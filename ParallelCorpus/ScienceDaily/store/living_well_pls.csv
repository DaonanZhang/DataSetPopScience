pls,fk_score,ari_score,reference,abstract
"
Babies and toddlers exposed to television or video viewing may be more likely to exhibit atypical sensory behaviors, such as being disengaged and disinterested in activities, seeking more intense stimulation in an environment, or being overwhelmed by sensations like loud sounds or bright lights, according to data from researchers at Drexel's College of Medicine published today in the journal JAMA Pediatrics.

According to the researchers, children exposed to greater TV viewing by their second birthday were more likely to develop atypical sensory processing behaviors, such as ""sensation seeking"" and ""sensation avoiding,"" as well as ""low registration"" -- being less sensitive or slower to respond to stimuli, such as their name being called, by 33 months old.
Sensory processing skills reflect the body's ability to respond efficiently and appropriately to information and stimuli received by its sensory systems, such as what the toddler hears, sees, touches, and tastes.
The team pulled 2011-2014 data on television or DVD-watching by babies and toddlers at 12- 18- and 24-months from the National Children's Study of 1,471 children (50% male) nationwide.
Sensory processing outcomes were assessed at 33 months using the Infant/Toddler Sensory Profile (ITSP), a questionnaire completed by parents/caregivers, designed to give insights on how children process what they see, hear and smell, etc.
ITSP subscales examine children's patterns of low registration, sensation seeking, such as excessively touching or smelling objects; sensory sensitivity, such as being overly upset or irritated by lights and noise; and sensation avoiding -- actively trying to control their environment to avoid things like having their teeth brushed. Children score in ""typical,"" ""high"" or ""low"" groups based on how often they display various sensory-related behaviors. Scores were considered ""typical"" if they were within one standard deviation from the average of the ITSP norm.
Measurements of screen exposure at 12-months were based on caregiver responses to the question: ""Does your child watch TV and/or DVDs? (yes/no),"" and at 18- and 24- months based on the question: ""Over the past 30 days, on average, how many hours per day did your child watch TV and/or DVDs?""
The findings suggest: At 12 months, any screen exposure compared to no screen viewing was associated with a 105% greater likelihood of exhibiting ""high"" sensory behaviors instead of ""typical"" sensory behaviors related to low registration at 33 months At 18 months, each additional hour of daily screen time was associated with 23% increased odds of exhibiting ""high"" sensory behaviors related to later sensation avoiding and low registration. At 24 months, each additional hour of daily screen time was associated with a 20% increased odds of ""high"" sensation seeking, sensory sensitivity, and sensation avoiding at 33 months.

The researchers adjusted for age, whether the child was born prematurely, caregiver education, race/ethnicity and other factors, such as how often the child engages in play or walks with the caregiver.
The findings add to a growing list of concerning health and developmental outcomes linked to screen time in infants and toddlers, including language delay, autism spectrum disorder, behavioral issues, sleep struggles, attention problems and problem-solving delays.
""This association could have important implications for attention deficit hyperactivity disorder and autism, as atypical sensory processing is much more prevalent in these populations,"" said lead author Karen Heffler, MD, an associate professor of Psychiatry in Drexel's College of Medicine. ""Repetitive behavior, such as that seen in autism spectrum disorder, is highly correlated with atypical sensory processing. Future work may determine whether early life screen time could fuel the sensory brain hyperconnectivity seen in autism spectrum disorders, such as heightened brain responses to sensory stimulation.""
Atypical sensory processing in kids with autism spectrum disorder (ASD) and ADHD manifests in a range of detrimental behaviors. In children with ASD, greater sensation seeking or sensation avoiding, heightened sensory sensitivity and low registration have been associated with irritability, hyperactivity, eating and sleeping struggles, as well as social problems. In kids with ADHD, atypical sensory processing is linked to trouble with executive function, anxiety and lower quality of life.
""Considering this link between high screen time and a growing list of developmental and behavioral problems, it may be beneficial for toddlers exhibiting these symptoms to undergo a period of screen time reduction, along with sensory processing practices delivered by occupational therapists,"" said Heffler.
The American Academy of Pediatrics (AAP) discourages screen time for babies under 18-24 months. Live video chat is considered by the AAP to be okay, as there may be benefit from the interaction that takes place. AAP recommends time limitations on digital media use for children 2 to 5 years to typically no more than 1 hour per day.

""Parent training and education are key to minimizing, or hopefully even avoiding, screen time in children younger than two years,"" said senior author David Bennett, PhD, a professor of Psychiatry in Drexel's College of Medicine.""
Despite the evidence, many toddlers view screens more often. As of 2014, children age 2 and under in the United States averaged 3 hours, 3 minutes a day of screen time, up from 1 hour, 19 minutes a day in 1997, according to a 2019 research letter in JAMA Pediatrics. Some parents cite exhaustion and inability for affordable alternatives as reasons for the screen time, according to a July 2015 study in the Journal of Nutrition and Behavior.
Although the current paper looked strictly at television or DVD watching, and not media viewed on smartphones or tablets, it does provide some of the earliest data linking early-life digital media exposure with later atypical sensory processing across multiple behaviors. The authors said future research is needed to better understand the mechanisms that drive the association between early-life screen time and atypical sensory processing.
In addition to Heffler and Bennett, authors on this paper include Binod Acharya, who completed the work while at Drexel's Dornsife School of Public Health's Urban Health Collaborative, and Keshab Subedi from Christiana Care Health Systems. 

","score: 17.82107642728508, grade_level: '18'","score: 19.681052415405517, grade_levels: ['college_graduate'], ages: [24, 100]",10.1001/jamapediatrics.2023.5923,"Atypical sensory processing is challenging for children and families, yet there is limited understanding of its associated risk factors. To determine the association between early-life digital media exposure and sensory processing outcomes among toddlers. This multicenter US study used data that were analyzed from the National Children’s Study (NCS), a cohort study of environmental influences on child health and development, with enrollment from 2011 to 2014. Data analysis was performed in 2023. The study included children enrolled in the NCS at birth whose caregivers completed reports of digital media exposure and sensory processing. Children’s viewing of television or video at 12 months (yes or no), 18 months, and 24 months of age (hours per day). Sensory processing was reported at approximately 33 months of age on the Infant/Toddler Sensory Profile. Quadrant scores (low registration, sensation seeking, sensory sensitivity, and sensation avoiding) were categorized into groups representing typical, high, and low sensory-related behaviors, and multinomial regression analyses were performed. A total of 1471 children (50% male) were included. Screen exposure at 12 months of age was associated with a 2-fold increased odds of being in the high category of low registration (odds ratio [OR], 2.05; 95% CI, 1.31-3.20), while the odds of being in the low category instead of the typical category decreased for sensation seeking (OR, 0.55; 95% CI, 0.35-0.87), sensation avoiding (OR, 0.69; 95% CI, 0.50-0.94), and low registration (OR, 0.64; 95% CI, 0.44-0.92). At 18 months of age, greater screen exposure was associated with increased risk of high sensation avoiding (OR, 1.23; 95% CI, 1.03-1.46) and low registration (OR, 1.23; 95% CI, 1.04-1.44). At 24 months of age, greater screen exposure was associated with increased risk of high sensation seeking (OR, 1.20; 95% CI, 1.02-1.42), sensory sensitivity (OR, 1.25; 95% CI, 1.05-1.49), and sensation avoiding (OR, 1.21; 95% CI, 1.03-1.42). In this cohort study, early-life digital media exposure was associated with atypical sensory processing outcomes in multiple domains. These findings suggest that digital media exposure might be a potential risk factor for the development of atypical sensory profiles. Further research is needed to understand the relationship between screen time and specific sensory-related developmental and behavioral outcomes, and whether minimizing early-life exposure can improve subsequent sensory-related outcomes."
"
For children, the world is full of surprises. Adults, on the other hand, are much more difficult to surprise. And there are complex processes behind this apparently straightforward state of affairs. Researchers at the University of Basel have been using mice to decode how reactions to the unexpected develop in the growing brain.

Babies love playing peekaboo, continuing to react even on the tenth sudden appearance of their partner in the game. Recognizing the unexpected is an important cognitive ability. After all, new can also mean dangerous.
The exact way in which surprises are processed in the brain changes as we grow, however: unusual stimuli are much more quickly categorized as ""important"" or ""uninteresting,"" and are significantly less surprising the second and third time they appear. This increased efficiency makes perfect sense: new stimuli may gain our attention, but do not cause an unnecessarily strong reaction that costs us energy. While this may appear trivial at first, so far there has been very little research into this fact in the context of brain development.
Experiments with young mice conducted by Professor Tania Barkat's research team have now begun to decode how the developing brain processes surprising sounds and what changes as we grow up. The researchers have reported on their findings in the journal Science Advances.
Strange sounds
In their experiments, the researchers used sequences of sounds in which a different tone was heard at irregular intervals in between a series of identical ones. At the same time, they recorded the animals' brain waves. This process is known as the ""oddball paradigm,"" and is used by health professionals for purposes such as the diagnosis of schizophrenia.
Using these measurements, the researchers were able to understand how the reaction of different brain regions to the change of tone developed over time in the young mice. This reaction was initially very strong, but decreased as the relevant brain region matured, to a level comparable to that of measurements in adult animals. This development does not take place simultaneously in the various areas of the brain that process sound, however.

A region known as the inferior colliculus, located at the beginning of the path from the auditory nerve to the auditory cortex, was already fully mature in the animals at the age of 20 days, the earliest point in time studied by the team. A second site, the auditory thalamus, only showed an ""adult"" reaction to the differing tone at the age of 30 days.
Development in the cerebral cortex itself, the ""primary auditory cortex,"" took even longer, until day 50. ""This development of the surprise reaction thus begins in the periphery and ends in the cerebral cortex,"" says study leader Tania Barkat. The cerebral cortex therefore matures much later than expected -- in human years, this would equate roughly to the early 20s.
No development without experience
The researchers also observed that experiences play a key role in the development of the surprise response in the cerebral cortex. If the mice were reared in a noise-neutral environment, the processing of unexpected sounds in the auditory cortex was significantly delayed.
One possible explanation for this is that the brain -- and the cerebral cortex in particular -- forms an internal image of the world during growth, which it then compares with external stimuli. Anything that does not correspond to this ""worldview"" is a surprise, but may also result in an update. ""Without experience with sounds, however, the cerebral cortex in these mice is unable to develop such a model of the world,"" says neuroscientist Barkat. As a result, the animal is unable to categorize sounds properly into ""familiar"" and ""unexpected.""

","score: 12.347769672855879, grade_level: '12'","score: 12.554053381962866, grade_levels: ['college'], ages: [18, 24]",10.1126/sciadv.adi7624,"Stimulus-specific adaptation (SSA), the reduction of neural activity to a common stimulus that does not generalize to other, rare stimuli, is an essential property of our brain. Although well characterized in adults, it is still unknown how it develops during adolescence and what neuronal circuits are involved. Using in vivo electrophysiology and optogenetics in the lemniscal pathway of the mouse auditory system, we observed SSA to be stable from postnatal day 20 (P20) in the inferior colliculus, to develop until P30 in the auditory thalamus and even later in the primary auditory cortex (A1). We found this maturation process to be experience-dependent in A1 but not in thalamus and to be related to alterations in deep but not input layers of A1. We also identified corticothalamic projections to be implicated in thalamic SSA development. Together, our results reveal different circuits underlying the sequential SSA maturation and provide a unique perspective to understand predictive coding and surprise across sensory systems."
"
A UC Riverside study to motivate your new year's resolutions: it demonstrates that high-fat diets affect genes linked not only to obesity, colon cancer and irritable bowels, but also to the immune system, brain function, and potentially COVID-19 risk.

While other studies have examined the effects of a high-fat diet, this one is unusual in its scope. UCR researchers fed mice three different diets over the course of 24 weeks where at least 40% of the calories came from fat. Then, they looked not only at the microbiome, but also at genetic changes in all four parts of the intestines.
One group of mice ate a diet based on saturated fat from coconut oil, another got a monounsaturated, modified soybean oil, a third got an unmodified soybean oil high in polyunsaturated fat. Compared to a low-fat control diet, all three groups experienced concerning changes in gene expression, the process that turns genetic information into a functional product, such as a protein.
""Word on the street is that plant-based diets are better for you, and in many cases that's true. However, a diet high in fat, even from a plant, is one case where it's just not true,"" said Frances Sladek, a UCR cell biology professor and senior author of the new study.
A new Scientific Reports paper about the study documents the many impacts of high-fat diets. Some of the intestinal changes did not surprise the researchers, such as major changes in genes related to fat metabolism and the composition of gut bacteria. For example, they observed an increase in pathogenic E. coli and a suppression of Bacteroides, which helps protect the body against pathogens.
Other observations were more surprising, such as changes in genes regulating susceptibility to infectious diseases. ""We saw pattern recognition genes, ones that recognize infectious bacteria, take a hit. We saw cytokine signaling genes take a hit, which help the body control inflammation,"" Sladek said. 'So, it's a double whammy. These diets impair immune system genes in the host, and they also create an environment in which harmful gut bacteria can thrive.""
The team's previous work with soybean oil documents its link to obesity and diabetes, both major risk factors for COVID. This paper now shows that all three high-fat diets increase the expression of ACE2 and other host proteins that are used by COVID spike proteins to enter the body.

Additionally, the team observed that high-fat food increased signs of stem cells in the colon. ""You'd think that would be a good thing, but actually they can be precursors to cancer,"" Sladek said.
In terms of effects on gene expression, coconut oil showed the greatest number of changes, followed by the unmodified soybean oil. Differences between the two soybean oils suggest that polyunsaturated fatty acids in unmodified soybean oil, primarily linoleic acid, play a role in altering gene expression.
Negative changes to the microbiome in this study were more pronounced in mice fed the soybean oil diet. This was unsurprising, as the same research team previously documented other negative health effects of high soybean oil consumption.
In 2015, the team found that soybean oil induces obesity, diabetes, insulin resistance, and fatty liver in mice. In 2020, the researchers team demonstrated the oil could also affect genes in the brain related to conditions like autism, Alzheimer's disease, anxiety, and depression.
Interestingly, in their current work they also found the expression of several neurotransmitter genes were changed by the high fat diets, reinforcing the notion of a gut-brain axis that can be impacted by diet.
The researchers have noted that these findings only apply to soybean oil, and not to other soy products, tofu, or soybeans themselves. ""There are some really good things about soybeans. But too much of that oil is just not good for you,"" said UCR microbiologist Poonamjot Deol, who was co-first author of the current study along with UCR postdoctoral researcher Jose Martinez-Lomeli.

Also, the studies were conducted using mice, and mouse studies do not always translate to the same results in humans. However, humans and mice share 97.5% of their working DNA. Therefore, the findings are concerning, as soybean oil is the most commonly consumed oil in the United States, and is increasingly being used in other countries, including Brazil, China, and India.
By some estimates, Americans tend to get nearly 40% of their calories from fat, which mirrors what the mice were fed in this study. ""Some fat is necessary in the diet, perhaps 10 to 15%. Most people though, at least in this country, are getting at least three times the amount that they need,"" Deol said.
Readers should not panic about a single meal. It is the long-term high-fat habit that caused the observed changes. Recall that the mice were fed these diets for 24 weeks. ""In human terms, that is like starting from childhood and continuing until middle age. One night of indulgence is not what these mice ate. It's more like a lifetime of the food,"" Deol said.
That said, the researchers hope the study will cause people to closely examine their eating habits.
""Some people think, 'Oh, I'll just exercise more and be okay. But regularly eating this way could be impacting your immune system and how your brain functions,"" Deol said. ""You may not be able to just exercise away these effects.""

","score: 10.37580477972934, grade_level: '10'","score: 11.068455226029371, grade_levels: ['12'], ages: [17, 18]",10.1038/s41598-023-49555-7,"High fat diets (HFDs) have been linked to several diseases including obesity, diabetes, fatty liver, inflammatory bowel disease (IBD) and colon cancer. In this study, we examined the impact on intestinal gene expression of three isocaloric HFDs that differed only in their fatty acid composition—coconut oil (saturated fats), conventional soybean oil (polyunsaturated fats) and a genetically modified soybean oil (monounsaturated fats). Four functionally distinct segments of the mouse intestinal tract were analyzed using RNA-seq—duodenum, jejunum, terminal ileum and proximal colon. We found considerable dysregulation of genes in multiple tissues with the different diets, including those encoding nuclear receptors and genes involved in xenobiotic and drug metabolism, epithelial barrier function, IBD and colon cancer as well as genes associated with the microbiome and COVID-19. Network analysis shows that genes involved in metabolism tend to be upregulated by the HFDs while genes related to the immune system are downregulated; neurotransmitter signaling was also dysregulated by the HFDs. Genomic sequencing also revealed a microbiome altered by the HFDs. This study highlights the potential impact of different HFDs on gut health with implications for the organism as a whole and will serve as a reference for gene expression along the length of the intestines."
"
Acetaminophen is considered the safest over-the-counter pain reliever and fever reducer available during pregnancy. Studies have shown that 50%-65% of women in North America and Europe take acetaminophen during pregnancy. A new study from researchers at the University of Illinois Urbana-Champaign explored the relationship between acetaminophen use during pregnancy and language outcomes in early childhood. It found that increasing acetaminophen use was associated with language delays.

The findings are reported in the journal Pediatric Research.
Earlier studies have found associations between acetaminophen use during pregnancy and poorer child communication skills. But those studies used measures of language development that were less precise than the methods applied in the current study, said Megan Woodbury, who led the research as a graduate student with U. of I. comparative biosciences professor emerita Susan Schantz. The work was conducted as part of the Illinois Kids Development Study, which explores how environmental exposures in pregnancy and childhood influence child development. Schantz is the IKIDS principal investigator. Woodbury is now a postdoctoral researcher at Northeastern University in Boston.
""The previous studies had only asked pregnant people at most once a trimester about their acetaminophen use,"" Woodbury said. ""But with IKIDS, we talked to our participants every four to six weeks during pregnancy and then within 24 hours of the kid's birth, so we had six time points during pregnancy.""
The language analyses involved 298 2-year-old children who had been followed prenatally, 254 of whom returned for further study at age 3.
For the 2-year-olds, the researchers turned to the MacArthur-Bates Communicative Development Inventories, which asks a parent to report on the child's vocabulary, language complexity and the average length of the child's longest three utterances.
""We wanted to collect data at that age because it's the period called 'word explosion,' when kids are just adding words every day to their vocabulary,"" Schantz said.

The vocabulary measure asked parents to select words their child had used from a list of 680 words.
The parents assessed their child again at 3 years, comparing their language skills to those of their peers.
The analysis linked acetaminophen use in the second and third trimesters of pregnancy to modest but significant delays in early language development.
""We found that increased use of acetaminophen -- especially during the third trimester -- was associated with smaller vocabulary scores and shorter 'mean length of utterance' at two years,"" Woodbury said.
""At age three, greater acetaminophen use during the third trimester was related to parents ranking their kids as lower than their peers on their language abilities,"" Schantz said. ""That outcome was seen primarily in male children.""
The most dramatic finding was that each use of acetaminophen in the third trimester of pregnancy was associated with an almost two-word reduction in vocabulary in the 2-year-olds.

""This suggests that if a pregnant person took acetaminophen 13 times -- or once per week -- during the third trimester of that pregnancy, their child might express 26 fewer words at age 2 than other children that age,"" Woodbury said.
Fetal brain development occurs throughout pregnancy, but the second and third trimesters are especially critical times, Schantz said.
""Hearing is developing in the second trimester, but language development is already starting in the third trimester before the baby is even born,"" she said.
""It's thought that acetaminophen exerts its analgesic effect through the endocannabinoid system, which is also very important for fetal development,"" Woodbury said.
The findings need to be tested in larger studies, the researchers said. Until then, people should not be afraid to take acetaminophen for fever or serious pain and discomfort during pregnancy. Conditions like a very high fever can be dangerous and using a drug like acetaminophen will likely help.
""There aren't other options for people to take when they really need them,"" Schantz said. ""But perhaps people should use more caution when turning to the drug to treat minor aches and pains.""
This work was supported by the Children's Environmental Health and Disease Prevention Research Center funded by the National Institute of Environmental Health Sciences and the U.S. Environmental Protection Agency and the National Institutes of Health Environmental Influences on Child Health Outcomes program.

","score: 13.812358870967746, grade_level: '14'","score: 14.654467917251047, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41390-023-02924-4,"Acetaminophen is the only analgesic considered safe for use throughout pregnancy. Recent studies suggest that use during pregnancy may be associated with poorer neurodevelopmental outcomes in children, but few have examined language development. The Illinois Kids Development Study is a prospective birth cohort in east-central Illinois. Between December 2013 and March 2020, 532 newborns were enrolled and had exposure data available. Participants reported the number of times they took acetaminophen six times across pregnancy. Language data were collected at 26.5–28.5 months using the MacArthur-Bates Communicative Development Inventories (CDI; n = 298), and 36–38 months using the Speech and Language Assessment Scale (SLAS; n = 254). Taking more acetaminophen during the second or third trimester was associated with marginally smaller vocabularies and shorter utterance length (M3L) at 26.5–28.5 months. More acetaminophen use during the third trimester was also associated with increased odds of M3L scores ≤25th percentile in male children. More use during the second or third trimester was associated with lower SLAS scores at 36–38 months. Third trimester use was specifically related to lower SLAS scores in male children. Higher prenatal acetaminophen use during pregnancy may be associated with poorer early language development. Taking more acetaminophen during pregnancy, particularly during the second and third trimesters, was associated with poorer scores on measures of language development when children were 26.5–28.5 and 36–38 months of age. Only male children had lower scores in analyses stratified by child sex. To our knowledge, this is the first study that has used a standardized measure of language development to assess the potential impact of prenatal exposure to acetaminophen on language development. This study adds to the growing body of literature suggesting that the potential impact of acetaminophen use during pregnancy on fetal neurodevelopment should be carefully evaluated. Taking more acetaminophen during pregnancy, particularly during the second and third trimesters, was associated with poorer scores on measures of language development when children were 26.5–28.5 and 36–38 months of age. Only male children had lower scores in analyses stratified by child sex. To our knowledge, this is the first study that has used a standardized measure of language development to assess the potential impact of prenatal exposure to acetaminophen on language development. This study adds to the growing body of literature suggesting that the potential impact of acetaminophen use during pregnancy on fetal neurodevelopment should be carefully evaluated."
"
New Cornell University-led research finds that social media platforms and the metrics that reward content creators for revealing their innermost selves to fans open creators up to identity-based harassment.

""Creators share deeply personal -- often vulnerable -- elements of their lives with followers and the wider public,"" said Brooke Erin Duffy, associate professor of communication. ""Such disclosures are a key way that influencers build intimacy with audiences and form communities. There's a pervasive sense that internet users clamor for less polished, less idealized, more relatable moments -- especially since the pandemic.""
Duffy is the lead author of ""Influencers, Platforms, and the Politics of Vulnerability"" published in the European Journal of Cultural Studies.
The research team conducted in-depth interviews with content creators to get a sense of how they experience the demands to make their content -- and often themselves -- visible to audiences, sponsors and the platforms.
Among their findings: The value of vulnerability for platform-based influencers cannot be overstated -- authenticity sells, and that means projecting intimacies, insecurities and even secrets; These authentic revelations are often tied to one's identities, which can open a person up to attacks based on gender, race, sexuality and other perceived traits; Personal and social vulnerabilities were often compounded by the vulnerabilities of platform-dependent labor: Not only did participants identify the failures of their platforms to protect them from harm (as ""contractors"" instead of ""employees""), many felt these companies incentivize networked antagonism.""Influencers and creators have relatively few formal sources of support or protection,"" Duffy said. ""In contrast to those legally employed by Meta, Twitch and TikTok, creators are independent contractors. They're left wanting for a lot of the workplace protections traditionally afforded to employees.""
The researchers examined informal strategies -- both anticipatory and reactive -- that creators deploy to manage their vulnerabilities. The former included the use of platform filtering systems to sift out abusive, profane or hurtful language. The latter strategies ranged from simply not reading the comments to employing the platform's tools to minimize the impact of what, for many, felt like an inevitable onslaught of critique.
The authors acknowledge the difficulties of resolving endemic issues of internet hate and harassment. ""'Getting off the internet' is hardly a viable option for participants in the put-yourself-out-there neoliberal job economy,"" they wrote -- and offer a warning to those wishing to join the creator economy.
""It is something of a truism that 'everyone gets the same platform,'"" they wrote. ""We would caution, however, that the politics of visibility -- and hence, the politics of vulnerability -- are far less egalitarian that platforms lead us to believe.""

","score: 16.34158371040724, grade_level: '16'","score: 17.62418552036199, grade_levels: ['college_graduate'], ages: [24, 100]",10.1177/13675494231212346,"While workers of all stripes are compelled to embrace uncertainty under conditions of neoliberalism, ideologies of risk assume a particular guise in the platform economy, wherein laborers are exhorted to ‘put yourself out there’. Given the attendant harms associated with public visibility – especially for women and other marginalized groups – it seems crucial to explore platform-dependent laborers’ experiences of ‘putting themselves out there’. This article draws upon in-depth interviews with 23 social media influencers and content creators, sampled from across platforms, content niches and subjectivities. Our analysis revealed that vulnerability is a structuring concept in the influencer economy – one that operates at multiple, often overlapping levels. First, the commercial logic of authenticity casts personal vulnerability as a strategy for building community and accruing followers. But influencers’ individual disclosures were often entangled with their social identities (e.g., gender, race, sexuality, ability and body type), which rendered them socially vulnerable to targeted antagonism from audiences﻿. Interviewees experienced a range of harms, from identity-based hate and harassment to concerted take-down campaigns. These personal and social vulnerabilities were compounded by the vulnerabilities of platform-dependent labor: not only did participants identify the failures of platforms to protect them, some shared a sense that these companies exacerbated harms through a commercial logic that incentivizes antagonism. After examining the emotional labor necessary to manage such platform vulnerabilities, we close by reiterating the unique precarity of platform labor, wherein participants lack the social and legal protections typically afforded to ‘vulnerable workers’."
"
Low-carbohydrate diets comprised mostly of plant-based proteins and fats with healthy carbohydrates such as whole grains were associated with slower long-term weight gain than low-carbohydrate diets comprised mostly of animal proteins and fats with unhealthy carbohydrates like refined starches, according to a new study led by Harvard T.H. Chan School of Public Health.

The study will be published on December 27, 2023, in JAMA Network Open.
""Our study goes beyond the simple question of, 'To carb or not to carb?'"" said lead author Binkai Liu, research assistant in the Department of Nutrition. ""It dissects the low-carbohydrate diet and provides a nuanced look at how the composition of these diets can affect health over years, not just weeks or months.""
While many studies have shown the benefits of cutting carbohydrates for short-term weight loss, little research has been conducted on low-carbohydrate diets' effect on long-term weight maintenance and the role of food group quality.
Using data from the Nurses' Health Study, Nurses' Health Study II, and Health Professionals Follow-up Study, the researchers analyzed the diets and weights of 123,332 healthy adults from as early as 1986 to as recently as 2018. Each participant provided self-reports of their diets and weights every four years. The researchers scored participants' diets based on how well they adhered to five categories of low-carbohydrate diet: total low-carbohydrate diet (TLCD), emphasizing overall lower carbohydrate intake; animal-based low-carbohydrate diet (ALCD), emphasizing animal-based proteins and fats; vegetable-based low-carbohydrate diet (VLCD), emphasizing plant-based proteins and fats; healthy low-carbohydrate diet (HLCD), emphasizing plant-based proteins, healthy fats, and fewer refined carbohydrates; and unhealthy low-carbohydrate diet (ULCD), emphasizing animal-based proteins, unhealthy fats, and carbohydrates coming from unhealthy sources such as processed breads and cereals.
The study found that diets comprised of plant-based proteins and fats and healthy carbohydrates were significantly associated with slower long-term weight gain. Participants who increased their adherence to TLCD, ALCD, and ULCD on average gained more weight compared to those who increased their adherence to HLCD over time. These associations were most pronounced among participants who were younger (<55 years old), overweight or obese, and/or less physically active. The results for the vegetable-based low carbohydrate diet were more ambiguous: Data from the Nurses' Health Study II showed an association between higher VLCD scores and less weight gain over time, while data around VLCD scores from the Nurses' Health Study and Health Professionals Follow-up Study were more mixed.
""The key takeaway here is that not all low-carbohydrate diets are created equal when it comes to managing weight in the long-term,"" said senior author Qi Sun, associate professor in the Department of Nutrition. ""Our findings could shake up the way we think about popular low-carbohydrate diets and suggest that public health initiatives should continue to promote dietary patterns that emphasize healthful foods like whole grains, fruits, vegetables, and low-fat dairy products.""
Other Harvard Chan authors included Molin Wang, associate professor in the Departments of Epidemiology and Biostatistics, and Yang Hu, research scientist; Sharan Rai, postdoctoral research fellow; and Frank Hu, professor, in the Department of Nutrition.
The study was funded by research grants from the National Institutes of Health: UM1 CA186107, U01 CA176726, U01 CA167552, P01 CA87969, R01 HL034594, R01 HL035464, R01 HL60712, R01 DK120870, R01 DK126698, R01 DK119268, U2C DK129670, DK119268, R01 ES022981, and R21 AG070375.

","score: 16.258458781362005, grade_level: '16'","score: 20.523655913978494, grade_levels: ['college_graduate'], ages: [24, 100]",10.1001/jamanetworkopen.2023.49552,"The associations of low-carbohydrate diets (LCDs) with long-term weight management remains unclear, and the source and quality of macronutrients within LCDs are less explored. To prospectively examine associations between changes in LCD indices and weight change among US adults. This prospective cohort study included initially healthy participants at baseline from the Nurses’ Health Study (NHS; 1986-2010), Nurses’ Health Study II (NHSII; 1991-2015), and Health Professionals Follow-up Study (HPFS; 1986-2018). Data analysis was performed between November 2022 and April 2023. Five LCD indices were examined: (1) a total LCD (TLCD) emphasizing overall lower carbohydrate intake; (2) an animal-based LCD (ALCD) that emphasized animal-sourced protein and fat; (3) a vegetable-based LCD (VLCD) that emphasized plant-sourced protein and fat; (4) a healthy LCD (HLCD) emphasizing less refined carbohydrates, more plant protein, and healthy fat; and (5) an unhealthy LCD (ULCD) emphasizing less healthful carbohydrates, more animal protein, and unhealthy fat. The outcome of interest was 4-year changes in self-reported body weight. A total of 123 332 participants (mean [SD] age, 45.0 [9.7] years; 103 320 [83.8%] female) were included in this study. The median carbohydrate intake (as a percentage of energy) of the highest quintiles of TLCD score at baseline ranged from 38.3% in HPFS to 40.9% in NHSII. Mean weight gain over 4-year intervals among participants varied from 0.8 kg in the HPFS to 1.8 kg in the NHSII. After adjusting for demographics and baseline and concomitant changes of selected lifestyle factors, each 1-SD increase in TLCD score was associated with 0.06 (95% CI, 0.04-0.08) kg more weight gain over the 4-year periods. Similarly, participants gained 0.13 (95% CI, 0.11 to 0.14) kg per each 1-SD increase in ALCD score and 0.39 (95% CI, 0.37 to 0.40) kg per each 1-SD change in ULCD score. In contrast, each 1-SD increase in VLCD score was associated with 0.03 (95% CI, 0.01 to 0.04) kg less weight gain, and each 1-SD increase in HLCD score was associated with 0.36 (95% CI, 0.35 to 0.38) kg less weight gain. The associations were more pronounced among obese individuals (per 1-SD increase in HLCD score: BMI ≥30, 0.88 [95% CI, 0.80, 0.97] kg less weight gain; BMI &amp;lt;25, 0.23 [95% CI, 0.20, 0.26] kg less weight gain; P for interaction &amp;lt; .001). These findings suggest that the quality of LCDs may play a critical role in modulating long-term weight change. Only LCDs that emphasized high-quality protein, fat, and carbohydrates from whole grains and other plant-based foods were associated with less weight gain."
"
Light in the evening is thought to be bad for sleep. However, does the color of the light play a role? Researchers from the University of Basel and the Technical University of Munich (TUM) compared the influence of different light colors on the human body. The researchers' findings contradict the results of a previous study in mice.

Vision is a complex process. The visual perception of the environment is created by a combination of different wavelengths of light, which are decoded as colours and brightness in the brain. Photoreceptors in the retina first convert the light into electrical impulses: with sufficient light, the cones enable sharp, detailed, and coloured vision. Rods only contribute to vision in low light conditions allowing for different shades of grey to be distinguished but leaving vision much less precise. The electrical nerve impulses are finally transmitted to ganglion cells in the retina and then via the optic nerve to the visual cortex in the brain. This region of the brain processes the neural activity into a coloured image.
What influences the internal clock?
Ambient light however does not only allow us to see, it also influences our sleep-wake rhythm. Specialised ganglion cells are significantly involved in this process, which -- like the cones and rods -- are sensitive to light and react particularly strongly to short-wavelength light at a wavelength of around 490 nanometres. If light consists solely of short wavelengths of 440 to 490 nanometres, we perceive it as blue. If short-wavelength light activates the ganglion cells, they signal to the internal clock that it is daytime. The decisive factor here is how intense the light is per wavelength; the perceived colour is not relevant.
""However, the light-sensitive ganglion cells also receive information from the cones. This raises the question of whether the cones, and thereby the light colour, also influence the internal clock. After all, the most striking changes in brightness and light colour occur at sunrise and sunset, marking the beginning and end of a day,"" says Dr. Christine Blume. At the Centre for Chronobiology of the University of Basel, she investigates the effects of light on humans and is the first author of a study investigating the effects of different light colours on the internal clock and sleep. The team of researchers from the University of Basel and the TUM has now published its findings in the scientific journal ""Nature Human Behaviour."" 
Light colours in comparison
""A study in mice in 2019 suggested that yellowish light has a stronger influence on the internal clock than blueish light,"" says Christine Blume. In humans, the main effect of light on the internal clock and sleep is probably mediated via the light-sensitive ganglion cells. ""However, there is reason to believe that the colour of light, which is encoded by the cones, could also be relevant for the internal clock.""
To get to the bottom of this, the researchers exposed 16 healthy volunteers to a blueish or yellowish light stimulus for one hour in the late evening, as well as a white light stimulus as a control condition. The light stimuli were designed in such a way that they differentially activated the colour-sensitive cones in the retina in a very controlled manner. However, the stimulation of the light-sensitive ganglion cells was the same in all three conditions. Differences in the effect of the light were therefore directly attributable to the respective stimulation of the cones and ultimately the colour of the light.

""This method of light stimulation allows us to separate the light properties that may play a role in how light effects humans in a clean experimental way,"" says Manuel Spitschan, Professor of Chronobiology and Health at the Technical University of Munich, who was also involved in the study.
In order to understand the effects of the different light stimuli on the body, in the sleep laboratory the researchers determined whether the internal clock of the participants had changed depending on the colour of the light. Additionally, they assessed how long it took the volunteers to fall asleep and how deep their sleep was at the beginning of the night. The researchers also enquired about their tiredness and tested their ability to react, which decreases with increasing sleepiness.
Ganglion cells are crucial
The conclusion: ""We found no evidence that the variation of light colour along a blue-yellow dimension plays a relevant role for the human internal clock or sleep,"" says Christine Blume. This contradicts the results of the mouse study mentioned above. ""Rather, our results support the findings of many other studies that the light-sensitive ganglion cells are most important for the human internal clock,"" says the scientist.
Manuel Spitschan sees the study as an important step towards putting basic research into practice: ""Our findings show that it is probably most important to take into account the effect of light on the light-sensitive ganglion cells when planning and designing lighting. The cones and therefore the colour play a very subordinate role.""
It remains to be seen whether the colour of the light also has no effect on sleep if the parameters change and, for example, the duration of the light exposure is extended or takes place at a different time. Follow-up studies should answer questions like these.
Night mode on screens -- useful or not? 
We often hear that the short-wavelength component of light from smartphone and tablet screens affects biological rhythms and sleep. The recommendation is therefore to put your mobile phone away early in the evening or at least use the night shift mode, which reduces the short-wavelength light proportions and looks slightly yellowish. Christine Blume confirms this. However, the yellowish colour adjustment is a by-product that could be avoided. ""Technologically, it is possible to reduce the short-wavelength proportions even without colour adjustment of the display, however this has not yet been implemented in commercial mobile phone displays,"" says the sleep researcher.

","score: 12.251984413270986, grade_level: '12'","score: 13.336313738588288, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41562-023-01791-7,"Evening exposure to short-wavelength light can affect the circadian clock, sleep and alertness. Intrinsically photosensitive retinal ganglion cells expressing melanopsin are thought to be the primary drivers of these effects. Whether colour-sensitive cones also contribute is unclear. Here, using calibrated silent-substitution changes in light colour along the blue–yellow axis, we investigated whether mechanisms of colour vision affect the human circadian system and sleep. In a 32.5-h repeated within-subjects protocol, 16 healthy participants were exposed to three different light scenarios for 1 h starting 30 min after habitual bedtime: baseline control condition (93.5 photopic lux), intermittently flickering (1 Hz, 30 s on–off) yellow-bright light (123.5 photopic lux) and intermittently flickering blue-dim light (67.0 photopic lux), all calibrated to have equal melanopsin excitation. We did not find conclusive evidence for differences between the three lighting conditions regarding circadian melatonin phase delays, melatonin suppression, subjective sleepiness, psychomotor vigilance or sleep. The Stage 1 protocol for this Registered Report was accepted in principle on 9 September 2020. The protocol, as accepted by the journal, can be found at https://doi.org/10.6084/m9.figshare.13050215.v1."
"
New research, publishing December 21 in the open access journal in PLOS Biology, shows that tears from women contain chemicals that block aggression in men. The study led by Shani Agron at the Weizmann Institute of Science, Israel, finds that sniffing tears leads to reduced brain activity related to aggression, which results is less aggressive behavior.

Male aggression in rodents is known to be blocked when they smell female tears. This is an example of social chemosignaling, a process that is common in animals but less common -- or less understood -- in humans. To determine whether tears have the same affect in people, the researchers exposed a group of men to either women's emotional tears or saline while they played a two-person game. The game was designed to elicit aggressive behavior against the other player, whom the men were led to believe was cheating. When given the opportunity, the men could get revenge on the other player by causing them lose money. The men did not know what they were sniffing and could not distinguish between the tears or the saline, which were both odorless.
Revenge-seeking aggressive behavior during the game dropped more than 40% after the men sniffed women's emotional tears. When repeated in an MRI scanner, functional imaging showed two aggression-related brain regions -- the prefrontal cortex and anterior insula -- that became more active when the men were provoked during the game, but did not become as active in the same situations when the men were sniffing the tears. Individually, the greater the difference in this brain activity, the less often the player took revenge during the game. Finding this link between tears, brain activity, and aggressive behavior implies that social chemosignaling is a factor in human aggression, not simply an animal curiosity.
The authors add, ""We found that just like in mice, human tears contain a chemical signal that blocks conspecific male aggression. This goes against the notion that emotional tears are uniquely human.""

","score: 12.880209698558328, grade_level: '13'","score: 14.043433813892534, grade_levels: ['college_graduate'], ages: [24, 100]",10.1371/journal.pbio.3002442,"Rodent tears contain social chemosignals with diverse effects, including blocking male aggression. Human tears also contain a chemosignal that lowers male testosterone, but its behavioral significance was unclear. Because reduced testosterone is associated with reduced aggression, we tested the hypothesis that human tears act like rodent tears to block male aggression. Using a standard behavioral paradigm, we found that sniffing emotional tears with no odor percept reduced human male aggression by 43.7%. To probe the peripheral brain substrates of this effect, we applied tears to 62 human olfactory receptors in vitro. We identified 4 receptors that responded in a dose-dependent manner to this stimulus. Finally, to probe the central brain substrates of this effect, we repeated the experiment concurrent with functional brain imaging. We found that sniffing tears increased functional connectivity between the neural substrates of olfaction and aggression, reducing overall levels of neural activity in the latter. Taken together, our results imply that like in rodents, a human tear–bound chemosignal lowers male aggression, a mechanism that likely relies on the structural and functional overlap in the brain substrates of olfaction and aggression. We suggest that tears are a mammalian-wide mechanism that provides a chemical blanket protecting against aggression."
"
In Finland, there is a clear increase in the number of sick days taken due to depression, anxiety and sleep disorders in October and November, whereas the number of absences is lower than expected between June and September. In late autumn, the number of sick days taken is almost twice as high as in the summer and about a quarter higher than in early autumn. On the other hand, manic episodes related to bipolar disorder occur more frequently than expected during the spring and summer, when there are more daylight hours, and less frequently than expected during darker times of year.

The results can be found in a study funded by the Research Council of Finland. The study was conducted as a part of the Climate Change and Health research programme. The aim of the study was to investigate the connection between changing light levels and mental health. It is expected that due to climate change, winters in Finland will become darker while summers will become brighter.
During the study, Kela's sick leave register was used to analyse the seasonal timing of a total of 636,543 sick leaves that were due to mental health reasons over a period of 12 years. The analyses examined whether the expected number of absences was above or below the expected number of sick leaves.
""Previous studies have found that some people experience so-called winter depression (seasonal affective disorder) during the dark season. In addition to the typical symptoms of depression, kaamos depression involves an increased appetite and weight gain along with excess sleepiness, which means sleeping for longer and feeling tired during the day. The symptoms of winter depression can often be alleviated through bright light therapy,"" says Timo Partonen, a Research Professor at the Finnish Institute for Health and Welfare.
Seasonal variation can increase workloads in the workplace and in health services particularly in the autumn, when the most common types of sick leaves -- absences due to depression, anxiety and sleep disorders -- are starting to occur often.
""It's also worth considering if there are other explanations for the phenomenon apart from a dark season. For example, is there an exceptionally high amount of psychosocial stress in the workplace during autumn, which then leads to an increasing number of sick leaves,"" says Professor of Psychology Marianna Virtanen from the University of Eastern Finland.
If climate change causes summers in Finland to become brighter and winters to become darker, the study suggests that depression, anxiety and sleep disorders could increase during the winter because of those changes. However, with the exception of sleep disorders, they could also become less prevalent during the summer. In the case of bipolar disorder, darker winters could alleviate the symptoms of mania, while brighter summers could exacerbate them.

","score: 13.60012987012987, grade_level: '14'","score: 15.39170995670996, grade_levels: ['college_graduate'], ages: [24, 100]",10.1017/S2045796023000768,"Although seasonality has been documented for mental disorders, it is unknown whether similar patterns can be observed in employee sickness absence from work due to a wide range of mental disorders with different severity level, and to what extent the rate of change in light exposure plays a role. To address these limitations, we used daily based sickness absence records to examine seasonal patterns in employee sickness absence due to mental disorders. We used nationwide diagnosis-specific psychiatric sickness absence claims data from 2006 to 2017 for adult individuals aged 16–67 (n = 636,543 sickness absence episodes) in Finland, a high-latitude country with a profound variation in daylength. The smoothed time-series of the ratio of observed and expected (O/E) daily counts of episodes were estimated, adjusted for variation in all-cause sickness absence rates during the year. Unipolar depressive disorders peaked in October–November and dipped in July, with similar associations in all forms of depression. Also, anxiety and non-organic sleep disorders peaked in October–November. Anxiety disorders dipped in January–February and in July–August, while non-organic sleep disorders dipped in April–August. Manic episodes reached a peak from March to July and dipped in September–November and in January–February. Seasonality was not dependent on the severity of the depressive disorder. These results suggest a seasonal variation in sickness absence due to common mental disorders and bipolar disorder, with high peaks in depressive, anxiety and sleep disorders towards the end of the year and a peak in manic episodes starting in spring. Rapid changes in light exposure may contribute to sickness absence due to bipolar disorder. The findings can help clinicians and workplaces prepare for seasonal variations in healthcare needs."
"
Conventional wisdom suggests that searching online to evaluate the veracity of misinformation would reduce belief in it. But a new study by a team of researchers shows the opposite occurs: Searching to evaluate the truthfulness of false news articles actually increases the probability of believing misinformation.

The findings, which appear in the journal Nature, offer insights into the impact of search engines' output on their users -- a relatively under-studied area.
""Our study shows that the act of searching online to evaluate news increases belief in highly popular misinformation -- and by notable amounts,"" says Zeve Sanderson, founding executive director of New York University's Center for Social Media and Politics (CSMaP) and one of the paper's authors.
The reason for this outcome may be explained by search-engine outputs -- in the study, the researchers found that this phenomenon is concentrated among individuals for whom search engines return lower-quality information.
""This points to the danger that 'data voids' -- areas of the information ecosystem that are dominated by low quality, or even outright false, news and information -- may be playing a consequential role in the online search process, leading to low return of credible information or, more alarming, the appearance of non-credible information at the top of search results,"" observes lead author Kevin Aslett, an assistant professor at the University of Central Florida and a faculty research affiliate at CSMaP.
In the newly published Nature study, Aslett, Sanderson, and their colleagues studied the impact of using online search engines to evaluate false or misleading views -- an approach encouraged by technology companies and government agencies, among others.
To do so, they recruited participants through both Qualtrics and Amazon's Mechanical Turk -- tools frequently used in running behavioral science studies -- for a series of five experiments and with the aim of gauging the impact of a common behavior: searching online to evaluate news (SOTEN).

The first four studies tested the following aspects of online search behavior and impact: The effect of SOTEN on belief in both false or misleading and true news directly within two days an article's publication (false popular articles included stories on COVID-19 vaccines, the Trump impeachment proceedings, and climate events) Whether the effect of SOTEN can change an individual's evaluation after they had already assessed the veracity of a news story The effect of SOTEN months after publication The effect of SOTEN on recent news about a salient topic with significant news coverage -- in the case of this study, news about the Covid-19 pandemicA fifth study combined a survey with web-tracking data in order to identify the effect of exposure to both low- and high-quality search-engine results on belief in misinformation. By collecting search results using a custom web browser plug-in, the researchers could identify how the quality of these search results may affect users' belief in the misinformation being evaluated.
The study's source credibility ratings were determined by NewsGuard, a browser extension that rates news and other information sites in order to guide users in assessing the trustworthiness of the content they come across online.
Across the five studies, the authors found that the act of searching online to evaluate news led to a statistically significant increase in belief in misinformation. This occurred whether it was shortly after the publication of misinformation or months later. This finding suggests that the passage of time -- and ostensibly opportunities for fact checks to enter the information ecosystem -- does not lessen the impact of SOTEN on increasing the likelihood of believing false news stories to be true. Moreover, the fifth study showed that this phenomenon is concentrated among individuals for whom search engines return lower-quality information.
""The findings highlight the need for media literacy programs to ground recommendations in empirically tested interventions and search engines to invest in solutions to the challenges identified by this research,"" concludes Joshua A. Tucker, professor of politics and co-director of CSMaP, another of the paper's authors.
The paper's other authors included William Godel and Jonathan Nagler of NYU's Center for Social Media and Politics, and Nathaniel Persily of Stanford Law School.
The study was supported by a grant from the National Science Foundation (2029610).

","score: 20.463190223508608, grade_level: '20'","score: 23.27069625341695, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41586-023-06883-y,"Considerable scholarly attention has been paid to understanding belief in online misinformation1,2, with a particular focus on social networks. However, the dominant role of search engines in the information environment remains underexplored, even though the use of online search to evaluate the veracity of information is a central component of media literacy interventions3–5. Although conventional wisdom suggests that searching online when evaluating misinformation would reduce belief in it, there is little empirical evidence to evaluate this claim. Here, across five experiments, we present consistent evidence that online search to evaluate the truthfulness of false news articles actually increases the probability of believing them. To shed light on this relationship, we combine survey data with digital trace data collected using a custom browser extension. We find that the search effect is concentrated among individuals for whom search engines return lower-quality information. Our results indicate that those who search online to evaluate misinformation risk falling into data voids, or informational spaces in which there is corroborating evidence from low-quality sources. We also find consistent evidence that searching online to evaluate news increases belief in true news from low-quality sources, but inconsistent evidence that it increases belief in true news from mainstream sources. Our findings highlight the need for media literacy programmes to ground their recommendations in empirically tested strategies and for search engines to invest in solutions to the challenges identified here."
"
In a new study, viewers of Facebook users' posts came away with perceptions of the users that differed from the users' own self-perceptions. Qi Wang and colleagues at Cornell University, New York, US, present these findings in the open-access journal PLOS ONE on December 20, 2023.

Many people post on social media platforms in order to express themselves and connect with others. Prior research has shown that viewers of personal websites, such as blogs or online profiles, form largely accurate perceptions of the authors' personalities. However, social media posts, such as Facebook status updates, are often isolated and lack context. Few studies have explored how users' self-perceptions align with how others perceive them after viewing such posts.
To shed new light, Wang and colleagues asked 158 undergraduate students to answer questions about their own personal characteristics, including their extraversion, disclosiveness, connectedness, self-esteem, independence, and interdependence. The students also shared their last 20 Facebook status updates.
Then, two groups of additional participants viewed the Facebook updates and answered questions about the users' characteristics. One group viewed the updates in a multimedia format with text and any accompanying images or hyperlinks, and the other saw text-only versions.
Overall, the viewers' perceptions of the Facebook users differed from users' self-perceptions. For instance, viewers tended to see users as being more disclosive, having lower self-esteem, and being less interdependent than how users perceived themselves. However, viewers' perceptions and self-perceptions of connectedness aligned, perhaps reflecting that a primary aim of social media posts is to connect with others.
Compared to text-only viewers' perceptions, multimedia viewers' perceptions were more in line with users' self-perceptions. However, there was more variation among multimedia perceptions, while text-only viewers showed more consensus. In addition, both groups' perceptions varied with users' gender and ethnicity, in line with judgments observed in offline contexts in prior research.
These findings provide new insights into the dynamics of online self-presentation and impression formation. The authors note that such understanding is important for fostering good communication and relationships. Future work could deepen understanding by, for instance, including a longer timeline of updates or considering other platforms such as TikTok.
The authors add: ""Can people form accurate impressions about us from our social media posts? Our study finds that there are substantial discrepancies between how people view Facebook users based on their status updates and how the users view themselves. Multimedia channels make the impressions more accurate, and user characteristics related to relationship-building, gender and ethnicity are more accurately perceived.""

","score: 13.862281468531474, grade_level: '14'","score: 15.60521853146853, grade_levels: ['college_graduate'], ages: [24, 100]",10.1371/journal.pone.0294990,"This study examines the cyber audience’s perception of social media users’ persona based on their online posts from a cognitive meaning-making perspective. Participants (N = 158) answered questions about their personal characteristics and provided their 20 most recent Facebook status updates. Two groups of viewers, who viewed either the text-only or multimedia version of the status updates, answered questions about the Facebook users’ personal characteristics. The viewers’ perceptions of Facebook users deviated from the users’ self-perceptions, although user characteristics that serve social motives were more accurately perceived. Multimedia viewers were more accurate than text viewers, whereas the latter showed a greater consensus. Gender and ethnic differences of Facebook users also emerged in online person perceptions, in line with gendered and cultured characteristics. These findings shed critical light on the dynamic interplay between social media users and the cyber audience in the co-construction of a digitally extended self."
"
Recent controversy has surrounded the concept of loot boxes -- the purchasable video game features that offer randomised rewards but are not governed by gambling laws.

Now research led by the University of Plymouth has shown that at-risk individuals, such as those with known gaming and gambling problems, are more likely to engage with loot boxes than those without.
The study is one of the largest, most complex and robustly designed surveys yet conducted on loot boxes, and has prompted experts to reiterate the call for stricter enforcement around them.
Existing studies have shown that the items are structurally and psychologically akin to gambling but, despite the evidence, they still remain accessible to children.
The new findings, which add to the evidence base linking loot boxes to gambling, are published in the journal Royal Society Open Science.
The surveys captured the thoughts of 1,495 loot box purchasing gamers, and 1,223 gamers who purchase other, non-randomised game content.
They highlighted that taking the risk of opening a loot box was associated with people who had experienced problem gambling, problem gaming, impulsivity and gambling cognitions -- including the perceived inability to stop buying them.

It also showed that any financial or psychological impacts from loot box purchasing are liable to disproportionately affect various at-risk cohorts, such as those who have previously had issues with gambling.
Lead author Dr James Close, Lecturer in Clinical Education at the University of Plymouth, said: ""Loot boxes are paid-for rewards in video games, but the gamer does not know what's inside. With the risk/reward mindset and behaviours associated with accessing loot boxes, we know there are similarities with gambling, and these new papers provide a longer, more robust description exploring the complexities of the issue.
""Among the findings, the work shows that loot box use is driven by beliefs such as 'I'll win in a minute' -- which really echoes the psychology we see in gambling. The studies contribute to a substantial body of evidence establishing that, for some, loot boxes can lead to financial and psychological harm. However, it's not about making loot boxes illegal, but ensuring that their impact is understood as akin to gambling, and that policies are in place to ensure consumers are protected from these harms.""
The research was funded by GambleAware, supported by the National Institute for Health and Care Research (NIHR) Applied Research Collaboration South West Peninsula (PenARC), and conducted alongside the University of Wolverhampton and other collaborators.
An earlier paper from this study also found evidence that under-18s who engaged with loot boxes progressed onto other forms of gambling. The overall findings remain consistent with narratives that policy action on loot boxes will take steps to minimise harm in future.
Co-lead Dr Stuart Spicer, PenARC Research Fellow in the University of Plymouth's Peninsula Medical School, added: ""We know loot boxes have attracted a lot of controversy and the UK government has adopted an approach of industry self-regulation. However, industry compliance to safety features is currently unsatisfactory, and there is a pressing need to see tangible results. Our research adds to the evidence base that they pose a problem for at-risk groups, such as people with dysfunctional thoughts about gambling, lower income, and problematic levels of video gaming. We really hope that these findings will add to the evidence base showing the link between loot boxes, gambling, and other risky behaviours, and that there will be more of a push to take action and minimise harm.""

","score: 15.469655172413795, grade_level: '15'","score: 17.691931034482757, grade_levels: ['college_graduate'], ages: [24, 100]",10.1098/rsos.231045,"Loot boxes are purchasable randomized rewards in video games that share structural and psychological similarities with gambling. Systematic review evidence has established reproducible associations between loot box purchasing and both problem gambling and problem video gaming, perhaps driven by a range of overlapping psychological processes (e.g. impulsivity, gambling-related cognitions, etc.) It has also been argued that loot box engagement may have negative influences on player financial and psychological wellbeing. We conducted a pre-registered survey of 1495 loot box purchasing gamers (LB cohort) and 1223 gamers who purchase other, non-randomized game content (nLB cohort). Our survey confirms 15 of our 23 pre-registered hypotheses against our primary outcome (risky loot box engagement), establishing associations with problem gambling, problem gaming, impulsivity, gambling cognitions, experiences of game-related ‘flow’ and specific ‘distraction and compulsion’ motivations for purchase. Results with hypotheses concerning potential harms established that risky loot box engagement was negatively correlated with wellbeing and positively correlated with distress. Overall, results indicate that any risks from loot boxes are liable to disproportionately affect various ‘at risk’ cohorts (e.g. those experiencing problem gambling or video gaming), thereby reiterating calls for policy action on loot boxes."
"
Air quality in the office may affect our level of creativity at work, scientists at Nanyang Technological University, Singapore (NTU Singapore) have found.

Working with the global air filter manufacturer Camfil on a shared research project, the NTU Singapore scientists found in a study that high levels of volatile organic compounds -- gases released from products such as detergents, pesticides, perfumes, aerosol sprays and paint -- affected the study participants' creativity when they were asked to build 3D models with LEGO bricks.
Using a statistical analysis, the NTU team estimated that reducing total volatile organic compounds (TVOC) by 72 per cent could improve a student's creative potential by 12 per cent.
TVOC is an indicator that refers to the volume of volatile organic compounds in the air. Indoor VOCs are emitted from interior decoration sources such as paints and carpets and household products such as detergents and air fresheners.
This study, conducted on the NTU Smart Campus, is part of a partnership between the University and Camfil to investigate the impact of indoor air quality on the cognitive performance of adults, test various air filter technologies in tropical weather conditions, and deliver innovative clean air solutions combined with optimised energy efficiency.
The findings detailed in the study, published in Scientific Reports in September, shed light on the importance of indoor air quality on our creative cognition, said the research team led by Assistant Professor Ng Bing Feng and Associate Professor Wan Man Pun, Cluster Directors for Smart & Sustainable Building Technologies at the Energy Research Institute @ NTU (ERI@N).
Asst Prof Ng said: ""While most people would correctly associate indoor air quality with effects on the lungs, especially since we just emerged from a pandemic, our study shows that it could also have an impact on the mind and creative cognition, or the ability to use knowledge in an unconventional way. Our findings suggest that relatively low TVOC levels, even if well within the accepted threshold, could impact an individual's creative potential.""
Assoc Prof Wan added: ""This could have serious consequences for industries that rely on creativity for the bulk of their work. For instance, artists often use paints and thinners that release high levels of volatile organic compounds and may not know they need adequate ventilation to clear them from their workplace. The findings also point to how making minor adjustments in the office, such as reducing the use of aroma diffusers or ensuring adequate ventilation, could positively impact employees and their productivity.""

The study also aligns with the Health & Society and Brain & Learning research clusters under the research pillar of NTU 2025, the University's five-year strategic plan.
The other scientists on the research team were NTU PhD graduate Dr Shmitha Arikrishnan, former NTU senior research fellow Dr Adam Charles Robert, who is currently a postdoctoral researcher at Singapore-ETH Centre, and NTU graduate Lau Wee Siang.
Assessing creativity through LEGO 3D models
To quantifiably assess creative potential in this study, the NTU team developed the Serious Brick Play method, which is largely adapted from the LEGO Serious Play framework. This tool involves expressing thoughts and ideas using 3D models built with LEGO bricks.
A typical LEGO Serious Play session involves a facilitator who introduces a challenge, to which participants respond by building a model using LEGO bricks. Participants then discuss their models and reflect on the building process, prompted by the facilitator.
In the Serious Brick Play method designed by the NTU team, participants do not discuss their models and share their reflections in a group. Instead, they provide written descriptions of their LEGO models. These written descriptions and LEGO models are then scored by a panel of judges for creativity.

Asst Prof Ng explained: ""While the LEGO Serious Play framework has been used in various settings to unleash creative thinking and has even been used to support dementia patients, it does not have a quantitative assessment component and cannot systematically assess creativity. This is why we added a component to score participants on their creativity.""
The scoring guidelines for the participants' LEGO models were developed based on the Creative Product Analysis Matrix model, which is used to grade creativity and has been validated in earlier studies, he added.
The NTU researchers tested the scoring guidelines to measure the degree of consistency among the different judges when they independently assessed the LEGO models, and concluded that the scoring guidelines provided were reliable.
The researchers also tested the Serious Brick Play method's ability to measure what it was designed for through statistical analyses and found that the method was able to cover the key aspects of the Alternative Uses Task, a well-known tool that assesses creativity. Specifically, it assesses divergent thinking, a thought process used to generate creative ideas by exploring many possible solutions.
The researchers said that the Serious Brick Play method further assesses another thought process called convergent thinking, which focuses on coming up with a single, well-established answer to a problem.
""Divergent and convergent thinking are thought to be the central components of creativity, but most existing tools are designed around divergent thinking. Our Serious Brick Play method adds value by also covering the aspect of convergent thinking,"" said Asst Prof Ng.
How the study was done
Over six weeks, the researchers gathered data from a sample size of 87 undergraduate and postgraduate students in a controlled environment simulating an indoor workspace. Every week across three 40-minute sessions, the study participants read a summary of a global issue -- such as climate change, mental health, and poverty -- and then offered a solution by building a 3D model using LEGO bricks. The participants were then asked to give a written description and explanation for their models.
In each session, researchers varied the air quality of the workspace using different combinations of air filters contributed by Camfil. This varied the level of pollutants in the air, including carbon dioxide, PM2.5 (air pollutants less than 2.5 micrometres in diameter), and total volatile organic compounds (TVOC).
The participants' LEGO models and descriptions were then graded by seven randomly selected adults, who were trained to familiarise themselves with the scoring guidelines based on: Originality: whether the solution is usual or unusual, Fluency: the level of elaboration in the description of the solution, and Build: how sophisticated, complex, or aesthetic the solution is.Link between TVOC levels and creativity
The NTU team's statistical analysis of the participants' average scores and indoor air quality data gathered from 18 sessions revealed that participants tended to turn in creative solutions with lower scores -- an indicator of lower creative potential -- when the workspace had higher TVOC levels.
Using a statistical model, the team calculated that reducing TVOC from an acceptable threshold of 1,000 parts per billion to 281 parts per billion -- or a 72 per cent reduction in TVOC levels -- led to a 12 per cent increase in creative potential in the study cohort.
Less significant relationships were found between PM2.5 and creativity as well as carbon dioxide levels and creativity.
Asst Prof Ng said: ""The results from this study indicate that creativity levels can be linked to the concentration of pollutants in a room. Improving the air quality could be an economical solution to improve occupants' creativity.""
Having uncovered a link between TVOC levels and creativity, the research team is now studying how TVOC and other indoor air pollutants affect cognitive processes by measuring participants' brain activity.

","score: 16.971392604248624, grade_level: '17'","score: 18.753709284028325, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41598-023-42355-z,"Companies are increasingly asking their employees to find creative solutions to their problems. However, the office environment may reduce an employee’s creative potential. In this study, the role of indoor air quality parameters (PM2.5, TVOC, and CO2) in maintaining a creative environment (involving lateral thinking ability) was evaluated by Serious Brick Play (SBP), an adaptation of the LEGO Serious Play (LSP) framework. This study was conducted in a simulated office space with 92 participants over a period of 6 weeks. The SBP required participants to address a challenge by building using Lego bricks, and then describe the solution within a given timeframe. The creations and descriptions were then graded in terms of originality, fluency, and build. The results indicated that higher TVOC levels were significantly associated with lower-rated creative solutions. A 71.9% reduction in TVOC (from 1000 ppb), improves an individual’s full creative potential by 11.5%. Thus, maintaining a low TVOC level will critically enhance creativity in offices."
"
A warmer environment could mean more mosquitoes as it becomes harder for their predators to control the population, according to a recent study led by Virginia Commonwealth University researchers.

As the cover feature in Ecology, a journal published by the Ecological Society of America, the study -- ""Warming and Top-Down Control of Stage-Structured Prey: Linking Theory to Patterns in Natural Systems"" -- found that rising temperatures, often linked to climate change, can make predators of mosquito larvae less effective at controlling mosquito populations. Warmer temperatures accelerate development time of larvae, leading to a smaller window of time that dragonflies could eat them.
This means there could be nearly twice as many mosquito larvae that make it to adulthood in the study area. The researchers looked at riverine rock pools at Belle Isle along the James River in Richmond and found that warmer temperature pools had more aquatic mosquito larvae, even when their predators that naturally control the populations were present.
The native rock pool mosquito is not an important disease vector, but it is one of the few local mosquitoes that doesn't have to feed as an adult to lay eggs. So the findings might apply to similar taxa, like the invasive Asian rock pool mosquito.
""We might see larger populations of everyone's least favorite bug, mosquitoes. While the mosquito larvae we studied here [are] the North American rock pool mosquito, these findings likely apply to species of mosquito that do act as vectors for diseases like West Nile or even Zika virus,"" said Andrew T. Davidson, Ph.D., lead researcher on the study. He conducted the research through the Ph.D. program in VCU's Center for Integrative Life Sciences Education.
Predators help stabilize ecosystems and food webs, and the study looked at predator-prey interaction between dragonfly nymphs and mosquito larvae. Before field work, the research was rooted in concepts of thermal physiology and short-term lab experiments that yielded predictive models of the relationship between predators, prey and temperature in the field. The field study then tested the models in a complete natural environment.
The study builds on Davidson's earlier research in Functional Ecology as well as work by lab mate C. Ryland Stunkle and the rest of the VCU rock pool team. The team also acknowledges the collaborative support of professor Brian Byrd of Western Carolina University's College of Health and Human Sciences.
The recent work led by Davidson was part of a larger National Science Foundation grant that has involved scientists from VCU, the University of Richmond, Radford University, Western Carolina University and Eastern Carolina University. The collaborative award of nearly $1 million has included nearly $400,000 for VCU.
Contributors to the new study include Stunkle, Joshua T. Armstrong and James R. Vonesh of VCU; Elizabeth A. Hamman of St. Mary's College of Maryland; and Michael W. McCoy of Florida Atlantic University.

","score: 15.336257554625757, grade_level: '15'","score: 16.284597861459787, grade_levels: ['college_graduate'], ages: [24, 100]",10.1002/ecy.4213,"Warming has broad and often nonlinear impacts on organismal physiology and traits, allowing it to impact species interactions like predation through a variety of pathways that may be difficult to predict. Predictions are commonly based on short‐term experiments and models, and these studies often yield conflicting results depending on the environmental context, spatiotemporal scale, and the predator and prey species considered. Thus, the accuracy of predicted changes in interaction strength, and their importance to the broader ecosystems they take place in, remain unclear. Here, we attempted to link one such set of predictions generated using theory, modeling, and controlled experiments to patterns in the natural abundance of prey across a broad thermal gradient. To do so, we first predicted how warming would impact a stage‐structured predator–prey interaction in riverine rock pools between Pantala spp. dragonfly nymph predators and Aedes atropalpus mosquito larval prey. We then described temperature variation across a set of hundreds of riverine rock pools (n = 775) and leveraged this natural gradient to look for evidence for or against our model's predictions. Our model's predictions suggested that warming should weaken predator control of mosquito larval prey by accelerating their development and shrinking the window of time during which aquatic dragonfly nymphs could consume them. This was consistent with data collected in rock pool ecosystems, where the negative effects of dragonfly nymph predators on mosquito larval abundance were weaker in warmer pools. Our findings provide additional evidence to substantiate our model‐derived predictions while emphasizing the importance of assessing similar predictions using natural gradients of temperature whenever possible."
"
Researchers at LMU, the Max Planck Institute for Human Development, and the University of Oxford have investigated how sleep affects memory. They found a link between breathing and the emergence of certain brain activity patterns in sleep that are associated with the reactivation of memory contents. The data points to possible consequences of unhealthy breathing on memory.

How are memories consolidated during sleep? In 2021, researchers led by Dr. Thomas Schreiner, leader of the Emmy Noether junior research group at LMU's Department of Psychology, had already shown there was a direct relationship between the emergence of certain sleep-related brain activity patterns and the reactivation of memory contents during sleep. However, it was still unclear whether these rhythms are orchestrated by a central pacemaker. So the researchers joined up with scientists from the Max Planck Institute for Human Development in Berlin and the University of Oxford to reanalyze the data. Their results have identified respiration as a potential pacemaker. ""That is to say, our breathing influences how memories are consolidated during sleep,"" says Schreiner.
Learning processes investigated in sleep laboratory
For their original study, the researchers showed 20 study participants 120 images over the course of two sessions. All the pictures were associated with certain words. Then the participants slept for around two hours in the sleep laboratory. When they awoke, they were questioned about the associations they had learned. During the entire learning and sleep period, their brain activity was recorded by means of EEG, along with their breathing.
The researchers discovered that previously learned contents were spontaneously reactivated by the sleeping brain during the presence of so-called slow oscillations and sleep spindles (short phases of increased brain activity). ""The precision of the coupling of these sleep-related brain rhythms increases from childhood to adolescence and then declines again during aging,"" says Schreiner.
Breathing and brain activity are linked
Because respiration frequency also changes with age, the researchers then analyzed the data in relation to the recorded breathing and were able to establish a connection between them: ""Our results show that our breathing and the emergence of characteristic slow oscillation and spindle patterns are linked,"" says Schreiner. ""Although other studies had already established a connection between breathing and cognition during wake, our work makes clear that respiration is also important for memory processing during sleep.""
Older people often suffer from sleep disorders, respiratory disorders, and declining memory function. Schreiner plans to further investigate whether there are connections between these phenomena and whether interventions -- such as the use of CPAP masks, which are already used to treat sleep apnea -- make sense from a cognitive perspective.

","score: 14.165466063348422, grade_level: '14'","score: 15.876651583710405, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41467-023-43450-5,"The beneficial effect of sleep on memory consolidation relies on the precise interplay of slow oscillations and spindles. However, whether these rhythms are orchestrated by an underlying pacemaker has remained elusive. Here, we tested the relationship between respiration, which has been shown to impact brain rhythms and cognition during wake, sleep-related oscillations and memory reactivation in humans. We re-analysed an existing dataset, where scalp electroencephalography and respiration were recorded throughout an experiment in which participants (N = 20) acquired associative memories before taking a nap. Our results reveal that respiration modulates the emergence of sleep oscillations. Specifically, slow oscillations, spindles as well as their interplay (i.e., slow-oscillation_spindle complexes) systematically increase towards inhalation peaks. Moreover, the strength of respiration - slow-oscillation_spindle coupling is linked to the extent of memory reactivation (i.e., classifier evidence in favour of the previously learned stimulus category) during slow-oscillation_spindles. Our results identify a clear association between respiration and memory consolidation in humans and highlight the role of brain-body interactions during sleep."
"
People who abandon New Year's resolutions or other commitments can maintain the respect of their peers by blaming external factors such as lack of money, new research suggests.

Studies have found that people were more likely to be seen as having good self-control despite abandoning a commitment to live a healthier life if they claimed they did not have the money for a gym membership or expensive new cooking equipment. People who instead claimed they didn't have the time to exercise or to replace a takeaway habit with healthy, home-cooked food, were more likely to be seen as having poor self-control.
Dr Janina Steinmetz, Reader in Marketing at Bayes Business School (formerly Cass), who conducted the research, analysed which excuses boost the chances of people appearing to have good self-control even after they fail to keep a resolution or pledge.
She said: ""Many resolutions or commitments involve either time or money so the lack of one or the other seems to provide a good excuse for breaking it without adversely affecting how others see us. However, these two excuses are not equally effective. My six experiments involving around 1,200 people found that pleading a lack of money leads to better outcomes -- in terms of perceptions about the individual -- than citing lack of time.""
For example, in one experiment, 200 online participants read about people who failed to keep a commitment to eat healthier food. Some of those they read about blamed the cost of cooking good meals while others said they were defeated by a lack of time. Participants saw the first group as having better self-control and were more likely to consider them as potentially good gym partners.
The differences appear to reflect how much the excuse is seen as being within the person's control, Dr Steinmetz suggests.
She said: ""These results are surprising because people like to use lack of time as an excuse when they can't do something. They equate lack of time with high status. However, the studies suggest we tend to think others could find the time to exercise or cook healthy meals if they were sufficiently motivated. That is why citing factors many of us have less control over, such as lack of money, can produce perceptions of having better self-control even when we abandon our New Year's resolution or break a commitment.""
The results, published last week in the European Journal of Social Psychology, could have implications for local authorities, NHS organisations and others campaigning on public health issues -- and health professionals working with obese people.

Dr Steinmetz explained: ""People often justify a diet heavy in fast food or TV dinners by saying it is quicker than buying and cooking healthy ingredients. Organisations promoting or marketing healthy lifestyles or working with patients around behaviour change can challenge that self-aggrandising claim that people are 'just too busy' to choose the healthy option. They can promote healthy but easy-to-prepare meals using affordable ingredients, or the benefits of even half an hour's aerobic activity. That would undermine the credibility of an all-too-familiar excuse.""
There might also be lessons in the research for anyone in the market for a new job or romance.
Dr Steinmetz said: ""In job interviews and on dating website questionnaires people are often invited to talk about a failure they've had in life. Obviously, we've all had them but when explaining why, whether you're looking for a job or for romance, blaming uncontrollable factors might help you convey a positive image. Although my research didn't look at those contexts, it might be wise to avoid the temptation to blame lack of time.""

","score: 13.052464305326748, grade_level: '13'","score: 14.633922295442062, grade_levels: ['college_graduate'], ages: [24, 100]",10.1002/ejsp.3010,"Research has shown that people frequently fail at exerting self‐control. Yet, having good self‐control is essential for being trusted and relied on. In this research, I test which common and frequent excuses for self‐control failures (i.e., resulting from lack of time vs. money) allow people to maintain an image of good self‐control despite failure. In six studies (five pre‐registered), using different types of self‐control domains, I show that participants perceived someone who failed at a resolution to nevertheless have good self‐control if they failed because they lacked money (vs. time) to follow through (Study 1). This effect was due to the mediated (Study 2a) and manipulated (Study 2b) perceived controllability of the excuse. This effect had downstream consequences for participants’ hypothetical and real behaviour toward the individual when their outcomes were interdependent (Studies 3 and 4). Finally, participants lacked insight into these patterns when communicating their own self‐control failures, which they attributed to a lack of time over money (Study 5)."
"
Snacks constitute almost a quarter of a day's calories in U.S. adults and account for about one-third of daily added sugar, a new study suggests.

Researchers analyzing data from surveys of over 20,000 people found that Americans averaged about 400 to 500 calories in snacks a day - often more than what they consumed at breakfast - that offered little nutritional value.
Though dietitians are very aware of Americans' propensity to snack, ""the magnitude of the impact isn't realized until you actually look at it,"" said senior study author Christopher Taylor, professor of medical dietetics in the School of Health and Rehabilitation Sciences at The Ohio State University.
""Snacks are contributing a meal's worth of intake to what we eat without it actually being a meal,"" Taylor said. ""You know what dinner is going to be: a protein, a side dish or two. But if you eat a meal of what you eat for snacks, it becomes a completely different scenario of, generally, carbohydrates, sugars, not much protein, not much fruit, not a vegetable. So it's not a fully well-rounded meal.""
Survey participants who were controlling their type 2 diabetes ate fewer sugary foods and snacked less overall than participants without diabetes and those whose blood sugar levels indicated they were prediabetic.
""Diabetes education looks like it's working, but we might need to bump education back to people who are at risk for diabetes and even to people with normal blood glucose levels to start improving dietary behaviors before people develop chronic disease,"" Taylor said.
The study was published recently in PLOS Global Public Health.

Researchers analyzed data from 23,708 U.S. adults over 30 years of age who had participated from 2005 to 2016 in the National Health and Nutrition Examination Survey. The survey collects 24-hour dietary recalls from each participant - detailing not just what, but when, all food was consumed.
Respondents were categorized according to their HbA1c level, a measure of glucose control, into four groups: nondiabetes, prediabetes, controlled diabetes and poorly controlled diabetes.
Among the whole survey sample, snacks accounted for between 19.5% and 22.4% of total energy intake - while contributing very little nutritional quality.
In descending order of proportion, snacks consisted of convenience foods high in carbohydrates and fats, sweets, alcoholic beverages, non-alcoholic drinks that include sugar-sweetened beverages, protein, milk and dairy, fruits, grains and, lagging far behind, vegetables.
Noting that capturing 24 hours of food consumption doesn't necessary reflect how people usually eat, ""it gives us a really good snapshot of a large number of people,"" Taylor said. ""And that can help us understand what's going on, where nutritional gaps might be and the education we can provide.""
Finding that people with diabetes had healthier snacking habits was an indicator that dietary education is beneficial to people with the disease. But it's information that just about everyone can use, Taylor said - and it's about more than just cutting back on sugar and carbs.

""We need to go from just less added sugar to healthier snacking patterns,"" he said. ""We've gotten to a point of demonizing individual foods, but we have to look at the total picture. Removing added sugars won't automatically make the vitamin C, vitamin D, phosphorus and iron better. And if we take out refined grains, we lose nutrients that come with fortification.
""When you take something out, you have to put something back in, and the substitution becomes just as important as the removal.""
And so, rather than offering tips on what foods to snack on, Taylor emphasizes looking at a day's total dietary picture and seeing whether snacks will fulfill our nutritional needs.
""Especially during the holidays, it's all about the environment and what you have available, and planning accordingly. And it's about shopping behavior: What do we have in the home?"" he said.
""We think about what we're going to pack for lunch and cook for dinner. But we don't plan that way for our snacks. So then you're at the mercy of what's available in your environment.""
This work was supported by Abbott Nutrition and Ohio State. Co-authors included Kristen Heitman, Owen Kelly, Stephanie Fanelli and Jessica Krok-Schoen of Ohio State and Sara Thomas and Menghua Luo of Abbott Nutrition.

","score: 12.141997321279092, grade_level: '12'","score: 12.790442826050558, grade_levels: ['college'], ages: [18, 24]",10.1371/journal.pgph.0000802,"Little is known about the snacking patterns among adults with type 2 diabetes. The contribution of snacks to energy and nutrient intakes is important to further understand dietary patterns and glycemic control. The purpose of this study is to evaluate snack consumption among adults according to diabetes status in the United States. One NHANES 24-hour dietary recall for each participant collected between 2005–2016 was utilized for analysis (n = 23,708). Analysis of covariance was used to compare differences in nutrient and food groups intakes from snacks across levels of glycemic control, while controlling for age, race/ethnicity, income, marital status, and gender. Results of this analysis inform that adults with type 2 diabetes consume less energy, carbohydrates, and total sugars from snacks than adults without diabetes. Those with controlled type 2 diabetes consumed more vegetables and less fruit juice than other groups, yet adults with type 2 diabetes in general consumed more cured and luncheon meats than adults without diabetes or with prediabetes. Protein from all snacks for those without diabetes is higher than all other groups. This study elucidates common snacking patterns among US adults with diabetes and highlights the need for clinicians and policymakers to take snacking into consideration when evaluating and providing dietary recommendations."
"
The average menstruator will use over 11,000 tampons or sanitary pads in their lifetime. Vaginal and vulvar tissue that touch pads and tampons is highly permeable. Through this permeable tissue chemicals are absorbed without being metabolized, which makes endocrine-disrupting chemicals potentially dangerous when found in menstrual products. Endocrine-disrupting chemicals can interfere with human hormones and cause medical issues, including gynecological conditions such as endometriosis and uterine fibroids.

Joanna Marroquin, a Mason PhD in Public Health student, and Associate Professor Anna Pollack, reviewed studies conducted since 2103 that measured chemicals in menstrual products and that measured human biomarkers of chemical exposure and determined that endocrine-disrupting chemicals were found in menstrual products including tampons, pads, and liners.
""Identifying chemicals in menstrual products that menstruators regularly use is important because exposure through these products can impact menstruators' reproductive health,"" said Marroquin, the paper's first author.
The study found that menstrual products contain a variety of endocrine-disrupting chemicals including phthalates, volatile organic compounds, parabens, environmental phenols, fragrance chemicals, dioxins and dioxin-like compounds.
This issue is even more relevant thanks to the Robin Danielson Menstrual Product and Intimate Care Product Safety Act of 2023, which was introduced in the U.S. House of Representatives in October 2023. The Act would establish a program of research regarding the risks posed by the presence of dioxins, phthalates, pesticides, chemical fragrances, and other components in menstrual products and intimate care products.
This literature reviewed 15 papers published between 2013 and 2023 that tested menstrual products in the U.S., Japan, and South Korea. The researchers note that there are few publications available that measure chemicals in menstrual products.
Additionally, though forever chemicals (PFAS) have been found in menstrual underwear, there is a lack of peer-reviewed research on menstrual underwear and other newly-popular-in-the-U.S. products such as menstrual cups and discs.
Chemicals in menstrual products: A systematic review was published in BJOG, an international journal of obstetrics and gynecology in September 2023. Additional authors include Marianthi-Anna Kiomourtzoglou from Mailman School of Public Health, Columbia University and Alexandra Scranton from Women's Voices for the Earth.
The research was supported by Pollack's National Institute of Environmental Health Sciences R01ES31079 award.

","score: 16.665028624192065, grade_level: '17'","score: 19.776629732225302, grade_levels: ['college_graduate'], ages: [24, 100]",10.1111/1471-0528.17668,"From menarche until menopause, the average menstruator will use over 11 000 tampons or sanitary pads. Vaginal and vulvar tissue is highly permeable, and chemicals are absorbed without undergoing first‐pass metabolism. To conduct a review of the literature to determine exposure to environmental chemicals in menstrual products. This review identified 15 papers over the past 10 years. Papers that measured chemicals in menstrual products and that measured human biomarkers of chemical exposure were included. Papers had to also be available in English. Reviewers assessed the articles and data provided. Multiple chemical groups were found. Phthalates, volatile organic compounds, parabens, environmental phenols, fragrance chemicals, dioxins and dioxin‐like compounds were detected in menstrual products. Research gaps were identified, including the lack of studies on newer products such as menstrual underwear and cups/discs. In addition to measuring chemicals in these products, future research should focus on clarifying the exposure per menstrual cycle to these chemicals to understand how menorrhagia and cycle length influence exposure from menstrual products. Menstrual products contained measurable levels of a range of endocrine disrupting chemicals including phthalates, phenols and parabens. This reflects a potentially important route of exposure to chemicals that can impact women's reproductive health."
"
Artificial Intelligence (AI) and 3D images of the human tongue have revealed that the surface of our tongues are unique to each of us, new findings suggest.

The results offer an unprecedented insight into the biological make-up of our tongue's surface and how our sense of taste and touch differ from person to person.
The research has huge potential for discovering individual food preferences, developing healthy food alternatives and early diagnosis of oral cancers in the future, experts say.
The human tongue is a highly sophisticated and complex organ. It's surface is made up of hundreds of small buds -- known as papillae -- that assist with taste, talking and swallowing.
Of these numerous projections, the mushroom-shaped fungiform papillae hold our taste buds whereas the crown-shaped filiform papillae give the tongue its texture and sense of touch.
The taste function of our fungiform papillae has been well researched but little is known about the difference in shape, size and pattern of both forms of papillae between individuals.
A team of researchers led by the University of Edinburgh's School of Informatics, in collaboration with the University of Leeds, trained AI computer models to learn from three-dimensional microscopic scans of the human tongue, showing the unique features of papillae.

They fed the data from over two thousand detailed scans of individual papillae -- taken from silicone moulds of fifteen people's tongues -- to the AI tool.
The AI models were designed to gain a better understanding of individual features of the participant's papillae and to predict the age and gender of each volunteer.
The team used small volumes of data to train the AI models about the different features of the papillae, combined with a significant use of topology -- an area of mathematics which studies how certain spaces are structured and connected.
This enabled the AI tool to predict the type of papillae to within 85 per cent accuracy and to map the position of filiform and fungiform papillae on the tongue's surface.
Remarkably, the papillae were also found to be distinctive across all fifteen subjects and individuals could be identified with an accuracy of 48 per cent from a single papilla.
The findings have been published in the journal Scientific Reports.

The study received funding from the United Kingdom Research and Innovation (UKRI) CDT in Biomedical AI and European Research Council (ERC) under the European Union's Horizon 2020 research and innovation program.
Senior author, Professor Rik Sakar, Reader, School of Informatics, University of Edinburgh, said:
""This study brings us closer to understanding the complex architecture of tongue surfaces.
""We were surprised to see how unique these micron-sized features are to each individual. Imagine being able to design personalized food customised to the conditions of specific people and vulnerable populations and thus ensure they can get proper nutrition whilst enjoying their food.
Professor Sakar, added:
""We are now planning to use this technique combining AI with geometry and topology to identify micron-sized features in other biological surfaces. This can help in early detection and diagnosis of unusual growths in human tissues.
Lead author, Rayna Andreeva, PhD student at the Centre for Doctoral Training (CDT) in Biomedical AI, University of Edinburgh, said:
""It was remarkable that the features based on topology worked so well for most types of analysis, and they were the most distinctive across individuals. This needs further study not only for the papillae, but also for other kinds of biological surfaces and medical conditions.""

","score: 15.273992094861658, grade_level: '15'","score: 16.228477470355735, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41598-023-46535-9,"The tongue surface houses a range of papillae that are integral to the mechanics and chemistry of taste and textural sensation. Although gustatory function of papillae is well investigated, the uniqueness of papillae within and across individuals remains elusive. Here, we present the first machine learning framework on 3D microscopic scans of human papillae ($$n=2092$$ n = 2092 ), uncovering the uniqueness of geometric and topological features of papillae. The finer differences in shapes of papillae are investigated computationally based on a number of features derived from discrete differential geometry and computational topology. Interpretable machine learning techniques show that persistent homology features of the papillae shape are the most effective in predicting the biological variables. Models trained on these features with small volumes of data samples predict the type of papillae with an accuracy of 85%. The papillae type classification models can map the spatial arrangement of filiform and fungiform papillae on a surface. Remarkably, the papillae are found to be distinctive across individuals and an individual can be identified with an accuracy of 48% among the 15 participants from a single papillae. Collectively, this is the first evidence demonstrating that tongue papillae can serve as a unique identifier, and inspires a new research direction for food preferences and oral diagnostics."
"
Increased sedentary time in childhood can raise cholesterol levels by two thirds as an adult, leading to heart problems and even premature death -- but a new study has found light physical activity may completely reverse the risks and is far more effective than moderate-to-vigorous physical activity.

The study was conducted in collaboration between the University of Exeter, University of Eastern Finland, and University of Bristol and published in The Journal of Clinical Endocrinology & Metabolism. Researchers used data from the University of Bristol study Children of the 90s (also known as the Avon Longitudinal Study of Parents and Children), which included 792 children aged 11 years who were followed up until the age of 24.
Results from this study found that accumulated sedentary time from childhood can increase cholesterol levels by two thirds (67 percent) by the time someone reaches their mid-twenties. Elevated cholesterol and dyslipidaemia from childhood and adolescence have been associated with premature death in the mid-forties and heart problems such as subclinical atherosclerosis and cardiac damage in the mid-twenties.
Healthy lifestyles are considered important in the prevention of dyslipidaemia and one of the primary ways of lowering cholesterol, apart from diet, is movement behaviour. For the first time, this study objectively examined the long-term effects of sedentary time, light physical activity, and moderate-to-vigorous physical activity on childhood cholesterol levels.
The World Health Organization currently recommends children and adolescents should accumulate on average 60 minutes of moderate-to-vigorous physical activity a day and reduce sedentary time but have limited guidelines for light physical activity. Yet this new study and other recent studies has found light physical activity -- which includes exercises such as long walks, house chores, or slow dancing, swimming, or cycling -- is up to five times more effective than moderate-to-vigorous physical activity at promoting healthy hearts and lowering inflammation in the young population.
Dr Andrew Agbaje from the University of Exeter led the study and said: ""These findings emphasise the incredible health importance of light physical activity and shows it could be the key to preventing elevated cholesterol and dyslipidaemia from early life. We have evidence that light physical activity is considerably more effective than moderate-to-vigorous physical activity in this regard, and therefore it's perhaps time the World Health Organization updated their guidelines on childhood exercise -- and public health experts, paediatricians, and health policymakers encouraged more participation in light physical activity from childhood.""
During the research, accelerometer measures of sedentary time, light physical activity, and moderate-to-vigorous physical activity were collected at ages 11, 15, and 24 years. High-density lipoprotein cholesterol, low-density lipoprotein cholesterol, triglyceride, and total cholesterol were repeatedly measured at ages 15, 17, and 24 years. These children also had repeated measurement of dual-energy Xray absorptiometry assessment of total body fat mass and muscle mass, as well as fasting blood glucose, insulin, and high sensitivity C-reactive protein, with smoking status, socio-economic status, and family history of cardiovascular disease.

During the 13-year follow-up, sedentary time increased from approximately six hours a day to nine hours a day. Light physical activity decreased from six hours a day to three hours a day while moderate-to-vigorous physical activity was relatively stable at around 50 minutes a day from childhood until young adulthood. The average increase in total cholesterol was 0.69 mmol/l. It was observed without any influence from body fat.
An average of four-and-a-half hours a day of light physical activity from childhood through young adulthood causally decreased total cholesterol by (-0.53 mmol/l), however, body fat mass could reduce the effect of light physical activity on total cholesterol by up to six percent. Approximately 50 minutes a day of moderate-to-vigorous physical activity from childhood was also associated with slightly reduced total cholesterol (-0.05 mmol/L), but total body fat mass decreased the effect of moderate-to-vigorous physical activity on total cholesterol by up to 48 percent. Importantly, the increase in fat mass neutralised the small effect of moderate-to-vigorous physical activity on total cholesterol.
The paper is entitled Associations of Sedentary Time and Physical Activity from Childhood with Lipids: A 13-Year Mediation and Temporal Studyand published in The Journal of Clinical Endocrinology & Metabolism. These findings come shortly after another study led by Dr Andrew Agbaje published this week in Nature Communications found light physical activity may completely reverse childhood obesity linked to increased sedentary time in more than 6000 children. Sedentary time contributed seven to ten percent of the total fat mass gained during growth from childhood until young adulthood. Light physical activity decreased the overall gain in fat mass by 9.5-15 percent, while moderate-to-vigorous physical activity decreased fat mass by 0.7-1.7 percent.
Dr Andrew Agbaje of the University of Exeter said: ""Our research suggests light physical activity may be an unsung hero and it is about time the world replaced the mantra of 'an average of 60 minutes a day of moderate-to-vigorous physical activity' with 'at least 3 hours a day of light physical activity'. Light physical activity appears to be the antidote to the catastrophic effect of sedentary time in the young population.""
Dr Andrew Agbaje's research group (urFIT-child) is supported by research grants from Jenny and Antti Wihuri Foundation, the Finnish Cultural Foundation Central Fund, the Finnish Cultural Foundation North Savo Regional Fund, the Orion Research Foundation, the Aarne Koskelo Foundation, the Antti and Tyyne Soininen Foundation, the Paulo Foundation, the Yrjö Jahnsson Foundation, the Paavo Nurmi Foundation, the Finnish Foundation for Cardiovascular Research, Ida Montin Foundation, the Foundation for Pediatric Research, and Alfred Kordelin Foundation.

","score: 20.36337626854021, grade_level: '20'","score: 21.9029906323185, grade_levels: ['college_graduate'], ages: [24, 100]",10.1210/clinem/dgad688,"Among children, evidence on long-term longitudinal associations of accelerometer-measured sedentary time, light physical activity (LPA), and moderate to vigorous PA (MVPA) with lipid indices are few. The mediating role of body composition and other metabolic indices in these associations remains unclear and whether poor movement behavior precedes altered lipid levels is unknown. This study examined the associations of sedentary time, LPA, and MVPA from childhood through young adulthood with increased lipids, the mediating role of body composition, and whether temporal interrelations exist. Data from 792 children (58% female; mean [SD] age at baseline, 11.7 [0.2] years), drawn from the Avon Longitudinal Study of Parents and Children (ALSPAC) UK birth cohort, who had at least 2 time-point measures of accelerometer-based sedentary time, LPA, and MVPA during clinic visits at ages 11, 15, and 24 years and complete fasting plasma high-density lipoprotein cholesterol, low-density lipoprotein cholesterol, triglyceride, and total cholesterol measured during follow-up visits at ages 15, 17, and 24 years were analyzed. Total fat mass partly mediated the inverse associations of LPA with low-density lipoprotein cholesterol by 13%, triglyceride by 28%, and total cholesterol by 6%. Total fat mass mediated the inverse associations of MVPA with low-density lipoprotein cholesterol by 37% and total cholesterol by 48%, attenuating the effect on total cholesterol to nonsignificance (P = .077). In the temporal path analyses, higher MVPA at age 15 years was associated with lower low-density lipoprotein cholesterol at 24 years (β = −0.08, SE, 0.01, P = .022) but not vice versa. Sedentary time worsens lipid indices, but increased LPA had a 5- to 8-fold total cholesterol-lowering effect and was more resistant to the attenuating effect of fat mass than MVPA."
"
Researchers at Texas A&M University have already shown that paternal drinking habits prior to conception can have a negative effect on fetal development -- with semen from men who regularly consume alcohol impacting placenta development, fetal alcohol syndrome (FAS)-associated brain and facial defects, and even IVF outcomes.

In an article published this month in Andrology, the lab of Dr. Michael Golding has now demonstrated that it takes much longer than previously believed, longer than a month, for the effects of alcohol consumption to leave the father's sperm.
""When someone is consuming alcohol on a regular basis and then stops, their body goes through withdrawal, where it has to learn how to operate without the chemical present,"" said Golding, a professor in the School of Veterinary Medicine & Biomedical Sciences' Department of Veterinary Physiology & Pharmacology. ""What we discovered is that a father's sperm are still negatively impacted by drinking even during the withdrawal process, meaning it takes much longer than we previously thought for the sperm to return to normal.""
The Dangers Of Paternal Drinking
One of the major risks associated with alcohol consumption before and during pregnancy is FAS, which causes abnormal facial features, low birth weight and/or height, attention and hyperactivity issues, and poor coordination.
Currently, doctors are required to confirm only that the mother has consumed alcohol -- not the father -- to diagnose a child with FAS.
""For years, there's really been no consideration of male alcohol use whatsoever,"" Golding said. ""Within the last five to eight years, we've started to notice that there are certain conditions where there's a very strong paternal influence when it comes to alcohol exposure and fetal development.

""With this project, we wanted to see how long it would take for the effects of alcohol on sperm to wear off,"" he said. ""We thought it would be a relatively quick change back to normal, but it wasn't. The withdrawal process took over a month.""
When drinking alcohol, an individual's liver experiences oxidative stress, leading the body to overproduce certain chemicals, which then interrupts normal cellular activity. Golding's team discovered that withdrawal causes the same kind of oxidative stress, effectively lengthening the duration of alcohol's effects on the body beyond what was previously thought.
""During withdrawal, the liver experiences perpetual oxidative stress and sends a signal throughout the male body,"" Golding said. ""The reproductive system interprets that signal and says, 'Oh, we are living in an environment that has a really strong oxidative stressor in it. I need to program the offspring to be able to adapt to that kind of environment.' But Golding suspects that the adaptations to the sperm aren't beneficial -- they lead to problems like FAS.""
He also noted that it doesn't take excessive alcohol use for a person to experience withdrawal.
""In the models we're using, even drinking three to four beers after work several days a week can induce withdrawal when the behavior ceases,"" Golding said. ""You may not feel inebriated, but your body is going through chemical changes.""
Changing The Narrative
Golding's work is vital to improving pregnancy outcomes by changing the conversation about who is responsible for alcohol-related birth defects, since society has historically placed all blame on mothers, even when they do not consume alcohol during their pregnancy.

""There's psychological trauma associated with the question, 'Did you drink while you were pregnant?' It's also difficult for physicians to have that conversation,"" he said. ""But if they don't, then FAS doesn't get diagnosed right away and the child may not get the support that they need until later in life.""
Because of this, it's crucial that couples planning on getting pregnant know how far in advance to stop drinking in order to prevent birth defects.
While Golding and his lab will continue to research the effects of paternal drinking to help doctors advise couples, he suggests that fathers abstain from alcohol at least three months prior to conceiving, given this groundbreaking discovery.
""There's still a lot of work to be done to get a hard answer, but we know that sperm are made over the course of 60 days, and the withdrawal process takes at least one month,"" he said. ""So, my estimate would be to wait at least three months.""

","score: 13.688548009367683, grade_level: '14'","score: 14.97265807962529, grade_levels: ['college_graduate'], ages: [24, 100]",10.1111/andr.13566,"Chronic preconception paternal alcohol use adversely modifies the sperm epigenome, inducing fetoplacental and craniofacial growth defects in the offspring of exposed males. A crucial outstanding question in the field of paternal epigenetic inheritance concerns the resilience of the male germline and its capacity to recover and correct sperm‐inherited epigenetic errors after stressor withdrawal. We set out to determine if measures of the sperm‐inherited epigenetic program revert to match the control treatment 1 month after withdrawing the daily alcohol treatments. Using a voluntary access model, we exposed C57BL/6J males to 6% or 10% alcohol for 10 weeks, withdrew the alcohol treatments for 4 weeks, and used RNA sequencing to examine gene expression patterns in the caput section of the epididymis. We then compared the abundance of sperm small RNA species between treatments. In the caput section of the epididymis, chronic alcohol exposure induced changes in the transcriptional control of genetic pathways related to the mitochondrial function, oxidative phosphorylation, and the generalized stress response (EIF2 signaling). Subsequent analysis identified region‐specific, alcohol‐induced changes in mitochondrial DNA copy number across the epididymis, which correlated with increases in the mitochondrial DNA content of alcohol‐exposed sperm. Notably, in the corpus section of the epididymis, increases in mitochondrial DNA copy number persisted 1 month after alcohol cessation. Analysis of sperm noncoding RNAs between control and alcohol‐exposed males 1 month after alcohol withdrawal revealed a ∼100‐fold increase in mir‐196a, a microRNA induced as part of the nuclear factor erythroid 2‐related factor 2 (Nrf2)‐driven cellular antioxidant response. Our data reveal that alcohol‐induced epididymal mitochondrial dysfunction and differences in sperm noncoding RNA content persist after alcohol withdrawal. Further, differences in mir‐196a and sperm mitochondrial DNA copy number may serve as viable biomarkers of adverse alterations in the sperm‐inherited epigenetic program."
"
Breastfeeding, even partially alongside formula feeding, changes the chemical makeup -- or metabolome -- of an infant's gut in ways that positively influence brain development and may boost test scores years later, suggests new CU Boulder research.

""For those who struggle with exclusively breastfeeding, this study suggests your baby can still get significant benefits if you breastfeed as much as you can,"" said senior author Tanya Alderete, an assistant professor of integrative physiology at CU Boulder.
The study, published Dec. 13 in the journal npj Metabolic Health and Disease, also identifies specific metabolites that manufacturers may want to consider adding to infant formula to optimize healthy brain development and concerning compounds they should try to leave out.
""Our research suggests that even at low levels, some contaminants found in formula may have negative neurodevelopmental effects downstream,"" said first author Bridget Chalifour, a postdoctoral researcher in Alderete's lab.
A health report card for the gut
For the study, the research team examined what is known as the ""fecal metabolome"" -- the diverse collection of metabolites found in the gut and shed in poop. Metabolites are small molecules that are churned out by gut bacteria as a byproduct of metabolizing food and make their way into the bloodstream, impacting the brain and other organs.
Breastmilk, formula and solid food also contain metabolites.

While scientists have long studied our resident bacteria, or microbiome, to better understand human health, the emerging field of ""metabolomics"" goes a step further.
""Looking at the gut microbiome tells us which bacteria are there, while looking at the fecal metabolome can help tell us what they are doing,"" said Chalifour. ""It's like a health report card for the gut.""
The team collected fecal samples from 112 infants at 1- and 6-months-old and worked with Donghai Liang, assistant professor of environmental health at Emory University in Atlanta, and other colleagues to chemically analyze which metabolites were present. They grouped infants based on how much they were breastfed vs. formula fed. At age 2, the children took cognitive, motor and language tests.
The study found that the samples from infants in different feeding groups contained significantly different levels of metabolites.
For instance, at 1 month old, 17 metabolites were more abundant the more a baby was breastfed, and 40 were more abundant the more a baby was formula fed.
When looking more closely at specific metabolites, the researchers identified 14 that were also associated with differences in test scores at age 2.

With only one notable exception, caffeine, the more metabolites associated with breast milk a baby had in their stool, the better they did on cognitive tests as toddlers (more on caffeine later.)
The more metabolites associated with formula feeding they had, the worst they did.
""The consistency of these results is striking and supports the benefits of breastfeeding as much as possible in early life,"" said Alderete.
Some metabolites associated with formula concerning
One particularly beneficial metabolite was cholesterol: At both 1 and 6 months old, the more a baby was breastfed the more cholesterol they had in their stool. And the more cholesterol babies had in their stool, the better they did on cognitive tests. This makes sense, as the fatty acid is critical for forming healthy circuits between brain cells. As the authors note, 80% to 90% of the brain's volume grows in the first two years of life.
In contrast, the more a baby was formula fed, the higher their levels of a metabolite called cadaverine, a known contaminant formed via fermentation.
In the study, the more a child was formula fed, the higher their levels of cadaverine and the lower their test scores at age 2. While the compound is considered a toxin at higher levels, the Food and Drug Administration permits low levels in infant formula.
""It may be that formula manufacturers should be more vigilant in getting levels of this compound down to zero,"" said Chalifour.
Interestingly, babies who were breastfed had higher levels of caffeine in their stool -- perhaps because moms may have been breastfeeding over a cup of coffee.
Not surprisingly, higher levels of caffeine, a stimulant, were associated with poorer cognitive scores. Prenatal caffeine exposure has previously been associated with lower neurodevelopmental scores, and experts recommend no more than 12 ounces, or a cup and a half, of coffee per day for pregnant women.
Not all or nothing
The World Health Organization recommends that infants be exclusively breastfed for the first six months of life, but in the United States, only 63% of infants are exclusively breastfed immediately following birth. By six months, only a quarter of U.S. babies are exclusively breastfed.
Alderete acknowledges that for some parents, breastfeeding isn't possible. She hopes her research can ultimately help manufacturers improve formula to make it as close to breastmilk as it can be. And she stresses that just because a child was not breastfed does not mean they'll have neurodevelopmental deficits. Early feeding patterns are just one of many factors that contribute to how a brain develops.
Her takeaway to new parents having trouble breastfeeding exclusively: Don't give up. It doesn't have to be all or nothing.
""Just increasing the proportion of breastmilk relative to formula may have a positive impact on your developing child,"" she said.

","score: 12.97149596577368, grade_level: '13'","score: 13.895728027105406, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s44324-023-00001-2,"Infant fecal metabolomics can provide valuable insights into the associations of nutrition, dietary patterns, and health outcomes in early life. Breastmilk is typically classified as the best source of nutrition for nearly all infants. However, exclusive breastfeeding may not always be possible for all infants. This study aimed to characterize associations between levels of mixed breastfeeding and formula feeding, along with solid food consumption and the infant fecal metabolome at 1- and 6-months of age. As a secondary aim, we examined how feeding-associated metabolites may be associated with early life neurodevelopmental outcomes. Fecal samples were collected at 1- and 6-months, and metabolic features were assessed via untargeted liquid chromatography/high-resolution mass spectrometry. Feeding groups were defined at 1-month as 1) exclusively breastfed, 2) breastfed >50% of feedings, or 3) formula fed ≥50% of feedings. Six-month groups were defined as majority breastmilk (>50%) or majority formula fed (≥50%) complemented by solid foods. Neurodevelopmental outcomes were assessed using the Bayley Scales of Infant Development at 2 years. Changes in the infant fecal metabolome were associated with feeding patterns at 1- and 6-months. Feeding patterns were associated with the intensities of a total of 57 fecal metabolites at 1-month and 25 metabolites at 6-months, which were either associated with increased breastmilk or increased formula feeding. Most breastmilk-associated metabolites, which are involved in lipid metabolism and cellular processes like cell signaling, were associated with higher neurodevelopmental scores, while formula-associated metabolites were associated with lower neurodevelopmental scores. These findings offer preliminary evidence that feeding patterns are associated with altered infant fecal metabolomes, which may be associated with cognitive development later in life."
"
Practicing yoga nidra -- a kind of mindfulness training -- might improve sleep, cognition, learning, and memory, even in novices, according to a pilot study publishing in the open-access journal PLOS ONE on December 13 by Karuna Datta of the Armed Forces Medical College in India, and colleagues. After a two-week intervention with a cohort of novice practitioners, the researchers found that the percentage of delta-waves in deep sleep increased and that all tested cognitive abilities improved.

Unlike more active forms of yoga, which focus on physical postures, breathing, and muscle control, yoga nidra guides people into a state of conscious relaxation while they are lying down. While it has reported to improve sleep and cognitive ability, those reports were based more on subjective measures than on objective data. The new study used objective polysomnographic measures of sleep and a battery of cognitive tests. Measurements were taken before and after two weeks of yoga nidra practice, which was carried out during the daytime using a 20 minute audio recording.
Among other things, polysomnography measures brain activity to determine how long each sleep stage lasts and how frequently each stage occurs. After two weeks of yoga nidra, the researchers observed that participants exhibited a significantly increased sleep efficiency and percentage of delta-waves in deep sleep. They also saw faster responses in all cognitive tests with no loss in accuracy and faster and more accurate responses in tasks including tests of working memory, abstraction, fear and anger recognition, and spatial learning and memory tasks. The findings support previous studies which link delta-wave sleep to improved sleep quality as well as better attention and memory.
The authors believe their study provides objective evidence that yoga nidra is an effective means of improving sleep quality and cognitive performance. Yoga nidra is a low-cost and highly accessible activity from which many people might therefore benefit.
The authors add: ""Yoga nidra practice improves sleep and makes brain processing faster. Accuracy also increased, especially with learning and memory related tasks.""

","score: 14.957880737880739, grade_level: '15'","score: 16.09204633204633, grade_levels: ['college_graduate'], ages: [24, 100]",10.1371/journal.pone.0294678,"Complementary and Alternative medicine is known to have health benefits. Yoga nidra practice is an easy-to-do practice and has shown beneficial effects on stress reduction and is found to improve sleep in insomnia patients. Effect of yoga nidra practice on subjective sleep is known but its effect on sleep and cognition objectively is not documented. The aim of the study was to study the effect of yoga nidra practice on cognition and sleep using objective parameters. 41 participants were enrolled, and baseline sleep diary (SD) collected. Participants volunteered for overnight polysomnography (PSG) and cognition testing battery (CTB) comprising of Motor praxis test, emotion recognition task (ERT), digital symbol substitution task, visual object learning task (VOLT), abstract matching (AIM), line orientation task, matrix reasoning task, fractal-2-back test (NBACK), psychomotor vigilance task and balloon analog risk task. Baseline CTB and after one and two weeks of practice was compared. Power spectra density for EEG at central, frontal, and occipital locations during CTB was compared. Repeat SD and PSG after four weeks of practice were done. After yoga nidra practice, improved reaction times for all cognition tasks were seen. Post intervention compared to baseline (95%CI; p-value, effect size) showed a significant improvement in sleep efficiency of +3.62% (0.3, 5.15; p = 0.03, r = 0.42), -20min (-35.78, -5.02; p = 0.003, d = 0.84) for wake after sleep onset and +4.19 μV2 (0.5, 9.5; p = 0.04, r = 0.43) in delta during deep sleep. Accuracy increased in VOLT (95% CI: 0.08, 0.17; p = 0.002, d = 0.79), AIM (95% CI: 0.03, 0.12; p = 0.02, d = 0.61) and NBACK (95% CI: 0.02, 0.13; p = 0.04, d = 0.56); ERT accuracy increased for happy, fear and anger (95% CI: 0.07, 0.24; p = 0.004, d = 0.75) but reduced for neutral stimuli (95% CI: -0.31, -0.12; p = 0.04, r = 0.33) after yoga nidra practice. Yoga Nidra practice improved cognitive processing and night-time sleep."
"
New research from Binghamton University, State University of New York shows how some workplace gossip could reduce the likelihood of employee turnover and, as a result, potentially boost an organization's effectiveness.

""Organizations should be aware of the impact of positive gossip because turnover can be a very important factor in dictating an organization's success,"" said Jinhee Moon, a doctoral student at the Binghamton University School of Management who conducted the study with a team of other researchers. ""To make employees participate in positive gossip, the organization should do the right things by treating their employees well, and being aware their behaviors can show they care about their employees.""
While studies linked to workplace gossip aren't new, Moon's work builds upon previous research by exploring how employees who gossip might experience social gains. Moon previously worked on a study that dealt with why people participate in gossipy behavior at their workplace, and the recent publication is connected to her own leadership research focus at SOM, which centers on interpersonal relationships and social networks.
For the recent study, Moon and fellow researchers surveyed 338 health workers in South Korea on positive and negative forms of workplace gossip related to their organizations and management. Some of the topics included: ""At work, I sometimes complain about my organization when management is absent."" ""If I feel treated badly by management, I talk about this to my colleagues."" ""I sometimes praise my organization's capability when the management is absent.""Moon said the research showed gossip is viewed as more valuable when people positively talk about their management or organization. Health workers who participated in the survey expressed more interest in information they could use to enhance or maintain their organizational status.
The study also indicated no relationship between negative gossip and coercive power in the workplace, which Moon said proved contrary to what researchers had expected.
""We expected that if you participate in negative gossip, maybe you're trying to appear powerful or controlling or want to 'beat someone up,' but we couldn't find any supportive results,"" Moon said. ""If anything, we found that people didn't value that type of gossip as information and just saw it as someone who wants to complain. So, if you're thinking about negative workplace gossip, you might want to save your time because there's no positive impact for you.""
But one of the most helpful aspects of the research, as Moon saw it, was how it highlighted that participating in positive gossip among one's coworkers could reduce the chances of voluntary employee turnover.
""It can be very hard just to quit your job, and if you're experiencing difficulty where you work, maybe you want to participate in positive gossip with your colleagues and talk about some of the more bearable aspects of the organization,"" Moon said. ""Eventually, that can help you gain some personal power. It's a very convenient way to reduce negative feelings toward your own workplace, which can help you more in the long run.""

","score: 15.007120622568095, grade_level: '15'","score: 16.113650419823877, grade_levels: ['college_graduate'], ages: [24, 100]",10.1177/10596011231203758,"Although workplace gossip is ubiquitous, more scholarship is needed to determine how employees may use gossip to attain valuable social resources at work—namely, their experience of power. Drawing from the gossip literature and research on power in the workplace, we identify proximal (i.e., increased power accrual) and distal (i.e., diminished voluntary turnover) positive outcomes for employees enacting negative and positive gossip about the organization at work. Using a sample of 338 nurses, we found that positive workplace gossip about the organization increases expert power. Our analysis further revealed that positive workplace gossip about the organization had a negative indirect effect on the voluntary turnover of gossip actors via their expert power. Our findings contribute to the organizational literature on the benefits of gossip to actors and serve to further enrich the emerging literature which has considered the relationship between power and turnover. An important implication of our research is that organizations need to recognize the dynamics of organization-directed gossip and its potential to serve as a source of social power for employees and a retention driver for those who accrue power in expertise."
"
A Cambridge-led study has shown why many women experience nausea and vomiting during pregnancy -- and why some women, including the Duchess of Cambridge, become so sick they need to be admitted to hospital.

The culprit is a hormone produced by the fetus -- a protein known as GDF15. But how sick the mother feels depends on a combination of how much of the hormone is produced by the fetus and how much exposure the mother had to this hormone before becoming pregnant.
The discovery, published today in Nature, points to a potential way to prevent pregnancy sickness by exposing mothers to GDF15 ahead of pregnancy to build up their resilience.
As many as seven in ten pregnancies are affected by nausea and vomiting. In some women -- thought to be between one and three in 100 pregnancies -- it can be severe, even threatening the life of the fetus and the mother and requiring intravenous fluid replacement to prevent dangerous levels of dehydration. So-called hyperemesis gravidarum is the commonest cause of admission to hospital of women in the first three months of pregnancy.
Although some therapies exist to treat pregnancy sickness and are at least partially effective, widespread ignorance of the disorder compounded by fear of using medication in pregnancy mean that many women with this condition are inadequately treated.
Until recently, the cause of pregnancy sickness was entirely unknown. Recently, some evidence, from biochemical and genetic studies has suggested that it might relate to the production by the placenta of the hormone GDF15, which acts on the mother's brain to cause her to feel nauseous and vomit.
Now, an international study, involving scientists at the University of Cambridge and researchers in Scotland, the USA and Sri Lanka, has made a major advance in understanding the role of GDF15 in pregnancy sickness, including hyperemesis gravidarum.

The team studied data from women recruited to a number of studies, including at the Rosie Maternity Hospital, part of Cambridge University Hospitals NHS Foundation Trust and Peterborough City Hospital, North West Anglia NHS Foundation Trust. They used a combination of approaches including human genetics, new ways of measuring hormones in pregnant women's blood, and studies in cells and mice.
The researchers showed that the degree of nausea and vomiting that a woman experiences in pregnancy is directly related to both the amount of GDF15 made by the fetal part of placenta and sent into her bloodstream, and how sensitive she is to the nauseating effect of this hormone.
GDF15 is made at low levels in all tissues outside of pregnancy. How sensitive the mother is to the hormone during pregnancy is influenced by how much of it she was exposed to prior to pregnancy -- women with normally low levels of GDF15 in blood have a higher risk of developing severe nausea and vomiting in pregnancy.
The team found that a rare genetic variant that puts women at a much greater risk of hyperemesis gravidarum was associated with lower levels of the hormone in the blood and tissues outside of pregnancy. Similarly, women with the inherited blood disorder beta thalassemia, which causes them to have naturally very high levels of GDF15 prior to pregnancy, experience little or no nausea or vomiting.
Professor Sir Stephen O'Rahilly, Co-Director of the Wellcome-Medical Research Council Institute of Metabolic Science at the University of Cambridge, who led the collaboration, said: ""Most women who become pregnant will experience nausea and sickness at some point, and while this is not pleasant, for some women it can be much worse -- they'll become so sick they require treatment and even hospitalisation.
""We now know why: the baby growing in the womb is producing a hormone at levels the mother is not used to. The more sensitive she is to this hormone, the sicker she will become. Knowing this gives us a clue as to how we might prevent this from happening. It also makes us more confident that preventing GDF15 from accessing its highly specific receptor in the mother's brain will ultimately form the basis for an effective and safe way of treating this disorder.""
Mice exposed to acute, high levels of GDF15 showed signs of loss of appetite, suggesting that they were experiencing nausea, but mice treated with a long-acting form of GDF15 did not show similar behaviour when exposed to acute levels of the hormone. The researchers believe that building up woman's tolerance to the hormone prior to pregnancy could hold the key to preventing sickness.

Co-author Dr Marlena Fejzo from the Department of Population and Public Health Sciences at the University of Southern California whose team had previously identified the genetic association between GDF15 and hyperemesis gravidarum, has first-hand experience with the condition. ""When I was pregnant, I became so ill that I could barely move without being sick. When I tried to find out why, I realized how little was known about my condition, despite pregnancy nausea being very common.
""Hopefully, now that we understand the cause of hyperemesis gravidarum, we're a step closer to developing effective treatments to stop other mothers going through what I and many other women have experienced.""
The work involved collaboration between scientists at the University of Cambridge, University of Southern California, University of Edinburgh, University of Glasgow and Kelaniya University, Colombo, Sri Lanka. The principal UK funders of the study were the Medical Research Council and Wellcome, with support from the National Institute for Health and Care Research Cambridge Biomedical Research Centre.

","score: 15.910759095516024, grade_level: '16'","score: 17.114258204312726, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41586-023-06921-9,"GDF15, a hormone acting on the brainstem, has been implicated in the nausea and vomiting of pregnancy, including its most severe form, hyperemesis gravidarum (HG), but a full mechanistic understanding is lacking1–4. Here we report that fetal production of GDF15 and maternal sensitivity to it both contribute substantially to the risk of HG. We confirmed that higher GDF15 levels in maternal blood are associated with vomiting in pregnancy and HG. Using mass spectrometry to detect a naturally labelled GDF15 variant, we demonstrate that the vast majority of GDF15 in the maternal plasma is derived from the feto-placental unit. By studying carriers of rare and common genetic variants, we found that low levels of GDF15 in the non-pregnant state increase the risk of developing HG. Conversely, women with β-thalassaemia, a condition in which GDF15 levels are chronically high5, report very low levels of nausea and vomiting of pregnancy. In mice, the acute food intake response to a bolus of GDF15 is influenced bi-directionally by prior levels of circulating GDF15 in a manner suggesting that this system is susceptible to desensitization. Our findings support a putative causal role for fetally derived GDF15 in the nausea and vomiting of human pregnancy, with maternal sensitivity, at least partly determined by prepregnancy exposure to the hormone, being a major influence on its severity. They also suggest mechanism-based approaches to the treatment and prevention of HG."
"
How do women picture the partner of their dreams? And how does this vary between women based on their age? 

A team of researchers led by the University of Göttingen investigated the complex relationships between age and preferences for a partner in a large, international sample of single women. The study found that most preferences for a partner showed no variation between women of different ages. However, higher age was linked to a preference for confident and assertive partners, as well as acceptance of a larger age range, in particular a higher acceptance of a partner being younger than oneself. Age was also linked to the parenting intentions of the ideal partner: consistently high in importance until approximately age 28 and then decreasing thereafter. The results were published in the Journal Human Nature.
To answer the question whether love knows no age, researchers from the University of Göttingen, Indiana University, and Queen's University Belfast, collaborated with the female health app CLUE to reach over 20,000 single women aged 18 to 67 years from nearly 150 countries via an online questionnaire. In addition to heterosexual women, this study also included two groups often neglected in psychological research: bisexual and lesbian women. Respondents were asked to rate how important attributes such as attractiveness, kindness and supportiveness, financial security and successfulness, as well as education and intelligence were to them in their partner. They were also asked to specify the youngest and oldest ages they would be happy to accept in a romantic partner. Using rigorous methods, the role of age in partner preferences was thoroughly investigated in these three groups.
Most partner preferences -- including the preference for a kind and supportive partner -- were consistently important, regardless of age. The study, however, revealed links between age and some specific preferences. ""What was particularly interesting for us is that for heterosexual women up to the age of 28, the importance of the ideal partner wanting to be or become a father remained equally high, but decreased thereafter,"" explains Laura Botzet from Göttingen University's Department of Biological Personality Psychology. Both evolutionary theories and psychological research on the ""biological clock"" would have suggested a later decline, namely between the ages of 40 and 50, when women approach the end of their reproductive phase. This unexpected earlier decrease could be linked to changing life plans, with younger women re-evaluating family goals, whilst older women, who already have children, prioritize different aspects of their relationship. The pattern varied by sexual orientation, potentially indicating different attitudes towards own children among the groups.
Botzet concludes: ""Love, it turns out, is not entirely ageless; it's nuanced. A woman's age is related to certain aspects of her desired partner, such as the preference for partners with stronger parenting intentions or the ideal age of a partner. These insights are exciting because they challenge conventional notions of how age is linked to the way women picture the partner of their dreams.""

","score: 14.65378648233487, grade_level: '15'","score: 15.496358486943166, grade_levels: ['college_graduate'], ages: [24, 100]",10.1007/s12110-023-09460-4,"Women’s capacity to reproduce varies over the life span, and developmental goals such as family formation are age-graded and shaped by social norms about the appropriate age for completing specific developmental tasks. Thus, a woman’s age may be linked to her ideas about what an ideal partner should be like. With the goals of replicating and extending prior research, in this study we examined the role of age in women’s partner preferences across the globe. We investigated associations of age with ideal long-term partner preferences in a cross-cultural sample of 17,254 single (i.e., unpartnered) heterosexual women, ages 18 to 67, from 147 countries. Data were collected via an online questionnaire, the Ideal Partner Survey. Confirming our preregistered hypotheses, we found no or only negligible age effects on preferences for kindness-supportiveness, attractiveness, financial security-successfulness, or education-intelligence. Age was, however, positively associated with preferences for confidence-assertiveness. Consistent with family formation goals, age was associated with an ideal partner’s parenting intentions (high until approximately age 30, then decreasing afterward). Age range deemed acceptable (and in particular, the discrepancy between one’s own age and the minimum ideal age of a partner) increased with age. This latter pattern also replicated in exploratory analyses based on subsamples of lesbian and bisexual women. In summary, age has a limited impact on partner preferences. Of the attributes investigated, only preference for confidence-assertiveness was linked with age. However, age range deemed acceptable and an ideal partner’s parenting intention, a dimension mostly neglected in earlier research, substantially vary with age."
"
The current class of anti-obesity drugs is proving remarkably effective at removing excess pounds. However, a phase 3 randomized clinical trial led by researchers at Weill Cornell Medicine and NewYork-Presbyterian found that people who stopped taking the medication regained much of that weight within a year. At the same time, the study shows that remaining on the drug not only promotes additional weight loss but preserves improvements in metabolic and cardiovascular health.

The results from the SURMOUNT-4 study, which appeared Dec. 11 in JAMA and was sponsored by Eli Lilly and Company, demonstrated that the drug can substantially help people struggling with health issues related to their weight, but it is not a quick-fix to weight loss.
""Obesity is a leading driver of many diseases that we spend our time treating in medicine; illnesses like hypertension, heart disease, diabetes and fatty liver disease are either caused by or worsened by obesity,"" said lead study author Dr. Louis Aronne, the Sanford I. Weill Professor of Metabolic Research and director of the Comprehensive Weight Control Center, which is part of the Division of Endocrinology, Diabetes, and Metabolism at Weill Cornell Medicine. ""The fact that we now have drugs that are proving to be effective is exciting and rewarding.""
Diabetes Drug Promotes Weight Loss
Tirzepatide is part of a new class of drugs called called GLP-1 receptor agonists that were developed to treat type 2 diabetes. Besides controlling blood sugar, the drugs also resulted in weight loss, so pharma companies created specific formulations to help patients shed pounds.
In 2022, a phase 3 randomized, controlled clinical trial demonstrated that tirzepatide led to a 20 percent reduction in body weight over 72 weeks. The findings prompted the U.S. Food and Drug Administration to approve the drug last month, with the trade name Zepbound, for weight loss in individuals with a body mass index (BMI) of 30 or higher -- or for those with a BMI of 27 or greater who also had health conditions such as high cholesterol or hypertension.
Although the initial effects were dramatic, the researchers were uncertain whether the weight loss would persist beyond the period of active treatment. To find out, they launched the SURMOUNT-4 trial, which was conducted at 70 sites in Argentina, Brazil, Taiwan and the United States between March 2021 and May 2023. The participants took a maximum tolerated dose of tirzepatide for 36 weeks, which yielded the expected weight reduction of 20.9 percent with improvements in blood pressure, blood sugar metrics and lipid levels.

Then 670 eligible participants were randomly assigned to either continue with the tirzepatide for an additional year (52 weeks) or to switch to a placebo. Those who continued on tirzepatide lost an additional 5.5 percent versus the placebo group which regained 14 percent of their weight.
Though the placebo group was still almost 10 percent lighter than their initial weight, the improvements in cardiometabolic risk factors had been reversed. Relative to placebo, tirzepatide was associated with significant improvements in BMI, lipid levels, diabetes indicators and blood pressure.
""Those who went on the placebo regained about half the weight they had lost,"" said Dr. Aronne, who is also an internist specializing in diabetes and obesity at NewYork-Presbyterian/Weill Cornell Medical Center. ""Whereas those who continued on the drug lost another 5 percent, so their overall weight loss was about 25 percent.""
The findings indicate that people may need to remain on tirzepatide to keep off the pounds. ""If you stop the medication, you regain the weight. There's no question that will happen,"" said Dr. Aronne. But that shouldn't be surprising. ""Obesity is a chronic condition, like diabetes or high blood pressure. So, it must be treated chronically.""
The researchers noted that they didn't evaluate the effects of intensive behavioral therapy on the maintenance of body weight reduction, which could make a difference in preventing weight regain after coming off the drug.
Tirzepatide Mimics Natural Hormones that Foster Feeling Full
Tirzepatide works by mimicking the GLP-1 and GIP hormones that are naturally secreted by the intestine after a meal, which prompts insulin secretion. It also reduces appetite by slowing down the time it takes the stomach to empty and interacting with areas in the brain harboring GLP-1 receptors to signal satiety.

""Instead of counting calories, the medicine helps a person eat less because it signals to the brain that you're full,"" said Dr. Aronne. ""The dual mechanism of action helps overcome the plateau phenomenon that is seen at some point and produces additive weight loss.""
Since the drug mimics hormones that are produced in the gastrointestinal system, side effects tended to be nausea, vomiting, diarrhea or constipation and resolved with time. The study had few people dropout because of side effects.
""People feel much better when they lose this kind of weight, so they are extremely enthusiastic about these treatments. But they also should realize this may require them to stay on the drug long term,"" said Dr. Aronne. Further studies will need to assess the long-term risks and benefits associated with these drugs, especially considering the potential for their lifelong use.

","score: 13.307443988846977, grade_level: '13'","score: 14.768345087748074, grade_levels: ['college_graduate'], ages: [24, 100]",10.1001/jama.2023.24945,"The effect of continued treatment with tirzepatide on maintaining initial weight reduction is unknown. To assess the effect of tirzepatide, with diet and physical activity, on the maintenance of weight reduction. This phase 3, randomized withdrawal clinical trial conducted at 70 sites in 4 countries with a 36-week, open-label tirzepatide lead-in period followed by a 52-week, double-blind, placebo-controlled period included adults with a body mass index greater than or equal to 30 or greater than or equal to 27 and a weight-related complication, excluding diabetes. Participants (n = 783) enrolled in an open-label lead-in period received once-weekly subcutaneous maximum tolerated dose (10 or 15 mg) of tirzepatide for 36 weeks. At week 36, a total of 670 participants were randomized (1:1) to continue receiving tirzepatide (n = 335) or switch to placebo (n = 335) for 52 weeks. The primary end point was the mean percent change in weight from week 36 (randomization) to week 88. Key secondary end points included the proportion of participants at week 88 who maintained at least 80% of the weight loss during the lead-in period. Participants (n = 670; mean age, 48 years; 473 [71%] women; mean weight, 107.3 kg) who completed the 36-week lead-in period experienced a mean weight reduction of 20.9%. The mean percent weight change from week 36 to week 88 was −5.5% with tirzepatide vs 14.0% with placebo (difference, −19.4% [95% CI, −21.2% to −17.7%]; P &amp;lt; .001). Overall, 300 participants (89.5%) receiving tirzepatide at 88 weeks maintained at least 80% of the weight loss during the lead-in period compared with 16.6% receiving placebo (P &amp;lt; .001). The overall mean weight reduction from week 0 to 88 was 25.3% for tirzepatide and 9.9% for placebo. The most common adverse events were mostly mild to moderate gastrointestinal events, which occurred more commonly with tirzepatide vs placebo. In participants with obesity or overweight, withdrawing tirzepatide led to substantial regain of lost weight, whereas continued treatment maintained and augmented initial weight reduction. ClinicalTrials.gov Identifier: NCT04660643"
"
On the cusp of summer holidays, Aussie kids are looking forward to some well-deserved time off. But too much downtime could create health problems, as new research shows that holidays are the prime time for excessive weight gain in kids.

Conducted by the University of South Australia's Alliance for Research in Exercise, Nutrition and Activity team, the 'Life on Holidays' study assessed changes to children's fitness and fatness during the holidays. It found that children's body fat increased at a faster pace during school holidays than in-school periods.
Funded by the NHMRC, the study found that young children (in Grades 4 and 5) expended less energy during holidays than during the school year.
Specifically, children: slept 12 minutes less per day spent 12 minutes less per being physically active spent an additional 70 minutes per day on screen time.Children's body fat increased at a greater rate during the holidays, and aerobic fitness declined faster than during the in-school periods.
It is the first study of its kind outside of the US.
In Australia, one in four children and teenagers are overweight or obese. Globally, more than 124 million children and adolescents (6% of girls and 8% of boys) are obese.

Lead researcher, UniSA's Professor Tim Olds says promoting physical activities for children in the school holidays could help address unhealthy weight gain and declining fitness.
""Like all of us, kids (and their parents) deserve some holiday downtime, but the way they spend their time on holidays is very different from the school term -- and in ways which is not always good for children's health,"" Prof Olds says.
""On school holidays, kids are significantly less active than when they're at school, and this translates into higher body fat percentages and lower levels of fitness.
""During the holidays, kids spend about 12 minutes less each day in moderate to vigorous physical activity -- almost an hour and a half a week -- and get more than an hour's extra screen time each day. They also spend an extra 20 minutes each day in transport, and a quarter of an hour more per day just chilling.
""It's not surprising to find that kids get fatter at a faster rate on school holidays compared to school term, and lose a lot of fitness. If kids spent the whole year on holidays, their percentage of body fat would increase by about 4% more each year than if they had no holidays, and their fitness would decline by about 10% each year.
""Kids who are not getting enough exercise and movement have a greater risk of developing health issues, such as cardiovascular disease and Type 2 diabetes later in life, so it's important that we encourage kids to stay active and embrace a balance of downtime and exercise.""
The two-year study focused on children aged 9-10 years, with data collected at the beginning and end of Terms 1 and 4 in both Grades 4 and 5. More than 150 participants from the 'Life on Holidays' study took part in this study.

Co-researcher, UniSA's Dr Dot Dumuid says that one solution could be for Australia to adopt the American institution of summer camps and holiday programs to improve kids' use of time during holidays.
""A defining factor of school holidays is that they're unstructured -- they can get food from the fridge when they want it, and generally have access to computers and devices -- and there's no doubt that screen time plays a key role in increased sedentary time during school holidays,"" Dr Dumuid says.
""When you compare this to the structure of a school day, where kids have a prepared lunch, and scheduled PE lessons and playtimes, it's vastly different.
""In contrast, summer camps and holiday programs get a big tick of approval as they provide kids with physical activities in a semi-structured way. Already popular in America, summer camps may be worth investigating as a viable option over Australian holidays.
""We all want our kids to be healthy. And while devices and TV may provide a bit of babysitting, is it really worth your child's health?""

","score: 12.502386502386504, grade_level: '13'","score: 13.965278055278056, grade_levels: ['college_graduate'], ages: [24, 100]",10.1186/s12889-023-17009-4,"Emerging evidence suggests that children’s fatness increases and fitness declines at a greater rate during the summer holiday period, compared with the school year. The aim of this study was to compare rates of change in fitness and fatness over the in-term and summer holiday periods among Australian schoolchildren. A secondary aim was to explore whether rates of change differed according to the child’s sex, socio-economic status (SES), pubertal status and weight status. Children (n = 381) initially in Grade 4 (age 9) were recruited for this 2-year longitudinal study. Fatness (% body fat, BMI z-score, waist-to-height ratio) and fitness (20-m shuttle run and standing broad jump) were measured at the start and end of two consecutive years. Rates of change were calculated for the two in-school periods (Grades 4 and 5) and for the summer holiday period. Rates of change in fatness and fitness between in-school and holiday periods were compared, and differences in rates of change according to sex, socio-economic status, and weight status were explored. During the holidays, percentage body fat increased at a greater rate (annualised rate of change [RoC]: +3.9 vs. Grade 4 and + 4.7 vs. Grade 5), and aerobic fitness declined at a greater rate (RoC − 4.7 vs. Grade 4 and − 4.4 vs. Grade 5), than during the in-school periods. There were no differences in rates of change for BMI z-score, waist-to-height ratio or standing broad jump. Body fatness increased faster in the holidays (relative to the in-school period) in children who are overweight and from low-SES families. Aerobic fitness declined more rapidly in the holidays in children who are overweight. This study highlights that during the summer holiday period, children experience greater increases in fatness and declines in fitness, with children who live with low-SES families and are overweight being more affected. The findings suggest the need for targeted interventions during this period to address these negative health trends. Australia New Zealand Clinical Trials Registry, identifier ACTRN12618002008202. Retrospectively registered on 14 December 2018."
"
Birds are affected by the mass use of fireworks on New Year's Eve up to a distance of 10 km away. With data from weather radars and bird counts an international team of researchers revealed how many birds take off immediately after the start of the fireworks, at what distance from fireworks this occurs and which species groups mainly react. 'We already knew that many water birds react strongly, but now we also see the effect on other birds throughout the Netherlands', says ecologist Bart Hoekstra of the University of Amsterdam. In the scientific journal Frontiers in Ecology and the Environment, the researchers therefore argue for large fireworks-free zones.

On New Year's Eve, an average of 1,000 times as many birds are in the air close to where fireworks are set off as on other nights, with peaks of 10,000 to 100,000 times the normal number of birds. The effects are strongest within the first 5 km of fireworks, but up to 10 km there are still an average of at least 10 times as many birds flying as normal.
'Birds take off as a result of an acute flight response due to sudden noise and light. In a country like the Netherlands, with many wintering birds, we are talking about millions of birds being affected by the lighting of fireworks,' says Hoekstra.
Weather radar and bird counts
Last year, other researchers at IBED discovered that geese are so affected by fireworks that they spend an average of 10% longer looking for food than normal during at least the next 11 days. They apparently need that time to replenish the lost energy or to compensate for the unknown foraging area in which they have ended up, after fleeing from the fireworks.
Hoekstra's study looked at which species take off after fireworks and when this occurs. He used information from Royal Netherlands Meteorological Institute weather radars during both a clear New Year's Eve and on other normal nights. He combined this with distribution data from Sovon -- the Dutch Centre for Field Ornithology -- based on bird counts by hundreds of volunteers. 'We already knew that many water birds react strongly, but it was still unclear how birds outside these waterbodies react to fireworks. Through the counts we know exactly where which birds are and using the radar images we can see where they actually take off because of fireworks.' Using the data, Hoekstra was able to calculate how many birds take off immediately after the start of the fireworks, at what distance from fireworks this happens, and which species groups mainly react.
Panic in the air
The analysis makes it clear that in the study areas around the radars in Den Helder and Herwijnen alone, almost 400,000 birds take off immediately at the start of the fireworks during New Year's Eve. Moreover, it appears that larger birds in open areas in particular fly around for hours after and at remarkable altitudes. Hoekstra: 'Larger birds such as geese, ducks and gulls fly to a height of hundreds of metres due to the large-scale discharge of fireworks and remain in the air for up to an hour. There is a risk that they will end up in bad winter weather, or that they will not know where they are flying due to panic and accidents could occur.'

'Restrict fireworks in central areas'
Because 62% of all birds in the Netherlands live within a radius of 2.5 km of inhabited areas, the consequences of fireworks are high for all birds throughout the country. 'Flying requires a lot of energy, so ideally birds should be disturbed as little as possible during the cold winter months. Measures to ensure this are especially important in open areas such as grasslands, where many larger birds spend the winter. The effects of fireworks on birds are less pronounced near forests and semi-open habitats. In addition, smaller birds such as tits and finches live there, which are less likely to fly away from disturbance.'
The authors argue for fireworks-free zones in areas where large birds live. Hoekstra: 'These buffer zones could be smaller in areas where light and sound travel less far, such as near forests. Furthermore, fireworks should mainly be lit at central locations in built-up areas, as far away from birds as possible. It would be best for birds if we moved towards light shows without sound, such as drone shows or decorative fireworks without very loud bangs.'

","score: 11.865302132424993, grade_level: '12'","score: 13.61217768652886, grade_levels: ['college_graduate'], ages: [24, 100]",10.1002/fee.2694,"Fireworks are important elements of celebrations globally, but little is known about their effects on wildlife. The synchronized and extraordinary use of fireworks on New Year's Eve triggers strong flight responses in birds. We used weather radar and systematic bird counts to quantify how flight responses differed across habitats and corresponding bird communities, and determined the distance‐dependence of this relationship. On average, approximately 1000 times as many birds were in flight on New Year's Eve than on other nights. We found that fireworks‐related disturbance decreased with distance, most strongly in the first five kilometers, but overall flight activity remained elevated tenfold at distances up to about 10 km. Communities of large‐bodied species displayed a stronger response than communities of small‐bodied species. Given the pervasive nature of this disturbance, the establishment of large fireworks‐free zones or centralizing fireworks within urban centers could help to mitigate their effects on birds. Conservation action should prioritize avian communities with the most disturbance‐prone, large‐bodied bird species."
"
As holidays near, people are sneaking shakes of their presents to try to figure out what they're getting. But present shakers might be a little less sly than they think. New research shows it's incredibly easy for people watching others shake boxes to tell what they're up to.

""There are few things more delightful than seeing a child's eyes light up as they pick up a present and wonder what might be inside,"" said author Chaz Firestone, a Johns Hopkins University assistant professor of psychological and brain sciences who investigates how vision and thought interact. ""What our work shows is that your mind is able to track the information they are seeking. Just as they might be able to tell what's inside the box by shaking it around, you can tell what they are trying to figure out when they shake it.""
In a series of experiments, the researchers asked hundreds of people to watch others shake boxes. It took just seconds for most of them to know whether the box shaker was trying to learn either how many things were in the box or the shape of things in the box. Although the boxes weren't presents, and the contents weren't smart watches, Legos or Red Ryder BB guns, if they were, the results would have been exactly the same, the researchers say.
""The way you would shake a present to find out if it's one thing or many things, or if it's a small thing versus a big thing, can be subtly different,"" said lead author Sholei Croom, a Johns Hopkins graduate student. ""But people are amazing at picking up on such subtleties.""
The deceptively simple work by Johns Hopkins University perception researchers is the first to demonstrate that people can tell what others are trying to learn just by watching their actions. The work, newly published just in time for the holidays in the journal Proceedings of the National Academy of Sciences, reveals a key, yet neglected, aspect of human cognition.
""When we present this work we always talk about Christmas presents,"" Firestone said. ""It's the perfect real-life example of our experiment.""

","score: 10.479267759562841, grade_level: '10'","score: 12.067950819672127, grade_levels: ['college'], ages: [18, 24]",10.1073/pnas.2303162120,"Many actions have instrumental aims, in which we move our bodies to achieve a physical outcome in the environment. However, we also perform actions with epistemic aims, in which we move our bodies to acquire information and learn about the world. A large literature on action recognition investigates how observers represent and understand the former class of actions; but what about the latter class? Can one person tell, just by observing another person’s movements, what they are trying to learn? Here, five experiments exploreepistemic action understanding. We filmed volunteers playing a “physics game” consisting of two rounds: Players shook an opaque box and attempted to determine i) the number of objects hidden inside, or ii) the shape of the objects inside. Then, independent subjects watched these videos and were asked to determine which videos came from which round: Who was shaking for number and who was shaking for shape? Across several variations, observers successfully determined what an actor was trying to learn, based only on their actions (i.e., how they shook the box)—even when the box’s contents were identical across rounds. These results demonstrate that humans can infer epistemic intent from physical behaviors, adding a new dimension to research on action understanding."
"
Exclusive breastfeeding for the first six months of life is proven to protect both mother and child health. According to the World Health Organization (WHO), between 2015 and 2021, 48% of mothers exclusively breastfed, meaning that their babies were not given any other food or liquids. However, this figure is based on data collected from surveys which report what a child was given in the previous 24 hours. A research team, including members from the University of Tokyo, has found that this ""24-hour recall"" method overestimates exclusive breastfeeding by about six times compared to a ""since-birth recall"" method. The 24-hour recall data also do not reflect the positive impact of in-hospital breastfeeding support and guidance. More indicators to assess child-feeding practices and mothers' experiences are needed to increase exclusive breastfeeding and to improve breastfeeding outcomes for both.

Breastfeeding is a natural behavior but also a learned behavior that requires appropriate support. The World Health Organization and the United Nations Children's Fund (UNICEF) recommend that babies be exclusively breastfed until the age of six months. Breast milk contains antibodies and hormones, along with nutrients, which can help build babies' resistance to common childhood illnesses and can even reduce the risk of some diseases in adulthood. According to the Centers for Disease Control and Prevention in the U.S., breastfeeding also benefits the mother by reducing the risk of breast and ovarian cancer, type 2 diabetes and high blood pressure. However, many women breastfeed without sufficient help or guidance, which can result in mothers struggling with this demanding task and stopping earlier.
The WHO has set a global target to increase exclusive breastfeeding in the first six months from an estimated 38% between 2006 and 2010, to over 50% by 2025. To assess progress toward this target, the WHO and UNICEF collect data on child feeding from population-based household surveys every three to five years. These surveys ask what babies under the age of 5 months were fed, and how often, within the past 24 hours. However, this 24-hour recall method has been criticized for not giving a true picture of breastfeeding practices.
""We have found that merely asking mothers whether they are currently breastfeeding overestimates the prevalence of breastfeeding and also overlooks the importance of providing proper support in maternity facilities,"" said Assistant Professor Keiko Nanishi from the Graduate School of Medicine at the University of Tokyo. ""I have long thought that 24-hour recall used by the WHO as an indicator does not reflect the responsibilities of health staff and facilities. As a mother, a pediatrician, a lactation consultant and a researcher in maternal and child health, I think breastfeeding promotion should focus on creating a mother- and baby-friendly environment including health staff and facilities implementing evidence-based infant-feeding practices.""
In a study of over 4,000 mothers in Japan, Nanishi and team compared responses to questions about breastfeeding using the 24-hour recall method and the since-birth recall method. For the latter, additional questions were asked about when breastfeeding started and finished during the months since birth, when formula milk was introduced and stopped, and when complementary feeding began. Participants were also asked about in-hospital breastfeeding support, measured against the WHO's recommended Ten Steps to Successful Breastfeeding, along with their intentions to breastfeed, social background and factors related to their experience of childbirth.
Results of the surveys showed that when using the 24-hour recall method, exclusive breastfeeding for children under 5 months was estimated to be much higher at 29.8% compared to since-birth recall, which was 4.4%. Also, exclusive breastfeeding was clearly more common when more in-hospital breastfeeding support was provided (following the WHO's Ten Steps). However, the connection between in-hospital support and exclusive breastfeeding falsely appeared to be weaker and inconsistent when relying on data from 24-hour recall, compared to since-birth recall.
""The development, implementation and improvement of health policies require appropriate indicators to evaluate factors such as the prevalence of breastfeeding in a country or region, who needs explicit support, whether the support is effective and whether breastfeeding rates are improving,"" explained Nanishi. ""While the 24-hour recall method has been widely used (for example, in The State of the World's Children report by UNICEF), we have found that using it risks misleading policymakers.""
Based on these results, Nanishi suggests that to improve breastfeeding rates, more supportive environments and policies are needed. ""Medical professionals tend to unconsciously use the 24-hour recall method in their practice. They tend to ask their clients, 'Are you currently breastfeeding your infant?' and then try to find the cause of the failure of breastfeeding in the mother. Instead, they must ask themselves, 'When this mother gave birth, did we provide her with appropriate care for breastfeeding?'"" said Nanishi.
""I would like the general public, especially mothers, to know that successful breastfeeding is not their sole responsibility, and that proper hospital care and appropriate health policies are very important,"" Nanishi explained. ""Mothers tend to blame themselves when breastfeeding does not work. But instead of blaming themselves, they have the right to ask for more appropriate policies and support. I hope for a healthy and sustainable society, and I believe breastfeeding support is essential for that.""

","score: 14.059287684976173, grade_level: '14'","score: 16.20642463004765, grade_levels: ['college_graduate'], ages: [24, 100]",10.1136/bmjgh-2023-013737,"WHO recommends exclusive breast feeding from birth to 6 months. However, to monitor populations, it recommends using the proportion of infants under 6 months who were exclusively breastfed during the previous 24 hours. To assess the usefulness of 24-hour recall, we (1) compared the prevalence of exclusive breast feeding measured by since-birth recall to the prevalence measured by 24-hour recall and (2) quantified each indicator’s association with WHO-recommended, well-established methods for in-hospital breastfeeding support. We conducted two online surveys of mothers in Japan (total n=4247) who had a healthy singleton delivery in the previous 25 months. They reported on their breast feeding (a) from birth to 5 months; or (b) during the previous 24 hours, for those with infants under 5 months; or (c) both, for those who participated in the initial survey and also in the follow-up survey. All mothers also reported on their in-hospital support. The strength of each indicator’s association with provision of in-hospital support was quantified as the area under the curve (AUC). The prevalences of exclusive breast feeding by since-birth recall were 4.4% (first survey) and 2.5% (second survey). By 24-hour recall, the prevalence appeared to be 29.8%. More in-hospital support was moderately well associated with more exclusive breast feeding measured by since-birth recall: AUC 0.72 (95%CI 0.66 to 0.78). That association is consistent with the known benefits of in-hospital support. In contrast, when exclusive breast feeding was measured by 24-hour recall, its association with in-hospital support appeared to be extremely weak: AUC 0.59 (95% CI 0.54 to 0.65). Using 24-hour recall substantially overestimates the prevalence of exclusive breast feeding since birth, and it conceals the benefits of in-hospital breastfeeding support. To monitor population achievement of exclusive breast feeding for the first 6 months, or to evaluate breastfeeding interventions, 24-hour recall of exclusive breast feeding should not be used alone."
"
Fewer women pursue careers in physics than biology, and scientists from around the world believe these differences come down to personal preferences, according to a new Rice University study of international scientists. The study's researchers warn that merely chalking this imbalance up to individual choice may diminish the push for gender equality in the sciences.

""Scientists explain the underrepresentation of women in physics compared to biology in four national contexts"" appears in a recent edition of Gender, Work and Organization. Using survey data collected from academic biologists and physicists in the U.S. (1,777 total), Italy (1,257), France (648) and Taiwan (780), the researchers examine how scientists' social identities and the countries in which they reside shape their explanations of gender inequality in science.
Elaine Howard Ecklund, one of the study's authors and the Herbert S. Autrey Chair, professor of sociology and director of Rice's Boniuk Institute, said regardless of the scientists they surveyed, the decisions of women to not pursue careers in physics were interpreted by the respondents through a lens of individualism. The danger in this, Ecklund said, is ignoring the way preferences themselves are shaped by gendered processes. For example, previous studies have demonstrated that women are more likely to be excluded from professional networks because of their gender, penalized for being or potentially becoming mothers and not having sufficient access to professional mentoring -- all of which are factors that can affect the choices they make for pursuing or avoiding a particular field of science.
""These barriers ultimately prevent women from entering, persisting and advancing in academic science along different points in the pipeline,"" noted Di Di, one of the study's lead authors from Santa Clara University.
Ecklund further noted how gendered processes are at work long before women make decisions about their field of study, families or other aspects of life. Prior research suggests women are influenced early on by their parents' gender roles in the family and their occupations, which shape young women's decisions to go into fields like science, technology, engineering, math and other gendered occupations. These occupational selections are viewed as individual choices by scientists surveyed for this study.
""When scientists draw on individualist arguments to explain gender inequality -- thus ignoring these gendered processes -- they may blunt initiatives that can promote women's equity in STEM,"" said Esther Chan, one of the lead authors of the study from the University of Wisconsin-Milwaukee.
The study was funded by the Templeton Religion Trust.

","score: 17.73945054945055, grade_level: '18'","score: 20.080208604954365, grade_levels: ['college_graduate'], ages: [24, 100]",10.1111/gwao.13076,"Women are consistently underrepresented in physics when compared to biology. Yet how scientists themselves explain the causes of this underrepresentation is understudied outside the US context. In this research, we ask the following question: How do scientists in different national/regional contexts explain why there are fewer women in physics than biology? Using original survey data collected among academic biologists and physicists in the US (N = 1777), Italy (N = 1257), France (N = 648), and Taiwan (N = 780), we examine how scientists' social identities, social locations, and country context shape essentialist, individualist, and structural explanations of gender inequality. Findings indicate that scientists across national contexts attribute the unequal gender distribution in physics and biology to women's individual choices. Explanations for the gender distribution also vary by social identities and social locations (gender, discipline, and seniority) in country‐specific ways. Scientists and advocates ought to engage conversations that explicitly confront scientists' assumptions about individual choices in global science."
"
In a new study of more than 50,000 Korean adolescents, those who used a smartphone for more than 4 hours per day had higher rates of adverse mental health and substance use. Jin-Hwa Moon and Jong Ho Cha of Hanyang University Medical Center, Korea, and colleagues present these findings in the open-access journal PLOS ONE on December 6, 2023.

Prior research has shown that smartphone use among adolescents has increased in recent years, and that this usage may be associated with higher risk of adverse health -- such as psychiatric disorders, sleep issues, eye-related problems, and musculoskeletal disorders. However, growing evidence suggests that at least some daily internet usage may be associated with better physical and mental health for adolescents.
To deepen understanding of the relationship between adolescents' use of smartphones and health, Moon, Cha and colleagues analyzed data on more than 50,000 adolescent participants in the ongoing Korea Youth Risk Behavior Web-based Survey collected in 2017 and in 2020. The data included the approximate number of daily hours each participant spent on a smartphone as well as various health measures. The statistical analysis employed propensity score matching to help account for other factors that could be linked to health outcomes, such as age, sex, and socioeconomic status.
The researchers found that in 2020, the percentage of adolescents in the study who used a smartphone more than 2 hours per day was 85.7 percent -- up from 64.3 percent in 2017. Adolescents who used a smartphone for more than 4 hours per day had higher rates of stress, thoughts of suicide, and substance use than those with usage below 4 hours per day. However, adolescents that used a smartphone 1-2 hours per day encountered fewer problems than adolescents who did not use a smartphone at all.
The authors note that this study does not confirm a causal relationship between smartphone use and adverse health outcomes. Nonetheless, the findings could help inform usage guidelines for adolescents -- especially if daily usage continues to rise.
The authors add: ""This research shows the impact of using smart devices for more than 4 hours a day on adolescent health.""

","score: 13.891025641025646, grade_level: '14'","score: 16.28089308996089, grade_levels: ['college_graduate'], ages: [24, 100]",10.1371/journal.pone.0294553,"We aimed to investigate the association between smartphone use and adverse behavioral health outcomes using nationwide Korea Youth Risk Behavior Web-based Survey data for 2017 and 2020. The 2020 data (N = 54,809) were used to analyze the relationships between daily smartphone usage time (non-user, 0–2 h [hour], 2–4 h, 4–6 h, 6–8 h, and > 8 h), and adverse health outcomes (stress, sleep, depression, suicide, substance use, and smartphone overdependence). A 1:1 propensity score matching (PSM) was used to control for confounding variables. A total of 40,998 adolescents with < 4 h/day and > 4 h/day of usage were included. Adolescents’ mean smartphone usage time in 2020 increased compared to that in 2017 (weighted % of > 2 h/day; 64.3% vs. 85.7%). The curvilinear relationships between smartphone usage time and adverse health outcomes were prominent after > 4 h/day. Adolescents using smartphones 2–4 h/day showed no increased adverse health outcomes compared to non-users, except for smartphone overdependence. Using a smartphone > 4 h/day was significantly associated with stress perception (1.16; 1.11–1.22), suicidal ideation (1.22; 1.13–1.31), and substance use (alcohol, 1.66; 1.57–1.75) after PSM. Our study demonstrated the curvilinear relationship between smartphone usage time and adverse health outcomes in adolescents. Our findings can help establish smartphone usage guidelines for adolescents."
"
If one spouse or partner in a heterosexual couple has high blood pressure, the other partner often does too, according to new research published today in the Journal of the American Heart Association, an open access, peer-reviewed journal of the American Heart Association.

""Many people know that high blood pressure is common in middle-aged and older adults, yet we were surprised to find that among many older couples, both husband and wife had high blood pressure in the U.S., England, China and India,"" said senior author Chihua Li, Dr.P.H., a post-doctoral fellow at the University of Michigan and the study's corresponding author. ""For instance, in the U.S., among more than 35% of couples who were ages 50 or older, both had high blood pressure.""
Researchers investigated whether heterosexual partners in the U.S., England, China and India mirrored each other's high blood-pressure status. Previous studies have explored the union of high blood pressure and other diseases among couples in a single country setting or used small regional samples.
""Ours is the first study examining the union of high blood pressure within couples from both high- and middle-income countries,"" said study co-lead author Jithin Sam Varghese, Ph.D., an assistant research professor at the Emory Global Diabetes Research Center at Emory University in Atlanta. ""We wanted to find out if many married couples who often have the same interests, living environment, lifestyle habits and health outcomes may also share high blood pressure.""
The researchers analyzed blood pressure measures for 3,989 U.S. couples, 1,086 English couples, 6,514 Chinese couples and 22,389 Indian couples and found: The prevalence of both spouses or partners having high blood pressure was about 47% in England; 38% in the U.S.; 21% in China and 20% in India. Compared to wives married to husbands without high blood pressure, wives whose husbands had high blood pressure were 9% more likely to have high blood pressure in the U.S. and England, 19% more likely in India and 26% more likely in China. Within each country, similar associations were observed for husbands. The association was consistent when the analyses were stratified by area of residence within each country, household wealth, length of marriage, age groups and education levels.""High blood pressure is more common in the U.S. and England than in China and India, however, the association between couples' blood pressure status was stronger in China and India than in the U.S and England. One reason might be cultural. In China and India, there's a strong belief in sticking together as a family, so couples might influence each other's health more,"" said study co-lead author Peiyi Lu, Ph.D., a post-doctoral fellow in epidemiology at Columbia University Mailman School of Public Health. ""In collectivist societies in China and India, couples are expected to depend and support each other, emotionally and instrumentally, so health may be more closely entwined.""
These findings highlight the potential of using couple-based approaches for high blood pressure diagnosis and management, such as couple-based screening, skills training or joint participation in programs, Li noted.

Study background and details: The researchers used cross-sectional data -- capturing a single point in time -- taken from studies of aging that are representative of populations across entire countries, including the 2016-17 Health and Retirement Study in the U.S., the 2016-17 English Longitudinal Study on Ageing, the 2015-16 China Health and Retirement Longitudinal Study, and the 2017-19 Longitudinal Aging Study in India. These four studies have harmonized design and measures, and each adopted a household survey that first recruited a primary participant that met the age eligibility -- 50 and older for the studies in the U.S. and England and 45 and older for the studies in China and India -- and then invited his or her spouse or partner to participate regardless of their age. Couples were defined as heterosexual participants living in the same household who reported to be married or partnered to one another, and those who were older than legal age for marriage for their country at the time of the survey. The average age of husbands in the study was 65.7 years in the U.S.; 74.2 years in England; 61.5 in China; and 57.2 years in India. The average age of wives in the study was 62.9 years in the U.S; 72.5 years in England; 59.2 years in China and 51.1 years in India. High blood pressure was defined based on measurements at one time point. Participants were noted as having hypertension if they had one of the following: systolic blood pressure higher than 140 mm Hg or diastolic greater than 90 mm Hg, as measured by health professionals; or if they answered yes when asked if they had a history of high blood pressure.Among the study's limitations were its cross-sectional design, meaning it captured a single point in time and thus only one blood-pressure measurement, and that the surveys included only heterosexual couples.
According to the American Heart Association's 2023 statistics, in 2020, nearly 120,000 deaths were primarily attributable to high blood pressure, and from 2017 to 2020, 122.4 million (46.7%) U.S. adults had high blood pressure.
""Varghese, Lu and colleagues report an important finding among middle-aged and older adults -- if your spouse has hypertension, you are more likely to have hypertension, too.
These findings are important because hypertension is among the most dominant modifiable cardiovascular risk factors and remains highly prevalent and poorly controlled on an increasingly global level. As the authors point out, the current focus of clinical and public health strategies to control hypertension on the individual level is not adequate. The authors suggest that interventions that target spouses may, thus, be especially effective,"" said Bethany Barone Gibbs, Ph.D., FAHA, an associate professor and chair of the department of epidemiology and biostatistics at the School of Public Health at West Virginia University, and chair of the writing committee for the Association's 2021 Statement on Physical Activity as a Critical Component of First-Line Treatment for Elevated Blood Pressure or Cholesterol.
""Following this idea, making lifestyle changes, such as being more active, reducing stress or eating a healthier diet, can all reduce blood pressure; however, these changes may be difficult to achieve and, more importantly, sustain if your spouse or partner (and greater family unit) are not making changes with you,"" she said. ""These findings also hint at a broader approach -- interventions using a socioecological model considering determinants of hypertension across individual, interpersonal, environmental and policy levels are likely going to be necessary to reduce the global public health burden of hypertension.""

","score: 18.302301943198803, grade_level: '18'","score: 20.96523467862481, grade_levels: ['college_graduate'], ages: [24, 100]",10.1161/JAHA.123.030765,"Health concordance within couples presents a promising opportunity to design interventions for disease management, including hypertension. We compared the concordance of prevalent hypertension within middle‐aged and older heterosexual couples in the United States, England, China, and India. Cross‐sectional dyadic data on heterosexual couples were used from contemporaneous waves of the HRS (US Health and Retirement Study, 2016/17, n=3989 couples), ELSA (English Longitudinal Study on Aging, 2016/17, n=1086), CHARLS (China Health and Retirement Longitudinal Study, 2015/16, n=6514), and LASI (Longitudinal Aging Study in India, 2017/19, n=22 389). Concordant hypertension was defined as both husband and wife in a couple having hypertension. The prevalence of concordant hypertension within couples was 37.9% (95% CI, 35.8–40.0) in the United States, 47.1% (95% CI, 43.2–50.9) in England, 20.8% (95% CI, 19.6–21.9) in China, and 19.8% (95% CI, 19.0–20.5) in India. Compared with wives married to husbands without hypertension, wives married to husbands with hypertension were more likely to have hypertension in the United States (prevalence ratio, 1.09 [95% CI, 1.01– 1.17), England (prevalence ratio, 1.09, 95% CI, 0.98–1.21), China (prevalence ratio, 1.26 [95% CI, 1.17–1.35), and India (prevalence ratio, 1.19 [95% CI, 1.15–1.24]). Within each country, similar associations were observed for husbands. Across countries, associations in the United States and England were similar, whereas they were slightly larger in China and India. Concordance of hypertension within heterosexual couples was consistently observed across these 4 socially and economically diverse countries. Couple‐centered interventions may be an efficient strategy to prevent and manage hypertension in these countries."
"
Pregnant women are not getting the essential nutrients they and their babies need from modern diets say scientists, who have warned that the situation will likely worsen as more people turn to plant-based foods.

A study looking at the health of expecting mothers from high-income countries, including the UK, New Zealand and Singapore, found that 90 per cent were lacking key vitamins necessary for healthy pregnancies and the wellbeing of unborn infants.
Scientists from the University of Southampton, working with experts worldwide, surveyed more than 1,700 women and found most were missing essential nutrients found in abundance in meat and dairy products.
These included vitamins B12, B6 and D, folic acid and riboflavin which are essential for the development of foetuses in the womb.
Lead author and Professor of Epidemiology Keith Godfrey, from the University of Southampton, said the prevalence of vitamin deficiencies among women attempting to become pregnant in wealthy countries is a serious concern.
He added: ""The push to reduce our dependence on meat and dairy to achieve net-zero carbon emissions is likely to further deplete expecting mothers of vital nutrients, which could have lasting effects on unborn children.
""Our study shows that almost every woman trying to conceive had insufficient levels of one or more vitamin, and this figure is only going to get worse as the world moves towards plant-based diets.

""People think that nutrient deficiency only affects people in underdeveloped countries -- but it is also affecting the majority of women living in high-income nations.""
The study, which was published in PLOS Medicine, assessed 1,729 women between the ages of 18 and 38 at conception and followed many during subsequent pregnancies.
It was undertaken by researchers from Southampton and its National Institute for Health and Care Research (NIHR) Biomedical Research Centre, the University of Auckland, National University of Singapore, and Agency for Science, Research and Technology, Singapore.
Results showed that nine out of ten women had marginal or low levels of folate, riboflavin, vitamins B12 and D around the time of conception, and that many developed vitamin B6 deficiency in late pregnancy.
Co-author Professor of Paediatric Endocrinology Wayne Cutfield, from the University of Auckland, said while folic acid is recommended for women planning conception and during pregnancy, expecting mothers should be given over-the-counter multivitamins to reduce nutrient deficiencies.
He added: ""The wellbeing of a mother ahead of conceiving and during a pregnancy has a direct influence on the health of the infant, their lifelong physical development, and ability to learn.""
The PLOS Medicine trial was the first to show that supplements, available over the counter, can reduce vitamin insufficiencies during the preconception, pregnancy and lactational periods.
Associate Professor Shiao-Yng Chan at the National University of Singapore said: ""If we continue to move towards diets with less meat and dairy products, reducing intakes of micronutrients essential for a child's development, vitamin deficiencies will continue to grow unless women start taking more supplements or are supported with specific advice about nutrient-rich foods.""

","score: 18.4002416998672, grade_level: '18'","score: 20.870564409030543, grade_levels: ['college_graduate'], ages: [24, 100]",10.1371/journal.pmed.1004260,"Maternal vitamin status preconception and during pregnancy has important consequences for pregnancy outcome and offspring development. Changes in vitamin status from preconception through early and late pregnancy and postpartum have been inferred from cross-sectional data, but longitudinal data on vitamin status from preconception throughout pregnancy and postdelivery are sparse. As such, the influence of vitamin supplementation on vitamin status during pregnancy remains uncertain. This study presents one prespecified outcome from the randomized controlled NiPPeR trial, aiming to identify longitudinal patterns of maternal vitamin status from preconception, through early and late pregnancy, to 6 months postdelivery, and determine the influence of vitamin supplementation. In the NiPPeR trial, 1,729 women (from the United Kingdom, Singapore, and New Zealand) aged 18 to 38 years and planning conception were randomized to receive a standard vitamin supplement (control; n = 859) or an enhanced vitamin supplement (intervention; n = 870) starting in preconception and continued throughout pregnancy, with blinding of participants and research staff. Supplement components common to both treatment groups included folic acid, β-carotene, iron, calcium, and iodine; components additionally included in the intervention group were riboflavin, vitamins B6, B12, and D (in amounts available in over-the-counter supplements), myo-inositol, probiotics, and zinc. The primary outcome of the study was glucose tolerance at 28 weeks’ gestation, measured by oral glucose tolerance test. The secondary outcome reported in this study was the reduction in maternal micronutrient insufficiency in riboflavin, vitamin B6, vitamin B12, and vitamin D, before and during pregnancy. We measured maternal plasma concentrations of B-vitamins, vitamin D, and markers of insufficiency/deficiency (homocysteine, hydroxykynurenine-ratio, methylmalonic acid) at recruitment, 1 month after commencing intervention preconception, in early pregnancy (7 to 11 weeks’ gestation) and late pregnancy (around 28 weeks’ gestation), and postdelivery (6 months after supplement discontinuation). We derived standard deviation scores (SDS) to characterize longitudinal changes among participants in the control group and measured differences between the 2 groups. At recruitment, the proportion of patients with marginal or low plasma status was 29.2% for folate (<13.6 nmol/L), 7.5% and 82.0% for riboflavin (<5 nmol/L and ≤26.5 nmol/L, respectively), 9.1% for vitamin B12 (<221 pmol/L), and 48.7% for vitamin D (<50 nmol/L); these proportions were balanced between the groups. Over 90% of all participants had low or marginal status for one or more of these vitamins at recruitment. Among participants in the control group, plasma concentrations of riboflavin declined through early and late pregnancy, whereas concentrations of 25-hydroxyvitamin D were unchanged in early pregnancy, and concentrations of vitamin B6 and B12 declined throughout pregnancy, becoming >1 SDS lower than baseline by 28 weeks gestation. In the control group, 54.2% of participants developed low late-pregnancy vitamin B6 concentrations (pyridoxal 5-phosphate <20 nmol/L). After 1 month of supplementation, plasma concentrations of supplement components were substantially higher among participants in the intervention group than those in the control group: riboflavin by 0.77 SDS (95% CI 0.68 to 0.87, p < 0.0001), vitamin B6 by 1.07 SDS (0.99 to 1.14, p < 0.0001), vitamin B12 by 0.55 SDS (0.46 to 0.64, p < 0.0001), and vitamin D by 0.51 SDS (0.43 to 0.60, p < 0.0001), with higher levels in the intervention group maintained during pregnancy. Markers of vitamin insufficiency/deficiency were reduced in the intervention group, and the proportion of participants with vitamin D insufficiency (<50 nmol/L) during late pregnancy was lower in the intervention group (35.1% versus 8.5%; p < 0.0001). Plasma vitamin B12 remained higher in the intervention group than in the control group 6 months postdelivery (by 0.30 SDS (0.14, 0.46), p = 0.0003). The main limitation is that generalizability to the global population is limited by the high-resource settings and the lack of African and Amerindian women in particular. Over 90% of the trial participants had marginal or low concentrations of one or more of folate, riboflavin, vitamin B12, or vitamin D during preconception, and many developed markers of vitamin B6 deficiency in late pregnancy. Preconception/pregnancy supplementation in amounts available in over-the-counter supplements substantially reduces the prevalence of vitamin deficiency and depletion markers before and during pregnancy, with higher maternal plasma vitamin B12 maintained during the recommended lactational period. ClinicalTrials.gov NCT02509988; U1111-1171-8056."
"
Nearly 1,000 birds were killed Oct. 4-5 when they collided with an illuminated glass building in Chicago. Though mass fatalities of this magnitude are rare, light pollution poses a serious -- and growing -- threat to migrating birds.

In the largest study of its kind, published in Nature Communications, scientists used weather radar data to map bird stopover density in the United States and found that artificial light is a top indicator of where birds will land. City lights lure birds into what can be an ecological trap, said lead author Kyle Horton, an assistant professor in Colorado State University's Department of Fish, Wildlife and Conservation Biology.
Buildings that lead to collisions, less habitat, scarcer food, and more people and cats can make cities less-than-ideal rest stops for migrating birds. Urban parks can be decent stopover sites, but birds that rest there might need to compete over limited resources.
Migration is a risky and exhausting time in a bird's life. Birds migrate hundreds to thousands of miles -- sometimes burning half their body mass along the way. Finding a good place to rest and refuel is critical for migrating birds to survive and thrive once they reach their destination.
""These stopover locations are the fueling stations,"" Horton said. ""If you're on a cross-country trip and there's no fueling stations, then you're stranded. If they don't have a good spot to rebuild energy supplies, migration can't happen.""
The study provides the first continent-wide maps of migration stopover hotspots in the contiguous United States, and knowing these broadscale layover patterns can help in the development of conservation plans.
""Cities pose multiple risks to migrating birds,"" said co-author and Michigan State University Professor Geoff Henebry. ""They also offer resources for the tired birds to rest and refuel. Our study is notable in that it combines big data -- and a lot of processing -- from the weather surveillance radar network with big data from multiple spaceborne sensors to address key questions regarding the influence of urban areas on bird migration.""
The study pairs more than 10 million radar observations with landscape and other place-based information to try to explain why birds choose to rest where they do. Out of 49 predictors, light pollution was the No. 2 predictor of stopover density.

The top predictor was elevation, which provides context for where birds are flying but doesn't explain why they are flying there. The patterns created by migrating birds that are picked up by radar tend to follow coastlines or a particular elevation. Light pollution is the top predictor of human influence on bird migration.
Unsuspected hazard
Birds' attraction to cities creates a conservation conundrum: Should urban centers be conserved as important stopover locations or targeted for lights-out campaigns? Horton and his colleagues are working with nonprofit and government organizations to do both, but urban lighting involves lots of stakeholders, making it a complicated issue.
There can be social pressure to leave lights on, and some people find them aesthetically pleasing. But light pollution harms people too. It can disrupt humans' circadian rhythms, leading to health problems including depression, insomnia, cardiovascular disease and cancer.
""We don't often think about light as a pollutant, but it checks all the boxes of what pollution is,"" Horton said.
Tools like BirdCast -- a collaborative project among CSU, the Cornell Lab of Ornithology and University of Massachusetts -- can help. BirdCast provides migration forecasts and real-time maps from weather radar. Anyone can create alerts to be notified when birds are flocking near their city. Forecasts pinpoint which nights are most important for reducing light pollution.

Retrofitting windows with decals like gridded dots or lines can help prevent collisions by revealing the barrier to birds. Lowering the brightness and softening the color of lights can help too. Bright white or blue lights are the worst for wildlife, while warmer hues, like red, orange and yellow, are less attractive.
Communication towers used to beam continuous red or white light to warn aircraft. Birds would circle the towers, hitting the wires that secured them. In 2016, based on conservation research, the Federal Aviation Administration started requiring communication towers to use flashing red lights, dramatically reducing bird collisions in a literal blink.
Flipping a switch to help feathered friends
The Chicago convention center collisions might be an extreme example of birds dying because of light pollution, but Horton said mass fatalities involving 100 or more birds are all too common. It is estimated that nearly one billion birds collide with buildings in the United States every year.
The casualties at McCormick Place Convention Center Oct. 4-5 were mostly songbirds -- 33 species were tallied, according to the Chicago Field Museum. These birds benefit people by eating insects that plague crops and gardens, pollinating plants and distributing seeds.
Public awareness of bird migration habits would be a good place to start to help protect them from light pollution, Horton said. ""Most people might not realize that birds migrate at night.""
For a complicated problem, this one has a simple solution -- at least when it comes to birds.
""If we turned off all lights tonight, there would be no birds colliding because of lights tonight,"" Horton said. ""The impact is immediate and positive for birds.""

","score: 10.927848162475822, grade_level: '11'","score: 12.27383849129594, grade_levels: ['college'], ages: [18, 24]",10.1038/s41467-023-43046-z,"As billions of nocturnal avian migrants traverse North America, twice a year they must contend with landscape changes driven by natural and anthropogenic forces, including the rapid growth of the artificial glow of the night sky. While airspaces facilitate migrant passage, terrestrial landscapes serve as essential areas to restore energy reserves and often act as refugia—making it critical to holistically identify stopover locations and understand drivers of use. Here, we leverage over 10 million remote sensing observations to develop seasonal contiguous United States layers of bird migrant stopover density. In over 70% of our models, we identify skyglow as a highly influential and consistently positive predictor of bird migration stopover density across the United States. This finding points to the potential of an expanding threat to avian migrants: peri-urban illuminated areas may act as ecological traps at macroscales that increase the mortality of birds during migration."
"
Following an 18-month meditation programme can improve the wellbeing of older adults, finds a new randomised controlled trial by an international team co-led by UCL.

The findings, published in PLOS ONE, show that meditation can improve people's awareness, connection to others, and insight.
While the meditation training did not confer significant benefits on two commonly used measures of psychological wellbeing and quality of life, the researchers say their findings may reveal limitations in existing methods of tracking wellbeing.
Lead author Marco Schlosser (UCL Psychiatry and University of Geneva) said: ""As the global population ages, it is increasingly crucial to understand how we can support older adults in maintaining and deepening their psychological wellbeing. In our study, we tested whether long-term meditation training can enhance important dimensions of wellbeing. Our findings suggest that meditation is a promising non-pharmacological approach to support human flourishing in late life.""
The study is the longest randomised meditation training trial conducted to date, and explored the impact of an 18-month meditation programme on the psychological wellbeing of more than 130 healthy French-speaking people aged 65 to 84. The study, led by Principal Investigator Professor Gaël Chételat, took place in Caen, France. It was conducted by the European Union's Horizon 2020-funded Medit-Ageing (Silver Santé Study) research group which involves UCL, Inserm, University of Geneva, Université de Caen Normandy, Lyon Neuroscience Research Center, University of Liège, Technische Universität Dresden, and Friedrich Schiller University Jena.
The researchers compared a meditation programme, which included a nine-month mindfulness module followed by a nine-month loving kindness and compassion module, delivered by weekly group sessions (two hours long), daily home practice (at least 20 minutes), and one retreat day, with a group that did English language training (as a comparison group) and a no-intervention control group.
The team found that meditation training significantly impacted a global score that measures the wellbeing dimensions of awareness, connection, and insight. Awareness describes an undistracted and intimate attentiveness to one's thoughts, feelings, and surroundings, which can support a sense of calm and deep satisfaction. Connection captures feelings such as respect, gratitude, and kinship that can support more positive relationships with others. Insight refers to a self-knowledge and understanding of how thoughts and feelings participate in shaping our perception -- and how to transform unhelpful patterns of thought relating to ourselves and the world.

The benefits of meditation training to an established measure of psychological quality of life were not superior to English language training, while neither intervention significantly impacted another widely used measure of psychological wellbeing. The researchers suggest this may be because these two established measures do not cover the qualities and depth of human flourishing that can potentially be cultivated by longer-term meditation training, so benefits to awareness, connection and insight are missed.
The programme did not benefit everyone equally, as participants who reported lower levels of psychological wellbeing at the start of the trial showed greater improvements compared to those who already had higher levels of wellbeing.
Co-author Dr Natalie Marchant (UCL Psychiatry) said: ""We hope that further research will clarify which people are most likely to benefit from meditation training, as it may confer stronger benefits on some specific groups. Now that we have evidence that meditation training can help older adults, we hope that further refinements in partnership with colleagues from other research disciplines could make meditation programmes even more beneficial.""
Senior author Dr Antoine Lutz (Lyon Neuroscience Research Center, Inserm, France) said: ""By showing the potential of meditation programmes, our findings pave the way for more targeted and effective programmes that can help older adults flourish, as we seek to go beyond simply preventing disease or ill-health, and instead take a holistic approach to helping people across the full spectrum of human wellbeing.""

","score: 18.387936507936512, grade_level: '18'","score: 21.27914285714286, grade_levels: ['college_graduate'], ages: [24, 100]",10.1371/journal.pone.0294753,"As the world population is ageing, it is vital to understand how older adults can maintain and deepen their psychological well-being as they are confronted with the unique challenges of ageing in a complex world. Theoretical work has highlighted the promising role of intentional mental training such as meditation practice for enhancing human flourishing. However, meditation-based randomised controlled trials in older adults are lacking. We aimed to investigate the effects of meditation training on psychological well-being in older adults. This study presents a secondary analysis of the Age-Well trial (ClinicalTrials.gov: NCT02977819), which randomised 137 healthy older adults (age range: 65 to 84 years) to an 18-month meditation training, an active comparator (English language training), or a passive control. Well-being was measured at baseline, mid-intervention, and 18-month post-randomisation using the Psychological Well-being Scale (PWBS), the World Health Organisation’s Quality of Life (QoL) Assessment psychological subscale, and composite scores reflecting the meditation-based well-being dimensions of awareness, connection, insight, and a global score comprising the average of these meditation-based dimensions. The 18-month meditation training was superior to English training on changes in the global score (0.54 [95% CI: 0.26, 0.82], p = 0.0002) and the subscales of awareness, connection, insight, and superior to no-intervention only on changes in the global score (0.54 [95% CI: 0.26, 0.82], p = 0.0002) and awareness. Between-group differences in psychological QoL in favour of meditation did not remain significant after adjusting for multiple comparisons. There were no between-group differences in PWBS total score. Within the meditation group, psychological QoL, awareness, insight, and the global score increased significantly from baseline to 18-month post-randomisation. The longest randomised meditation training conducted to date enhanced a global composite score reflecting the meditation-based well-being dimensions of awareness, connection, and insight in older adults. Future research is needed to delineate the cognitive, affective, and behavioural factors that predict responsiveness to meditation and thus help refine the development of tailored meditation training."
"
What happens in the human brain when we learn from positive and negative experiences? To help answer that question and better understand decision-making and human behavior, scientists are studying dopamine.

Dopamine is a neurotransmitter produced in the brain that serves as a chemical messenger, facilitating communication between nerve cells in the brain and the body. It is involved in functions such as movement, cognition and learning. While dopamine is most known for its association with positive emotions, scientists are also exploring its role in negative experiences.
Now, a new study from researchers at Wake Forest University School of Medicine shows that dopamine release in the human brain plays a crucial role in encoding both reward and punishment prediction errors. This means that dopamine is involved in the process of learning from both positive and negative experiences, allowing the brain to adjust and adapt its behavior based on the outcomes of these experiences.
The study was published today in Science Advances.
""Previously, research has shown that dopamine plays an important role in how animals learn from 'rewarding' (and possibly 'punishing') experiences. But, little work has been done to directly assess what dopamine does on fast timescales in the human brain,"" said Kenneth T. Kishida, Ph.D., associate professor of physiology and pharmacology and neurosurgery at Wake Forest University School of Medicine. ""This is the first study in humans to examine how dopamine encodes rewards and punishments and whether dopamine reflects an 'optimal' teaching signal that is used in today's most advanced artificial intelligence research.""
For the study, researchers on Kishida's team utilized fast-scan cyclic voltammetry, an electrochemical technique, paired with machine learning, to detect and measure dopamine levels in real-time (i.e., 10 measurements per second). However, this method is challenging and can only be performed during invasive procedures such as deep-brain stimulation (DBS) brain surgery. DBS is commonly employed to treat conditions such as Parkinson's disease, essential tremor, obsessive-compulsive disorder and epilepsy.
Kishida's team collaborated with Atrium Health Wake Forest Baptist neurosurgeons Stephen B. Tatter, M.D., and Adrian W. Laxton, M.D., who are also both faculty members in the Department of Neurosurgery at Wake Forest University School of Medicine, to insert a carbon fiber microelectrode deep into the brain of three participants at Atrium Health Wake Forest Baptist Medical Center who were scheduled to receive DBS to treat essential tremor.

While the participants were awake in the operating room, they played a simple computer game. As they played the game, dopamine measurements were taken in the striatum, a part of the brain that is important for cognition, decision-making, and coordinated movements.
During the game, participants' choices were either rewarded or punished with real monetary gains or losses. The game was divided into three stages in which participants learned from positive or negative feedback to make choices that maximized rewards and minimized penalties. Dopamine levels were measured continuously, once every 100 milliseconds, throughout each of the three stages of the game.
""We found that dopamine not only plays a role in signaling both positive and negative experiences in the brain, but it seems to do so in a way that is optimal when trying to learn from those outcomes. What was also interesting, is that it seems like there may be independent pathways in the brain that separately engage the dopamine system for rewarding versus punishing experiences. Our results reveal a surprising result that these two pathways may encode rewarding and punishing experiences on slightly shifted timescales separated by only 200 to 400 milliseconds in time,"" Kishida said.
Kishida believes that this level of understanding may lead to a better understanding of how the dopamine system is affected in humans with psychiatric and neurological disorders. Kishida said additional research is needed to understand how dopamine signaling is altered in psychiatric and neurological disorders.
""Traditionally, dopamine is often referred to as 'the pleasure neurotransmitter,""' Kishida said. ""However, our work provides evidence that this is not the way to think about dopamine. Instead, dopamine is a crucial part of a sophisticated system that teaches our brain and guides our behavior. That dopamine is also involved in teaching our brain about punishing experiences is an important discovery and may provide new directions in research to help us better understand the mechanisms underlying depression, addiction, and related psychiatric and neurological disorders.""
This study was supported by grants from the National Institutes of Health: R01MH121099, R01DA048096, R01MH124115, P50DA006634, 5KL2TR001420, F31DA053174, T32DA041349 and F30DA053176.

","score: 15.32306666666667, grade_level: '15'","score: 16.836840000000002, grade_levels: ['college_graduate'], ages: [24, 100]",10.1126/sciadv.adi4927,"In the mammalian brain, midbrain dopamine neuron activity is hypothesized to encode reward prediction errors that promote learning and guide behavior by causing rapid changes in dopamine levels in target brain regions. This hypothesis (and alternatives regarding dopamine’s role in punishment-learning) has limited direct evidence in humans. We report intracranial, subsecond measurements of dopamine release in human striatum measured, while volunteers (i.e., patients undergoing deep brain stimulation surgery) performed a probabilistic reward and punishment learning choice task designed to test whether dopamine release encodes only reward prediction errors or whether dopamine release may also encode adaptive punishment learning signals. Results demonstrate that extracellular dopamine levels can encode both reward and punishment prediction errors within distinct time intervals via independent valence-specific pathways in the human brain."
"
Parents should speak to their babies using sing-song speech, like nursery rhymes, as soon as possible, say researchers. That's because babies learn languages from rhythmic information, not phonetic information, in their first months.

Phonetic information -- the smallest sound elements of speech, typically represented by the alphabet -- is considered by many linguists to be the foundation of language. Infants are thought to learn these small sound elements and add them together to make words. But a new study suggests that phonetic information is learnt too late and slowly for this to be the case.
Instead, rhythmic speech helps babies learn language by emphasising the boundaries of individual words and is effective even in the first months of life.
Researchers from the University of Cambridge and Trinity College Dublin investigated babies' ability to process phonetic information during their first year.
Their study, published today in the journal Nature Communications, found that phonetic information wasn't successfully encoded until seven months old, and was still sparse at 11 months old when babies began to say their first words.
""Our research shows that the individual sounds of speech are not processed reliably until around seven months, even though most infants can recognise familiar words like 'bottle' by this point,"" said Cambridge neuroscientist, Professor Usha Goswami. ""From then individual speech sounds are still added in very slowly -- too slowly to form the basis of language.""
The researchers recorded patterns of electrical brain activity in 50 infants at four, seven and eleven months old as they watched a video of a primary school teacher singing 18 nursery rhymes to an infant. Low frequency bands of brainwaves were fed through a special algorithm, which produced a 'read out' of the phonological information that was being encoded.

The researchers found that phonetic encoding in babies emerged gradually over the first year of life, beginning with labial sounds (e.g. d for ""daddy"") and nasal sounds (e.g. m for ""mummy""), with the 'read out' progressively looking more like that of adults
First author, Professor Giovanni Di Liberto, a cognitive and computer scientist at Trinity College Dublin and a researcher at the ADAPT Centre, said: ""This is the first evidence we have of how brain activity relates to phonetic information changes over time in response to continuous speech.""
Previously, studies have relied on comparing the responses to nonsense syllables, like ""bif"" and ""bof"" instead.
The current study forms part of the BabyRhythm project led by Goswami, which is investigating how language is learnt and how this is related to dyslexia and developmental language disorder.
Goswami believes that it is rhythmic information -- the stress or emphasis on different syllables of words and the rise and fall of tone -- that is the key to language learning. A sister study, also part of the BabyRhythm project, has shown that rhythmic speech information was processed by babies at two months old -- and individual differences predicted later language outcomes. The experiment was also conducted with adults who showed an identical 'read out' of rhythm and syllables to babies.
""We believe that speech rhythm information is the hidden glue underpinning the development of a well-functioning language system,"" said Goswami. ""Infants can use rhythmic information like a scaffold or skeleton to add phonetic information on to. For example, they might learn that the rhythm pattern of English words is typically strong-weak, as in 'daddy' or 'mummy', with the stress on the first syllable. They can use this rhythm pattern to guess where one word ends and another begins when listening to natural speech.""
""Parents should talk and sing to their babies as much as possible or use infant directed speech like nursery rhymes because it will make a difference to language outcome,"" she added.

Goswami explained that rhythm is a universal aspect of every language all over the world. ""In all language that babies are exposed to there is a strong beat structure with a strong syllable twice a second. We're biologically programmed to emphasise this when speaking to babies.""
Goswami says that there is a long history in trying to explain dyslexia and developmental language disorder in terms of phonetic problems but that the evidence doesn't add up. She believes that individual differences in children's language originate with rhythm.
The research was funded by the European Research Council under the European Union's Horizon 2020 research and innovation programme and by Science Foundation Ireland.

","score: 13.158179508161421, grade_level: '13'","score: 14.640832077178175, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41467-023-43490-x,"Even prior to producing their first words, infants are developing a sophisticated speech processing system, with robust word recognition present by 4–6 months of age. These emergent linguistic skills, observed with behavioural investigations, are likely to rely on increasingly sophisticated neural underpinnings. The infant brain is known to robustly track the speech envelope, however previous cortical tracking studies were unable to demonstrate the presence of phonetic feature encoding. Here we utilise temporal response functions computed from electrophysiological responses to nursery rhymes to investigate the cortical encoding of phonetic features in a longitudinal cohort of infants when aged 4, 7 and 11 months, as well as adults. The analyses reveal an increasingly detailed and acoustically invariant phonetic encoding emerging over the first year of life, providing neurophysiological evidence that the pre-verbal human cortex learns phonetic categories. By contrast, we found no credible evidence for age-related increases in cortical tracking of the acoustic spectrogram."
"
University of Massachusetts Amherst civil and environmental engineers have determined the factors that may help identify the schools and daycare centers at greatest risk for elevated levels of lead in drinking water. The most telling characteristic for schools in Massachusetts is building age, with facilities built in the 1960s and 1970s -- nearly a third of the facilities tested -- at the greatest risk for having dangerously high water lead levels.

There is no safe exposure level to lead. The Massachusetts Department of Environmental Protection (MassDEP) recommends that schools and childcare facilities achieve the lowest lead levels possible, with a goal of 1 ppb (parts per billion) or less, often the lowest measurement that a laboratory can make. Childhood exposure can cause brain and nervous system damage, slowed growth and development, learning and behavioral problems, and hearing and speech issues. (In fact, today, the Environmental Protection Agency announced proposed changes to the Lead and Copper Rule Improvements.)
The UMass study, published in the American Water Works Association's journal Water Science, compared water lead-level data to a myriad of other characteristics that could influence these levels. ""Is it certain types of fixtures? Is it certain types of buildings? Is it certain places? Is the chemistry of the water supply? Is there anything about the water treatment process?"" says Emily Kumpel, one of the study authors and assistant professor of civil and environmental engineering at UMass Amherst.
Building age was the most important contributing factor, for a few reasons. Legislation has been passed over the years to improve the safety of school water. The federal Safe Drinking Water Act Amendment in 1986 required the use of ""lead-free"" piping, solder and flux in buildings. The definition of ""lead-free"" was then refined to more stringent levels in 2011.
Kumpel explains that there is a clear ""before"" and ""after"" around each of these time points: 50% of water samples from buildings constructed in 1986 and earlier had a water lead level of 2.1 ppb or higher and 13.7% of samples were greater than 15 parts per billion (or 0.0015 mg of lead per liter of water). After 1986, this declined so that half of samples had one ppb or less of lead, and only 4.6% of samples had lead levels higher than 15 ppb.
Importantly, these results represent water lead levels captured at ""first draw,"" meaning the water had been stagnant in the pipes overnight. The pattern was similar when looking at samples after the pipes had been flushed for 30 seconds, though less pronounced and with significantly lower lead levels after flushing.
Buildings constructed in the 1960s and 1970s -- about 30% of all schools tested -- were the most likely to have faucets, water fountains or other fixtures with elevated water lead levels at first draw. Half of the first draw water samples taken from schools built in these decades had lead concentrations at or above 2.8 and 2.9 ppb respectively. Plus, 16% of fixtures in 1960s buildings and 19.5% of fixtures in 1970s buildings had first draw levels over 15 ppb.

""That means that if you go into a facility built in the '60s or '70s and are the first one to get a glass of water in the morning or after a long school break, you'd have a high chance of it having a dangerously high level of lead. However, if the tap was flushed or had been used throughout the day, this would drop substantially. This is why flushing or other remediation actions are important,"" Kumpel says.
Schools built in the 1950s and 1980s were also at slightly lower, but still elevated, risk.
Kumpel explains that this reflects certain construction decisions that were made in particular places at particular times. She also notes that trends in Massachusetts likely extend to other parts of New England that aren't so geographically different and likely had similar building trends and best practices over the years.
The study's data came from the Assistance Program for Lead in School Drinking Water, a MassDEP and UMass Amherst water monitoring collaboration that began in 2016. This initiative now has information from more than 1,500 schools and childcare facilities. ""This publicly available large data set has been used for previous studies as well, and provides a basis for facilities to take action to protect children's health, including applying for funding to install filtered bottle fill stations under the Massachusetts School Water Improvement Grant (SWIG) program,"" notes John Tobiason, professor and head of the UMass Amherst civil and environmental engineering department, who leads the UMass work in support of the MassDEP initiative and is a co-author on this paper.
Part of this current analysis of the results from MassDEP's voluntary testing program included evaluating if the results may also serve a predictive purpose by identifying the risk factors for elevated water lead levels.
""As of last year, around 60% [of Massachusetts schools] have had sampling done and reported to this public database, but 40% have not, though each month more schools and childcare facilities are testing,"" says Kumpel. ""That was what we were trying to get at with this model: of those that haven't yet tested, can we prioritize the places that we might need to look at the most? Using these factors, can we then predict where we should make sure to follow up?""
There are obvious implications for school administrators and legislators looking to enact environmental protections, but what are the takeaways for parents?
""It's close to home for me as someone with young children,"" says Kumpel. ""But I'm also an engineer that works particularly on water distribution systems and providing safe tap water. This is why there are programs to test the water and report results, that way you can have the assurance that there is monitoring. This is where, as engineers, we strive to achieve due diligence and transparency."" Where lead was detected, technical assistance was offered for remediation actions, including help in applying for the SWIG grants for water bottle filling stations, she adds.
Her advice: stay informed. Massachusetts makes it particularly easy to do that. ""Massachusetts has made the data available in a public database,"" she says. ""See if your child's school or daycare has been tested. There is this free testing program so, as a parent, it could be advocating that your childcare provider or school sign up for the testing program and get that information.""

","score: 11.705232240437159, grade_level: '12'","score: 12.926926229508197, grade_levels: ['college'], ages: [18, 24]",10.1002/aws2.1358,"Exposure to lead through drinking water is of concern for children, particularly at schools and early education and care facilities (EECFs), where they spend much of their time. We use lead and copper data from monitoring in schools and EECFs in Massachusetts (USA) and create risk indices based on the percentage of fixtures in a school above three water lead level (WLL) thresholds (15, 5, and 1 ppb) to model which building characteristics, water source, and water treatment practices are associated with a school exceeding these thresholds. Local building characteristics had larger effects than information about the public water supplier (PWS), and buildings built from 1950 to 1980 were most at risk. Daily flushing and fixture replacement often decreased elevated WLLs, and water coolers had lower WLLs than other fixtures. These findings highlight the value of WLL monitoring programs and can be used to prioritize future investment in monitoring and remediation."
"
Optimistic thinking has long been immortalized in self-help books as the key to happiness, good health and longevity but it can also lead to poor decision making, with particularly serious implications for people's financial wellbeing.

Research from the University of Bath shows that excessive optimism is actually associated with lower cognitive skills such as verbal fluency, fluid reasoning, numerical reasoning, and memory. Whereas those high on cognitive ability tend to be both more realistic and pessimistic in their expectations about the future.
""Forecasting the future with accuracy is difficult and for that reason we night expect those with low cognitive ability to make more errors in judgments, both pessimistic and optimistic. But the results are clear: low cognitive ability leads to more self-flattering biases -- people essentially deluding themselves to a degree."" said Dr Chris Dawson of the University's School of Management.
""This points to the idea that whilst humans may be primed by evolution to expect the best, those high on cognitive ability are more able to override this automatic response when it comes to important decisions. Plans based on overly optimistic beliefs make for poor decisions and are bound to deliver worse outcomes than would realistic beliefs,"" Dr Dawson added.
Decisions on major financial issues such as employment, investments or savings, and any choice involving risk and uncertainty, were particularly prone to this effect and posed serious implications for individuals.
""Unrealistically optimistic financial expectations can lead to excessive levels of consumption and debt, as well as insufficient savings. It can also lead to excessive business entries and subsequent failures. The chances of starting a successful business are tiny, but optimists always think they have a shot and will start businesses destined to fail,"" Dr Dawson said.
The study -- ""Looking on the (B)right Side of Life: Cognitive Ability and Miscalibrated Financial Expectations"" -- took data from a UK survey of over 36,000 households and looked at people's expectations of their financial wellbeing and compared them with their actual financial outcomes. The research found that those highest on cognitive ability experienced a 22% increase in the probability of ""realism"" and a 35 per cent decrease in the probability of ""extreme optimism.""
""The problem with our being programmed to think positively is that it can adversely affect our quality of decision making, particularly when we have to make serious decisions. We need to be able to over-ride that and this research shows that people with high cognitive ability manage this better than those with low cognitive ability,"" he said.
""Unrealistic optimism is one of the most pervasive human traits and research has shown people consistently underestimate the negative and accentuate the positive. The concept of 'positive thinking' is almost unquestioningly embedded in our culture -- and it would be healthy to revisit that belief,"" Dr Dawson added

","score: 15.660416666666666, grade_level: '16'","score: 16.601395833333335, grade_levels: ['college_graduate'], ages: [24, 100]",10.1177/01461672231209400,"It is a puzzle why humans tend toward unrealistic optimism, as it can lead to excessively risky behavior and a failure to take precautionary action. Using data from a large nationally representative U.K. sample [Formula: see text] our claim is that optimism bias is partly a consequence of low cognition—as measured by a broad range of cognitive skills, including memory, verbal fluency, fluid reasoning and numerical reasoning. We operationalize unrealistic optimism as the difference between a person’s financial expectation and the financial realization that follows, measured annually over a decade. All else being equal, those highest on cognitive ability experience a 22% (53.2%) increase in the probability of realism (pessimism) and a 34.8% reduction in optimism compared with those lowest on cognitive ability. This suggests that the negative consequences of an excessively optimistic mindset may, in part, be a side product of the true driver, low cognitive ability."
"
In a study with 22 pairs of identical twins, Stanford Medicine researchers and their colleagues have found that a vegan diet improves cardiovascular health in as little as eight weeks.

Although it's well-known that eating less meat improves cardiovascular health, diet studies are often hampered by factors such as genetic differences, upbringing and lifestyle choices. By studying identical twins, however, the researchers were able to control for genetics and limit the other factors, as the twins grew up in the same households and reported similar lifestyles.
""Not only did this study provide a groundbreaking way to assert that a vegan diet is healthier than the conventional omnivore diet, but the twins were also a riot to work with,"" said Christopher Gardner, PhD, the Rehnborg Farquhar Professor and a professor of medicine. ""They dressed the same, they talked the same and they had a banter between them that you could have only if you spent an inordinate amount of time together.""
The study will publish Nov. 30 in JAMA Network Open. Gardner is the senior author. The study was co-first authored by Matthew Landry, PhD, a former Stanford Prevention Research Center postdoctoral scholar, now at the University of California, Irvine, and Catherine Ward, PhD, a post-doctoral scholar at the center.
Twin participants
The trial, conducted from May to July 2022, consisted of 22 pairs of identical twins for a total of 44 participants. The study authors selected healthy participants without cardiovascular disease from the Stanford Twin Registry -- a database of fraternal and identical twins who have agreed to participate in research studies -- and matched one twin from each pair with either a vegan or omnivore diet.
Both diets were healthy, replete with vegetables, legumes, fruits and whole grains and void of sugars and refined starches. The vegan diet was entirely plant-based, included no meat or animal products such as eggs or milk. The omnivore diet included chicken, fish, eggs, cheese, dairy and other animal-sourced foods.

During the first four weeks, a meal service delivered 21 meals per week -- seven breakfasts, lunches and dinners. For the remaining four weeks, the participants prepared their own meals.
A registered dietitian, or ""diet whisperer,"" according to Gardner, was on call to offer suggestions and answer questions regarding the diets during the duration of the study. The participants were interviewed about their dietary intake and kept a log of the food they ate.
Forty-three participants completed the study which, Gardner said, demonstrates how feasible it is to learn how to a prepare a healthy diet in four weeks.
""Our study used a generalizable diet that is accessible to anyone, because 21 out of the 22 vegans followed through with the diet,"" said Gardner, who is a professor in the Stanford Prevention Research Center. ""This suggests that anyone who chooses a vegan diet can improve their long-term health in two months, with the most change seen in the first month.""
Improving health
The authors found the most improvement over the first four weeks of the diet change. The participants with a vegan diet had significantly lower low-density lipoprotein cholesterol (LDL-C) levels, insulin and body weight -- all of which are associated with improved cardiovascular health -- than the omnivore participants.

At three time points -- at the beginning of the trial, at four weeks and at eight weeks  -- researchers weighed the participants and drew their blood. The average baseline LDL-C level for the vegans was 110.7 mg/dL and 118.5 mg/dL for the omnivore participants; it dropped to 95.5 for vegans and 116.1 for omnivores at the end of the study. The optimal healthy LDL-C level is less than 100.
Because the participants already had healthy LDL-C levels, there was less room for improvement, Gardner said, speculating that participants who had higher baseline levels would show greater change.
The vegan participants also showed about a 20% drop in fasting insulin -- higher insulin level is a risk factor for developing diabetes. The vegans also lost an average of 4.2 more pounds than the omnivores.
""Based on these results and thinking about longevity, most of us would benefit from going to a more plant-based diet,"" Gardner said.
The vegan participants (and the omnivores to some extent) did the three most important things to improve cardiovascular health, according to Gardner: They cut back on saturated fats, increased dietary fiber and lost weight.
A global flair
Gardner emphasizes that although most people will probably not go vegan, a nudge in the plant-based direction could improve health. ""A vegan diet can confer additional benefits such as increased gut bacteria and the reduction of telomere loss, which slows aging in the body,"" Gardner said.
""What's more important than going strictly vegan is including more plant-based foods into your diet,"" said Gardner, who has been ""mostly vegan"" for the last 40 years. ""Luckily, having fun with vegan multicultural foods like Indian masala, Asian stir-fry and African lentil-based dishes can be a great first step.""
Gardner is a member of the Stanford Cardiovascular Institute, the Wu Tsai Human Performance Alliance, the Maternal and Child Health Research Institute, and the Stanford Cancer Institute.
The study was funded by the Vogt Foundation; the Stanford Clinical and Translational Science Award; and the National Heart, Lung and Blood Institute.

","score: 12.984909909909913, grade_level: '13'","score: 14.596542792792796, grade_levels: ['college_graduate'], ages: [24, 100]",10.1001/jamanetworkopen.2023.44457,"Increasing evidence suggests that, compared with an omnivorous diet, a vegan diet confers potential cardiovascular benefits from improved diet quality (ie, higher consumption of vegetables, legumes, fruits, whole grains, nuts, and seeds). To compare the effects of a healthy vegan vs healthy omnivorous diet on cardiometabolic measures during an 8-week intervention. This single-center, population-based randomized clinical trial of 22 pairs of twins (N = 44) randomized participants to a vegan or omnivorous diet (1 twin per diet). Participant enrollment began March 28, 2022, and continued through May 5, 2022. The date of final follow-up data collection was July 20, 2022. This 8-week, open-label, parallel, dietary randomized clinical trial compared the health impact of a vegan diet vs an omnivorous diet in identical twins. Primary analysis included all available data. Twin pairs were randomized to follow a healthy vegan diet or a healthy omnivorous diet for 8 weeks. Diet-specific meals were provided via a meal delivery service from baseline through week 4, and from weeks 5 to 8 participants prepared their own diet-appropriate meals and snacks. The primary outcome was difference in low-density lipoprotein cholesterol concentration from baseline to end point (week 8). Secondary outcome measures were changes in cardiometabolic factors (plasma lipids, glucose, and insulin levels and serum trimethylamine N-oxide level), plasma vitamin B12 level, and body weight. Exploratory measures were adherence to study diets, ease or difficulty in following the diets, participant energy levels, and sense of well-being. A total of 22 pairs (N = 44) of twins (34 [77.3%] female; mean [SD] age, 39.6 [12.7] years; mean [SD] body mass index, 25.9 [4.7]) were enrolled in the study. After 8 weeks, compared with twins randomized to an omnivorous diet, the twins randomized to the vegan diet experienced significant mean (SD) decreases in low-density lipoprotein cholesterol concentration (−13.9 [5.8] mg/dL; 95% CI, −25.3 to −2.4 mg/dL), fasting insulin level (−2.9 [1.3] μIU/mL; 95% CI, −5.3 to −0.4 μIU/mL), and body weight (−1.9 [0.7] kg; 95% CI, −3.3 to −0.6 kg). In this randomized clinical trial of the cardiometabolic effects of omnivorous vs vegan diets in identical twins, the healthy vegan diet led to improved cardiometabolic outcomes compared with a healthy omnivorous diet. Clinicians can consider this dietary approach as a healthy alternative for their patients. ClinicalTrials.gov Identifier: NCT05297825"
"
A new study, published in PLOS ONE, has uncovered a remarkable connection between individuals' musical preferences and their moral values, shedding new light on the profound influence that music can have on our moral compass.

The research, conducted by a team of scientists at Queen Mary University of London and ISI Foundation in Turin, Italy, employed machine learning techniques to analyse the lyrics and audio features of individuals' favorite songs, revealing a complex interplay between music and morality.
""Our study provides compelling evidence that music preferences can serve as a window into an individual's moral values,"" stated Dr Charalampos Saitis, one of the senior authors of the study and Lecturer in Digital Music Processing at Queen Mary University of London's School of Electronic Engineering and Computer Science.
The study involved an existing dataset of over 1,400 participants who completed psychometric questionnaires assessing their moral values and provided information about their favorite artists through Facebook Page Likes. The researchers then extracted acoustic and lyrical features from the top five songs of each participant's preferred artists.
Using ML algorithms, the team analysed the extracted features to predict participants' moral values. Various text processing techniques, including lexicon-based methods and BERT-based embeddings, were employed to analyze narrative, moral values, sentiment, and emotions in lyrics. Additionally, low- and high-level audio features provided via Spotify's API were used to understand encoded information in participants' musical choices, enhancing moral inferences.
The results demonstrated that a combination of lyrical and audio features outperformed basic demographic information in predicting individuals' moral compass. Specifically, musical elements like pitch and timbre emerged as crucial predictors for values of Care and Fairness, while sentiments and emotions expressed in lyrics were more effective in predicting traits of Loyalty, Authority, and Purity.
""Our findings reveal that music is not merely a source of entertainment or aesthetic pleasure; it is also a powerful medium that reflects and shapes our moral sensibilities,"" remarked Vjosa Preniqi, lead author of the study and a PhD student in Queen Mary's Centre for Doctoral Training in Data-informed Audience-centric Media Engineering. ""By understanding this connection, we can open up new avenues for music-based interventions that promote positive moral development.""
The study's implications extend beyond mere academic curiosity, holding the potential to impact how we engage with and utilise music in diverse aspects of life. ""Our breakthrough can pave the way for applications ranging from personalised music experiences to innovative music therapy and communication campaigns,"" commented Dr Kyriaki Kalimeri, senior co-author of the study and researcher at ISI Foundation.
""Our research has uncovered an important link between music and morality, paving the way for a deeper understanding of the psychological dimensions of our musical experiences,"" concluded Vjosa Preniqi. ""We are excited to continue exploring this rich and uncharted territory.""
This study was conducted using data from the LikeYouth Facebook application, a research-focused survey tool that has engaged over 64,000 participants primarily in Italy. Participants provided voluntary and informed consent and completed various psychometric surveys, including the Moral Foundations Questionnaire (MFQ), in addition to sharing demographic details and Facebook Page Likes.

","score: 19.214700854700855, grade_level: '19'","score: 20.432079059829064, grade_levels: ['college_graduate'], ages: [24, 100]",10.1371/journal.pone.0294402,"Music is a fundamental element in every culture, serving as a universal means of expressing our emotions, feelings, and beliefs. This work investigates the link between our moral values and musical choices through lyrics and audio analyses. We align the psychometric scores of 1,480 participants to acoustics and lyrics features obtained from the top 5 songs of their preferred music artists from Facebook Page Likes. We employ a variety of lyric text processing techniques, including lexicon-based approaches and BERT-based embeddings, to identify each song’s narrative, moral valence, attitude, and emotions. In addition, we extract both low- and high-level audio features to comprehend the encoded information in participants’ musical choices and improve the moral inferences. We propose a Machine Learning approach and assess the predictive power of lyrical and acoustic features separately and in a multimodal framework for predicting moral values. Results indicate that lyrics and audio features from the artists people like inform us about their morality. Though the most predictive features vary per moral value, the models that utilised a combination of lyrics and audio characteristics were the most successful in predicting moral values, outperforming the models that only used basic features such as user demographics, the popularity of the artists, and the number of likes per user. Audio features boosted the accuracy in the prediction of empathy and equality compared to textual features, while the opposite happened for hierarchy and tradition, where higher prediction scores were driven by lyrical features. This demonstrates the importance of both lyrics and audio features in capturing moral values. The insights gained from our study have a broad range of potential uses, including customising the music experience to meet individual needs, music rehabilitation, or even effective communication campaign crafting."
"
Research gives insight into importance of sleep on cognitive performance and emotional well-being to those who find themselves under stress

Politicians, military generals and first responders are just some high-stress positions which should avoid taking important decisions after a night without sleep, new research from the University of Ottawa indicates.
We all understand the power of sleep and the vital role it plays in human health, cognitive performance and in regulating our emotional well-being. Numerous studies into a lack of sleep have shown drops in neurocognitive functions, particularly vigilant attention, motor responses, inhibition control, and working memory. Despite this, sleep loss continues to challenge public health and affect people of all ages.
Sleep and risky decision-making
With little insight into the impact of a lack of sleep on risky decision-making at the neuroimaging level, researchers from the University of Ottawa and the University of Pennsylvania found a 24-hour period of sleep deprivation significantly impacted individuals' decision-making processes by dampening neural responses to the outcomes of their choices.
In other words, people tend to exhibit reduced positive emotions in response to winning outcomes and diminished negative emotions when faced with losses after pulling an all-nighter compared to their well-rested baseline condition.
""Common sense does dictate if people incur sleep loss, sleep disturbance or a sleep disorder that their cognitive function will be impacted, their attention and efficiency will decrease. But there is an emotional impact, too,"" says Zhuo Fang, a Data Scientist in the Department of Psychology at the Faculty of Social Sciences.

""If you experience even just one night of sleep deprivation, there will be an impact, even on a neural level. So, we wanted to combine brain imaging and behaviour to see that impact,"" adds Fang, who is affiliated with uOttawa's Brain and Mind Research Institute and The Royal.
The study, which evaluated the impact of one night of total sleep deprivation on 56 healthy adults, found: A single night of total sleep loss significantly decreased the brain activation to win and loss outcomes, suggesting that acute sleep loss can have a dampening effect on neural responses to decision outcomes during risk-taking. Total sleep deprivation had the detrimental effect by disrupting the relationship between neural response and individual's risk-taking behavior, which might be related to the altered perception for risk-taking.While numerous studies have previously illustrated the wide-ranging effects of sleep deprivation on various brain and cognitive functions, including attention processing, memory consolidation, and learning, this study addresses the specific impact of sleep loss on decision-making.
""These results underscore the importance of maintaining adequate sleep and how individuals should refrain from making important decisions when experiencing chronic or acute sleep deprivation,"" says Fang, who co-first authored the study with Tianxin Mao of the University of Pennsylvania alongside corresponding author Hengyi Rao.
""In specific professions where decision-makers are required to operate under accumulated sleep loss, specialized training or fatigue risk management might be necessary to enable them to handle such situations effectively.""

","score: 20.165428571428574, grade_level: '20'","score: 23.132842857142855, grade_levels: ['college_graduate'], ages: [24, 100]",10.1111/psyp.14465,"Sleep loss impacts a broad range of brain and cognitive functions. However, how sleep deprivation affects risky decision‐making remains inconclusive. This study used functional MRI to examine the impact of one night of total sleep deprivation (TSD) on risky decision‐making behavior and the underlying brain responses in healthy adults. In this study, we analyzed data from N = 56 participants in a strictly controlled 5‐day and 4‐night in‐laboratory study using a modified Balloon Analogue Risk Task. Participants completed two scan sessions in counter‐balanced order, including one scan during rested wakefulness (RW) and another scan after one night of TSD. Results showed no differences in participants' risk‐taking propensity and risk‐induced activation between RW and TSD. However, participants showed significantly reduced neural activity in the anterior cingulate cortex and bilateral insula for loss outcomes, and in bilateral putamen for win outcomes during TSD compared with RW. Moreover, risk‐induced activation in the insula negatively correlated with participants' risk‐taking propensity during RW, while no such correlations were observed after TSD. These findings suggest that sleep loss may impact risky decision‐making by attenuating neural responses to decision outcomes and impairing brain‐behavior associations."
"
It's one of those days. On the drive home from work, the car in the next lane cuts you off. You slam on the brakes, lay on the horn, and yell choice words at the offending driver. When you walk into your house half an hour later, you're still angry, and snap at your partner when they ask about your day.

Fruit flies may not have to worry about the lingering effects of road rage, but they also experience states of persistent aggression. In the case of female fruit flies, this behavior is a survival mechanism, causing the flies to headbutt, shove, and fence other female fruit flies to guard prime egg-laying territory on a ripe banana.
Now, researchers at Janelia and the California Institute of Technology are homing in on the neurons, circuits, and mechanisms responsible for this tenacious behavior.
In a new study, the researchers report they've teased out the cell types contributing to a persistent aggressive state in female fruit flies, showing that some cells associated with aggression can cause flies to remain angry for up to 10 minutes.
They also found that this persistent state may not be solely due to a recurrent connection between the aggression-associated cells, as had been thought. In a recurrent connection, signals loop back and feed into the same neural circuit, which could cause a behavior to persist.
Instead, the new research suggests persistent aggression could be regulated by other factors, including neuromodulators affecting neuronal activity, neurons downstream from the aggression-associated cells, or other circuits in the fly brain. Considering their findings, scientists may need to develop a new model that considers these other factors in addition to recurrent connections to explain this enduring behavior.
""It is interesting for the field because we talk about these recurrent connections as being key for the persistent state, and that's really what we thought,"" says Katie Schretter, a postdoc in the Rubin Lab who led the research. ""But now it seems less clear in this case.""
Understanding persistent internal states like aggression could help researchers better uncover how the brain makes decisions -- for instance, whether to stay mad or move on -- and the individual circuits involved in these choices. Figuring out the underlying mechanisms behind aggression could also help scientists better understand aggressive behavior in humans, including behaviors that can occur alongside neurodegenerative or psychiatric diseases.

""For our society, it's important to be able to decrease aggression and figure out how to stop persistent aggression,"" Schretter says. ""Figuring out how the circuit works can help us figure out how we might decrease it.""
Fighting fruit flies
Scientists had previously identified cell types associated with aggression in the brains of female fruit flies. They found that activating these cells caused the flies to fight. Given this, the team, led by Schretter, Cal Tech graduate student Hui (Vivian) Chiu, Janelia Senior Group Leader Gerry Rubin, and HHMI Investigator David Anderson, wanted to look at these cells to see how their signals might feed back into each other to generate a persistent aggressive state.
The researchers separated female flies with a barrier and then activated the different cell types associated with aggression for 30 seconds at a time. They kept the flies separated for specific periods of time, up to 30 minutes, before removing the barrier and letting them interact.
The team hypothesized that recurrent connections between certain aggression-associated cell types could cause the flies to remain aggressive for longer periods of time.
They found that one cell type associated with aggression -- aIPg -- contributes to persistent aggression. When these cells were activated, the flies would fight for up to 10 minutes after the barrier was removed. But another cell type previously found to be involved in aggression -- pC1d -- did not cause this same enduring anger.

pC1d also didn't affect whether aIPg caused persistent aggression, and neither pC1d nor aIPg showed persistent neuronal activity. These findings suggest that a persistent aggressive state doesn't depend on a recurrent connection between the two cell types.
Previous research had shown that stimulating another cell associated with aggression -- pC1e -- also does not cause persistent behavior on its own. However, Schretter and colleagues were surprised to find that when pC1d and pC1e were stimulated simultaneously, the flies remained persistently aggressive.
Taken together, the results suggest that the persistent aggressive state may be maintained by a mechanism different from what the researchers had originally thought. Instead of being due to a recurrent connection between aIPg and pC1d, as they had hypothesized, persistent aggression could involve pC1e. But it could also include other factors, such as a neuromodulator acting on the circuit or the effect of neurons downstream from aIPg, pC1d, and pC1e. Or aggression could be controlled by another circuit altogether.
Schretter says investigating these other models to explain persistent aggression is the next step.
""It's exciting to see what else could lengthen that persistence, because there could be other circuits that are also involved,"" she says. ""It is basically open for us to go after, so it is a fun place to be.""

","score: 12.319671361502348, grade_level: '12'","score: 14.15975166790215, grade_levels: ['college_graduate'], ages: [24, 100]",10.7554/eLife.88598.1,"Persistent internal states are important for maintaining survival-promoting behaviors, such as aggression. In female Drosophila melanogaster, we have previously shown that individually activating either aIPg or pC1d cell types can induce aggression. Here we investigate further the individual roles of these cholinergic, sexually dimorphic cell types, and the reciprocal connections between them, in generating a persistent aggressive internal state. We find that a brief 30-second optogenetic stimulation of aIPg neurons was sufficient to promote an aggressive internal state lasting at least 10 minutes, whereas similar stimulation of pC1d neurons did not. While we previously showed that stimulation of pC1e alone does not evoke aggression, persistent behavior could be promoted through simultaneous stimulation of pC1d and pC1e, suggesting an unexpected synergy of these cell types in establishing a persistent aggressive state. Neither aIPg nor pC1d show persistent neuronal activity themselves, implying that the persistent internal state is maintained by other mechanisms. Moreover, inactivation of pC1d did not significantly reduce aIPg-evoked persistent aggression arguing that the aggressive state did not depend on pC1d-aIPg recurrent connectivity. Our results suggest the need for alternative models to explain persistent female aggression."
"
People with personality traits such as conscientiousness, extraversion and positive affect are less likely to be diagnosed with dementia than those with neuroticism and negative affect, according to a new analysis by researchers at the University of California, Davis and Northwestern University. The difference was not linked to physical damage to brain tissue found in dementia patients, but more likely to how certain personality traits help people navigate dementia-related impairments.

The work is published Nov. 29 in Alzheimer's & Dementia: The Journal of the Alzheimer's Association.
Previous studies have tried to establish links between personality traits and dementia, but these were mostly small and represented only specific populations, said Emorie Beck, assistant professor of psychology at UC Davis and first author on the paper.
""We wanted to leverage new technology to synthesize these studies and test the strength and consistency of these associations,"" Beck said. If those links hold up, then targeting personality traits for change in interventions earlier in life could be a way to reduce dementia risk in the long term, she said.
Beck and colleagues analyzed data from eight published studies including over 44,000 people, of whom 1,703 developed dementia. They looked at measures of the ""big five"" personality traits (conscientiousness, extraversion, openness to experience, neuroticism and agreeableness) and subjective wellbeing (positive and negative affect, and life satisfaction) compared to clinical symptoms of dementia (performance on cognitive tests) and brain pathology at autopsy.
Personality is typically thought to be linked to dementia risk through behavior, Beck said. For example, people who score high on conscientiousness may be more likely to eat well and take care of their health, which results in better health in the long term.
The researchers found that high scores on negative traits (neuroticism, negative affect) and low scores on positive traits (conscientiousness, extraversion, positive affect) were associated with a higher risk of a dementia diagnosis. High scores on openness to experience, agreeableness, and life satisfaction had a protective effect in a smaller subset of studies.

Link to diagnosis but not pathology
To their surprise, however, no link was found between these personality traits and actual neuropathology in the brains of people after death.
""This was the most surprising finding to us,"" Beck said. ""If personality is predictive of performance on cognitive tests but not pathology, what might be happening?""
One explanation is that some personality traits could make people more resilient to the damage caused by diseases such as Alzheimer's. People with higher levels of some traits may find ways, whether they are aware of it or not, to cope with and work around impairments. Other work by members of the study team has shown that some people with quite extensive pathology can show little impairment on cognitive tests.
The researchers also looked at other factors that could moderate the relationship between personality and dementia risk and neuropathology, including age, gender and educational attainment.
""We found almost no evidence for effects, except that conscientiousness's protective effect increased with age,"" Beck said.
Many factors contribute to the development of dementia. Among those that aren't directly related to genetics, this study is a first step in teasing out the associations between personality and dementia, Beck said. The researchers plan to continue and expand the work, including looking at people who show little impairment in the face of a lot of pathology. They also hope to look at other everyday factors that might play a role in developing dementia.
Part of the work was conducted while Beck was a postdoctoral researcher at Northwestern University in Chicago. Coauthors are: Tomiko Yoneda, UC Davis and Northwestern; Daniel Mroczek and Eileen Graham, Northwestern; Bryan James, David Bennett and John Morris, Rush University Medical Center, Chicago; Jason Hassenstab, Washington University School of Medicine, St. Louis; Mindy Katz and Richard Lipton, Albert Einstein College of Medicine, the Bronx.
The work was supported by grants from the National Institute on Aging.

","score: 15.046575057606926, grade_level: '15'","score: 16.160532231776543, grade_levels: ['college_graduate'], ages: [24, 100]",10.1002/alz.13523,"The extent to which the Big Five personality traits and subjective well‐being (SWB) are discriminatory predictors of clinical manifestation of dementia versus dementia‐related neuropathology is unclear. Using data from eight independent studies (Ntotal= 44,531; Ndementia= 1703; baseline Mage= 49 to 81 years, 26 to 61% female; Mfollow‐up range = 3.53 to 21.00 years), Bayesian multilevel models tested whether personality traits and SWB differentially predicted neuropsychological and neuropathological characteristics of dementia. Synthesized and individual study results indicate that high neuroticism and negative affect and low conscientiousness, extraversion, and positive affect were associated with increased risk of long‐term dementia diagnosis. There were no consistent associations with neuropathology. This multistudy project provides robust, conceptually replicated and extended evidence that psychosocial factors are strong predictors of dementia diagnosis but not consistently associated with neuropathology at autopsy. N(+), C(−), E(−), PA(−), and NA(+) were associated with incident diagnosis. Results were consistent despite self‐report versus clinical diagnosis of dementia. Psychological factors were not associated with neuropathology at autopsy. Individuals with higher conscientiousness and no diagnosis had less neuropathology. High C individuals may withstand neuropathology for longer before death. N(+), C(−), E(−), PA(−), and NA(+) were associated with incident diagnosis. Results were consistent despite self‐report versus clinical diagnosis of dementia. Psychological factors were not associated with neuropathology at autopsy. Individuals with higher conscientiousness and no diagnosis had less neuropathology. High C individuals may withstand neuropathology for longer before death."
"
Many people are keen on making healthy as well as sustainable food choices, and they often intuitively equate ""healthy"" with being ""sustainable."" A study by researchers at the University of Konstanz, the Johannes Kepler University Linz and the Hamburg University of Applied Sciences is focusing on whether or not this perception corresponds to reality. It has just been published in the scientific journal PLOS Sustainability and Transformation.

The study shows that many consumers clearly correlate their perception of sustainability with how healthy their food choices and meals are. ""We examined just how widespread the perception is that healthy meals are also sustainable. We were especially interested in whether perceptions change based on the actual overlap between meal health and sustainability. We also explored whether the type of meal, such as a vegan meal, influences this presumed correlation,"" explains Professor Gudrun Sproesser, head of the Department of Health Psychology at Johannes Kepler University.
In the study, over 5,000 customers rated 29 different meal options at a public canteen -- i.e. the University of Konstanz's canteen, run by Seezeit student services -- as to what they believed to be a healthy and sustainable food choice. The exact values relating to environmental sustainability and healthy eating were also determined by applying a special algorithm to analyze the precise meal recipes. The findings were clear: Many participants automatically believed that healthy food was also sustainable.
Gudrun Sproesser points out: ""Interestingly, however, there was no association between this perception and the actual overlap between environmental sustainability and how healthy a meal actually is."" This is because healthier foods can be produced using methods that are less eco-friendly, and the reverse is also true: sustainable food can be less healthy.
Britta Renner, who leads the research team Psychological Assessment and Health Psychology at the University of Konstanz, adds: ""The findings clearly indicate that we consumers need better and more readily accessible information about the sustainability and healthiness of foods."" One useful approach, for example, is to use climate or sustainability labels on food, as suggested in a recent expert opinion on more sustainable food by the Scientific Advisory Board on Agricultural Policy, Food and Consumer Health Protection (WBAE) of the Federal Ministry of Food and Agriculture. Such labels would enable consumers to make more informed decisions about what they eat while simultaneously doing their part to protect the environment.

","score: 15.972425373134328, grade_level: '16'","score: 16.75630597014925, grade_levels: ['college_graduate'], ages: [24, 100]",10.1371/journal.pstr.0000086,"Research has found an association between the perceived sustainability and healthiness of foods and meals between individual consumers. The current study aimed to investigate whether the association between perceived sustainability and healthiness on the individual level is rooted in reality. Moreover, we investigated whether meal or individual characteristics affect this association. In total, 5021 customers of a public canteen rated the sustainability and healthiness of 29 meal options. For determining the actual environmental sustainability and healthiness scores, exact recipes of each meal were analyzed using the NAHGAST algorithm. Results showed a substantial association between perceived sustainability and healthiness at the individual level. However, this perceived relation was unrelated to the overlap between the actual environmental sustainability and healthiness scores of the meals. Moreover, this “healthier = more sustainable” perception was unrelated to other meal characteristics (e.g., vegan content) or individual characteristics (i.e., gender, eating style). However, this association was slightly higher in older than in younger participants. The present study shows in a real-world setting that food consumers seem to evaluate the sustainability and healthiness of meals based on a simple “healthy = sustainable” heuristic which is largely independent of the actual overlap of these dimensions. Future research is needed to shed more light on the nature, sources, and consequences of this heuristic."
"
Streptococcus agalactiae (known as Group B Streptococcus, or GBS) is present in the genital tract in around one in five women. Previous research by the team at the University of Cambridge and Rosie Hospital, Cambridge University Hospitals NHS Foundation Trust, identified GBS in the placenta of around 5% of women prior to the onset of labour. Although it can be treated with antibiotics, unless screened, women will not know they are carriers.

GBS can cause sepsis, a life-threatening reaction to an infection, in the newborn. Worldwide, GBS accounts for around 50,000 stillbirths and as many as 100,000 infant deaths per year.
In a study published today in Nature Microbiology, the team looked at the link between the presence of GBS in the placenta and the risk of admission of the baby to a neonatal unit. The researchers re-analysed data available from their previous study of 436 infants born at term, confirming their findings in a second cohort of 925 pregnancies.
From their analysis, the researchers estimate that placental GBS was associated with a two- to three-fold increased risk of neonatal unit admission, with one in 200 babies admitted with sepsis associated with GBS -- almost 10 times the previous estimate. The clinical assessment of these babies using the current diagnostic testing identified GBS in less than one in five of these cases.
In the USA, all pregnant women are routinely screened for GBS and treated with antibiotics if found to be positive. In the UK, women who test positive for GBS are also treated with antibiotics -- however, only a minority of pregnant women are tested for GBS, as the approach in the UK is to obtain samples only from women experiencing complications, or with other risk factors.
There are a number of reasons why women in the UK are not screened, including the fact that detecting GBS in the mother is not always straightforward and only a small minority of babies exposed to the bacteria were thought to become ill. A randomised controlled trial of screening for GBS for treatment with antibiotics is currently underway in the UK.
Dr Francesca Gaccioli from the Department of Obstetrics & Gynaecology at the University of Cambridge said: ""In the UK, we've traditionally not screened mothers for GBS, but our findings -- that significantly more newborns are admitted to the neonatal unit as a result of GBS-related sepsis than was previously thought -- profoundly changes the risk/benefit balance of universal screening.""
To improve detection, the researchers have developed an ultrasensitive PCR test, which amplifies tiny amounts of DNA or RNA from a suspected sample to check for the presence of GBS. They have filed a patent with Cambridge Enterprise, the University of Cambridge's technology transfer arm, for this test.

Professor Gordon Smith, Head of Obstetrics & Gynaecology at the University of Cambridge, said: ""Using this new test, we now realise that the clinically detected cases of GBS may represent the tip of the iceberg of complications arising from this infection. We hope that the ultra-sensitive test developed by our team might lead to viable point-of-care testing to inform immediate neonatal care.""
When the researchers analysed serum from the babies' umbilical cords, they found that over a third showed greatly increased levels of several cytokines -- protein messengers release by the immune system. This suggests that a so-called 'cytokine storm' -- an extreme immune response that causes collateral damage to the host -- was behind the increased risk of disease.
The research was funded by the Medical Research Council and supported by the National Institute for Health Research (NIHR) Cambridge Biomedical Research Centre.

","score: 15.90390756302521, grade_level: '16'","score: 17.47787394957983, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41564-023-01528-2,"Streptococcus agalactiae (Group B Streptococcus; GBS) is a common cause of sepsis in neonates. Previous work detected GBS DNA in the placenta in ~5% of women before the onset of labour, but the clinical significance of this finding is unknown. Here we re-analysed this dataset as a case control study of neonatal unit (NNU) admission. Of 436 infants born at term (≥37 weeks of gestation), 7/30 with placental GBS and 34/406 without placental GBS were admitted to the NNU (odds ratio (OR) 3.3, 95% confidence interval (CI) 1.3–7.8). We then performed a validation study using non-overlapping subjects from the same cohort. This included a further 239 cases of term NNU admission and 686 term controls: 16/36 with placental GBS and 223/889 without GBS were admitted to the NNU (OR 2.4, 95% CI 1.2–4.6). Of the 36 infants with placental GBS, 10 were admitted to the NNU with evidence of probable but culture-negative sepsis (OR 4.8, 95% CI 2.2–10.3), 2 were admitted with proven GBS sepsis (OR 66.6, 95% CI 7.3–963.7), 6 were admitted and had chorioamnionitis (inflammation of the foetal membranes) (OR 5.3, 95% CI 2.0–13.4), and 5 were admitted and had funisitis (inflammation of the umbilical cord) (OR 6.7, 95% CI 12.5–17.7). Foetal cytokine storm (two or more pro-inflammatory cytokines >10 times median control levels in umbilical cord blood) was present in 36% of infants with placental GBS DNA and 4% of cases where the placenta was negative (OR 14.2, 95% CI 3.6–60.8). Overall, ~1 in 200 term births had GBS detected in the placenta, which was associated with infant NNU admission and morbidity."
"
While past research has indicated that moderate alcohol consumption can lower one's risk of cardiovascular disease (CVD), more recent studiessuggest that moderate levels of drinking may be hazardous to heart health. A new analysis led by Boston University School of Public Health and Friedman School of Nutrition Science and Policy at Tufts University (Friedman School) now sheds new insight on this complex relationship between alcohol consumption and the progression of CVD.

Published in the journal BMC Medicine, the study found that alcohol consumption may have counteractive effects on CVD risk, depending on the biological presence of certain circulating metabolites -- molecules that are produced during or after a substance is metabolized and studied as biomarkers of many diseases.
The researchers observed a total of 60 alcohol consumption-related metabolites, identifying seven circulating metabolites that link long-term moderate alcohol consumption with an increased risk of CVD, and three circulating metabolites that link this same drinking pattern with a lower risk of CVD.
The findings provide a better understanding of the molecular pathway of long-term alcohol consumption and highlight the need for and direction of further research on these metabolites to inform targeted prevention and treatment of alcohol-related CVD.
""The study findings demonstrate that alcohol consumption may trigger changes of our metabolomic profiles, potentially yielding both beneficial and harmful outcomes,"" says Dr.Chunyu Liu, assistant professor of biostatistics at BUSPH and co-corresponding/co-senior author of the study along with Dr.Jiantao Ma, assistant professor in the Division of Nutrition Epidemiology and Data Science at the Friedman School. ""Because the majority of our study participants are moderate alcohol consumers, our findings contribute to the ongoing discussion about the relationship between moderate alcohol drinking and heart health.
""However, rather than definitively settling that debate, this study underscores the intricate effects of alcohol consumption on cardiovascular health and generates a useful hypothesis for future investigations,"" Dr. Liu says.
For the study, the researchers examined blood samples to measure the association between the cumulative average consumption of beer, wine, and liquor and 211 metabolites among 2,428Framingham Heart Study Offspring Study participants, who are the children of participants in the long-running Boston University-basedFramingham Heart Study, over 20 years. Among the participants, 636 developed CVD over the study period.

Among the 60 drinking-related metabolites, 13 metabolites had a stronger association with alcohol consumption in women than in men, perhaps due to women's generally smaller body size and likely higher blood alcohol concentration after consuming the same amount of alcohol as men.
The results also showed that consumption of different types of alcohol was linked to different metabolomic responses, with beer consumption generating a slightly weaker association overall than wine and liquor. In roughly two-thirds of the 60 metabolites, higher plasma levels were detected in participants who consumed greater amounts of alcohol.
Branched-chain amino acids (BCAAs), were among the metabolites that were not associated with alcohol consumption.
The researchers then calculated two alcohol consumption-associated metabolite scores, which had opposite associations with the development of CVD.
""While our study presents intriguing findings, validation through state-of-the-art methods and large and diverse study populations is crucial,"" Dr. Ma says. ""To enhance reliability, we aim to conduct larger-scale research involving a more diverse racial and ethnic background, as the current study participants are all white. In addition, we will expand our study to integrate with other molecular markers such as genetic information to illustrate the complex relationships between alcohol consumption, metabolite features, and cardiovascular risk.""
The study was funded by the National Institute on Alcohol Abuse and Alcoholism. Data collection in the Framingham Heart Study was supported by the National Heart, Lung, and Blood Institute.

","score: 19.251473684210527, grade_level: '19'","score: 21.418009868421052, grade_levels: ['college_graduate'], ages: [24, 100]",10.1186/s12916-023-03149-2,"Metabolite signatures of long-term alcohol consumption are lacking. To better understand the molecular basis linking alcohol drinking and cardiovascular disease (CVD), we investigated circulating metabolites associated with long-term alcohol consumption and examined whether these metabolites were associated with incident CVD. Cumulative average alcohol consumption (g/day) was derived from the total consumption of beer, wine, and liquor on average of 19 years in 2428 Framingham Heart Study Offspring participants (mean age 56 years, 52% women). We used linear mixed models to investigate the associations of alcohol consumption with 211 log-transformed plasma metabolites, adjusting for age, sex, batch, smoking, diet, physical activity, BMI, and familial relationship. Cox models were used to test the association of alcohol-related metabolite scores with fatal and nonfatal incident CVD (myocardial infarction, coronary heart disease, stroke, and heart failure). We identified 60 metabolites associated with cumulative average alcohol consumption (p < 0.05/211 ≈ 0.00024). For example, 1 g/day increase of alcohol consumption was associated with higher levels of cholesteryl esters (e.g., CE 16:1, beta = 0.023 ± 0.002, p = 6.3e − 45) and phosphatidylcholine (e.g., PC 32:1, beta = 0.021 ± 0.002, p = 3.1e − 38). Survival analysis identified that 10 alcohol-associated metabolites were also associated with a differential CVD risk after adjusting for age, sex, and batch. Further, we built two alcohol consumption weighted metabolite scores using these 10 metabolites and showed that, with adjustment age, sex, batch, and common CVD risk factors, the two scores had comparable but opposite associations with incident CVD, hazard ratio 1.11 (95% CI = [1.02, 1.21], p = 0.02) vs 0.88 (95% CI = [0.78, 0.98], p = 0.02). We identified 60 long-term alcohol consumption-associated metabolites. The association analysis with incident CVD suggests a complex metabolic basis between alcohol consumption and CVD."
"
Taking into account whether people believe they are receiving a real treatment or a fake one (placebo) could provide better insights that could help improve interventions for conditions such as depression and ADHD.

A team of psychologists, led by Professor Roi Cohen Kadosh from the University of Surrey, analysed five independent studies that covered different types of neurostimulation treatments to understand the role of patients' subjective beliefs. These patients included both clinical patients being treated for ADHD and depression, as well as healthy adults.
The study found that patients' beliefs about whether they were receiving real or placebo treatments explained the treatment outcomes in four of the five studies. On some occasions, the subjects' beliefs explained the treatment's results better than the actual treatment itself. Assumptions about the treatment intensity also played a significant role in the treatment.
Professor Roi Cohen Kadosh from the University of Surrey said that the results have provided a twist that scientists must consider in future research:
""The common wisdom is that the same medical treatment would produce similar results across patients, but our latest study suggests a fascinating twist. While you'd expect uniform improvements in a group of people with depression undergoing the same neurostimulation treatment, outcomes can vary widely.
""What's truly eye-opening is that this variability could be largely influenced by the participants' own beliefs about the treatment they're receiving. In essence, if an individual believes they're receiving an effective treatment -- even when given a placebo -- that belief alone might contribute to significant improvements in their condition.""
In the first study analysed, 121 participants were treated with different forms of Transcranial Magnetic Stimulation (rTMS) for depression. The results showed that participants' perceptions about receiving real or placebo treatment mattered more than the actual type of rTMS in reducing depression.

The second study involved 52 older people with late-life depression who received either a real or placebo of deep rTMS. Surrey researchers found that the effect of treatment on reducing depression scores depended on the combination of the participants' perceptions about receiving real or placebo treatment and the actual treatment they received.
In the third dataset, researchers investigated the effects of home-based Transcranial Direct Current Stimulation (tDCS) treatment on 64 adults diagnosed with ADHD. At the end of the study, participants' beliefs about the treatment they thought they had received were also collected. This study differed from the first two as both the subjects' beliefs and the actual treatment had a dual effect on reducing inattention scores.
In the fourth study, 150 healthy participants got varying doses of tDCS for mind wandering. Those who believed they got a more potent dose reported more mind wandering, even if the actual treatment wasn't a factor.
The fifth study analysed the impact of transcranial random noise stimulation on working memory. Unlike previous studies, participants' beliefs didn't affect the results, highlighting the varying influence of beliefs in brain stimulation research. Thus, Roi Cohen Kadosh and his team show how subjective beliefs can vary in their effect on research -- from fully explaining results beyond the actual treatment, to interacting with the treatment, to having no influence at all.
Dr Shachar Hochman, a co-author on this work from the University of Surrey, said:
""The concept that a placebo or sham treatment can mimic genuine treatment effects is well-established in science. While researchers have closely monitored this phenomenon, it has been typically catalogued separately from the in-depth analyses of the actual treatment outcomes. What sets our study apart is that we have brought together these two datasets -- subjective beliefs and objective treatment measures. This has the potential to reveal new insights into treatment efficacy.""
Professor Roi Cohen Kadosh added:
""Our findings show that there could be real value in recording participants' subjective beliefs at multiple points in the experiment to better understand their impact and put forward the importance of sharing this data and incorporating it within the research process. Recording beliefs might be useful beyond the realms of neurostimulation -- we may find similar results in pharmacological studies and more state-of-the-art interventions such as virtual reality, and I would encourage other scientists to use our analytical approach to re-examine results in past interventions and to incorporate it in future ones.""

","score: 15.284912990692032, grade_level: '15'","score: 17.429055038445973, grade_levels: ['college_graduate'], ages: [24, 100]",10.7554/eLife.88889.1,"In recent years, there has been debate about the effectiveness of treatments from different fields, such as neurostimulation, neurofeedback, brain training, and pharmacotherapy. This debate has been fuelled by contradictory and nuanced experimental findings. Notably, the effectiveness of a given treatment is commonly evaluated by comparing the effect of the active treatment versus the placebo on human health and/or behaviour. However, this approach neglects the individual’s subjective experience of the type of treatment s/he received in establishing treatment efficacy. Here, we show that individual differences in subjective treatment—the thought of receiving the active or placebo condition during an experiment—can explain variability in outcomes better than the actual treatment. We analysed four independent datasets (N=387 participants), including clinical patients and healthy adults from different age groups who were exposed to different neurostimulation treatments (transcranial magnetic stimulation: Study 1 & 2; transcranial direct current stimulation: Study 3 & 4). Our findings consistently show that the inclusion of subjective treatment provides a better model fit than objective treatment alone—the condition to which participants are assigned in the experiment. These results demonstrate the significant contribution of subjective experience in explaining the variability of clinical, cognitive and behavioural outcomes. Based on these findings, we advocate for existing and future studies in clinical and non-clinical research to start accounting for participants’ subjective beliefs when assessing the efficacy of treatments. This approach will be crucial in providing a more accurate estimation of the treatment effect and its source, allowing the development of effective and reproducible interventions. We demonstrate that individual differences in subjective treatment—the belief of receiving the active or placebo condition during an experiment—can explain variability in research outcomes better than objective treatment, the actual treatment to which participants are assigned. Even though it is a standard practice for intervention studies to collect data on subjective treatment, its contribution to research outcomes has been overlooked. By demonstrating the explanatory power of subjective treatment beyond objective treatment in four independent datasets, we show its potential to provide further insights into the effectiveness of different interventions. We, therefore, encourage researchers to adopt our approach in existing and new studies, to improve experimental design and ultimately increase the rigour and robustness of clinical and non-clinical interventions."
"
Trying something new is a risk every child undertakes as they explore and learn about the world. While risk can be costly, it can also pay off in rewards or knowledge. But new research suggests children without predictable support from the adults in their lives are less willing to take those risks -- and reap those rewards.

""If you're in a resource-rich environment -- meaning for a child that you're safe, your meals are coming, someone is at home for you, you're surrounded by adults that are protecting you -- you'll try new things,"" says Seth Pollak, a University of Wisconsin-Madison professor of psychology who studies childhood adversity. ""And that's how you discover and learn about the world.""
But not every exploration will be rewarding and, according to a new study of childhood exploration and parental predictability that Pollak and collaborators today published in the Proceedings of the National Academy of Sciences, kids who don't believe they have the support of reliable parents are less willing to risk the unknown.
""What's unseen around that corner could be golden, but you could also end up in some bad situations,"" Pollak says. ""You could end up ordering a bad meal or touching something that hurts you. You could end up in a bad relationship or with an empty wallet. And so, we thought, in order to have the confidence to try something new, you have to feel like you're supported and relatively safe -- like you can afford to make a bad call.""
The researchers studied decisions that more than 150 children ages 10 to 13 made while playing games designed by C. Shawn Green, a UW-Madison psychology professor. The games offered the children opportunities to risk a little and explore for potential gains.
One game, fashioned after a pair of casino slot machines, gave players a history of payouts on just one of the machines -- information that helped them understand their expected winnings if they kept pulling that machine's handle. The other machine's history was a mystery, and investing a pull there was more of a risk, but also potentially a bigger return.
The other game, in which the kids collected apples in virtual orchards, featured diminishing returns as players continued to pick from an individual tree. With limited time, would the players move to new trees, with unknown bounties? Or would they plug away at the tree they knew best?

The kids and their parents also participated in a battery of surveys and assessments. The researchers gauged the stress the children experience and the predictability of their lives -- based on factors like parental job loss, divorce, death or illness in the family, and changing schools and homes -- as well as children's own views about whether or not their parents were reliable and predictable.
Yuyan Xu, a UW-Madison graduate student and first author of the study, asked children to respond to questions about how they've experienced their relationships, such as: When my parents say they're going to pick me up, can I count on them to be there? When my parent makes a promise, do they follow through on it? Do I typically know how my parents are going to react to different kinds of situations?
The less reliable and predictable the kids felt their parents were, the less likely they were to take exploratory risks in the games they played. They were less likely to give the mysterious slot machine a chance or choose to move to a different apple tree.
""The children from more stable backgrounds, they play around and experiment in our games. They use that to get a sense of how things work, maybe earning them more money or more points,"" Pollak says. ""Kids from unstable backgrounds just don't play that way. They stay within a narrower range of possibilities. They prefer to stick with what they already know, even if it's limited, rather than taking a chance at a higher possible reward.""
The researchers found those self-imposed limits on risk were not related to the more objective measures of stress and unpredictability on the kids' lives or even on parental reports that didn't necessarily agree with their child's perceptions of their relationships. There wasn't a correlation between lack of risk-taking and levels of anxiety or neuroses, or of the kids' feelings about the rest of the world outside their family. If they felt their parents were unreliable and unpredictable, they were less willing to explore.
""I think it makes sense,"" Pollak says. ""Their brains are doing exactly what we want our brains to do, right? If you really feel things are not predictable and you don't know how things are going to land, you'd stick to what works and what's familiar. You wouldn't waste your resources on something that could all fall apart.""
The researchers ran their experiments first with a group of nearly 80 kids, then repeated it with a second group of just over 80 more to confirm their results.

""The interesting thing here is that there seems to be a way in which our early childhood experiences are calibrating how we decide to make these decisions years and years down the line and in these really different kinds of situations,"" says Xu.
Openness to exploration wouldn't be the only important aspect of childhood enhanced by stability. Language development, sleep quality, stress regulation and other subjects of childhood development research have been tied to predictability in children's lives. Pollak plans to delve further into the relationship between predictability and exploration to see how rifts might be healed.
""What can we do for kids who view their history of interpersonal relationships as unstable?"" he says. ""We might not be able to change the relationships by the time we understand them to be unpredictable. But could we change the way kids think about them, how they act on them? If that is flexible, maybe we can tune those kids into the benefits and rewards of exploration to help foster kids' learning.""
This research was supported by grants from the National Institutes of Health (R01MH61285 and P50HD105353).

","score: 10.748139324735074, grade_level: '11'","score: 11.904674279142363, grade_levels: ['12'], ages: [17, 18]",10.1073/pnas.2303869120,"Early in development, the process of exploration helps children gather new information that fosters learning about the world. Yet, it is unclear how childhood experiences may influence the way humans approach new learning. What influences decisions to exploit known, familiar options versus trying a novel alternative? We found that childhood unpredictability, characterized by unpredictable caregiving and unstable living environments, was associated with reduced exploratory behavior. This effect holds while controlling for individual differences, including anxiety and stress. Individuals who perceived their childhoods as unpredictable explored less and were instead more likely to repeat previous choices (habitual responding). They were also more sensitive to uncertainty than to potential rewards, even when the familiar options yielded lower rewards. We examined these effects across multiple task contexts and via both in-person (N = 78) and online replication (N = 84) studies among 10- to 13-y-olds. Results are discussed in terms of the potential cascading effects of unpredictable environments on the development of decision-making and the effects of early experience on subsequent learning."
"
Experiences of discrimination and acculturation are known to have a detrimental effect on a person's health. For pregnant women, these painful experiences can also affect the brain circuitry of their children, a new study from Yale and Columbia University finds. These effects, the researchers say, are separate from those caused by general stress and depression.

The study was published in the journal Neuropsychopharmacology.
Previous research has shown that not only are high levels of stress and depression harmful to the person experiencing them, but they can also have long-lasting effects on their children if experienced during pregnancy. In recent years, studies have also revealed that discrimination and acculturation -- or the changes that occur due to migration and the subsequent balancing of multiple, different cultures -- can affect the adult brain. What's less clear is how children might be affected by their parents' experiences of discrimination and acculturation.
For the new study, the researchers assessed the degree of discrimination, acculturation, and distress experienced by 165 people while pregnant using established questionnaires. The participants were 14 to 19 years old, mostly Hispanic (88%), and lived in or near the Washington Heights neighborhood of New York City. The researchers then performed magnetic resonance imaging (MRI) to evaluate brain connectivity in 38 of the participants' infants after birth.
The first step, researchers said, was to determine whether discrimination and acculturation are distinct from other types of stress or depression.
""We thought that some of these experiences might go hand-in-hand or overlap, in which case it would be difficult to measure the effects of discrimination or acculturation on their own,"" said Dustin Scheinost, associate professor of radiology and biomedical imaging at Yale School of Medicine and senior author of the study.
Scheinost and his colleagues from Columbia and Children's Hospital of Los Angeles used a data analysis program that assessed all of their separate questionnaire measures of acculturation, discrimination, stress, depression, childhood trauma, and socioeconomic status, and organized them into groups by how similar the data anlaysis program determined them to be. Doing this, researchers say, helped them understand the degree to which different measures might be used to evaluate similar experiences.

""That analysis clustered measures of stress and depression and separately pulled out discrimination and acculturation measures as their own distinct variables,"" said Scheinost. ""That told us that while these experiences of discrimination are related to stress and depression, they are separate enough that we can look at their unique effects.""
When the research team analyzed the MRI images of the infants' brains, they found differences in the children whose parents reported experiencing discrimination while pregnant.
The amygdala is an area of the brain associated with emotional processing and it's very vulnerable to prenatal stress, said the researchers. Prior research has found that early experiences of adversity can have measurable impacts on amygdala connectivity in infants, children, adolescents, and adults. A growing body of evidence also suggests the amygdala is involved in ethnic and racial processing, such as differentiating faces of people from different races or ethnicities, for example.
When the researchers assessed connectivity between the amygdala and another region of the brain called the prefrontal cortex, which is associated with higher-order functioning, they found that children of people who experienced more discrimination while pregnant had weaker connectivity between the two brain regions.
""Our finding was consistent with what you expect to see in the brain of those affected by early life adversity either pre- or postnatally,"" said Scheinost.
The takeaway, said Scheinost, is that while discrimination and acculturation affect the brain in ways other types of stress do, there is something unique and important about these particular experiences that should be better understood. Future research, he said, should focus on whether other populations are affected in similar ways and what underlies the effects.
""We don't fully know why this happens,"" said Scheinost. ""So we need to investigate the biological mechanisms that carry these experiences of adversity from parent to offspring.""

","score: 15.900397838335056, grade_level: '16'","score: 17.548075198344257, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41386-023-01765-3,"The experience of ethnic, racial, and structural inequalities is increasingly recognized as detrimental to health, and early studies suggest that its experience in pregnant mothers may affect the developing fetus. We characterized discrimination and acculturation experiences in a predominantly Hispanic sample of pregnant adolescent women and assessed their association with functional connectivity in their neonate’s brain. We collected self-report measures of acculturation, discrimination, maternal distress (i.e., perceived stress, childhood trauma, and depressive symptoms), and socioeconomic status in 165 women. Then, we performed a data-driven clustering of acculturation, discrimination, perceived stress, depressive symptoms, trauma, and socioeconomic status variables during pregnancy to determine whether discrimination or acculturation clustered into distinct factors. Discrimination and acculturation styles loaded onto different factors from perceived stress, depressive symptoms, trauma, and socioeconomic status, suggesting that they were distinct from other factors in our sample. We associated these data-driven maternal phenotypes (discrimination and acculturation styles) with measures of resting-state functional MRI connectivity of the infant amygdala (n = 38). Higher maternal report of assimilation was associated with weaker connectivity between their neonate’s amygdala and bilateral fusiform gyrus. Maternal experience of discrimination was associated with weaker connectivity between the amygdala and prefrontal cortex and stronger connectivity between the amygdala and fusiform of their neonate. Cautiously, the results may suggest a similarity to self-contained studies with adults, noting that the experience of discrimination and acculturation may influence amygdala circuitry across generations. Further prospective studies are essential that consider a more diverse population of minoritized individuals and with a comprehensive assessment of ethnic, racial, and structural factors."
"
If you've had a near miss accident in your car or suffered the intimidation of a menacing person, you've probably felt it -- a psychological reaction to a threat called a fight or flight response. Your heart rate climbs, anxiety washes over you, you might shake or sweat.

But hours after that stress passes, you may feel another response -- a powerful desire for comfort food, that highly processed, high-fat stuff you know isn't good for you. It can relieve stress and tension and provide a sense of control. Emotional eating following a stress-triggering interaction is familiar to many of us, and to scientists as well.
But how a threat signals your brain to want comfort food has been unknown.
Now, a Virginia Tech scientist has pinpointed a molecule found in a region of the brain called the hypothalamus that is connected to changes in the brain that lead to emotional overeating. Sora Shin, assistant professor at the Fralin Biomedical Research Institute at VTC, and her research team described the discovery in a paper published Oct. 28 in Nature Communications.
""We don't always eat because we are hungry and we have certain physical needs,"" said Shin, who is also an assistant professor in the Department of Human Nutrition, Foods, and Exercise in Virginia Tech's College of Agriculture and Life Sciences. ""Whenever we get stressed or feel some threat, then it can also trigger our eating motivation. We think this molecule is the culprit.""
Shin and her research team began their study by investigating a small molecule, Proenkephalin. This molecule is common in multiple parts of the brain, but little research had examined its role in the hypothalamus. Shin suspected it played a role in stress and eating because the hypothalamus is a center for regulating eating behavior.
The lab exposed mice to the odor of cat feces. The odor of a natural predator triggered a threat response in the mice, and 24 hours later, the mice exhibited a negative emotional state, overeating behavior, and neurons in their brains showed sensitivity to consumption of high-fat foods.

To confirm the role of the molecule in stress-induced eating, the researchers activated the same neurons artificially with light stimulating a genetically encoded molecule expressed in the neuronal cell's membrane, without the predator scent, and saw a similar response. In addition, when they exposed the mice to the cat odor and quieted the reaction of the neurons expressing that molecule with the same technique, the mice showed no negative emotional state and didn't overeat.
""So something about this molecule itself is very critical to inducing overconsumption after the threat,"" Shin said.
The discovery points toward a possible target for therapy to alleviate emotionally triggered eating.
""We have much more to learn about this molecule,"" Shin said, ""but we found its location and it could be a good starting point.""
Shin's first-authors on the study are In-Jee You, a former research associate at the institute, and Yeeun Bae, a human nutrition, foods, and exercise graduate student working in her lab.

","score: 13.120042613636365, grade_level: '13'","score: 13.415992542613637, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41467-023-42623-6,"Psychological stressors, like the nearby presence of a predator, can be strong enough to induce physiological/hormonal alterations, leading to appetite changes. However, little is known about how threats can alter feeding-related hypothalamic circuit functions. Here, we found that proenkephalin (Penk)-expressing lateral hypothalamic (LHPenk) neurons of mice exposed to predator scent stimulus (PSS) show sensitized responses to high-fat diet (HFD) eating, whereas silencing of the same neurons normalizes PSS-induced HFD overconsumption associated with a negative emotional state. Downregulation of endogenous enkephalin peptides in the LH is crucial for inhibiting the neuronal and behavioral changes developed after PSS exposure. Furthermore, elevated corticosterone after PSS contributes to enhance the reactivity of glucocorticoid receptor (GR)-containing LHPenk neurons to HFD, whereas pharmacological inhibition of GR in the LH suppresses PSS-induced maladaptive behavioral responses. We have thus identified the LHPenk neurons as a critical component in the threat-induced neuronal adaptation that leads to emotional overconsumption."
"
Parents who send their children to child care can breathe a little easier -- research published in JAMA Network Open from experts at Michigan Medicine, the University of Pittsburgh School of Medicine, and UPMC Children's Hospital of Pittsburgh shows that children in daycare were not significant spreaders of COVID-19.

The study found that transmission rates of SARS-CoV-2 within child care centers was only about 2% to 3%, suggesting that children and caregivers were not spreading COVID at significant rates to others in the centers.
The study also found low rates of infection among households that had kids attending child care centers, as only 17% of household infections resulted from children who caught COVID at their centers.
Overall, the study found that only 1 in 20 symptomatic children attending child care centers tested positive for the virus.
In contrast, once someone in a household tested positive for the coronavirus, transmission to other household members was high, at 50% for children and 67% for adults.
Young children frequently contracted COVID-19 from individuals outside their child care center.
Additional safety measures
Despite the low rates of transmission in child care centers, experts still highly recommend that families get themselves and their children vaccinated against COVID-19, as additional research shows that vaccines are a safe and effective way of preventing against serious infection.

""We strongly recommend the COVID-19 vaccine for young children to disrupt the high rates of transmission that we saw occur in households that can lead to missed work and school,"" said Andrew Hashikawa, M.D., clinical professor of emergency medicine.
The Centers for Disease Control and Prevention currently advises that kids with congestion, runny noses or other respiratory symptoms get tested for COVID and stay home if positive.
The findings suggest that these recommendations could be revised to align with those of other serious respiratory viruses, like influenza and respiratory syncytial virus, commonly known as RSV.
""While it's crucial to remain vigilant in our efforts to manage the spread of SARS-CoV-2, it seems that prioritizing testing and extended exclusion periods for children in child care centers may not be the most practical approach, as it can place undue financial burden on families from frequent testing, result in missed work, and hinder children's critical access to quality care and education,"" said Hashikawa.
This study was supported by Merck Investigator Studies Program grant 60418, the Henry L. Hillman Foundation and Flu Lab.

","score: 17.45941176470588, grade_level: '17'","score: 20.33213235294118, grade_levels: ['college_graduate'], ages: [24, 100]",10.1001/jamanetworkopen.2023.39355,"SARS-CoV-2 surveillance studies in US child care centers (CCCs) in the post–COVID-19 vaccine era are needed to provide information on incidence and transmission in this setting. To characterize SARS-CoV-2 incidence and transmission in children attending CCCs (students) and their child care providers (CCPs) and household contacts. This prospective surveillance cohort study was conducted from April 22, 2021, through March 31, 2022, and included 11 CCCs in 2 cities. A subset (surveillance group) of CCPs and students participated in active surveillance (weekly reverse transcription–polymerase chain reaction [RT-PCR] swabs, symptom diaries, and optional baseline and end-of-study SARS-CoV-2 serologic testing), as well as all household contacts of surveillance students. Child care center directors reported weekly deidentified self-reported COVID-19 cases from all CCPs and students (self-report group). SARS-CoV-2 infection in CCC students. SARS-CoV-2 incidence, secondary attack rates, and transmission patterns were determined from diary entries, self-reports to CCC directors, and case logs. Incidence rate ratios were measured using Poisson regression clustering on centers with a random intercept and unstructured matrix. From a total population of 1154 students and 402 CCPs who self-reported cases to center directors, 83 students (7.2%; mean [SD] age, 3.86 [1.64] years; 55 male [66%]), their 134 household contacts (118 adults [mean (SD) age, 38.39 (5.07) years; 62 female (53%)], 16 children [mean (SD) age, 4.73 (3.37) years; 8 female (50%)]), and 21 CCPs (5.2%; mean [SD] age, 38.5 [12.9] years; 18 female [86%]) participated in weekly active surveillance. There were 154 student cases (13%) and 87 CCP cases (22%), as defined by positive SARS-CoV-2 RT-PCR or home antigen results. Surveillance students had a higher incidence rate than self-report students (incidence rate ratio, 1.9; 95% CI, 1.1-3.3; P = .01). Students were more likely than CCPs to have asymptomatic infection (34% vs 8%, P &amp;lt; .001). The CCC secondary attack rate was 2.7% to 3.0%, with the upper range representing possible but not definite secondary cases. Whether the index case was a student or CCP, transmission within the CCC was not significantly different. Household cumulative incidence was 20.5%, with no significant difference in incidence rate ratio between adults and children. Household secondary attack rates were 50% for children and 67% for adults. Of 30 household cases, only 5 (17%) represented secondary infections caused by 3 students who acquired SARS-CoV-2 from their CCC. Pre- and poststudy seroprevalence rates were 3% and 22%, respectively, with 90% concordance with antigen or RT-PCR results. In this study of SARS-CoV-2 incidence and transmission in CCCs and students’ households, transmission within CCCs and from children infected at CCCs into households was low. These findings suggest that current testing and exclusion recommendations for SARS-CoV-2 in CCCs should be aligned with those for other respiratory viruses with similar morbidity and greater transmission to households."
"
Whether infants at five months of age look mostly at faces or non-social objects such as cars or mobile phones is largely determined by genes. This has now been demonstrated by researchers at Uppsala University and Karolinska Institutet. The findings suggest that there is a biological basis for how infants create their unique visual experiences and which things they learn most about. The study has been published in the scientific journal Nature Human Behaviour.

The way in which we explore our environment with our eyes affects what we notice, think about and learn. The new study analysed preference for faces versus non-social objects in more than 500 infant twins.
""Our results suggest that even before infants can influence and choose their environment by pointing, crawling or walking, they create their own unique perceptual experiences by systematically looking more at social or non-social objects, preferences that can be largely explained by genetic differences between children,"" says Ana Maria Portugal, Postdoctoral Researcher and first author of the study.
The children's gaze was measured using an infant friendly eye tracker. The results showed that individual infants' preference for faces could be largely explained by their genetics. The family environment did not explain preferences for social versus non-social information this early in life.
The researchers found that more looking at faces versus non-social objects at five months of age was associated with having a larger vocabulary in the second year of life. This supports the view that there is a link between early looking preferences and later development.
Differences in looking behaviour can potentially affect the interaction between parent and child.
Whether an infant looks at faces or not is a strong signal to other people and can influence parents' behaviour towards their child. However, it should be remembered that looking at a lot of non-social objects is not necessarily negative -- it is also important for cognitive development, explains Portugal.

The study is part of the Babytwins Study Sweden (BATSS) research project where identical and fraternal twins have been tested using different child-friendly methods at the Karolinska Institutet Center for Neurodevelopmental Disorders (KIND). The children were followed from five months to three years of age, but eye tracking was only conducted at 5 months.
Portugal found that the looking preferences of the genetically identical twins were more similar than those of the fraternal twins. For example: if one identical twin in the pair looked mostly at non-social objects, there was a large chance that the other twin had the same preference. In contrast to identical twins, fraternal twins on average share only 50% of their genes. Their looking preferences tended to be less similar within each pair of twins.
In addition to the link to later language development, the researchers examined whether visual preferences could predict whether the infants, later in childhood, exhibited behaviours characteristic of autism, which is defined by difficulties with social communication. They also tested whether there were gender differences in facial preference.
""Our results indicate that face preference in infants is not strongly associated with social communication ability later in childhood. We also found no difference between boys and girls in terms of preference for faces versus non-social objects,"" notes Terje Falck-Ytter, Professor at the Department of Psychology at Uppsala university and principal investigator in the BATSS study.
""Moreover, our data showed that the genes which influence facial preference are not the same as those involved in eye contact -- that is, whether infants looked primarily at the eyes or the mouth when looking at a face. It's fascinating that two basic social behaviours like looking at faces and looking at eyes have different genetic and probably evolutionary bases,"" adds Professor Falck-Ytter.

","score: 13.939732877123479, grade_level: '14'","score: 15.301679572603398, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41562-023-01764-w,"To what extent do individual differences in infants’ early preference for faces versus non-facial objects reflect genetic and environmental factors? Here in a sample of 536 5-month-old same-sex twins, we assessed attention to faces using eye tracking in two ways: initial orienting to faces at the start of the trial (thought to reflect subcortical processing) and sustained face preference throughout the trial (thought to reflect emerging attention control). Twin model fitting suggested an influence of genetic and unique environmental effects, but there was no evidence for an effect of shared environment. The heritability of face orienting and preference were 0.19 (95% confidence interval (CI) 0.04 to 0.33) and 0.46 (95% CI 0.33 to 0.57), respectively. Face preference was associated positively with later parent-reported verbal competence (β = 0.14, 95% CI 0.03 to 0.25, P = 0.014, R2 = 0.018, N = 420). This study suggests that individual differences in young infants’ selection of perceptual input—social versus non-social—are heritable, providing a developmental perspective on gene–environment interplay occurring at the level of eye movements."
"
The use of synthetic phonics to teach reading to children in reception (age 4-5) classes has improved attainment. A new study shows that extra help in blending the sounds in words is most effective in improving the skills essential for reading.

New research at Aston University has shown that extra practice in blending printed letter sounds can help struggling beginner readers in reception classes to learn to read.
Children in England learn to read through a system known as synthetic phonics, where they are taught the sounds of letters, 'phonemes', and how these sounds are written, 'graphemes'. For example, 'my' and 'lie' have the same phoneme at the end, but different graphemes. Pupils learn to identify graphemes, match them with phonemes, and blend the phonemes together to form the sound of the complete words (for example c-a-t = ""k -- æ -- t"" = ""cat""). This is known as blending. To learn successfully in this way, children need 'letter sound knowledge' (LSK) -- awareness of the sounds represented by letters/graphemes and 'phonological awareness' (PA) -- the awareness of individual sounds in words.
While the use of phonics has been shown to increase reading attainment in children, many teachers are not sure what additional support is most beneficial for those who are still struggling. They will often give extra LSK training using flashcards showing each letter or letter combination.
The new research, led by Dr Laura Shapiro in the School of Psychology, shows that extra training on how to blend printed letter sounds is most beneficial because this type of training had the biggest impact on PA, which is an essential skill for learning to read.
The researchers worked with teachers to identify children struggling with reading and recruited 222 children from 12 primary schools for the study, working with each child for half a term.
The researchers compared three key components of early reading -- sounding out printed letters, blending the sounds out loud, and both sounding out and blending printed letter sounds. In each session, the researchers showed the children one word at a time, without a picture or sentence context, to allow the child to focus on the target word. After the child was helped to read the word, they were shown an illustration relating to the word and a related sentence was read out loud to them. This context made the task enjoyable and meaningful and often prompted chats and interaction.

Dr Shapiro said:
""Many teachers already give extra support to struggling readers in reception, and often give support on learning letter sounds. Although practice on letter sounds is helpful, our study suggests it is more beneficial to give children extra practice in sounding out the letters AND blending the sounds together to make a word."" The researchers carefully controlled the conditions of the study to identify exactly which component of reading it was most crucial to support. The training used the same standardised instructions, pictures, and context sentences, with only the focus of the training changing.
Dr Shapiro says that they will now work with teachers to develop a strategy suitable for the classroom. Together, they will identify the most practical and enjoyable ways to provide PA support to children and develop effective strategies that can be shared for other teachers to use and adapt.
Dr Shapiro has this advice for teachers and parents:
""Help children to practice blending the sounds in words, such as 'm-u-ch' makes 'much', and do this whilst pointing at the letters in the printed word so that they can see the connection between the letters, their sounds and the blended word. Children enjoyed seeing the word put into context with a picture and a fun sentence. It helps to keep the reading part simple, for example, show them just one word and hide the rest of the page and the picture. Then once you've supported them to sound this word out, show the picture, read the remainder out loud and give them an opportunity to talk about the story.""
 

","score: 12.148874856486803, grade_level: '12'","score: 14.3508392652124, grade_levels: ['college_graduate'], ages: [24, 100]",10.1111/bjep.12641,"Despite evidence that synthetic phonics teaching has increased reading attainments, a sizable minority of children struggle to acquire phonics skills and teachers lack clear principles for deciding what types of additional support are most beneficial. Synthetic phonics teaches children to read using a decoding strategy to translate letters into sounds and blend them (e.g., c‐a‐t = “k ‐ æ – t” = “cat”). To use a decoding strategy, children require letter‐sound knowledge (LSK) and the ability to blend sound units (phonological awareness; PA). Training on PA has been shown to benefit struggling beginning readers. However, teachers in English primary schools do not routinely check PA. Instead, struggling beginner readers usually receive additional LSK support. Until now, there has been no systematic comparison of the effectiveness of training on each component of the decoding process. Should additional support for struggling readers focus on improving PA, or on supplementary LSK and/or decoding instruction? We aim to increase understanding of the roles of LSK and PA in children's acquisition of phonics skills and uncover which types of additional training are most likely to be effective for struggling beginner readers. We will compare training on each of these components, using a carefully controlled experimental design. We will identify reception‐age children at risk of reading difficulties (target n = 225) and randomly allocate them to either PA, LSK or decoding (DEC) training. We will test whether training type influences post‐test performance on word reading and whether any effects depend on participants' pre‐test PA and/or LSK. Two hundred and twenty‐two participants completed the training. Planned analyses showed no effects of condition on word reading. However, exploratory analyses indicated that the advantage of trained over untrained words was significantly greater for the PA and DEC conditions. There was also a significantly greater improvement in PA for the DEC condition. Overall, our findings suggest a potential advantage of training that includes blending skills, particularly when decoding words that had been included in training. Future research is needed to develop a programme of training on blending skills combined with direct vocabulary instruction for struggling beginner readers."
"
When you eagerly dig into a long-awaited dinner, signals from your stomach to your brain keep you from eating so much you'll regret it -- or so it's been thought. That theory had never really been directly tested until a team of scientists at UC San Francisco recently took up the question.

The picture, it turns out, is a little different.
The team, led by Zachary Knight, PhD, a UCSF professor of physiology in the Kavli Institute for Fundamental Neuroscience, discovered that it's our sense of taste that pulls us back from the brink of food inhalation on a hungry day. Stimulated by the perception of flavor, a set of neurons -- a type of brain cell -- leaps to attention almost immediately to curtail our food intake.
""We've uncovered a logic the brainstem uses to control how fast and how much we eat, using two different kinds of signals, one coming from the mouth, and one coming much later from the gut,"" said Knight, who is also an investigator with the Howard Hughes Medical Institute and a member of the UCSF Weill Institute for Neurosciences. ""This discovery gives us a new framework to understand how we control our eating.""
The study, which appears Nov. 22, 2023 in Nature, could help reveal exactly how weight-loss drugs like Ozempic work, and how to make them more effective.
New views into the brainstem 
Pavlov proposed over a century ago that the sight, smell and taste of food are important for regulating digestion. More recent studies in the 1970s and 1980s have also suggested that the taste of food may restrain how fast we eat, but it's been impossible to study the relevant brain activity during eating because the brain cells that control this process are located deep in the brainstem, making them hard to access or record in an animal that's awake.

Over the years, the idea had been forgotten, Knight said.
New techniques developed by lead author Truong Ly, PhD, a graduate student in Knight's lab, allowed for the first-ever imaging and recording of a brainstem structure critical for feeling full, called the nucleus of the solitary tract, or NTS, in an awake, active mouse. He used those techniques to look at two types of neurons that have been known for decades to have a role in food intake.
The team found that when they put food directly into the mouse's stomach, brain cells called PRLH (for prolactin-releasing hormone) were activated by nutrient signals sent from the GI tract, in line with traditional thinking and the results of prior studies.
However, when they allowed the mice to eat the food as they normally would, those signals from the gut didn't show up. Instead, the PRLH brain cells switched to a new activity pattern that was entirely controlled by signals from the mouth.
""It was a total surprise that these cells were activated by the perception of taste,"" said Ly. ""It shows that there are other components of the appetite-control system that we should be thinking about.""
While it may seem counterintuitive for our brains to slow eating when we're hungry, the brain is actually using the taste of food in two different ways at the same time. One part is saying, ""This tastes good, eat more,"" and another part is watching how fast you're eating and saying, ""Slow down or you're going to be sick.""
""The balance between those is how fast you eat,"" said Knight.

The activity of the PRLH neurons seems to affect how palatable the mice found the food, Ly said. That meshes with our human experience that food is less appetizing once you've had your fill of it.
Brain cells that inspire weight-loss drugs
The PRLH-neuron-induced slowdown also makes sense in terms of timing. The taste of food triggers these neurons to switch their activity in seconds, from keeping tabs on the gut to responding to signals from the mouth.
Meanwhile, it takes many minutes for a different group of brain cells, called CGC neurons, to begin responding to signals from the stomach and intestines. These cells act over much slower time scales -- tens of minutes -- and can hold back hunger for a much longer period of time.
""Together, these two sets of neurons create a feed-forward, feed-back loop,"" said Knight. ""One is using taste to slow things down and anticipate what's coming. The other is using a gut signal to say, 'This is how much I really ate. Ok, I'm full now!'""
The CGC brain cells' response to stretch signals from the gut is to release GLP-1, the hormone mimicked by Ozempic, Wegovy and other new weight-loss drugs.
These drugs act on the same region of the brainstem that Ly's technology has finally allowed researchers to study. ""Now we have a way of teasing apart what's happening in the brain that makes these drugs work,"" he said.
A deeper understanding of how signals from different parts of the body control appetite would open doors to designing weight-loss regimens designed for the individual ways people eat by optimizing how the signals from the two sets of brain cells interact, the researchers said.
The team plans to investigate those interactions, seeking to better understand how taste signals from food interact with feedback from the gut to suppress our appetite during a meal.

","score: 11.199403794037945, grade_level: '11'","score: 12.757511702389756, grade_levels: ['college'], ages: [18, 24]",10.1038/s41586-023-06758-2,"The termination of a meal is controlled by dedicated neural circuits in the caudal brainstem. A key challenge is to understand how these circuits transform the sensory signals generated during feeding into dynamic control of behaviour. The caudal nucleus of the solitary tract (cNTS) is the first site in the brain where many meal-related signals are sensed and integrated1–4, but how the cNTS processes ingestive feedback during behaviour is unknown. Here we describe how prolactin-releasing hormone (PRLH) and GCG neurons, two principal cNTS cell types that promote non-aversive satiety, are regulated during ingestion. PRLH neurons showed sustained activation by visceral feedback when nutrients were infused into the stomach, but these sustained responses were substantially reduced during oral consumption. Instead, PRLH neurons shifted to a phasic activity pattern that was time-locked to ingestion and linked to the taste of food. Optogenetic manipulations revealed that PRLH neurons control the duration of seconds-timescale feeding bursts, revealing a mechanism by which orosensory signals feed back to restrain the pace of ingestion. By contrast, GCG neurons were activated by mechanical feedback from the gut, tracked the amount of food consumed and promoted satiety that lasted for tens of minutes. These findings reveal that sequential negative feedback signals from the mouth and gut engage distinct circuits in the caudal brainstem, which in turn control elements of feeding behaviour operating on short and long timescales."
"
Eating more ultra-processed foods (UPFs) may be associated with a higher risk of developing cancers of upper aerodigestive tract (including the mouth, throat and esophagus), according to a new study led by researchers from the University of Bristol and the International Agency for Research on Cancer (IARC). The authors of this international study, which analysed diet and lifestyle data on 450,111 adults who were followed for approximately 14 years, sayobesity associated with the consumption of UPFs may not be the only factor to blame. The study is published today [22 November] in the European Journal of Nutrition.

Several studies have identified an association between UPF consumption and cancer, including a recent study which looked at the association between UPFs and 34 different cancers in the largest cohort study in Europe, the European Prospective Investigation into Cancer and Nutrition (EPIC) cohort.
As more evidence emerges about the associations between eating UPFs and adverse health outcomes, researchers from the Bristol Medical School and IARC wanted to explore this further. Since many UPFs have an unhealthy nutritional profile, the team sought to establish whether the association between UPF consumption and head and neck cancer and esophageal adenocarcinoma (a cancer of the esophagus) in EPIC could be explained by an increase in body fat.
Results from the team's analyses showed that eating 10% more UPFs is associated with a 23% higher risk of head and neck cancer and a 24% higher risk of esophageal adenocarcinoma in EPIC. Increased body fat only explained a small proportion of the statistical association between UPF consumption and the risk of these upper-aerodigestive tract cancers.
Fernanda Morales-Berstein, a Wellcome Trust PhD student at the University of Bristol and the study's lead author, explained: ""UPFs have been associated with excess weight and increased body fat in several observational studies. This makes sense, as they are generally tasty, convenient and cheap, favouring the consumption of large portions and an excessive number of calories. However, it was interesting that in our study the link between eating UPFs and upper-aerodigestive tract cancer didn't seem to be greatly explained by body mass index and waist-to-hip ratio.""
The authors suggest that other mechanisms could explain the association. For example, additives including emulsifiers and artificial sweeteners which have been previously associated with disease risk, and contaminants from food packaging and the manufacturing process, may partly explain the link between UPF consumption and upper-aerodigestive tract cancer in this study.
However, Fernanda Morales-Berstein and colleagues did add caution regarding their findings and suggest that the associations between UPF consumption and upper-aerodigestive tract cancers found in the study could be affected by certain types of bias. This would explain why they found evidence of an association between higher UPF consumption and increased risk of accidental deaths, which is highly unlikely to be causal.

George Davey Smith, Professor of Clinical Epidemiology and Director of the MRC Integrative Epidemiology Unit at the University of Bristol, and co-author on the paper, said: ""UPFs are clearly associated with many adverse health outcomes, yet whether they actually cause these, or whether underlying factors such as general health-related behaviours and socioeconomic position are responsible for the link, is still unclear, as the association with accidental deaths draws attention to.""
Inge Huybrechts, Team head of the Lifestyle exposures and interventions team at IARC, added: ""Cohorts with long-term dietary follow-up intake assessments, considering also contemporary consumption habits, are needed to replicate these study's findings, as the EPIC dietary data were collected in the 1990s, when the consumption of UPFs was still relatively low. As such associations may potentially be stronger in cohorts including recent dietary follow-up assessments.""
Further research is needed to identify other mechanisms, such as food additives and contaminants, which may explain the links observed. However, based on the finding that body fat did not greatly explain the link between UPF consumption and upper-aerodigestive tract cancer risk in this study, Fernanda Morales-Berstein, suggested: ""Focussing solely on weight loss treatment, such as Semaglutide, is unlikely to greatly contribute to the prevention of upper-aerodigestive tract cancers related to eating UPFs.""
Dr Helen Croker, Assistant Director of Research and Policy at World Cancer Research Fund, added: ""This study adds to a growing pool of evidence suggesting a link between UPFs and cancer risk. The association between a higher consumption of UPFs and an increased risk of developing upper-aerodigestive tract cancer supports our Cancer Prevention Recommendations to eat a healthy diet, rich in wholegrains, vegetables, fruit, and beans.""
The study was funded by the Wellcome Trust; Cancer Research UK; World Cancer Research Fund International; Institut National du Cancer; Horizon 2020 'Dynamic longitudinal exposome trajectories in cardiovascular and metabolic non-communicable diseases' study; University of Bristol Vice Chancellor's Fellowship; British Heart Foundation and the Medical Research Council.

","score: 19.95224256292906, grade_level: '20'","score: 22.365833605753515, grade_levels: ['college_graduate'], ages: [24, 100]",10.1007/s00394-023-03270-1,"To investigate the role of adiposity in the associations between ultra-processed food (UPF) consumption and head and neck cancer (HNC) and oesophageal adenocarcinoma (OAC) in the European Prospective Investigation into Cancer and Nutrition (EPIC) cohort. Our study included 450,111 EPIC participants. We used Cox regressions to investigate the associations between the consumption of UPFs and HNC and OAC risk. A mediation analysis was performed to assess the role of body mass index (BMI) and waist-to-hip ratio (WHR) in these associations. In sensitivity analyses, we investigated accidental death as a negative control outcome. During a mean follow-up of 14.13 ± 3.98 years, 910 and 215 participants developed HNC and OAC, respectively. A 10% g/d higher consumption of UPFs was associated with an increased risk of HNC (hazard ratio [HR] = 1.23, 95% confidence interval [CI] 1.14–1.34) and OAC (HR = 1.24, 95% CI 1.05–1.47). WHR mediated 5% (95% CI 3–10%) of the association between the consumption of UPFs and HNC risk, while BMI and WHR, respectively, mediated 13% (95% CI 6–53%) and 15% (95% CI 8–72%) of the association between the consumption of UPFs and OAC risk. UPF consumption was positively associated with accidental death in the negative control analysis. We reaffirmed that higher UPF consumption is associated with greater risk of HNC and OAC in EPIC. The proportion mediated via adiposity was small. Further research is required to investigate other mechanisms that may be at play (if there is indeed any causal effect of UPF consumption on these cancers)."
"
Babies as young as four months old can make sense of how their bodies interact with the space around them, according to new research from the University of Birmingham.

The findings, published today (21 November 2023) in Scientific Reports, shed new light on how self-awareness develops.
Experts from the Birmingham BabyLab showed babies a ball on a screen moving towards or away from them. When the ball was closest to them on the screen, the babies were presented with a 'touch' (a small vibration) on their hands, whilst their brain activity was being measured. The data collection for the study was conducted at Goldsmiths (University of London).
The researchers found that from just four months old, babies show enhanced somatosensory (tactile) brain activity when a touch is preceded by an object moving towards them.
Dr Giulia Orioli, Research Fellow in Psychology at the University of Birmingham, who led the study said: ""Our findings indicate that even in the first few months of life, before babies have even learned to reach for objects, the multisensory brain is wired up to make links between what babies see and what they feel. This means they can sense the space around them and understand how their bodies interact with that space. This is sometimes referred to as peripersonal space.
""Of course, humans do this all the time as adults, using our combined senses to perceive where we are in space and making predictions about when we will touch an object or not. But now that we know that babies in the early stages of their development begin to show signs of this, it opens up questions about how much of these abilities are learnt, or innate.""
The researchers also explored how an unexpected 'touch' would affect some of the older babies in the study. They found that in babies aged eight months old when the touch on their hand was preceded by the ball on the screen moving away from them, the babies' brain activity showed signs that they were surprised.
Andrew Bremner, Professor of Developmental Psychology, commented: ""Seeing the older babies show surprise responses suggests that they had not expected the touch due to the visual direction the object was moving in. This indicates that as babies proceed through their first year of life, their brains construct a more sophisticated awareness of how their body exists in the space around them.""
Next, the researchers are hoping to follow up this study with younger and older participants. Research with adults can illuminate the kinds of brain activity which infants are developing towards. They are also hoping to be able to see if there are early signs of these ""multisensory"" abilities in newborn babies.
Dr Orioli concluded: ""It is a challenge working with newborns, as they spend such a large portion of their time sleeping and eating, but we are starting to have some success working with this age group, and it is going to be fascinating to see if babies only a few days old have the foundations of a sense of their bodies in space. If so, it could be that we are looking at the origins of human consciousness.""

","score: 12.558179104477613, grade_level: '13'","score: 13.973432835820901, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41598-023-45897-4,"We asked whether, in the first year of life, the infant brain can support the dynamic crossmodal interactions between vision and somatosensation that are required to represent peripersonal space. Infants aged 4 (n = 20, 9 female) and 8 (n = 20, 10 female) months were presented with a visual object that moved towards their body or receded away from it. This was presented in the bottom half of the screen and not fixated upon by the infants, who were instead focusing on an attention getter at the top of the screen. The visual moving object then disappeared and was followed by a vibrotactile stimulus occurring later in time and in a different location in space (on their hands). The 4-month-olds’ somatosensory evoked potentials (SEPs) were enhanced when tactile stimuli were preceded by unattended approaching visual motion, demonstrating that the dynamic visual-somatosensory cortical interactions underpinning representations of the body and peripersonal space begin early in the first year of life. Within the 8-month-olds’ sample, SEPs were increasingly enhanced by (unexpected) tactile stimuli following receding visual motion as age in days increased, demonstrating changes in the neural underpinnings of the representations of peripersonal space across the first year of life."
"
Intense focus pervades the EEG laboratory at the University of Konstanz on this day of experimentation. In separate labs, two participants, connected by screens, engage in the computer game Pacman. The burning question: Can strangers, unable to communicate directly, synchronize their efforts to conquer the digital realm together?

Doctoral candidate Karl-Philipp Flösch is leading today's experiment. He states: ""Our research revolves around cooperative behaviour and the adoption of social roles."" However, understanding brain processes underlying cooperative behaviour is still in its infancy, presenting a central challenge for cognitive neuroscience. How can cooperative behaviour be brought into a highly structured EEG laboratory environment without making it feel artificial or boring for study participants?
Pacman as a scientific ""playground""
The research team, led by Harald Schupp, Professor of Biological Psychology at the University of Konstanz, envisioned using the well-known computer game Pacman as a natural medium to study cooperative behaviour in the EEG laboratory. Conducting the study as part of the Cluster of Excellence Centre for the Advanced Study of Collective Behaviour, they recently published their findings in Psychophysiology.
""Pacman is a cultural icon. Many have navigated the voracious Pacman through mazes in their youth, aiming to devour fruits and outsmart hostile ghosts,"" reminisces Karl-Philipp Flösch. Collaborating with colleagues, co-author Tobias Flaisch adapted the game. In the EEG version, two players instead of one must collaboratively guide Pacman to the goal. Flaisch explains: ""Success hinges on cooperative behaviour, as players must seamlessly work together.""
However, the researchers have built in a special hurdle: the labyrinth's path is concealed. Only one of the two players can see where Pacman is going next. Flösch elaborates: ""The active player can communicate the direction to the partner, but only indirectly using pre-agreed symbols, communicated solely through the computer screen."" If you do not remember quickly enough that a crescent moon on the screen means that Pacman should move right, and that only the banana on the keyboard can make Pacman move to the right, you're making a mistake. ""From the perspective of classical psychological research, the game combines various skills inherent in natural social situations,"" notes Harald Schupp.
EEG measures event-related potentials
During each game, the players' brain reactions were measured using EEG. Calculating event-related potentials provides a detailed view of the effects elicited by different game roles with millisecond-level temporal precision. The team hypothesized that the game role significantly influences brain reactions. Therefore, they examined the P3 component, a well-studied brain reaction exhibiting a stronger deflection in the presence of significant and task-relevant stimuli. The results confirmed their assumption: ""The P3 was increased not only when the symbol indicated the next move's direction but also when observing whether the game partner selected the correct symbol,"" says Flösch. The team concludes that the role we take on during cooperation determines the informational value of environmental stimuli situationally. EEG measurements allow the brain processes involved to be dynamically mapped.
""Cooperative role adoption structures our entire society,"" summarizes Schupp, providing context for the study. ""An individual achieves little alone, but collectively, humanity even reaches the moon. Our technological society hinges on cooperative behavior,"" says Flösch, adding that children early on take individual roles, thereby learning the art of complex cooperation. Consequently, this role adoption occurs nearly effortlessly and automatically for us every day. ""Our brains are practically 'built' for it, as evidenced by the results of our study.""

","score: 14.008798359383661, grade_level: '14'","score: 14.493333887595611, grade_levels: ['college_graduate'], ages: [24, 100]",10.1111/psyp.14433,"Humans are highly co‐operative and thus cognitively, affectively, and motivationally tuned to pursue shared goals. Yet, cooperative tasks typically require people to constantly take and switch individual roles. Task relevance is dictated by these roles and thereby dynamically changing. Here, we designed a dyadic game to test whether the family of P3 components can trace this dynamic allocation of task relevance. We demonstrate that late positive event‐related potential (ERP) modulations not only reflect predictable asymmetries between receiving and sending information but also differentiate whether the receiver's role is related to correct decision making or action monitoring. Furthermore, similar results were observed when playing the game with a computer, suggesting that experimental games may motivate humans to similarly cooperate with an artificial agent. Overall, late positive ERP waves provide a real‐time measure of how role taking dynamically shapes the meaning and relevance of stimuli within collaborative contexts. Our results, therefore, shed light on how the processes of mutual coordination unfold during dyadic cooperation."
"
Hearing loss affects more than 60 percent of adults aged 70 and older in the United States and is known to be related to an increased risk of dementia. The reason for this association is not fully understood.

To better understand the connection, a team of University of California San Diego and Kaiser Permanente Washington Health Research Institute researchers employed hearing tests and magnetic resonance imaging (MRI) to determine whether hearing impairment is associated with differences in specific brain regions.
In the November 21, 2023 issue of the Journal of Alzheimer's Disease, researchers reported that individuals enrolled in this observational study who had hearing impairment exhibited microstructural differences in the auditory areas of the temporal lobe and in areas of the frontal cortex involved with speech and language processing, as well as areas involved with executive function.
""These results suggest that hearing impairment may lead to changes in brain areas related to processing of sounds, as well as in areas of the brain that are related to attention. The extra effort involved with trying to understand sounds may produce changes in the brain that lead to increased risk of dementia,"" said principal investigator Linda K. McEvoy, Ph.D., UC San Diego Herbert Wertheim School of Public Health and Human Longevity Science professor emeritus and senior investigator at the Kaiser Permanente Washington Health Research Institute.
""If so, interventions that help reduce the cognitive effort required to understand speech -- such as the use of subtitles on television and movies, live captioning or speech-to-text apps, hearing aids, and visiting with people in quiet environments instead of noisy spaces -- could be important for protecting the brain and reduce the risk of dementia.""
McEvoy designed and led the study while at UC San Diego, in collaboration with Reas and UC San Diego School of Medicine investigators who gathered data from the Rancho Bernardo Study of Health Aging, a longitudinal cohort study of residents of the Rancho Bernardo suburb in San Diego that launched in 1972. For this analysis, 130 study participants underwent hearing threshold tests in research clinic visits between 2003 and 2005 and subsequently had MRI scans between 2014 and 2016.
The results of the study show that hearing impairment is associated with regionally specific brain changes that may occur due to sensory deprivation and to the increased effort required to understand auditory processing stimulations.
""The findings emphasize the importance of protecting one's hearing by avoiding prolonged exposure to loud sounds, wearing hearing protection when using loud tools and reducing the use of ototoxic medications,"" said co-author Emilie T. Reas, Ph.D., assistant professor at the UC San Diego School of Medicine.
Disclosures: Donald J. Hagler Jr is listed as an inventor on US Patent 9,568,580, 2017, ""Identifying white matter fiber tracts using magnetic resonance imaging (MRI)."" Other authors report no conflicts of interest.

","score: 19.27709169744661, grade_level: '19'","score: 21.90002890637546, grade_levels: ['college_graduate'], ages: [24, 100]",10.3233/JAD-230767,"Background: Hearing loss is associated with cognitive decline and increased risk for Alzheimer’s disease, but the basis of this association is not understood. Objective: To determine whether hearing impairment is associated with advanced brain aging or altered microstructure in areas involved with auditory and cognitive processing. Methods: 130 participants, (mean 76.4±7.3 years; 65% women) of the Rancho Bernardo Study of Healthy Aging had a screening audiogram in 2003–2005 and brain magnetic resonance imaging in 2014–2016. Hearing ability was defined as the average pure tone threshold (PTA) at 500, 1000, 2000, and 4000 Hz in the better-hearing ear. Brain-predicted age difference (Brain-pad) was calculated as the difference between brain-predicted age based on a validated structural imaging biomarker of brain age, and chronological age. Regional diffusion metrics in temporal and frontal cortex regions were obtained from diffusion-weighted MRIs. Linear regression analyses adjusted for age, gender, education, and health-related measures. Results: PTAs were not associated with brain-PAD (β= 0.09; 95% CI: –0.084 to 0.243; p = 0.34). PTAs were associated with reduced restricted diffusion and increased free water diffusion primarily in right hemisphere temporal and frontal areas (restricted diffusion: βs = –0.21 to –0.30; 95% CIs from –0.48 to –0.02; ps < 0.03; free water: βs = 0.18 to 0.26; 95% CIs 0.01 to 0.438; ps < 0.04). Conclusions: Hearing impairment is not associated with advanced brain aging but is associated with differences in brain regions involved with auditory processing and attentional control. It is thus possible that increased dementia risk associated with hearing impairment arises, in part, from compensatory brain changes that may decrease resilience."
"
An analysis of 5.8 million authors across all scientific disciplines shows that the gender gap is closing, but there is still a long distance to go. The new research by John Ioannidis of the Meta-Research Innovation Center at Stanford (METRICs) at Stanford University, US, and colleagues, publishes November 21 in the open access journal PLOS Biology.

There is a strong gender gap in science which manifests itself in many ways. One of the most prominent ones is the relative representation of men and women among the scientists whose work receive the most attention in the scientific literature.
In the new study, the researchers evaluated the entire Scopus database, a database that includes papers across all scientific fields. The database included 5.8 million authors who could have their gender identity assigned with high certainty. Of those, 3.8 million were men and 2.0 million were women.
The researchers found thatmen outnumbered women 3.93 times among those authors who started publishing before 1992, but only 1.36 times among those authors who started publishing after 2011. However, when limited to the authors who had the highest impact (namely, those who were in the top 2% of their discipline based on a citation indicator), men outnumbered women 3.21 times among top-cited authors, decreasing from 6.41 times in the senior groups to 2.28 times in the youngest group who started publishing after 2011. In the youngest group, 32 of the 174 fields of science (18%) had at least as many women as men, or even more women than men, within the category of top-cited authors.
Gender imbalances in author numbers decreased sharply over time in both high-income countries (including the USA) and other countries, but the latter had little improvement in gender imbalances for top-cited authors. In random samples of 100 women and 100 men from the youngest group, in-depth assessment showed that most were working in academic environments as of 2023, but promotion to full professor was very rare. The authors argue that both gender imbalances and slow promotion pathways for the most gifted scientists, regardless of gender, need to be improved.
Ioannidis adds, ""Our work documents substantial shrinkage over time of the inequalities between men and women in the top echelons of scientific citation impact, but there is substantial room for further improvements in most scientific fields.""

","score: 14.230389610389611, grade_level: '14'","score: 16.42051948051948, grade_levels: ['college_graduate'], ages: [24, 100]",10.1371/journal.pbio.3002385,"We evaluated how the gender composition of top-cited authors within different subfields of research has evolved over time. We considered 9,071,122 authors with at least 5 full papers in Scopus as of September 1, 2022. Using a previously validated composite citation indicator, we identified the 2% top-cited authors for each of 174 science subfields (Science-Metrix classification) in 4 separate publication age cohorts (first publication pre-1992, 1992 to 2001, 2002 to 2011, and post-2011). Using NamSor, we assigned 3,784,507 authors as men and 2,011,616 as women (for 36.1% gender assignment uncertain). Men outnumbered women 1.88-fold among all authors, decreasing from 3.93-fold to 1.36-fold over time. Men outnumbered women 3.21-fold among top-cited authors, decreasing from 6.41-fold to 2.28-fold over time. In the youngest (post-2011) cohort, 32/174 (18%) subfields had > = 50% women, 97/174 (56%) subfields had > = 30% women, and 3 subfields had = <10% women among the top-cited authors. Gender imbalances in author numbers decreased sharply over time in both high-income countries (including the United States of America) and other countries, but the latter had little improvement in gender imbalances for top-cited authors. In random samples of 100 women and 100 men from the youngest (post-2011) cohort, in-depth assessment showed that most were currently (April 2023) working in academic environments. 32 women and 44 men had some faculty appointment, but only 2 women and 2 men were full professors. Our analysis shows large heterogeneity across scientific disciplines in the amelioration of gender imbalances with more prominent imbalances persisting among top-cited authors and slow promotion pathways even for the most-cited young scientists."
"
People who speak two languages may be better at shifting their attention from one thing to another compared to those who speak one, according to a study published this month in the journal Bilingualism: Language and Cognition.

The study examined differences between bilingual and monolingual individuals when it comes to attentional control and ignoring information that isn't important at the time, said its authors Grace deMeurisse, a University of Florida Ph.D. candidate studying linguistics, and Edith Kaan, a UF professor in the department of linguistics.
""Our results showed that bilinguals seem to be more efficient at ignoring information that's irrelevant, rather than suppressing -- or inhibiting information,"" deMeurisse said. ""One explanation for this is that bilinguals are constantly switching between two languages and need to shift their attention away from the language not in use.""
For example, if an English- and Spanish-speaking person is having a conversation in Spanish, both languages are active, but English is put on hold but always ready to be deployed as needed.
Numerous studies have examined the distinctions between the two groups in broad cognitive mechanisms, which are mental processes that our brains use, like memory, attention, problem-solving, and decision-making, deMeurisse said.
""The effects of speaking two languages on a person's cognitive control is often debated,"" she said. ""Some of the literature says these differences aren't so pronounced, but that could be because of the tasks linguists use to research differences between bilinguals and monolinguals.""
DeMeurisse and Kaan set out to see if differences between the two groups would surface and used a task that has not been applied in psycholinguistics before called the Partial Repetition Cost task to measure the participants' abilities to deal with incoming information and control their attention.

""We found that bilinguals seem to be better at ignoring information that's irrelevant,"" Kaan said.
The two groups of subjects included functional monolinguals and bilinguals. Functional monolinguals were defined as those who had two years or less of a foreign language experience in a classroom and use only the first language that they learned as a child.
Bilinguals were categorized as people who had learned both their first and second language before the ages of 9 to 12 and were still using both languages.
Kaan explained that an individual's cognitive traits continuously adapt to external factors, and as humans, we have very few traits that remain fixed throughout our lifetime.
""Our cognition is continuously adapting to the situation, so in this case it's adapting to being bilingual,"" she said. ""It doesn't mean it won't change, so if you stop using the second language, your cognition may change as well.""
The UF study demonstrates a need to build more consistencies among the varied experiments used to understand differences between those who speak one language and those who speak more than one.
""In the study of bilingualism and cognition, we are redefining the way we talk about differences between bilinguals and monolinguals and searching for more factors to consider and more methods to conduct that research,"" deMeurisse said.
The researchers were also clear to point out that their study was not intended to show that people who speak two or more languages have an advantage over those who speak one.
""We are not looking for advantages or disadvantages,"" deMeurisse said. ""However, regardless of cognitive differences, learning a second language is always going to be something that can benefit you, whether those benefits are cognitive, social, or environmental. It will never be a negative to be exposed to a second language.""

","score: 14.55917355371901, grade_level: '15'","score: 16.50838016528926, grade_levels: ['college_graduate'], ages: [24, 100]",10.1017/S1366728923000731,"The effects of bilingual language experience on cognitive control are still debated. A recent proposal is that being bilingual enhances attentional control. This is based on studies showing smaller effects of the nature of the preceding trial on the current trial in bilinguals (Grundy et al., 2017). However, performance on such tasks can also be accounted for by lower-level processes such as the binding and unbinding of stimulus and response features. The current study used a Partial Repetition Cost paradigm to explicitly test whether language experience can affect such processes. Results showed that bi- and monolinguals did not differ in their responses when the stimulus features were task-relevant. However, the bilinguals showed smaller partial repetition costs when the features were task-irrelevant. These findings suggest that language experience does not affect lower-level processes, and supports the view that bilinguals exhibit enhanced attentional disengagement."
"
Time flows in a continuous stream -- yet our memories are divided into separate episodes, all of which become part of our personal narrative. How emotions shape this memory formation process is a mystery that science has only recently begun to unravel. The latest clue comes from UCLA psychologists, who have discovered that fluctuating emotions elicited by music helps form separate and durable memories.

The study, published in Nature Communications, used music to manipulate the emotions of volunteers performing simple tasks on a computer. The researchers found that the dynamics of people's emotions molded otherwise neutral experiences into memorable events.
""Changes in emotion evoked by music created boundaries between episodes that made it easier for people to remember what they had seen and when they had seen it,"" said lead author Mason McClay, a doctoral student in psychology at UCLA. ""We think this finding has great therapeutic promise for helping people with PTSD and depression.""
As time unfolds, people need to group information, since there is too much to remember (and not all of it useful). Two processes appear to be involved in turning experiences into memories over time: The first integrates our memories, compressing and linking them into individualized episodes; the other expands and separates each memory as the experience recedes into the past. There's a constant tug of war between integrating memories and separating them, and it's this push and pull that helps to form distinct memories. This flexible process helps a person understand and find meaning in their experiences, as well as retain information.
""It's like putting items into boxes for long-term storage,"" said corresponding author David Clewett, an assistant professor of psychology at UCLA. ""When we need to retrieve a piece of information, we open the box that holds it. What this research shows is that emotions seem to be an effective box for doing this sort of organization and for making memories more accessible.""
A similar effect may help explain why Taylor Swift's ""Eras Tour"" has been so effective at creating vivid and lasting memories: Her concert contains meaningful chapters that can be opened and closed to relive highly emotional experiences.
McClay and Clewett, along with Matthew Sachs at Columbia University, hired composers to create music specifically designed to elicit joyous, anxious, sad or calm feelings of varied intensity. Study participants listened to the music while imagining a narrative to accompany a series of neutral images on a computer screen, such as a watermelon slice, a wallet or a soccer ball. They also used the computer mouse to track moment-to-moment changes in their feelings on a novel tool developed for tracking emotional reactions to music.

Then, after performing a task meant to distract them, participants were shown pairs of images again in a random order. For each pair, they were asked which image they had seen first, then how far apart in time they felt they had seen the two objects. Pairs of objects that participants had seen immediately before and after a change of emotional state -- whether of high, low, or medium intensity -- were remembered as having occurred farther apart in time compared to images that did not span an emotional change. Participants also had worse memory for the order of items that spanned emotional changes compared to items they had viewed while in a more stable emotional state. These effects suggest that a change in emotion resulting from listening to music was pushing new memories apart.
""This tells us that intense moments of emotional change and suspense, like the musical phrases in Queen's 'Bohemian Rhapsody,' could be remembered as having lasted longer than less emotive experiences of similar length,"" McClay said. ""Musicians and composers who weave emotional events together to tell a story may be imbuing our memories with a rich temporal structure and longer sense of time.""
The direction of the change in emotion also mattered. Memory integration was best -- that is, memories of sequential items felt closer together in time, and participants were better at recalling their order -- when the shift was toward more positive emotions. On the other hand, a shift toward more negative emotions (from calmer to sadder, for example) tended to separate and expand the mental distance between new memories.
Participants were also surveyed the following day to assess their longer-term memory, and showed better memory for items and moments when their emotions changed, especially if they were experiencing intense positive emotions. This suggests that feeling more positive and energized can fuse different elements of an experience together in memory.
Sachs emphasized the utility of music as an intervention technique.
""Most music-based therapies for disorders rely on the fact that listening to music can help patients relax or feel enjoyment, which reduces negative emotional symptoms,"" he said. The benefits of music-listening in these cases are therefore secondary and indirect. Here, we are suggesting a possible mechanism by which emotionally dynamic music might be able to directly treat the memory issues that characterize such disorders.""
Clewett said these findings could help people reintegrate the memories that have caused post-traumatic stress disorder.
""If traumatic memories are not stored away properly, their contents will come spilling out when the closet door opens, often without warning. This is why ordinary events, such as fireworks, can trigger flashbacks of traumatic experiences, such as surviving a bombing or gunfire,"" he said. ""We think we can deploy positive emotions, possibly using music, to help people with PTSD put that original memory in a box and reintegrate it, so that negative emotions don't spill over into everyday life.""
The research was supported by the National Science Foundation, UCLA and Columbia University.

","score: 14.386751241777421, grade_level: '14'","score: 15.029401261914352, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41467-023-42241-2,"Human emotions fluctuate over time. However, it is unclear how these shifting emotional states influence the organization of episodic memory. Here, we examine how emotion dynamics transform experiences into memorable events. Using custom musical pieces and a dynamic emotion-tracking tool to elicit and measure temporal fluctuations in felt valence and arousal, our results demonstrate that memory is organized around emotional states. While listening to music, fluctuations between different emotional valences bias temporal encoding process toward memory integration or separation. Whereas a large absolute or negative shift in valence helps segment memories into episodes, a positive emotional shift binds sequential representations together. Both discrete and dynamic shifts in music-evoked valence and arousal also enhance delayed item and temporal source memory for concurrent neutral items, signaling the beginning of new emotional events. These findings are in line with the idea that the rise and fall of emotions can sculpt unfolding experiences into memories of meaningful events."
"
Consumer preferences in floral arrangements don't necessarily match the designs that florists are taught to make, according to a new study by researchers at North Carolina State University.

Particular flower species are important to consumers, while they also pay attention to the overall symmetry of the arrangement as well as the colors presented, the research suggests.
Specifically, the study shows that surveyed consumers absolutely love roses -- and are quite willing to pay more to have them included in floral arrangements -- while having little use for chrysanthemums, a Mother's Day staple.
The findings could be beneficial to florists, says Vanessa Woods, an NC State doctoral student in horticultural science and co-author of a paper describing the study.
""There's little research about what consumers prefer in floral arrangements, which is seen by many in the horticulture industry to be an art form,"" Woods said. ""This is an industry in which products need to be sold quickly, but there's not much information on what people actually want.""
The researchers brought in more than 120 people and used eye-trackers in the lab to test whether certain elements of floral design theory -- used by many florists to create arrangements -- are supported by consumer preferences.
The researchers tested a number of design theory elements, including line, whether a straight or moving line can be drawn through the center of the arrangement; symmetry, or whether the arrangements mirrors itself when you draw a straight line through the middle of the arrangement; and color, or whether the arrangements have one, contrasting, similar or assorted colored flowers.

To test these elements, the researchers provided photos of various flower arrangements and asked consumers about their preferences and the prices they'd be willing to pay for arrangements. The researchers used the eye-tracking technology to get insight into the flower arrangements that captured consumer attention and where the eye was drawn to in the designs.
The results showed that consumers were more concerned with flower species than with any of the design theory elements, which was not surprising to the research team.
""Americans have a love affair with roses,"" Woods said. ""When roses are part of an arrangement, people really value that. They think it's more expensive, they're willing to pay more for it and they spend more time looking at that arrangement.""
Consumers showed no preference about the line element -- they preferred straight lines and curving lines in designs equally. But they preferred floral arrangements that are more symmetrical rather than less. And consumers were wary of arrangements with just one flower color, preferring instead similar colors or colors close to each other on the color wheel -- purples with reds, for example.
""Our study is about general preferences of consumers,"" said Melinda Knuth, assistant professor of horticultural science at NC State and certified floral designer. ""There is still room for creativity and flair in floristry -- someone's perfect bouquet might not be everyone's. We are just trying to quantify these general preferences in a way that helps the floral industry thrive. This study gives us ground work to begin looking at niche preferences of floral consumers. Just like other art forms, there's variety in preferences.""
The study appears in the open access journal HortScience. The research was supported by the Floral Marketing Research Fund. Xuan (Jade) Wu, Charlie Hall and Marco Palma from Texas A&M University also co-authored the paper.

","score: 13.155399649737301, grade_level: '13'","score: 14.32362521891418, grade_levels: ['college_graduate'], ages: [24, 100]",10.21273/HORTSCI17273-23,"Florists use design theory to create arrangements that they assume will be pleasing to consumers, thus increasing purchase rates and spending. However, certain elements of design theory and their relationship with consumer acceptance and spending have not been empirically tested. Using mixed logit models and eye-tracking technology, we investigated whether consumer preferences support three key elements of existing floral design theory: line, color, and form. We also examined consumer preferences for floral species, which, although not a traditional element of design theory, may influence consumer purchasing decisions. Our findings challenge existing design theory because consumers did not uniformly favor it. Instead, they valued symmetrical form, arrangements with similar (but not identical) colors, and, surprisingly, the presence of roses in an arrangement was the most crucial factor in capturing consumer attention and increasing the willingness to pay."
"
A team led by researchers at Weill Cornell Medicine has used an AI-based approach to uncover underlying patterns among the conditions in which people are born, grow, live, work, and age, termed social determinants of health (SDoH), and then linked each pattern to children's health outcomes. Compared with traditional approaches, the strategy, in principle, provides a more objective and comprehensive picture of potential social factors that affect child health, which in turn, can enable better targeted interventions.

As reported Oct. 16 in JAMA Pediatrics, the researchers analyzed data on more than 10,500 American children, in communities across 17 U.S. states. Quantifying more than 80 neighborhood-level SDoH factors for each child, the analysis uncovered four broad patterns in the sample, including affluence, high-stigma environment, high socioeconomic deprivation, and high crime and drug sale rates coupled with lower education and densely populated areas. They found statistical associations between these patterns and outcomes relating to child developmental health, including mental, cognitive and physical health.
""A complex set of social factors can influence children's health, and I think our results underscore the importance of using methods that can handle such complexity,"" said study lead author Dr. Yunyu Xiao, an assistant professor of population health sciences at Weill Cornell Medicine.
Dr. Xiao co-led the study with Dr. Chang Su, also an assistant professor of population health sciences. Both are in the Division of Health Informatics in the Department of Population Health Sciences at Weill Cornell Medicine. Dr. Jyotishman Pathak and Dr. Fei Wang, also at Weill Cornell Medicine, are co-authors in this joint work.
The Weill Cornell Medicine investigators work with a multi-institutional, multidisciplinary team of experts to study potential social determinants of health for clues to persistent causes of bad health outcomes. The team includes psychiatry expert Dr. John Mann from Columbia University; Drs. Timothy Brown, Lonnie Snowden, and Julian Chun-Chung Chow, experts in health economics, health policy and social welfare, respectively, at the University of California; Berkeley School of Public Health, and social epidemiologist Dr. Alex Tsai of Harvard Medical School. Identifying health-influencing social factors also can guide social policies aimed at improving child health, such as legislation mandating free school lunches for children from low-income families coupled with holistic health care provisions at school and clinical settings, Dr. Xiao said.
A New Approach to a Complex Issue
Prior studies in this field have tended to focus on narrow sets of socioeconomic variables and health outcomes, and typically have examined outcomes that are averaged over large geographic areas such as counties or states.

In the new study, the researchers took a different approach. Drs. Xiao and Su are experts in the use of machine learning and other advanced AI techniques that allow relatively unbiased, fine-grained analyses of large datasets. In recent years, they have been bringing these ""big-data"" techniques to bear on important social epidemiology problems -- for example, examining factors potentially influencing children's mental health during the COVID-19 pandemic.
""Our approach is data-driven, allowing us to see what patterns there are in large datasets, without prior hypotheses and other biases getting in the way,"" Dr. Su said.
The dataset in the new study was generated by an ongoing, survey-based, National Institutes of Health (NIH)-sponsored project called the Adolescent Brain Cognitive Development (ABCD) Study. It covered a cohort of 10,504 children, aged 9-10 at the start, and their parents at 21 sites across the United States from 2016 to 2021. The sample's ethnic and racial mix broadly reflected that of the U.S. as a whole.
In the analysis, each child's record was scored on 84 different SDoH variables relating to educational resources, physical infrastructure, perceived bias and discrimination, household income, neighborhood crime and drugs. The machine learning algorithm identified underlying patterns in the children's SDoH profiles -- and also looked for statistical associations between these patterns and health outcomes.
Child Health Outcomes Vary Depending on Social Determinants
A key finding was that the data clustered into four broad SDoH patterns: affluent; high socioeconomic deprivation; urban high crime and low level of educational attainment and resources; and high-stigma -- the latter involving higher self-reported measures of bias and discrimination against women and immigrants and other underrepresented groups. White children were overrepresented in the affluent and high-stigma areas; Black and Hispanic children in the other two.
Each of the four profiles was associated with its own broad pattern of health outcomes, the ""high socioeconomic deprivation"" pattern being associated with the worst health outcomes on average, including more signs of mental illness, worse cognitive performance, and worse physical health. The other two non-affluent patterns were also associated generally with more adverse outcomes compared with the affluent pattern.
The study had some limitations, including the survey-based, self-reported nature of the ABCD data, which is generally considered less reliable than objectively measured data. Also, epidemiological analyses like these can reveal only associations between social factors and health outcomes -- they can't prove that the former influence the latter. Even so, the researchers said, the results demonstrate the power of a relatively unbiased, machine-learning approach to uncover potentially meaningful links, and should help inform future studies that can discover actual causative mechanisms connecting social factors to child health.
""This multi-dimensional, unbiased approach in principle can lead to more targeted and effective policy interventions that we are investigating in a current NIH-funded project,"" Dr. Xiao said.

","score: 16.740401785714287, grade_level: '17'","score: 19.142600446428567, grade_levels: ['college_graduate'], ages: [24, 100]",10.1001/jamapediatrics.2023.4218,"Social determinants of health (SDOH) influence child health. However, most previous studies have used individual, small-set, or cherry-picked SDOH variables without examining unbiased computed SDOH patterns from high-dimensional SDOH factors to investigate associations with child mental health, cognition, and physical health. To identify SDOH patterns and estimate their associations with children’s mental, cognitive, and physical developmental outcomes. This population-based cohort study included children aged 9 to 10 years at baseline and their caregivers enrolled in the Adolescent Brain Cognitive Development (ABCD) Study between 2016 and 2021. The ABCD Study includes 21 sites across 17 states. Eighty-four neighborhood-level, geocoded variables spanning 7 domains of SDOH, including bias, education, physical and health infrastructure, natural environment, socioeconomic status, social context, and crime and drugs, were studied. Hierarchical agglomerative clustering was used to identify SDOH patterns. Associations of SDOH and child mental health (internalizing and externalizing behaviors) and suicidal behaviors, cognitive function (performance, reading skills), and physical health (body mass index, exercise, sleep disorder) were estimated using mixed-effects linear and logistic regression models. Among 10 504 children (baseline median [SD] age, 9.9 [0.6] years; 5510 boys [52.5%] and 4994 girls [47.5%]; 229 Asian [2.2%], 1468 Black [14.0%], 2128 Hispanic [20.3%], 5565 White [53.0%], and 1108 multiracial [10.5%]), 4 SDOH patterns were identified: pattern 1, affluence (4078 children [38.8%]); pattern 2, high-stigma environment (2661 children [25.3%]); pattern 3, high socioeconomic deprivation (2653 children [25.3%]); and pattern 4, high crime and drug sales, low education, and high population density (1112 children [10.6%]). The SDOH patterns were distinctly associated with child health outcomes. Children exposed to socioeconomic deprivation (SDOH pattern 3) showed the worst health profiles, manifesting more internalizing (β = 0.75; 95% CI, 0.14-1.37) and externalizing (β = 1.43; 95% CI, 0.83-2.02) mental health problems, lower cognitive performance, and adverse physical health. This study shows that an unbiased quantitative analysis of multidimensional SDOH can permit the determination of how SDOH patterns are associated with child developmental outcomes. Children exposed to socioeconomic deprivation showed the worst outcomes relative to other SDOH categories. These findings suggest the need to determine whether improvement in socioeconomic conditions can enhance child developmental outcomes."
"
Machine learning algorithms designed to diagnose a common infection that affects women showed a diagnostic bias among ethnic groups, University of Florida researchers found.

While artificial intelligence tools offer great potential for improving health care delivery, practitioners and scientists warn of their risk for perpetuating racial inequities. Published Friday in the Nature journal Digital Medicine, this is the first paper to evaluate fairness among these tools in connection to a women's health issue.
""Machine learning can be a great tool in medical diagnostics, but we found it can show bias toward different ethnic groups,"" said Ruogu Fang, an associate professor in the J. Crayton Pruitt Family Department of Biomedical Engineering and the study's author. ""This is alarming for women's health as there already are existing disparities that vary by ethnicity.""
The researchers evaluated the fairness of machine learning in diagnosing bacterial vaginosis, or BV, a common condition affecting women of reproductive age, which has clear diagnostic differences among ethnic groups.
Fang and co-corresponding author Ivana Parker, both faculty members in the Herbert Wertheim College of Engineering, pulled data from 400 women, comprising 100 from each of the ethnic groups represented -- white, Black, Asian, and Hispanic.
In investigating the ability of four machine learning models to predict BV in women with no symptoms, researchers say the accuracy varied among ethnicities. Hispanic women had the most false-positive diagnoses, and Asian women received the most false-negative. Algorithm
""The models performed highest for white women and lowest for Asian women,"" said the Parker, an assistant professor of bioengineering. ""This tells us machine learning methods are not treating ethnic groups equally well.""
Parker said that while they were interested in understanding how AI tools predict disease for specific ethnicities, their study also helps medical scientists understand the factors associated with bacteria in women of varying ethnic backgrounds, which can lead to improved treatments.

BV, one of the most common vaginal infections, can cause discomfort and pain and happens when natural bacteria levels are out of balance. While there are symptoms associate with BV, many people have no symptoms, making it difficult to diagnose.
It doesn't often cause complications, but in some cases, BV can increase the risk of sexually transmitted infections, miscarriage, and premature births.
The researchers said their findings demonstrate the need for improved methods for building the AI tools to mitigate health care bias.

","score: 15.179483082706767, grade_level: '15'","score: 17.008674812030073, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41746-023-00953-1,"While machine learning (ML) has shown great promise in medical diagnostics, a major challenge is that ML models do not always perform equally well among ethnic groups. This is alarming for women’s health, as there are already existing health disparities that vary by ethnicity. Bacterial Vaginosis (BV) is a common vaginal syndrome among women of reproductive age and has clear diagnostic differences among ethnic groups. Here, we investigate the ability of four ML algorithms to diagnose BV. We determine the fairness in the prediction of asymptomatic BV using 16S rRNA sequencing data from Asian, Black, Hispanic, and white women. General purpose ML model performances vary based on ethnicity. When evaluating the metric of false positive or false negative rate, we find that models perform least effectively for Hispanic and Asian women. Models generally have the highest performance for white women and the lowest for Asian women. These findings demonstrate a need for improved methodologies to increase model fairness for predicting BV."
"
A red wine may pair nicely with the upcoming Thanksgiving meal. But for some people, drinking red wine even in small amounts causes a headache. Typically, a ""red wine headache"" can occur within 30 minutes to three hours after drinking as little as a small glass of wine.

What in wine causes headaches?
In a new study, scientists at the University of California, Davis, examined why this happens -- even to people who don't get headaches when drinking small amounts of other alcoholic beverages. Researchers think that a flavanol found naturally in red wines can interfere with the proper metabolism of alcohol and can lead to a headache. The study was published in the journal Scientific Reports.
The headache culprit: Quercetin, a flavanol
This flavanol is called quercetin and it is naturally present in all kinds of fruits and vegetables, including grapes. It's considered a healthy antioxidant and is even available in supplement form. But when metabolized with alcohol, it can be problematic.
""When it gets in your bloodstream, your body converts it to a different form called quercetin glucuronide,"" said wine chemist and corresponding author Andrew Waterhouse, professor emeritus with the UC Davis Department of Viticulture and Enology. ""In that form, it blocks the metabolism of alcohol.""
Acetaldehyde toxin buildup leads to flushing, headache, nausea
As a result, people can end up accumulating the toxin acetaldehyde, explains lead author Apramita Devi, postdoctoral researcher with the UC Davis Department of Viticulture and Enology.

""Acetaldehyde is a well-known toxin, irritant and inflammatory substance,"" said Devi. ""Researchers know that high levels of acetaldehyde can cause facial flushing, headache and nausea.""
The medication disulfiram prescribed to alcoholics to prevent them from drinking causes these same symptoms. Waterhouse said that's because the drug also causes the toxin to build up in the body when normally an enzyme in the body would break it down. About 40% of the East Asian population also has an enzyme that doesn't work very well, allowing acetaldehyde to build up in their system.
""We postulate that when susceptible people consume wine with even modest amounts of quercetin, they develop headaches, particularly if they have a preexisting migraine or another primary headache condition,"" said co-author Morris Levin, professor of neurology and director of the Headache Center at the University of California, San Francisco. ""We think we are finally on the right track toward explaining this millennia-old mystery. The next step is to test it scientifically on people who develop these headaches, so stay tuned.""
Sunlight increases headache-causing flavanol in grapes
Waterhouse said levels of this flavanol can vary dramatically in red wine.
""Quercetin is produced by the grapes in response to sunlight,"" Waterhouse said. ""If you grow grapes with the clusters exposed, such as they do in the Napa Valley for their cabernets, you get much higher levels of quercetin. In some cases, it can be four to five times higher.""
Levels of quercetin can also differ depending on how the wine is made, including skin contact during fermentation, fining processes and aging.

Clinical trial on wine headaches
Scientists will next compare red wines that contain a lot of quercetin with those that have very little to test their theory about red wine headaches on people. This small human clinical trial, funded by the Wine Spectator Scholarship Foundation, will be led by UCSF.
Researchers said there are still many unknowns about the causes of red wine headaches. It's unclear why some people seem more susceptible to them than others. Researchers don't know if the enzymes of people who suffer from red wine headaches are more easily inhibited by quercetin or if this population is just more easily affected by the buildup of the toxin acetaldehyde.
""If our hypothesis pans out, then we will have the tools to start addressing these important questions,"" Waterhouse said.
Funding for this initial investigation came from people who supported the project via 2022 Crowdfund UC Davis.

","score: 12.078842523596624, grade_level: '12'","score: 12.511286636860405, grade_levels: ['college'], ages: [18, 24]",10.1038/s41598-023-46203-y,"The consumption of red wine induces headaches in some subjects who can drink other alcoholic beverages without suffering. The cause for this effect has been attributed to a number of components, often the high level of phenolics in red wine, but a mechanism has been elusive. Some alcohol consumers exhibit flushing and experience headaches, and this is attributed to a dysfunctional ALDH2 variant, the enzyme that metabolizes acetaldehyde, allowing it to accumulate. Red wine contains much higher levels of quercetin and its glycosides than white wine or other alcoholic beverages. We show that quercetin-3-glucuronide, a typical circulating quercetin metabolite, inhibits ALDH2 with an IC50 of 9.6 µM. Consumption of red wine has been reported to result in comparable levels in circulation. Thus, we propose that quercetin-3-glucoronide, derived from the various forms of quercetin in red wines inhibits ALDH2, resulting in elevated acetaldehyde levels, and the subsequent appearance of headaches in susceptible subjects. Human-subject testing is needed to test this hypothesis."
"
As possibilities have changed and technology has advanced, memories and nostalgia are now a significant part of our use of social media. This is shown in a study from the University of Gothenburg and University West.

Researchers at the University of Gothenburg and University West have been following a group of eleven active social media users for ten years, allowing them to describe and reflect on how they use the platforms to document and share their lives. The study provides insight into the role of technology in creating experiences and reliving meaningful moments.
""These types of studies help us look back and understand the culture as it was in the 2010s and 2020s when social media was a central part of it,"" says Beata Jungselius, senior lecturer of informatics at University West and one of the researchers behind the study.
Social media users engage in what researchers define as ""social media nostalgizing,"" meaning they actively seek out content that evokes feelings of nostalgia.
Alexandra Weilenmann, professor of interaction design at the University of Gothenburg, explains that participants in the study have described it as ""treating themselves"" to a nostalgia trip now and then.
""Going back and remembering what has happened earlier in life becomes a bigger part of it over time than posting new content,"" she says, and explains that in later interviews, it becomes clear that the platforms often serve as diary-like tools that allow memories to be relived.
Social media platforms are introducing increasingly advanced features to help users interact with older content. Personal, music-infused photo albums generated for us or reminders of pictures we posted on the same date one, three, or ten years ago allow for nostalgic experiences, which are often seen as positive. The study describes how these features can lead to users reconnecting with old friends by ""tagging"" them in a shared memory. Alexandra Weilenmann and Beata Jungselius believe this could be a deliberate move by social media platforms to encourage users to stay active since the publication of new content has decreased.
The researchers have noted that it's not just the content itself that evokes feelings of nostalgia but also memories of the actual usage of social media play a significant role. For example, one of the interviewees reminisces about how rewarding the intense communication in forums was and how it often led to real-life meetings and interactions.
""It's only now that we've lived with social media long enough to make and draw conclusions from a study like this. Through our method of studying the same users over ten years, we've been able to follow how their usage and attitudes toward the platforms have changed as they have evolved,"" says Beata Jungselius.

","score: 15.144867136659439, grade_level: '15'","score: 16.413863882863346, grade_levels: ['college_graduate'], ages: [24, 100]",10.1177/20563051231207850,"In this article, we present findings from an analysis of social media users’ own descriptions of having lived with social media for over a decade. In doing so, we draw upon the users’ reflections as related in data collected over 10 years. We present findings from a unique dataset of 36 stimulated-recall interviews, where we have studied the same group of informants in 2012, 2017, and 2022. While previous work on reminiscing, memories, and social media have relied on descriptions of practices as they are remembered, our approach has allowed us to follow and examine how users reflect upon their own practices over time. In this article, we focus on social media reminiscing practices and show how social media users seek and engage with previously posted social media content to reminisce and how their reflecting upon how their social media practices have evolved over time evoke ambiguous feelings. Drawing upon previous work and our own empirical material, we define and discuss social media nostalgia. We describe how social media users experience both personal social media nostalgia (referring to how I was), and historical social media nostalgia (referring to how it was) when reflecting upon past social media practices and demonstrate how social media users nostalgize as they interact with and through social media memories. Finally, we discuss our findings in relation to the interplay between reminiscing practices and technology and point to how social media memories represent a detailed insight into an ongoing social transformation of everyday life."
"
There is a well-known relationship between good physical fitness at a young age and a lower risk of cardiovascular disease later in life. However, when researchers adjusted for familial factors by means of sibling analysis, they found a weaker association, although the link between high body mass index (BMI) and cardiovascular disease remained strong. The study, which was conducted by researchers from Karolinska Institutet and other universities, is published in JAMA Network Open.

""This does not mean that fitness is irrelevant,"" says the study's last author Viktor Ahlqvist, doctoral student at the Department of Global Public Health, Karolinska Institutet. ""We could still see an association, although it was weaker after taking into account factors shared by full siblings. We also think that adolescence is an important time in life for establishing good habits such as exercising and having a healthy diet.""
Challenging to prove causal associations
Many observational studies have previously demonstrated links between various risk factors at a young age and cardiovascular disease in adulthood. However, whether the associations are causal is challenging to prove because of the potential influence of unaccounted genetic and environmental factors. A collaborative team including researchers from Karolinska Institutet in Sweden has therefore tried to examine if a large proportion of cardiovascular diseases in adulthood could indeed be prevented with a lower BMI, lower blood pressure, improved physical fitness or improved muscle strength in adolescence.
Sourcing data from the Swedish Military Conscription Register and other Swedish registries, the researchers identified over a million 18-year-old males and followed them for 60 years. Almost half of them were full brothers.
""The strength of our study, which makes it more reliable than many other conventional observational studies, is that we have used sibling analyses,"" says the study's first author Marcel Ballin, researcher at Uppsala University and analyst at Region Stockholm's Centre for Epidemiology and Community Medicine. ""By doing so we could examine how the relationship changes when controlling for all shared sibling factors. This includes environmental factors such as childhood environment and half of the genetics.""
High BMI is a strong risk factor
The results show that a high BMI in late adolescence was strongly associated with future cardiovascular disease, even after the researchers had controlled for shared familial factors. However, the association between physical fitness and cardiovascular disease was considerably weaker in the sibling analysis, suggesting that many previous observational studies might have overestimated the relevance of adolescent fitness to cardiovascular health later in life.

""Our conclusion is that of the risk factors studied, high BMI is the strongest individual risk factor for cardiovascular disease, and that efforts to tackle the obesity epidemic should continue to be given high priority,"" says co-author Daniel Berglind, docent at the Department of Global Public Health, Karolinska Institutet. ""A good level of fitness and muscle strength in adolescence doesn't seem as crucial, but physical activity still remains important for public health, as it can bring other health benefits.""
Several limitations
The study examined the association between risk factors at a young age and future cardiovascular disease; other disease outcomes were not investigated. The researchers had no data on whether the participants' risk factors varied later in life, and they only studied men, which makes it difficult to extend their findings to women. The Military Conscription Register also lacks details on certain risk factors for future cardiovascular disease, such as diet, alcohol consumption, smoking, blood lipids and blood glucose.
The researchers received no specific grant for this study. Co-author Martin Neovius is on the advisory panels for Ethicon, Johnson & Johnson and Itrim and has been a consultant for the Swedish armed forces outside the scope of this study. No other conflicts of interest have been reported.

","score: 16.224670912951172, grade_level: '16'","score: 17.648333333333333, grade_levels: ['college_graduate'], ages: [24, 100]",10.1001/jamanetworkopen.2023.43947,"Cardiovascular risk factors in youth have been associated with future cardiovascular disease (CVD), but conventional observational studies are vulnerable to genetic and environmental confounding. To examine the role of genetic and environmental factors shared by full siblings in the association of adolescent cardiovascular risk factors with future CVD. This is a nationwide cohort study with full sibling comparisons. All men who underwent mandatory military conscription examinations in Sweden between 1972 and 1995 were followed up until December 31, 2016. Data analysis was performed from May 1 to November 10, 2022. Body mass index (BMI), cardiorespiratory fitness, blood pressure, handgrip strength, and a combined risk z score in late adolescence. The primary outcome was fatal or nonfatal CVD, as recorded in the National Inpatient Register or the Cause of Death Register before 2017. A total of 1 138 833 men (mean [SD] age, 18.3 [0.8] years), of whom 463 995 were full brothers, were followed up for a median (IQR) of 32.1 (26.7-37.7) years, during which 48 606 experienced a CVD outcome (18 598 among full brothers). All risk factors were associated with CVD, but the effect of controlling for unobserved genetic and environmental factors shared by full siblings varied. In the sibling analysis, hazard ratios for CVD (top vs bottom decile) were 2.10 (95% CI, 1.90-2.32) for BMI, 0.77 (95% CI, 0.68-0.88) for cardiorespiratory fitness, 1.45 (95% CI, 1.32-1.60) for systolic blood pressure, 0.90 (95% CI, 0.82-0.99) for handgrip strength, and 2.19 (95% CI, 1.96-2.46) for the combined z score. The percentage attenuation in these hazard ratios in the sibling vs total cohort analysis ranged from 1.1% for handgrip strength to 40.0% for cardiorespiratory fitness. Consequently, in the sibling analysis, the difference in cumulative CVD incidence at age 60 years (top vs bottom decile) was 7.2% (95% CI, 5.9%-8.6%) for BMI and 1.8% (95% CI, 1.0%-2.5%) for cardiorespiratory fitness. Similarly, in the sibling analysis, hypothetically shifting everyone in the worst deciles of BMI to the middle decile would prevent 14.9% of CVD at age 60 years, whereas the corresponding number for cardiorespiratory fitness was 5.3%. In this Swedish national cohort study, cardiovascular risk factors in late adolescence, especially a high BMI, were important targets for CVD prevention, independently of unobserved genetic and environmental factors shared by full siblings. However, the role of adolescent cardiorespiratory fitness in CVD may have been overstated by conventional observational studies."
"
Old people who follow a Mediterranean diet are at a lower risk of cognitive decline, according to a study published in the journal Molecular Nutrition and Food Research. The study provides new evidence for a better understanding of the biological mechanisms related to the impact of the diet on cognitive health in the ageing population.

The study is led by Mireia Urpí-Sardá, adjunct lecturer and member of the Biomarkers and Nutritional & Food Metabolomics research group of the Faculty of Pharmacy and Food Sciences, the Institute for Nutrition and Food Safety (INSA-UB), the Food and Nutrition Torribera Campus of the University of Barcelona, and the CIBER on Frailty and Healthy Ageing (CIBERFES).
This European study, part of the Joint Programming Initiative ""A Healthy Diet for a Healthy Life"" (JPI HDHL) was carried out over twelve years and it involved 840 people over 65 years of age (65% of whom were women) in the Bourdeaux and Dijon regions of France.
Healthy diet and cognitive performance
According to Cristina Andrés-Lacueva, UB professor and head of the CIBERFES group, ""within the framework of the study, a dietary metabolomic index has been designed -- based on biomarkers obtained from the participants' serum -- on the food groups that form part of the Mediterranean diet. Once this index is known, its association with cognitive impairment is evaluated.""
in the study, baseline levels of saturated and unsaturated fatty acids, gut microbiota-derived polyphenol metabolites and other phytochemicals in serum that reflect individual bioavailability were chosen as biomarkers. Some of these indicators have not only been recognized as marks of exposure to the main food groups of the Mediterranean diet but have also been held responsible for the health benefits of the Mediterranean dietary pattern.
The metabolome or set of metabolites -- related to food and derived from gut microbiota activity -- was studied through a large-scale quantitative metabolomic analysis from the serum of the participants without dementia, from the beginning of the study. Cognitive impairment was assessed by five neuropsychological tests over twelve years.

As a result, the study reveals a protective association between the score of the Mediterranean diet based on serum biomarkers and cognitive decline in older people.
Biomarkers to study the benefits of the diet
According to Mercè Pallàs, professor at the UB Neurosciences Institute (UBneuro), ""the use of dietary pattern indices based on food-intake biomarkers is a step forward towards the use of more accurate and objective dietary assessment methodologies that take into account important factors such as bioavailability.""
Expert Alba Tor-Roca, first author of the study and CIBERFES researcher at the UB, explains that ""we found that adherence to Mediterranean diet assessed by a panel of dietary biomarkers is inversely associated with long-term cognitive decline in older people. These results support the use of these indicators in long-term follow-up assessments to observe the health benefits associated with the Mediterranean diet or other dietary patterns and therefore, guide personalized counselling at older ages.""
The study was carried out in collaboration with teams from the Department of Genetics, Microbiology and Statistics of the Faculty of Biology and the Department of Pharmacology, Toxicology and Therapeutic Chemistry of the Faculty of Pharmacy and Food Sciences of the UB. Teams from the University of Bordeaux and the INRAE centre at Clermont-Ferrand University (France), King's College London (United Kingdom), the University of Amsterdam (the Netherlands) and the Parcelsus Medical University in Salzburg (Austria) have also participated.
Funding was obtained through the International Joint Programming Actions PCIN-2015-229, the European Regional Development Funds (ERDF) and from the former Ministry of Economy, Industry and Competitiveness (MINECO) through the Joint Programming Initiative ""A Healthy Diet for a Healthy Life.""

","score: 20.403380673639763, grade_level: '20'","score: 22.23933019863737, grade_levels: ['college_graduate'], ages: [24, 100]",10.1002/mnfr.202300271,"Evidence on the Mediterranean diet (MD) and age‐related cognitive decline (CD) is still inconclusive partly due to self‐reported dietary assessment. The aim of the current study is to develop an MD‐ metabolomic score (MDMS) and investigate its association with CD in community‐dwelling older adults. This study includes participants from the Three‐City Study from the Bordeaux (n = 418) and Dijon (n = 422) cohorts who are free of dementia at baseline. Repeated measures of cognition over 12 years are collected. An MDMS is designed based on serum biomarkers related to MD key food groups and using a targeted metabolomics platform. Associations with CD are investigated through conditional logistic regression (matched on age, sex, and education level) in both sample sets. The MDMS is found to be inversely associated with CD (odds ratio [OR] [95% confidence interval (CI)] = 0.90 [0.80–1.00]; p = 0.048) in the Bordeaux (discovery) cohort. Results are comparable in the Dijon (validation) cohort, with a trend toward significance (OR [95% CI] = 0.91 [0.83–1.01]; p = 0.084). A greater adherence to the MD, here assessed by a serum MDMS, is associated with lower odds of CD in older adults."
"
Air filtration systems do not reduce the risk of picking up viral infections, according to new research from the University of East Anglia.

A new study published today reveals that technologies designed to make social interactions safer in indoor spaces are not effective in the real world.
The team studied technologies including air filtration, germicidal lights and ionisers.
They looked at all the available evidence but found little to support hopes that these technologies can make air safe from respiratory or gastrointestinal infections.
Prof Paul Hunter, from UEA's Norwich Medical School, said: ""Air cleaners are designed to filter pollutants or contaminants out of the air that passes through them.
""When the Covid pandemic hit, many large companies and governments -- including the NHS, the British military, and New York City and regional German governments -- investigated installing this type of technology in a bid to reduce airborne virus particles in buildings and small spaces.
""But air treatment technologies can be expensive. So it's reasonable to weigh up the benefits against costs, and to understand the current capabilities of such technologies.""
The research team studied evidence about whether air cleaning technologies make people safe from catching airborne respiratory or gastrointestinal infections.

They analysed evidence about microbial infections or symptoms in people exposed or not to air treatment technologies in 32 studies, all conducted in real world settings like schools or care homes. So far none of the studies of air treatment started during the Covid era have been published.
Lead researcher Dr Julii Brainard, also from UEA's Norwich Medical School, said: ""The kinds of technologies that we considered included filtration, germicidal lights, ionisers and any other way of safely removing viruses or deactivating them in breathable air.
""In short, we found no strong evidence that air treatment technologies are likely to protect people in real world settings.
""There is a lot of existing evidence that environmental and surface contamination can be reduced by several air treatment strategies, especially germicidal lights and high efficiency particulate air filtration (HEPA). But the combined evidence was that these technologies don't stop or reduce illness.
""There was some weak evidence that the air treatment methods reduced likelihood of infection, but this evidence seems biased and imbalanced.
""We strongly suspect that there were some relevant studies with very minor or no effect but these were never published.

""Our findings are disappointing -- but it is vital that public health decision makers have a full picture.
""Hopefully those studies that have been done during Covid will be published soon and we can make a more informed judgement about what the value of air treatment may have been during the pandemic.""
This research was led by the University of East Anglia with collaborators at University College London, the University of Essex, the Norfolk and Norwich University Hospital Trust, and the University of Surrey.
It was funded by the National Institute for Health and Care Research Health Protection Unit in Emergency Preparedness and Response, led by Kings College London and UEA in collaboration with the UK Health Security Agency.

","score: 14.649742599742599, grade_level: '15'","score: 15.862696267696272, grade_levels: ['college_graduate'], ages: [24, 100]",10.1101/2023.06.15.23291419,"Installation of technologies to remove or deactivate respiratory pathogens from indoor air is a plausible non-pharmaceutical disease control strategy. We undertook a systematic review of observational and experimental studies, published 1970-2022, to synthesise evidence about the effectiveness of suitable indoor air treatment technologies to prevent respiratory or gastrointestinal infections. We searched for data about infection and symptom outcomes for persons who spent minimum 20 hours/week in shared indoor spaces subjected to air treatment strategies hypothesised to change risk of respiratory or gastrointestinal infections or symptoms. Pooled data suggested no net benefits for symptom severity or symptom presence, in absence of confirmed infection. There was weak evidence that air treatment technologies tended to reduce confirmed infections, but these data evinced strong publication bias. Although environmental and surface samples are reduced after air treatment by several air treatment strategies, especially germicidal lights and high efficiency particulate air filtration, robust evidence has yet to emerge to confirm that these technologies are effective in real world settings. Data from several relevant randomised trials have yet to report and will be welcome to the evidence base."
"
We've all heard it: Put a frog in boiling water, and it will jump out. But put the same frog in lukewarm water and heat it gradually, and you'll cook the frog. Often used as a metaphor for the unhurried and stubborn response many have to a slowly rising threat, the mechanisms underlying the urban myth have become a subject of scientific fascination.

This parable seems to have inspired new Northwestern University research, which identified a brain pathway responsible for rapid-threat detection.
""Animals are more likely to react to rapid rather than slow environmental change,"" said lead author Marco Gallio, associate professor of neurobiology in Northwestern's Weinberg College of Arts and Sciences. ""In the present study, we identify a brain circuit in fruit flies that selectively responds to rapid thermal change, priming behavior for escape.""
The findings were published last week in the journal Nature Communications.
Gallio generally uses fruit flies to understand sensory circuits and the ways they create perceptions of the physical world. Using the fly as a model, the lab studies basic decision-making principles in an animal that has a fraction of the number of neurons (100,000) than humans have (roughly 100 billion). As a well-studied model organism for biological research, flies also are useful subjects because of the pre-existing tools to study fly neurons and behavior.
""There are often two types of responses to external stimuli in the brain: Some neurons respond to a stimulus like light or temperature with very persistent activity,"" Gallio said. ""Other neurons fire just at the beginning, like when a light turns on, and then their activity is gone. We've always wondered what the significance of these short-lived responses is.""
In visual stimuli, brains are wired to notice a large contrast between light and dark. Gallio said that the response intuitively also makes sense for the sense of touch: You don't think about pressure when your hand is resting on a surface. Run your hand over something new, however, and you will detect subtle changes in texture. Gallio's team wanted to see if the same was true for the sense of temperature.

To explore how flies respond to rapid change, the team used a high-resolution camera to observe flies navigating different temperature environments. When flies encounter a rapid heat front, they always produce a U-turn away from it.
The lab found flies always responded in cases of rapid temperature change, but not for slow change.
The team also identified a circuit in the fly brain that responds only to rapid temperature change (more than 0.2 degrees Celsius per second). Much like light-ON cells of the visual system, these neurons fired at the beginning of rapid heating and then went quiet.
""Our hypothesis was that these heat-ON responses may indeed correlate with the rate of temperature change,"" said Jenna Jouandet, the study's first author and a Ph.D. student in the Gallio Lab. ""And therefore, may allow flies to anticipate dangerous thermal conditions and prepare to escape.""
Indeed, when the researchers experimentally inactivated those neurons, flies escaped less promptly.
To better understand how the activity of these neurons may be important for the behavior of the fly, the researchers collaborated with William Kath, applied math professor at Northwestern and deputy director of the new National Institute for Theory and Mathematics in Biology. Applied math Ph.D. student Richard Suhendra built a small computer model with two antennae and two wheels to demonstrate how adding a neuron that anticipates dangerous heat could improve the flexibility of the vehicle response. (Play with the model through a simple game on the Gallio Lab webpage.)
""The neurons that we initially discovered take input from the thermosensory neurons on the antennae and carry information to the higher brain,"" Gallio said. ""Flies are a great model to map brain circuits in that we were able to reconstruct the full circuit from sensory neurons all the way down to the centers that produce movement.""

Gallio explained that rapid changes are nearly always dangerous for a small fly.
""If the temperature is changing by half a degree per second -- which is not that much -- within 30 or 40 seconds, that fly could be dead,"" Gallio said. ""This system is an alarm bell that rings to prime an animal's behavior for escape. We see the fly escape.""
Gallio hypothesizes that the results are broadly generalizable, especially because he sees it play out in humans, whether someone is entering a room that's a different temperature or getting into a hot shower. He said these neurons seem to be able to sense something others do not -- they seem to be able to anticipate the future.
The research reported in this publication was supported by the National Institutes of Health (grants R01NS086859, R21EY031849 and R21NS130554), a Pew Scholars Program in the Biomedical Sciences and a McKnight Technological Innovations in Neuroscience Awards. The research was supported in part through the computational resources provided for the Quest high performance computing facility at Northwestern University, which is jointly supported by the Office of the Provost, the Office for Research and Northwestern University Information Technology; the Training Grant in Circadian and Sleep Research (T32HL007909) and the National Science Foundation research training grant (DMS-1547394).

","score: 12.634785553047404, grade_level: '13'","score: 13.942691576571228, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41467-023-42864-5,"Neurons that participate in sensory processing often display “ON” responses, i.e., fire transiently at the onset of a stimulus. ON transients are widespread, perhaps universal to sensory coding, yet their function is not always well-understood. Here, we show that ON responses in the Drosophila thermosensory system extrapolate the trajectory of temperature change, priming escape behavior if unsafe thermal conditions are imminent. First, we show that second-order thermosensory projection neurons (TPN-IIIs) and their Lateral Horn targets (TLHONs), display ON responses to thermal stimuli, independent of direction of change (heating or cooling) and of absolute temperature. Instead, they track the rate of temperature change, with TLHONs firing exclusively to rapid changes (>0.2 °C/s). Next, we use connectomics to track TLHONs’ output to descending neurons that control walking and escape, and modeling and genetic silencing to demonstrate how ON transients can flexibly amplify aversive responses to small thermal change. Our results suggest that, across sensory systems, ON transients may represent a general mechanism to systematically anticipate and respond to salient or dangerous conditions."
"
Quiz shows, where contestants answer rapid-fire questions in a high-stress, high-stakes environment, are an integral part of TV programming -- and now they are demonstrating that they have research value.

By analyzing contestants' behavior and patterns of blinking on the British TV show ""Mastermind,"" cognitive scientists at the University of Arizona have studied human physiology under conditions of stress that would be impossible to reproduce in the lab.
The results were published yesterday in the journal Psychophysiology.
""This is a dream I've had for a long time -- to try to get physiological information out of video signals,"" said Robert Wilson, senior author on the paper, who is an associate professor in cognition and neural systems at the UArizona Department of Psychology, in the College of Science.
On ""Mastermind,"" contestants sit in a big leather chair, Wilson said, answering rapid-fire questions under the glare of spotlights as a camera slowly zooms in on their face. The bright lights and slow camera work make it easy to identify blinks, and the stress of being interrogated on national TV cannot be re-created under lab conditions, he said.
""This is a feasible method for doing video-based neuroscience in outside-the-lab conditions so that we can get into situations that are closer to real-world scenarios,"" said Skyler Wyly, the lead author of the study, who began this work as an undergraduate at UArizona and is now a doctoral student at Duke University.
How blinking is related to cognitive effort still needs to be explored, especially in real world tasks, Wilson said. However, he mentioned that there is a hypothesis that the more stressed humans are, the more they blink.

""We are yet to know about all the cognitive processes that modulate blinking,"" Wilson said.
The researchers analyzed 25 episodes from two seasons of the game show to collect data from 100 contestants. A team of nearly 60 research assistants marked the onset and offset of every question and every response made by the contestants, as well as the time of every blink, all of which contributed to nearly 100,000 data points. The researchers then analyzed the data to determine how blinking varied across individuals at different points in the game and compared their findings to those from less-stressful experiments performed in previous lab-based studies.
One key finding from the lab is that blinking acts like a ""punctuation of thought,"" Wilson said, and this result held true on TV. Contestants blinked at the ""punctuation marks"" of the game -- at the start of each question and at the start of their response. They also reduced their blinking while they were thinking about how to answer, which is also in line with results from the lab, Wilson said. The stress of ""Mastermind"" was also apparent in contestants' blinking, with their blink rate nearly twice the number of the usual 20 blinks per minute of a person who is at rest.
However, some findings differed from lab tests. For example, in the TV show, older adults blinked more than younger adults, and women blinked more than men. In the lab, there are no differences between these groups, Wilson said.
There were also some behavioral differences. In lab tests, people tend to slow down and respond more carefully and accurately after they make an error. In ""Mastermind,"" this was not the case.
It's not clear what is driving these differences between the lab and TV, but according to Wilson, the differences are the most exciting part of the research. One of the biggest questions in psychology right now, Wilson said, is how much of what is observed in the lab relates to what is going on in the real world.
""This question is critical, not only for our basic understanding of the mind, but also more practically if we want to find lab tests that can diagnose mental illness,"" Wilson said.
For the researchers at UArizona, blinks are just the beginning, and there is so much information in video signals, Wilson said. The way people look, breathe and fidget in their seat can be pulled out by modern computer vision techniques. This can help researchers get a multidimensional measure of physiology.
""This is exactly what we need to study -- the psychophysiology of real human behavior and real human thinking,"" Wilson said.

","score: 12.265229366165073, grade_level: '12'","score: 13.253087095303236, grade_levels: ['college_graduate'], ages: [24, 100]",10.1111/psyp.14485,"Television game shows have proven to be a valuable resource for studying human behavior under conditions of high stress and high stakes. However, previous work has focused mostly on choices—ignoring much of the rich visual information that is available on screen. Here, we take a first step to extracting more of this information by investigating the response times and blinking of contestants in the BBC show Mastermind. In Mastermind, contestants answer rapid‐fire quiz questions while a camera slowly zooms in on their faces. By labeling contestants' behavior and blinks from 25 episodes, we asked how accuracy, response times, and blinking varied over the course of the game. For accuracy and response times, we tested whether contestants responded more accurately and more slowly after an error—exhibiting the “post‐error increase in accuracy” and “post‐error slowing” which has been repeatedly observed in the lab. For blinking, we tested whether blink rates varied according to the cognitive demands of the game—decreasing during periods of cognitive load, such as when pondering a response, and increasing at event boundaries in the task, such as the start of a question. In contrast to the lab, evidence for post‐error changes in accuracy and response time was weak, with only marginal effects observed. In line with the lab, blinking varied over the course of the game much as we predicted. Overall, our findings demonstrate the potential of extracting dynamic signals from game shows to study the psychophysiology of behavior in the real world."
"
Many mammals, from domestic cats and dogs to giant pandas, use scent to communicate with each other. A new study from the University of California, Davis shows how domestic cats send signals to each other using odors derived from families of bacteria living in their anal glands. The work was published Nov. 8 in Scientific Reports.

The study adds to a growing body of research on the relationship between microbes and odor in mammals, including domestic dogs, wild animals such as foxes, pandas and hyenas, and humans.
Cats' scent comes from a mix of volatile organic compounds, including aldehydes, alcohols, esters and ketones. While mostly undetectable to human noses, these scents are important in cats' behavior and social lives. They mark territory, attract mates and repel rivals.
Connie Rojas, a postdoctoral researcher working with Professor Jonathan Eisen at the UC Davis Department of Evolution and Ecology and Genome Center, led a three-part study of anal gland secretions from domestic cats. They used DNA sequencing, mass spectrometry and microbial culturing to look at the chemicals in the secretions and the microbes that make them.
The subjects in the study were 23 domestic cats seen at the UC Davis Veterinary Medical Teaching Hospital for elective procedures such as dental cleaning. Owners gave written permission for their cats to take part in the study.
Highly variable microbiome
Five genera of bacteria (Corynebacterium, Bacteroides, Proteus, Lactobacillus and Streptococcus) dominated overall, but the microbial makeup was highly variable between individual cats. Older cats generally had a different microbiome from younger animals. There were also some apparent differences in cats assessed as obese, but the sample size was not large enough to confirm this. Microbial populations might also be affected by factors such as the cat's diet, health conditions and its overall living environment.

Looking at the chemicals produced in the anal glands, the researchers detected hundreds of organic compounds. Genetic analysis showed that the bacteria living in the anal gland could be responsible for making these compounds.
The researchers hope to continue and expand the study to include more domestic cats as well as other species of cats.
Additional coauthors on the study, all at UC Davis, are: David Coil, Genome Center; Stanley Marks, School of Veterinary Medicine; Eva Borras, Mitchell McCartney and Cristina Davis, Department of Mechanical and Aerospace Engineering and UC Davis Lung Center; and Hira Lesea, Department of Microbiology and Molecular Genetics.
The work was supported in part by grants from the National Institutes of Health and by a UC Davis Chancellor's fellowship to Rojas.

","score: 13.675841607565012, grade_level: '14'","score: 14.109113475177303, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41598-023-45997-1,"Many mammals rely on volatile organic chemical compounds (VOCs) produced by bacteria for their communication and behavior, though little is known about the exact molecular mechanisms or bacterial species that are responsible. We used metagenomic sequencing, mass-spectrometry based metabolomics, and culturing to profile the microbial and volatile chemical constituents of anal gland secretions in twenty-three domestic cats (Felis catus), in attempts to identify organisms potentially involved in host odor production. We found that the anal gland microbiome was dominated by bacteria in the genera Corynebacterium, Bacteroides, Proteus, Lactobacillus, and Streptococcus, and showed striking variation among individual cats. Microbiome profiles also varied with host age and obesity. Metabolites such as fatty-acids, ketones, aldehydes and alcohols were detected in glandular secretions. Overall, microbiome and metabolome profiles were modestly correlated (r = 0.17), indicating that a relationship exists between the bacteria in the gland and the metabolites produced in the gland. Functional analyses revealed the presence of genes predicted to code for enzymes involved in VOC metabolism such as dehydrogenases, reductases, and decarboxylases. From metagenomic data, we generated 85 high-quality metagenome assembled genomes (MAGs). Of importance were four MAGs classified as Corynebacterium frankenforstense, Proteus mirabilis, Lactobacillus johnsonii, and Bacteroides fragilis. They represent strong candidates for further investigation of the mechanisms of volatile synthesis and scent production in the mammalian anal gland."
"
After a long day at work, you open the door to the place you call home. A chorus of furry happiness rushes toward you, the sound of unconditional canine love. With your return, your dog's world is whole.

Virginia Tech and Arizona State University researchers are working to help more shelter dogs experience this kind of love, safety, and happiness in an adoptive home.
The research team in Virginia Tech's College of Agriculture and Life Sciences found that implementing shorter-term fostering programs at animal shelters vastly improves adoptions for our canine friends.
Spending time with a dog is one of the most consistently effective ways to improve a dog's life in the shelter. Time out of the kennel with a person can reduce physiological measures of stress, as can a single night or more in a foster caregiver's home.
In this study, the researchers assessed the effects of outings of just a few hours and fostering stays of one to two nights on dogs' length of stay in the shelter and their adoption outcomes.
The researchers found that brief outings and temporary fostering stays increased dogs' likelihood of adoption by five and more than 14 times, respectively. The team also found that these programs were more successful when a greater proportion of community members were providing outings and stays to the shelters' dogs as well as when these programs were carried out by shelters with more resources.
While short in their duration, these fostering programs can make a big impact on the lives of shelter dogs.

The research was funded with a $1.7 million grant from Maddie's Fund, a national family foundation established by Dave and Cheryl Duffield to revolutionize the status and well-being of companion animals, and was published today in the journal Animals.
Helping man's best friend
At Virginia Tech, the project was spearheaded by Erica Feuerbacher, associate professor in the School of Animal Sciences, and Lisa Gunter, assistant professor in the school who originally worked on the project as the Maddie's Fund Research Fellow at Arizona State University.
In previous work, the team investigated how outings and temporary fostering stays influenced dogs' stress and activity levels but did not consider if these experiences helped homeless canines find their forever homes. The answer, based on the research, is yes.
""It's a really exciting finding. Our prior work showed how beneficial sleepovers were for reducing dogs' stress,"" Feuerbacher said. ""It's wonderful to know that it also helps them get adopted.""
The results showed that for foster outings, about 4 percent of the people ended up adopting the dog. For overnight stays, the number increased to about 12 percent. Both results show that the vast majority of adopters were not the foster families.

""We saw that the majority of people adopting the dogs weren't the caregivers that were taking the dogs on outings or letting them stay in their homes. These dogs were being seen in the community, meeting new people, and caregivers were sharing their stories,"" Gunter said. ""This increased exposure likely helped the dogs find their adopters.""
Data was analyzed from 51 animal shelters in the United States on 1,955 dogs that received these fostering interventions as well as 25,946 dogs residing at these shelters that served as the study's controls. Over the grant's four-year lifespan, 85 shelter partners helped the research team carry out studies on brief outings, temporary stays, foster caregiving during the pandemic, weeklong fostering, and safety net fostering for pets whose owners were experiencing hardship.
While dogs' lengths of stay in this study were longer in comparison to dogs that did not receive a brief outing or temporary fostering, this difference was present prior to the intervention, suggesting that shelters are using these programs for dogs that need more help in finding homes. After going on an outing or fostering stay, dogs waited just 10 days to be adopted.
""Our data show that these programs can help the dogs not only have an improved experience in the shelter, but also dramatically increase their likelihood of adoption, and for the shelters that get their communities involved in brief outings and temporary fostering stays, better performing programs,"" Gunter said.
Clive Wynne, at Arizona State University, led the project, who was assisted by Gunter when she was a research scientist at the university.
""It's great news that even short-term fostering has positive impacts on shelter dogs' welfare and helps them get adopted because there are so many dogs in shelters in the United States and even the best shelters are not good places for dogs to be living,"" Wynne said.
Future paw prints
The increase in dog adoptions with short-term foster programs underscores their value to local shelters, the researchers said. Their findings highlight the importance of having resources available to shelters to support these programs. These programs are not as easy for some shelters as they are for others -- it takes support both financial and human.
""These kinds of fostering programs can save the lives of dogs in shelters,"" Gunter said. ""Currently, shelters are struggling with dog adoptions, and we have evidence that these programs support placement into homes, which in turn can help shelters help more dogs.""
Wynne also saw the future of this work as easy to implement for shelters.
""One of the beauties of this program of research is that the fostering intervention is relatively low cost for shelters,"" Wynne said. ""More than anything else, what shelters need is education on how to implement fostering, and helping them with that was an important aspect of this research program.""

","score: 12.472718762718767, grade_level: '12'","score: 14.03601953601953, grade_levels: ['college_graduate'], ages: [24, 100]",10.3390/ani13223528,"Human interaction is one of the most consistently effective interventions that can improve the welfare of shelter-living dogs. Time out of the kennel with a person has been shown to reduce physiological measures of stress as can leaving the shelter for a night or more in a foster home. In this study, we assessed the effects of brief outings and temporary fostering stays on dogs’ length of stay and outcomes. In total, we analyzed data of 1955 dogs from 51 animal shelters that received these interventions as well as 25,946 dogs residing at these shelters that served as our controls. We found that brief outings and temporary fostering stays increased dogs’ likelihood of adoption by 5.0 and 14.3 times, respectively. While their lengths of stay were longer in comparison to control dogs, this difference was present prior to the intervention. Additionally, we found that these programs were more successful when greater percentages of community members (as compared to volunteers and staff) were involved in caregiving as well as when programs were implemented by better-resourced shelters. As such, animal welfare organizations should consider implementing these fostering programs as evidence-based best practices that can positively impact the outcomes of shelter dogs."
"
It's one of the most famous taglines in film history, immortalising sharks as ruthless predators. But beyond the horror generated by Spielberg's Jaws series, a persistent fear of sharks remains, with consequences that extend into reality.

Following human-shark interactions in South Australia, this fear has prompted the Education Department's ban on school-based sea activities for at least the remainder of the term. And while safety is at the core of such decisions, we should be cautious of scaremongering, says UniSA shark expert Dr Brianna Le Busque.
""When we hear about shark ""attacks,"" it definitely puts people on edge, especially when interactions and sightings are sensationalised by the media,"" Dr Le Busque says.
""As most people do not have personal interactions with sharks, most of what we know about sharks comes from what we see on TV or in movies. Movies such as Jaws, The Meg or The Shallows depict sharks as purposely hunting and attacking humans, which not only creates excessive fear but strengthens any negative views people may already hold.
""This is called the 'The Jaws Effect' -- a known phenomenon where people are excessively and irrationally scared of sharks -- today, nearly 50 years after the first Jaws movie, it still influences people's perceptions of sharks, impacts conservation efforts, and affects policy decisions.
""That's what we've seen with the current bans on sea-based water activities. And the problem is that it could have negative impacts on children's ideas of water and beach safety.""
In a new UniSA world first study, Dr Le Busque shows how over-represented sharks are in the realm of 'creature features' -- a subgenre of science fiction, horror, or action films where the creatures are the villain in the plot.

""Sharks are commonplace in 'creature feature' films -- they overrepresented, being the most common animal in this film category. Further, of all films that depict sharks (in various genres) 96% overtly portrayed shark-human interactions as threatening.""
In the past 50 years, oceanic sharks have declined by more than 70%, with one in three species now threatened by extinction.
Dr Le Busque says while she believes the bans on school activities are currently unwarranted, she welcomes the early deployment of aerial shark patrols.
""Earlier shark monitoring is a good move to protect beachgoers, but we need a balance between people's safety and access to the ocean,"" Dr Le Busque says.
""No one wants a shark attack to occur, but these bans are just creating the same fear as generated on the ill-fated Amity Island in Jaws. It's just not the right way to go.""
CEO Surf Life Saving SA, Damien Marangon, says the ban on beach-based aquatic programs, without consultation or understanding facts and broader impact, was disappointing.

""Whilst the shark attack is incredibly unfortunate, it's also important to remember that tragically, far more people sadly drown every year in South Australian waters, than there are shark attacks,"" Marangon says.
""Over the past 20 years, we've averaged just over one instance per year. Despite the incidents over the last month or so, our data shows that we have not seen an increase in shark numbers.
""We were concerned about the impact this would have on the 3,899 students, from 47 schools who were enrolled in that program for the remainder of this term, who would not have had the opportunity to learn vital water safety skills, jeopardising their future safety in and around the water.
""Decisions like this, made in isolation, also unfortunately promote a fear of the ocean, which could have wide ranging and long-lasting impacts on our communities, local traders, family businesses, and the travel industry, by unnecessarily exacerbating a fear of our ocean, and will impact visitors to our beaches and our State.
""We're actively working with the Water Safety Unit within the Department for Education on strategies to continue to ensure that all participants can complete these aquatic education programs at the beach safely, and continue to make informed decisions based on data, research and stakeholder engagement.""

","score: 14.050886803519067, grade_level: '14'","score: 15.442316715542525, grade_levels: ['college_graduate'], ages: [24, 100]",10.1386/jem_00096_1,"Media are conduits for people to obtain information about animal species and may therefore influence how people think about these species. This study advances our understanding of animals (and plants) in the media by analysing a final dataset of 638 films categorized in the genre ‘Creature Features’. Through analysing the biography, film poster and trailer on the IMDb database, it was found that sharks were the most depicted species in creature feature films, with insects and arachnids, dinosaurs and snakes also being frequently featured. There were changes in the types of animal species commonly portrayed in creature feature films across time, with dinosaurs and primates being more frequently depicted in the 1920s–30s and sharks being more frequently depicted in recent decades. This study is the first to investigate which animal/plant species are evident in creature feature films, which is a broader genre incorporating mythology, extant and general unrealistic portrayals of animals. This allows for new understandings regarding the influence the media can have on perceptions of animal and plant species."
"
Despite national medical guidelines supporting the use of antiviral medications in young children diagnosed with influenza, a new study reports an underuse of the treatment.

""Trends in Outpatient Influenza Antiviral Use Among Children and Adolescents in the United States"" was published in Pediatrics, a peer-reviewed journal of the American Academy of Pediatrics.
""Antiviral treatment, when used early, improves health outcomes with influenza,"" said lead author and principal investigator James Antoon, MD, PhD, MPH, assistant professor of Pediatrics and Hospital Medicine at Monroe Carell Jr. Children's Hospital at Vanderbilt.
Antoon and colleagues collaborated with researchers from the University of Illinois at Chicago on a large study examining outpatient and emergency department prescription claims for patients younger than 18 from all 50 states over a nine-year period.
Oseltamivir, also known at Tamiflu, is the only oral antiviral medication approved for use in children younger than 5 years old.
""We found that young children, less than 5 years old and especially those 2 years old and younger, are undertreated for influenza,"" said Antoon. ""We noted that about 40% of children were treated with an antiviral, when guidelines recommend all of them be treated. It's important to note that we found low rates of antiviral use in all age groups.""
The study also found wide geographic variation in the use of influenza antivirals -- there was a threefold to twentyfold difference in the rate of antiviral use based on geographic region that was not explained by differences in the incidence of flu, said Antoon.

""These findings highlight opportunities for improvement in the prevention and treatment of influenza, especially in the most vulnerable children,"" he added.
Potential reasons for underprescribing in children include a wide range of perceptions about efficacy, differences in interpretation of testing, a misunderstanding of the national guidelines and concern for adverse drug events associated with oseltamivir in children.
A previous 2023 study, led by Antoon, explored how often children diagnosed with influenza experience serious neuropsychiatric side effects.
In that study, Antoon and his team were able to quantify the number of pediatric neuropsychiatric events, describe which children are more likely to experience the events and showed that these relatively infrequent events occur in both those children treated and not treated with an antiviral.
""Treatment of children in the outpatient setting has been reported to decrease symptom duration, household transmission, antibiotic use and influenza- associated complications like ear infections,"" said Antoon.
The low rate of antiviral use in young children captured in the study along with recent evidence of low guideline-concordant antiviral treatment in children at high risk for influenza complications highlight the need for improved flu management in the most vulnerable children in the U.S., according to the study.
The study was funded by grants K23AI168496 from the National Institute of Allergy and Infectious Diseases (NIAID), which is part of the National Institutes of Health, an American Pediatrics Association Young Investigator Award and the Vanderbilt University Medical Center Turner Hazinski Research Award. Carlos Grijalva, MD, MPH, professor of Health Policy and Biomedical Informatics at Vanderbilt University Medical Center (K24AI148459) and Derek Williams, MD, MPH, chief of the Division of Hospital Medicine at Monroe Carell (RO1AI125642) both supported by the NIAID.

","score: 18.576138958355987, grade_level: '19'","score: 20.443354354680878, grade_levels: ['college_graduate'], ages: [24, 100]",10.1542/peds.2023-061960,"Influenza antivirals improve outcomes in children with duration of symptoms &lt;2 days and those at high risk for influenza complications. Real-world prescribing of influenza antivirals in the pediatric population is unknown. We performed a cross-sectional study of outpatient and emergency department prescription claims in individuals &lt;18 years of age included in the IBM Marketscan Commercial Claims and Encounters Database between July 1, 2010 and June 30, 2019. Influenza antiviral use was defined as any dispensing of oseltamivir, baloxavir, or zanamivir. The primary outcome was the rate of antiviral dispensing per 1000 enrolled children. Secondary outcomes included antiviral dispensing per 1000 influenza diagnoses and inflation-adjusted costs of antiviral agents. Outcomes were calculated and stratified by age, acute versus prophylactic treatment, influenza season, and geographic region. The analysis included 1 416 764 unique antiviral dispensings between 2010 and 2019. Oseltamivir was the most frequently prescribed antiviral (99.8%). Dispensing rates ranged from 4.4 to 48.6 per 1000 enrolled children. Treatment rates were highest among older children (12–17 years of age), during the 2017 to 2018 influenza season, and in the East South Central region. Guideline-concordant antiviral use among young children (&lt;2 years of age) at a high risk of influenza complications was low (&lt;40%). The inflation-adjusted cost for prescriptions was $208 458 979, and the median cost ranged from $111 to $151. There is wide variability and underuse associated with influenza antiviral use in children. These findings reveal opportunities for improvement in the prevention and treatment of influenza in children."
"
Following the COVID-19 pandemic, the increase in virtual interactions has created a new challenge: fatigue caused by video calls, also known as Zoom fatigue or videoconference fatigue. This exhaustion, characterized by a feeling of tiredness and alienation due to too long or inappropriate video-based communication, had previously only been investigated through surveys and self-assessments by users. An interdisciplinary research team led by René Riedl from the University of Applied Sciences Upper Austria/Campus Steyr and Gernot Müller-Putz from Graz University of Technology has now managed to provide neurophysiological evidence of videoconference fatigue.

In the ""Technostress in Organizations"" project funded by the Austrian Science Fund FWF, the researchers conducted a neuroscientific study with students to investigate videoconference fatigue in the context of online university lectures. The test subjects took part in lectures that were held both in-person in a traditional lecture hall and online via video conferencing. These two experimental conditions were then compared with each other. The research team measured fatigue parameters both neurophysiologically based on electroencephalography (EEG) and electrocardiography (ECG) and by questionnaires. This allowed them to record objective physiological parameters and subjective perceptions. The objective findings based on EEG and specific parameters of heart rate variability as well as the subjective perceptions of the respondents showed that a 50-minute video conference-based lecture exhausted the test subjects significantly more than a lecture of the same length in the traditional lecture hall format, where lecturers and students meet face to face.
The study has been published in the high-profile journal Scientific Reports. ""A better understanding of videoconference fatigue is important, as this phenomenon has a far-reaching impact on the well-being of individuals, interpersonal relationships and organizational communication,"" emphasizes René Riedl. Gernot Müller-Putz further explains that ""a holistic view of the underlying psychological and physiological mechanisms is required to develop effective strategies for coping with the harmful effects of videoconference fatigue.""
Together with two North American colleagues, the two scientists constitute the board of the Society for Neuro-Information Systems, a non-profit international scientific association based in Vienna that promotes and supports research and innovation at the intersection of neuroscience, information systems research and digitization. A key aim of this society is to make people more satisfied and productive when using digital technologies. ""A better understanding of the neurophysiological processes in the body and brain of users is essential to achieve these goals,"" the two scientists conclude.

","score: 18.590493827160497, grade_level: '19'","score: 20.306740740740743, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41598-023-45374-y,"In the recent past, many organizations and people have substituted face-to-face meetings with videoconferences. Among others, tools like Zoom, Teams, and Webex have become the “new normal” of human social interaction in many domains (e.g., business, education). However, this radical adoption and extensive use of videoconferencing tools also has a dark side, referred to as videoconference fatigue (VCF). To date only self-report evidence has shown that VCF is a serious issue. However, based on self-reports alone it is hardly possible to provide a comprehensive understanding of a cognitive phenomenon like VCF. Against this background, we examined VCF also from a neurophysiological perspective. Specifically, we collected and analyzed electroencephalography (continuous and event-related) and electrocardiography (heart rate and heart rate variability) data to investigate whether VCF can also be proven on a neurophysiological level. We conducted a laboratory experiment based on a within-subjects design (N = 35). The study context was a university lecture, which was given in a face-to-face and videoconferencing format. In essence, the neurophysiological data—together with questionnaire data that we also collected—show that 50 min videoconferencing, if compared to a face-to-face condition, results in changes in the human nervous system which, based on existing literature, can undoubtedly be interpreted as fatigue. Thus, individuals and organizations must not ignore the fatigue potential of videoconferencing. A major implication of our study is that videoconferencing should be considered as a possible complement to face-to-face interaction, but not as a substitute."
"
White faces generated by artificial intelligence (AI) now appear more real than human faces, according to new research led by experts at The Australian National University (ANU).

In the study, more people thought AI-generated white faces were human than the faces of real people. The same wasn't true for images of people of colour.
The reason for the discrepancy is that AI algorithms are trained disproportionately on white faces, Dr Amy Dawel, the senior author of the paper, said.
""If white AI faces are consistently perceived as more realistic, this technology could have serious implications for people of colour by ultimately reinforcing racial biases online,"" Dr Dawel said.
""This problem is already apparent in current AI technologies that are being used to create professional-looking headshots. When used for people of colour, the AI is altering their skin and eye colour to those of white people.""
One of the issues with AI 'hyper-realism' is that people often don't realise they're being fooled, the researchers found.
""Concerningly, people who thought that the AI faces were real most often were paradoxically the most confident their judgements were correct,"" Elizabeth Miller, study co-author and PhD candidate at ANU, said.

""This means people who are mistaking AI imposters for real people don't know they are being tricked.""
The researchers were also able to discover why AI faces are fooling people.
""It turns out that there are still physical differences between AI and human faces, but people tend to misinterpret them. For example, white AI faces tend to be more in-proportion and people mistake this as a sign of humanness,"" Dr Dawel said.
""However, we can't rely on these physical cues for long. AI technology is advancing so quickly that the differences between AI and human faces will probably disappear soon.""
The researchers argue this trend could have serious implications for the proliferation of misinformation and identity theft, and that action needs to be taken.
""AI technology can't become sectioned off so only tech companies know what's going on behind the scenes. There needs to be greater transparency around AI so researchers and civil society can identify issues before they become a major problem,"" Dr Dawel said.
Raising public awareness can also play a significant role in reducing the risks posed by the technology, the researchers argue.
""Given that humans can no longer detect AI faces, society needs tools that can accurately identify AI imposters,"" Dr Dawel said.
""Educating people about the perceived realism of AI faces could help make the public appropriately sceptical about the images they're seeing online.""

","score: 12.003570351217412, grade_level: '12'","score: 12.430687351863824, grade_levels: ['college'], ages: [18, 24]",10.1177/09567976231207095,"Recent evidence shows that AI-generated faces are now indistinguishable from human faces. However, algorithms are trained disproportionately on White faces, and thus White AI faces may appear especially realistic. In Experiment 1 ( N = 124 adults), alongside our reanalysis of previously published data, we showed that White AI faces are judged as human more often than actual human faces—a phenomenon we term AI hyperrealism. Paradoxically, people who made the most errors in this task were the most confident (a Dunning-Kruger effect). In Experiment 2 ( N = 610 adults), we used face-space theory and participant qualitative reports to identify key facial attributes that distinguish AI from human faces but were misinterpreted by participants, leading to AI hyperrealism. However, the attributes permitted high accuracy using machine learning. These findings illustrate how psychological theory can inform understanding of AI outputs and provide direction for debiasing AI algorithms, thereby promoting the ethical use of AI."
"
Nearly one in five school-aged children and preteens now take melatonin for sleep, and some parents routinely give the hormone to preschoolers, according to new research from the University of Colorado Boulder published Nov. 13 in JAMA Pediatrics.

This concerns the authors, who note that safety and efficacy data surrounding the products are slim, such dietary supplements lack full regulation by the Food and Drug Administration.
""We hope this paper raises awareness for parents and clinicians, and sounds the alarm for the scientific community,"" said lead author Lauren Hartstein, PhD, a postdoctoral fellow in the Sleep and Development Lab at CU Boulder. ""We are not saying that melatonin is necessarily harmful to children. But much more research needs to be done before we can state with confidence that it is safe for kids to be taking long-term.""
Calls to poison control centers up
Melatonin is produced naturally in the pineal gland to signal the body that it is time to sleep and regulate its circadian rhythm -- the physiological cycle over a 24-hour period.
In many countries, the hormone is classified as a drug and available by prescription only.
In the United States, however, chemically synthesized or animal-derived melatonin is available over the counter as a dietary supplement, and increasingly available in child-friendly gummies.

""All of a sudden, in 2022, we started noticing a lot of parents telling us that their healthy child was regularly taking melatonin,"" said Hartstein, who studies how environmental cues, including light at night, impact children's sleep quality and melatonin production.
During 2017-18, only about 1.3% of U.S. parents reported that their children used melatonin.
To get a sense of the current prevalence of use, Hartstein and colleagues surveyed about 1,000 parents in the first half of 2023.
Among children ages 5 to 9, 18.5% surveyed had been given melatonin in the previous 30 days. For preteens ages 10 to 13, that number rose to 19.4%. Nearly 6% of preschoolers ages 1 to 4 had used melatonin in the previous month.
Preschoolers who used melatonin had been taking it for a median length of a year. Grade-schoolers and preteens had used it for median lengths of 18 and 21 months, respectively.
The older the child, the greater the dosage, with preschoolers taking anywhere from 0.25 to 2 mg and preteens taking up to 10 mg.

A need for caution
In a study published in April, researchers analyzed 25 melatonin gummy products and found that 22 contained different amounts of melatonin than the label indicated. One had more than three times the amount on the label. One had none at all. In addition, some melatonin supplements have been found to contain other concerning substances, such as serotonin.
""Parents may not actually know what they are giving to their children when administering these supplements,"" said Hartstein.
Some scientists have also raised concerns that giving melatonin to young people whose brains and bodies are still developing could influence the timing of puberty onset.
The few small-scale human studies that have looked into this have yielded inconsistent results.
Gummies, specifically, also carry another risk: They look like and taste like candy.
From 2012 to 2021, the authors note, reports of melatonin ingestion to poison control centers increased 530%, largely occurring among children under age 5. More than 94% were unintentional and 85% were asymptomatic.
A place for judicious use
Co-author Julie Boergers, PhD, a psychologist and pediatric sleep specialist at Rhode Island Hospital and the Alpert Medical School of Brown University, said that when used under the supervision of a health care provider, melatonin can be a useful short-term aid, particularly in youth with autism or severe sleep problems.
""But it is almost never a first-line treatment,"" she said, noting that she often recommends that families look to behavioral changes first and use melatonin only temporarily. ""Although it's typically well-tolerated, whenever we're using any kind of medication or supplement in a young, developing body we want to exercise caution.""
Anecdotally, she has heard from parents that the supplement often works well in the beginning but over time children may need higher doses to achieve the same effect.
Introducing melatonin early in life could also have another unintended consequence, said Hartstein: It could send a message that, if you have trouble sleeping, a pill is the answer.
The authors caution that the study was relatively small and does not necessarily represent usage nationwide. It is telling nonetheless.
""If this many kids are taking melatonin, that suggests there are a lot of underlying sleep issues out there that need to be addressed,"" Hartstein said. ""Addressing the symptom doesn't necessarily address the cause.""

","score: 12.881127972408787, grade_level: '13'","score: 13.50649119622436, grade_levels: ['college_graduate'], ages: [24, 100]",10.1001/jamapediatrics.2023.4749,"This survey study describes parent-reported sleep practices, such as prevalence, frequency, and timing of melatonin use, among young people aged 1 to 13 years."
"
Nearly everyone can lower their blood pressure, even people currently on blood pressure-reducing drugs, by lowering their sodium intake, reports a new study from Northwestern Medicine, Vanderbilt University Medical Center and the University of Alabama at Birmingham.

""In the study, middle-aged to elderly participants reduced their salt intake by about 1 teaspoon a day compared with their usual diet. The result was a decline in systolic blood pressure by about 6 millimeters of mercury (mm Hg), which is comparable to the effect produced by a commonly utilized first-line medication for high blood pressure,"" said Dr. Deepak Gupta, associate professor of medicine at Vanderbilt University Medical Center and co-principal investigator.
""We found that 70-75% of all people, regardless of whether they are already on blood pressure medications or not, are likely to see a reduction in their blood pressure if they lower the sodium in their diet,"" said co-principal investigator Norrina Allen, professor of preventive medicine at Northwestern University Feinberg School of Medicine.
This is one of the largest studies to investigate the effect of reducing sodium in the diet on blood pressure to include people with hypertension and already on medications.
""We previously didn't know if people already on blood pressure medication could actually lower their blood pressure more by reducing their sodium,"" said Allen, also the Quentin D. Young Professor of Health Policy and director of the Center for Epidemiology and Population Health.
The study will be published Nov. 11 in the Journal of the American Medical Association and presented at the American Heart Association Scientific Sessions 2023 in Philadelphia.
The total daily sodium intake recommended by the AHA is to be below 1,500 milligrams and this study was designed to decrease it even lower than that, Allen said. ""It can be challenging but reducing your sodium in any amount will be beneficial,"" she said.

High blood pressure is the leading cause of morbidity and mortality in the world. ""High blood pressure can lead to heart failure, heart attacks, and strokes because it puts extra pressure on your arteries,"" Allen said. ""It affects the heart's ability to work effectively and pump blood.""
How the study worked
Middle-aged to elderly individuals in their 50s to 70s from Birmingham, Alabama, and Chicago were randomized to either a high-sodium diet (2,200 mg per day on top of their usual diet) or low-sodium diet (500 mg in total per day) for one week, after which they crossed over to the opposite diet for one week.
On the day before each study visit, participants wore blood pressure monitors and collected their urine for 24 hours. Among 213 participants, systolic blood pressure was significantly lowered by 7 to 8 mm Hg when they ate the low-sodium diet compared with high-sodium diet, and by 6 mm Hg compared with their usual diet.
Overall, 72% of participants experienced a lowering of their systolic blood pressure on the low-sodium diet compared with their usual diet.
""The effect of reduction in dietary sodium on blood pressure lowering was consistent across nearly all individuals, including those with normal blood pressure, high blood pressure, treated blood pressure and untreated blood pressure,"" Gupta said.

""Just as any physical activity is better than none for most people, any sodium reduction from the current usual diet is likely better than none for most people with regards to blood pressure,"" he said.
""This reinforces the importance of reduction in dietary sodium intake to help control blood pressure, even among individuals taking medications for hypertension,"" Allen added.
The blood pressure lowering effect of dietary sodium reduction was achieved rapidly and safely within one week.
""The fact that blood pressure dropped so significantly in just one week and was well tolerated is important and emphasizes the potential public health impact of dietary sodium reduction in the population, given that high blood pressure is such a huge health issue worldwide,"" said co-investigator Dr. Cora Lewis, professor and chair of the department of epidemiology and professor of medicine at the University of Alabama at Birmingham.
""It is particularly exciting that the products we used in the low-sodium diet are generally available, so people have a real shot at improving their health by modifying their diet in this way,"" Lewis said.
Other authors include Krista Varady, Yan Ru Su, Meena Madhur, Daniel Lackland, Jared Reis, Thomas J. Wang and Donald Lloyd-Jones.
The research was supported by grant R01HL148661 and contracts 75N92023D00005 and 75N92023D00004from the National Heart, Lung and Blood Institute of the National Institutes of Health.
The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.

","score: 16.810949683944376, grade_level: '17'","score: 18.41040455120101, grade_levels: ['college_graduate'], ages: [24, 100]",10.1001/jama.2023.23651,"Dietary sodium recommendations are debated partly due to variable blood pressure (BP) response to sodium intake. Furthermore, the BP effect of dietary sodium among individuals taking antihypertensive medications is understudied. To examine the distribution of within-individual BP response to dietary sodium, the difference in BP between individuals allocated to consume a high- or low-sodium diet first, and whether these varied according to baseline BP and antihypertensive medication use. Prospectively allocated diet order with crossover in community-based participants enrolled between April 2021 and February 2023 in 2 US cities. A total of 213 individuals aged 50 to 75 years, including those with normotension (25%), controlled hypertension (20%), uncontrolled hypertension (31%), and untreated hypertension (25%), attended a baseline visit while consuming their usual diet, then completed 1-week high- and low-sodium diets. High-sodium (approximately 2200 mg sodium added daily to usual diet) and low-sodium (approximately 500 mg daily total) diets. Average 24-hour ambulatory systolic and diastolic BP, mean arterial pressure, and pulse pressure. Among the 213 participants who completed both high- and low-sodium diet visits, the median age was 61 years, 65% were female and 64% were Black. While consuming usual, high-sodium, and low-sodium diets, participants’ median systolic BP measures were 125, 126, and 119 mm Hg, respectively. The median within-individual change in mean arterial pressure between high- and low-sodium diets was 4 mm Hg (IQR, 0-8 mm Hg; P &amp;lt; .001), which did not significantly differ by hypertension status. Compared with the high-sodium diet, the low-sodium diet induced a decline in mean arterial pressure in 73.4% of individuals. The commonly used threshold of a 5 mm Hg or greater decline in mean arterial pressure between a high-sodium and a low-sodium diet classified 46% of individuals as “salt sensitive.” At the end of the first dietary intervention week, the mean systolic BP difference between individuals allocated to a high-sodium vs a low-sodium diet was 8 mm Hg (95% CI, 4-11 mm Hg; P &amp;lt; .001), which was mostly similar across subgroups of age, sex, race, hypertension, baseline BP, diabetes, and body mass index. Adverse events were mild, reported by 9.9% and 8.0% of individuals while consuming the high- and low-sodium diets, respectively. Dietary sodium reduction significantly lowered BP in the majority of middle-aged to elderly adults. The decline in BP from a high- to low-sodium diet was independent of hypertension status and antihypertensive medication use, was generally consistent across subgroups, and did not result in excess adverse events. ClinicalTrials.gov Identifier: NCT04258332"
"
With Amazon aiming to make 10,000 deliveries with drones in Europe this year and Walmart planning to expand its drone delivery services to an additional 60,000 homes this year in the states, companies are investing more research and development funding into drone delivery, But are consumers ready to accept this change as the new normal?

Northwestern University's Mobility and Behavior Lab, led by Amanda Stathopoulos, an associate professor of civil and environmental engineering, wanted to know if consumers were ready for robots to replace delivery drivers, in the form of automated vehicles, drones and robots. The team found that societally, there's work to do to shift public perceptions of the near-future technology.
""We need to think really carefully about the effect of these new technologies on people and communities, and to tune in to what they think about these changes,"" Stathopoulos, the study's senior author, said.
The study, titled ""Robots at your doorstep: Acceptance of near-future technologies for automated parcel delivery,"" published last week in the journal Scientific Reports. Researchers noted a ""complex and multifaceted"" relationship between behavior and acceptance of near-future technologies for automated parcel delivery.
While people were generally more willing to accept an automated vehicle as a substitute for a delivery person -- perhaps because there already is familiarity with self-driving cars -- people disliked drones and robots as options. However, as delivery speed increased and price decreased, likelihood to accept the technology increased.
They also found that tech-savvy consumers were more accepting of the near-future technologies than populations less familiar with the technology.
Stathopoulos is the William Patterson Junior professor of civil and environmental engineering at Northwestern's McCormick School of Engineering, where she studies the human aspects of new systems of mobility. She also is a faculty affiliate of Northwestern's Transportation Center. She said especially after the pandemic, people have come to expect efficient delivery from e-commerce purchases as they increasingly work from home.

Maher Said, a graduate of Stathopoulos's lab, is the study's lead author.
""There's a paradox: We're having a hard time reconciling the convenience and the benefit of getting speedy, efficient delivery with its consequences, like poor labor conditions in warehouses, air pollution and congested streets,"" Stathopoulos said. ""We don't really see that other role that we play as citizens or as users of the city. And one role is directly affecting the other role, and we are both. With automated delivery, we could reduce some of these issues.""
The team designed a survey to assess preferences of 692 U.S. respondents, asking questions about different delivery options and variables like delivery speed, package handling and general perceptions.
Stathopoulos said that while new modes of delivery present an exciting opportunity, societally, ""we're not there just yet."" As companies ramp up drone deliveries due in part to labor shortages and in part because existing systems cannot satisfy the sheer volume of e-commerce deliveries, the researchers caution that these innovations may fail because of a lack of public acceptance.
Stathopoulos said she thinks shipping and logistics centers should be placed at the ""front and center"" of city planning and design, as in some European cities, to recognize its importance and role in quality of life. Policy makers will also need to become part of the conversation as more drones enter the airspace and labor shifts. None of this will work, Stathopoulos argued, until companies begin to consolidate their unique systems.
""On the planning side, we need to make sure that we embrace the fact that the massive amount of deliveries is going to shape our cities,"" Stathopoulos said. ""Collaboration, coordination, and information sharing between companies has been a running challenge -- but it's not going to work if everyone has their own technology. It just destroys the purpose and builds redundant and overlapping systems.""
However, by listening to and conducting more frequent assessments of user acceptance of technologies, Stathopoulos argues that policy makers and companies can prepare for the future and work to overcome anxiety and reluctance to accept new technologies.
The study was supported by the National Science Foundation Career program.

","score: 15.150353535353535, grade_level: '15'","score: 16.14655844155844, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41598-023-45371-1,"The logistics and delivery industry is undergoing a technology-driven transformation, with robotics, drones, and autonomous vehicles expected to play a key role in meeting the growing challenges of last-mile delivery. To understand the public acceptability of automated parcel delivery options, this U.S. study explores customer preferences for four innovations: autonomous vehicles, aerial drones, sidewalk robots, and bipedal robots. We use an Integrated Nested Choice and Correlated Latent Variable (INCLV) model to reveal substitution effects among automated delivery modes in a sample of U.S. respondents. The study finds that acceptance of automated delivery modes is strongly tied to shipment price and time, underscoring the importance of careful planning and incentives to maximize the trialability of innovative logistics options. Older individuals and those with concerns about package handling exhibit a lower preference for automated modes, while individuals with higher education and technology affinity exhibit greater acceptance. These findings provide valuable insights for logistics companies and retailers looking to introduce automation technologies in their last-mile delivery operations, emphasizing the need to tailor marketing and communication strategies to meet customer preferences. Additionally, providing information about appropriate package handling by automated technologies may alleviate concerns and increase the acceptance of these modes among all customer groups."
"
There is a general understanding that pets have a positive impact on one's well-being. A new study by Michigan State University found that although pet owners reported pets improving their lives, there was not a reliable association between pet ownership and well-being during the COVID-19 pandemic.

The study, published in the Personality and Social Psychology Bulletin, assessed 767 people over three times in May 2020. The researchers took a mixed-method approach that allowed them to look at several indicators of well-being while also asking people in an open-ended question to reflect on the role of pets from their point of view. Pet owners reported that pets made them happy. They claimed pets helped them feel more positive emotions and provided affection and companionship. They also reported negative aspects of pet ownership like being worried about their pet's well-being and having their pets interfere with working remotely.
However, when their happiness was compared to nonpet owners, the data showed no difference in the well-being of pet owners and nonpet owners over time. The researchers found that it did not matter what type of pet was owned, how many pets were owned or how close they were with their pet. The personalities of the owners were not a factor.
""People say that pets make them happy, but when we actually measure happiness, that doesn't appear to be the case,"" said William Chopik, an associate professor in MSU's Department of Psychology and co-author of the study. ""People see friends as lonely or wanting companionship, and they recommend getting a pet. But it's unlikely that it'll be as transformative as people think.""
The researchers explored several reasons why there is not a difference between the well-being of pet owners and nonpet owners. One of them being that nonpet owners may have filled their lives with a variety of other things that make them happy.

","score: 11.682063492063495, grade_level: '12'","score: 12.515333333333338, grade_levels: ['college'], ages: [18, 24]",10.1177/01461672231203417,"Pet ownership has often been lauded as a protective factor for well-being, particularly during the COVID-19 pandemic. We expanded this question to consider how pet (i.e., species, number) and owner (i.e., pet relationship quality, personality, attachment orientations) characteristics affected the association between pet ownership and well-being in a pre-registered mixed method analysis of 767 people assessed three times in May 2020. In our qualitative analyses, pet owners listed both benefits and costs of pet ownership during the COVID-19 pandemic. In our quantitative analyses, we found that pet ownership was not reliably associated with well-being. Furthermore, this association largely did not depend on the number of pets owned, the species of pet(s) owned, the quality of the human–pet relationship, or the owner’s psychological characteristics. Our findings are consistent with a large body of research showing null associations of pet ownership on well-being (quantitatively) but positive reports of pet ownership (qualitatively)."
"
Physical fitness since childhood is associated with cerebellar grey matter volume in adolescents. According to a recent study conducted at the University of Jyväskylä and the University of Eastern Finland, those who were stronger, faster and more agile, in other words, had better neuromuscular fitness since childhood, had larger Crus I grey matter volume in adolescence.

Despite the importance of the developing cerebellum on cognition and learning, the associations between physical fitness and cerebellar volume in adolescents have remained unclear. This study examined the associations of physical fitness with grey matter volume of cerebellar lobules related to cognition in adolescents, and whether these associations differed between females and males.
Those adolescents with better neuromuscular fitness since childhood had larger Crus I grey matter volume. However, adolescents with better cardiorespiratory fitness had smaller total cerebellar grey matter volume. Moreover, males with better neuromuscular fitness since childhood had smaller Crus II grey matter volume.
""Our study highlights the importance of physical activity through childhood and adolescence, leading to better physical fitness, as it might be relevant to cerebellar volumes related to cognition and learning. However, the associations we observed are in part contradictory,"" says Doctoral Researcher Petri Jalanko from the Faculty of Sport and Health Sciences at the University of Jyväskylä.
""The study sheds light on the associations between physical fitness and the cerebellum. Future randomised controlled trials utilising direct cardiorespiratory fitness measurements and novel brain imaging to assess a larger population and both sexes separately are needed to better understand the associations and causality between physical fitness and cerebellar volumes in adolescents,"" Jalanko says.
The findings are from the FitBrain study, which included 40 participants from the 8-year follow-up examinations of the Physical Activity and Nutrition in Children (PANIC) study. Of the participants, 22 were female and 18 were male, and their mean age was 17.9 years.
Cardiorespiratory fitness was assessed by maximal ramp test on a cycle ergometer, muscular strength with standing long jump, speed-agility with the 10 x 5 m shuttle-run test, coordination with the Box and Block Test and neuromuscular fitness as the sum of standing long jump, Box and Block Test and shuttle-run test z-scores. Cerebellar volumes were assessed by magnetic resonance imaging. The study was published in the Scandinavian Journal of Medicine and Science in Sports.

","score: 16.510113636363645, grade_level: '17'","score: 17.51553571428571, grade_levels: ['college_graduate'], ages: [24, 100]",10.1111/sms.14513,"Despite the importance of the developing cerebellum on cognition, the associations between physical fitness and cerebellar volume in adolescents remain unclear. We explored the associations of physical fitness with gray matter (GM) volume of VI, VIIb and Crus I & II, which are cerebellar lobules related to cognition, in 40 (22 females; 17.9 ± 0.8 year‐old) adolescents, and whether the associations were sex‐specific. Peak oxygen uptake (V̇O2peak) and power were assessed by maximal ramp test on a cycle ergometer, muscular strength with standing long jump (SLJ), speed‐agility with the shuttle‐run test (SRT), coordination with the Box and Block Test (BBT) and neuromuscular performance index (NPI) as the sum of SLJ, BBT and SRT z‐scores. Body composition was measured using a dual‐energy X‐ray absorptiometry. Cerebellar volumes were assessed by magnetic resonance imaging. V̇O2peak relative to lean mass was inversely associated with the GM volume of the cerebellum (standardized regression coefficient (β) = −0.038, 95% confidence interval (CI) ‐0.075 to 0.001, p = 0.044). Cumulative NPI was positively associated with the GM volume of Crus I (β = 0.362, 95% CI 0.045 to 0.679, p = 0.027). In females, better performance in SRT was associated with a larger GM volume of Crus I (β = −0.373, 95% CI ‐0.760 to −0.028, p = 0.036). In males, cumulative NPI was inversely associated with the GM volume of Crus II (β = −0.793, 95% CI ‐1.579 to −0.008 p = 0.048). Other associations were nonsignificant. In conclusion, cardiorespiratory fitness, neuromuscular performance and speed‐agility were associated with cerebellar GM volume, and the strength and direction of associations were sex‐specific."
"
The study, supported by the British Heart Foundation (BHF) and published in the European Heart Journal, is the first to assess how different movement patterns throughout the 24-hour day are linked to heart health. It is the first evidence to emerge from the international Prospective Physical Activity, Sitting and Sleep (ProPASS) consortium.

Cardiovascular disease, which refers to all diseases of the heart and circulation, is the number one cause of mortality globally. In 2021, it was responsible for one in three deaths (20.5m), with coronary heart disease alone the single biggest killer. Since 1997, the number of people living with cardiovascular disease across the world has doubled and is projected to rise further.
In this study, researchers at UCL analysed data from six studies, encompassing 15,246 people from five countries, to see how movement behaviour across the day is associated with heart health, as measured by six common indicators*. Each participant used a wearable device on their thigh to measure their activity throughout the 24-hour day and had their heart health measured.
The researchers identified a hierarchy of behaviours that make up a typical 24-hour day, with time spent doing moderate-vigorous activity providing the most benefit to heart health, followed by light activity, standing and sleeping compared with the adverse impact of sedentary behaviour.
The team modelled what would happen if an individual changed various amounts of one behaviour for another each day for a week, in order to estimate the effect on heart health for each scenario. When replacing sedentary behaviour, as little as five minutes of moderate-vigorous activity had a noticeable effect on heart health.
For a 54-year-old woman with an average BMI of 26.5, for example, a 30-minute change translated into a 0.64 decrease in BMI, which is a difference of 2.4%. Replacing 30 minutes of daily sitting or lying time with moderate or vigorous exercise could also translate into a 2.5 cm (2.7%) decrease in waist circumference or a 1.33 mmol/mol (3.6%) decrease in glycated haemoglobin.
Dr Jo Blodgett, first author of the study from UCL Surgery & Interventional Science and the Institute of Sport, Exercise & Health, said: ""The big takeaway from our research is that while small changes to how you move can have a positive effect on heart health, intensity of movement matters. The most beneficial change we observed was replacing sitting with moderate to vigorous activity -- which could be a run, a brisk walk, or stair climbing -- basically any activity that raises your heart rate and makes you breathe faster, even for a minute or two.""
The researchers pointed out that although time spent doing vigorous activity was the quickest way to improve heart health, there are ways to benefit for people of all abilities -- it's just that the lower the intensity of the activity, the longer the time is required to start having a tangible benefit. Using a standing desk for a few hours a day instead of a sitting desk, for example, is a change over a relatively large amount of time but is also one that could be integrated into a working routine fairly easily as it does not require any time commitment.

Those who are least active were also found to gain the greatest benefit from changing from sedentary behaviours to more active ones.
Professor Emmanuel Stamatakis, joint senior author of the study from the Charles Perkins Centre and Faculty of Medicine and Health at the University of Sydney, said: ""A key novelty of the ProPASS consortium is the use of wearable devices that better differentiate between types of physical activity and posture, allowing us to estimate the health effects of even subtle variations with greater precision.""
Though the findings cannot infer causality between movement behaviours and cardiovascular outcomes, they contribute to a growing body of evidence linking moderate to vigorous physical activity over 24 hours with improved body fat metrics. Further long-term studies will be crucial to better understanding the associations between movement and cardiovascular outcomes.
Professor Mark Hamer, joint senior author of the study from UCL Surgery & Interventional Science and the Institute of Sport, Exercise & Health, said: ""Though it may come as no surprise that becoming more active is beneficial for heart health, what's new in this study is considering a range of behaviours across the whole 24-hour day. This approach will allow us to ultimately provide personalised recommendations to get people more active in ways that are appropriate for them.""
James Leiper, Associate Medical Director at the British Heart Foundation, said: ""We already know that exercise can have real benefits for your cardiovascular health and this encouraging research shows that small adjustments to your daily routine could lower your chances of having a heart attack or stroke. This study shows that replacing even a few minutes of sitting with a few minutes of moderate activity can improve your BMI, cholesterol, waist size, and have many more physical benefits.
""Getting active isn't always easy, and it's important to make changes that you can stick to in the long-term and that you enjoy -- anything that gets your heart rate up can help. Incorporating 'activity snacks' such as walking while taking phone calls, or setting an alarm to get up and do some star jumps every hour is a great way to start building activity into your day, to get you in the habit of living a healthy, active lifestyle.""
This research was funded by the British Heart Foundation.
*The studies were part of the Prospective Physical Activity, Sitting and Sleep (ProPASS) consortium. Heart health was measured using six outcomes: body-mass index (BMI), waist circumference, HDL cholesterol, HDL-to-total cholesterol ratio, triglycerides and HbA1c.

","score: 16.649717901301717, grade_level: '17'","score: 18.670084847166322, grade_levels: ['college_graduate'], ages: [24, 100]",10.1093/eurheartj/ehad717,"Physical inactivity, sedentary behaviour (SB), and inadequate sleep are key behavioural risk factors of cardiometabolic diseases. Each behaviour is mainly considered in isolation, despite clear behavioural and biological interdependencies. The aim of this study was to investigate associations of five-part movement compositions with adiposity and cardiometabolic biomarkers. Cross-sectional data from six studies (n = 15 253 participants; five countries) from the Prospective Physical Activity, Sitting and Sleep consortium were analysed. Device-measured time spent in sleep, SB, standing, light-intensity physical activity (LIPA), and moderate-vigorous physical activity (MVPA) made up the composition. Outcomes included body mass index (BMI), waist circumference, HDL cholesterol, total:HDL cholesterol ratio, triglycerides, and glycated haemoglobin (HbA1c). Compositional linear regression examined associations between compositions and outcomes, including modelling time reallocation between behaviours. The average daily composition of the sample (age: 53.7 ± 9.7 years; 54.7% female) was 7.7 h sleeping, 10.4 h sedentary, 3.1 h standing, 1.5 h LIPA, and 1.3 h MVPA. A greater MVPA proportion and smaller SB proportion were associated with better outcomes. Reallocating time from SB, standing, LIPA, or sleep into MVPA resulted in better scores across all outcomes. For example, replacing 30 min of SB, sleep, standing, or LIPA with MVPA was associated with −0.63 (95% confidence interval −0.48, −0.79), −0.43 (−0.25, −0.59), −0.40 (−0.25, −0.56), and −0.15 (0.05, −0.34) kg/m2 lower BMI, respectively. Greater relative standing time was beneficial, whereas sleep had a detrimental association when replacing LIPA/MVPA and positive association when replacing SB. The minimal displacement of any behaviour into MVPA for improved cardiometabolic health ranged from 3.8 (HbA1c) to 12.7 (triglycerides) min/day. Compositional data analyses revealed a distinct hierarchy of behaviours. Moderate-vigorous physical activity demonstrated the strongest, most time-efficient protective associations with cardiometabolic outcomes. Theoretical benefits from reallocating SB into sleep, standing, or LIPA required substantial changes in daily activity."
"
People are beginning to reconsider their reproductive decisions due to complex concerns about climate change, with many choosing to forego childbearing, or reduce the number of children they have as a result, finds a new study by UCL researchers.

The research, published in PLOS Climate, is the first systematic review to explore how and why climate change-related concerns may be impacting reproductive decision-making.
The team examined 13 studies, involving 10,788 participants, which were conducted between 2012 and 2022, primarily in Global North countries such as the USA, Canada, New Zealand, and various European countries. They found that climate change concerns were typically associated with less positive attitudes towards reproduction and a desire or intent for fewer children or none at all.
Underpinning this finding were four key factors: uncertainty about the future of an unborn child, environmentalist views centred on overpopulation and overconsumption, meeting family subsistence needs, and political sentiments.
The term eco-anxiety has rapidly entered public discourse, describing a range of negative emotional responses including fear, worry, guilt and anger as a response to climate change. In 2018, a nationally representative New York Times survey found that 33% of childfree Americans aged 20-45 cited being ""worried about climate change"" as a reason for not having children.
Since then, ethical concerns about the quality of life children might have in a climate-changed future have been cited as the primary rationale for individuals choosing to not have children. However, the team behind this new study wanted to understand if there was an evidence base supporting the claims that climate change concerns were causing people to change their childbearing decisions, and if so, whether any other motivating factors, aside from ethical concerns, came into play.
The new analysis found that in 12 out of 13 studies, stronger concerns about climate change were associated with a desire for fewer children, or none at all.

One of the main reasons for this was the individual's concern for their children in a world affected by climate change. However, the review also highlighted three other factors, with a primary concern being the ecological impact of reproduction, as people feared that having children would contribute to overpopulation and overconsumption in a world with already stretched resources.
To a lesser extent, two studies in Zambia and Ethiopia also found that participants desired fewer children to meet subsistence needs during periods of declining agricultural productivity.
Finally, individuals in another study had political considerations resulting in their decision to not have children -- with two participants even reporting their refusal to have children as a method of 'striking' until systemic change was enacted.
Interestingly, these final two themes were also raised by some participants as reasons to have a greater number of children. For example, in Zambia, participants were concerned about their ability to support their family without the household labour provided by additional children helping with domestic work, as well as water and food collection.
Lead author, Hope Dillarstone (former MSc student at the UCL Institute for Global Health, said: ""Recent media attention has been paid to a growing number of individuals factoring their concerns about climate change into their childbearing plans. However, we were concerned that public discourse may have oversimplified this relationship.
""Our first-of-its-kind study shows that there is a complex and intricate relationship between climate change and reproductive choices, with differences noted both within and between countries across the world.
""Our analysis shows that not only are many people concerned about their child's welfare growing up in a world of uncertainty, but that they are also considering the impact of having children on the environment, their family's ability to subsist, and their politics.
""Understanding why some people choose to adjust their reproductive decisions as a result of climate change may prove instrumental for shaping public policy, showing a need for collaboration among policymakers to incorporate local-level environmental concerns within national and international climate change, mental health and sexual and reproductive health policies.""
The team are now calling for more research at the intersection of climate change, mental health, and reproductive decision-making, particularly among highly affected Global South populations where current research is lacking.

","score: 17.902655122655123, grade_level: '18'","score: 20.568225108225107, grade_levels: ['college_graduate'], ages: [24, 100]",10.1371/journal.pclm.0000236,"The impact of climate change on reproductive decision-making is becoming a significant issue, with anecdotal evidence indicating a growing number of people factoring their concerns about climate change into their childbearing plans. Although empirical research has explored climate change and its relationship to mental health, as well as the motivations behind reproductive decision-making independently, a gap in the literature remains that bridges these topics at their nexus. This review endeavours to fill this gap by synthesising the available evidence connecting climate change-related concerns with reproductive decision-making and exploring the reasons and motivations behind this relationship. A systematic review using six databases was conducted to identify relevant literature. Included studies reported quantitative, qualitative, and mixed-methods data related to: (1) climate change, (2) mental health and wellbeing concerns, and (3) reproductive decision-making. Findings were synthesised narratively using a parallel-results convergent synthesis design and the quality of studies was appraised using three validated assessment tools. Four hundred and forty-six documents were screened using pre-defined inclusion criteria, resulting in the inclusion of thirteen studies. The studies were conducted between 2012 and 2022 primarily in Global North countries (e.g., USA, Canada, New Zealand, and European countries). Climate change concerns were typically associated with less positive attitudes towards reproduction and a desire and/or intent for fewer children or none at all. Four themes explaining this relationship were identified: uncertainty about the future of an unborn child, environmentalist views centred on overpopulation and overconsumption, meeting family subsistence needs, and environmental and political sentiments. The current evidence reveals a complex relationship between climate change concerns and reproductive decision-making, grounded in ethical, environmental, livelihood, and political considerations. Further research is required to better understand and address this issue with an intercultural approach, particularly among many highly affected Global South populations, to ensure comparability and generalisable results."
"
The 11 young firefighters went through a rigorous training exercise, carrying up to 40 pounds of gear over hilly terrain during a 45-minute training exercise in the California sun. Gloves, helmets, flashlights, goggles, and more weighted them down as they sprinted through the countryside wearing fire-resistant clothing to show they were ready to serve as wildland firefighters.

When the training was over, they immediately went to the medical tent -- not to rest and recover but to give samples of their blood, saliva, and urine for analysis by a team of scientists equipped with needles, test tubes, cold packs, and the gear of their own trade.
Then, the scientists from the Department of Energy's Pacific Northwest National Laboratory (PNNL) analyzed more than 4,700 molecules -- proteins, lipids, and metabolites -- from each of the firefighters, looking to understand what happens when the body undergoes intense physical exercise. Measuring and interpreting the data from thousands of such measurements is a specialty of PNNL scientists who explore issues related to climate science and human health by analyzing millions of sensitive measurements using mass spectrometry each year.
For this study, the intent was to increase safety for first responders and others.
""Heat stress can be life threatening,"" said Kristin Burnum-Johnson, a corresponding author of the study. ""We wanted to take an in-depth look at what's happening in the body and see if we're able to detect danger from exhaustion in its earliest stages. Perhaps we can reduce the risk of strenuous exercise for first responders, athletes, and members of the military.""
As expected, the team detected hundreds of molecular changes in the firefighters. The differences before and after exercise underscored the body's efforts at tissue damage and repair, maintenance of fluid balance, efforts to keep up with increased energy and oxygen demand, and the body's attempts to repair and regenerate its proteins and other important substances.
But in the saliva, the team found some unexpected results. There was a change in the microbial mix of the mouth -- the oral microbiome -- showing that the body was increasingly on the lookout for bacterial invaders. Scientists also saw a decrease in signaling molecules important for inflammation and for fighting off viral infections.

A decrease in inflammation makes sense for people exercising vigorously; less inflammation allows people to breathe in air more quickly, meeting the body's eager demand for more oxygen. Having fewer inflammatory signals in the respiratory system helps the body improve respiration and blood flow.
Less inflammation, better breathing
But less inflammation leaves the body more vulnerable to viral respiratory infection -- which is exactly what other scientists have noted in elite athletes and others who exercise vigorously. Some studies have shown that a person is up to twice as likely to come down with a viral respiratory infection in the days after an especially energetic workout.
""People who are very fit might be more prone to viral respiratory infection immediately after vigorous exercise. Having less inflammatory activity to fight off an infection could be one cause,"" said Ernesto Nakayasu, a corresponding author of the paper. He notes that the work provides a molecular basis for what clinicians have noticed in their patients who do strenuous workouts.
The team hopes that the findings will help explain why come people are more vulnerable to respiratory infection after a workout.
The research was published Oct. 18 in Military Medical Research.
In addition to Nakayasu and Burnum-Johnson, PNNL authors included Marina A. Gritsenko, Young-Mo Kim, Jennifer E. Kyle, Kelly G. Stratton, Carrie D. Nicora, Nathalie Munoz, Yuqian Gao, Karl K. Weitz, Vanessa L. Paurus, Kent J. Bloodsworth, Kelsey A. Allen, Lisa M. Bramer, Fernando Montes, Kathleen A. Clark, Grant Tietje,and Justin Teeguarden.
The work was funded by PNNL. Some of the mass spectrometry measurements were done at the Environmental Molecular Sciences Laboratory, a Department of Energy Office of Science user facility at PNNL.

","score: 14.54993006993007, grade_level: '15'","score: 15.747425625222242, grade_levels: ['college_graduate'], ages: [24, 100]",10.1186/s40779-023-00477-5,"Physiological and biochemical processes across tissues of the body are regulated in response to the high demands of intense physical activity in several occupations, such as firefighting, law enforcement, military, and sports. A better understanding of such processes can ultimately help improve human performance and prevent illnesses in the work environment. To study regulatory processes in intense physical activity simulating real-life conditions, we performed a multi-omics analysis of three biofluids (blood plasma, urine, and saliva) collected from 11 wildland firefighters before and after a 45 min, intense exercise regimen. Omics profiles post- versus pre-exercise were compared by Student’s t-test followed by pathway analysis and comparison between the different omics modalities. Our multi-omics analysis identified and quantified 3835 proteins, 730 lipids and 182 metabolites combining the 3 different types of samples. The blood plasma analysis revealed signatures of tissue damage and acute repair response accompanied by enhanced carbon metabolism to meet energy demands. The urine analysis showed a strong, concomitant regulation of 6 out of 8 identified proteins from the renin-angiotensin system supporting increased excretion of catabolites, reabsorption of nutrients and maintenance of fluid balance. In saliva, we observed a decrease in 3 pro-inflammatory cytokines and an increase in 8 antimicrobial peptides. A systematic literature review identified 6 papers that support an altered susceptibility to respiratory infection. This study shows simultaneous regulatory signatures in biofluids indicative of homeostatic maintenance during intense physical activity with possible effects on increased infection susceptibility, suggesting that caution against respiratory diseases could benefit workers on highly physical demanding jobs."
"
Leonardo Da Vinci was one, so too Albert Einstein and Joan of Arc, while the footballer Diego Maradona was famous for using his against England, we are of course talking about left-handers.

It's been debated for decades, but now researchers at the University of York and University College London have suggested that left-handedness is not linked to better spatial skills.
By asking participants to download and play a video game that captured user information and tracked navigational challenges, researchers were able to measure demographic data -- including hand preference -- and activity from more than 420,000 international participants, across 41 different countries. They found that left handers were neither better nor worse than right handers at the tasks, clarifying a long-running debate about the links between handedness and spatial skills.
The brain has two hemispheres, controlling the opposite sides of the body; so in right handers, the left hemisphere controls the dominant right hand, whereas the situation is reversed in left-handers. Many cognitive abilities are also dominated by one of the two brain hemispheres, while right and left handers also show different patterns of lateralisation -- the specialisation of a particular area. As a result, many debates about cognitive differences related to handedness are also debates about the effects of brain lateralisation on cognitive abilities.
Spatial cognition, the ability of humans to perceive and navigate our physical environment, is a fundamental set of brain-based skills. It is also not clearly dominated by either hemisphere, leaving scientists unclear as to whether it has any link to handedness.
Some, inconclusive, research has suggested that left-handers might be better at navigating virtual and real games and left-handed athletes are known to be over-represented in the in professional sports requiring rapid and accurate responses.
However, it's been a tricky issue to research, partly because handedness prevalence changes from culture to culture, and partly because testing for handedness effects requires a large number of participants. Using the video game Sea Hero Quest, the researchers were able to overcome both challenges.

Dr Pablo Fernandez-Velasco, a researcher at the Department of Philosophy at the University of York, who co-led the study, said: ""Recruiting participants in our study through a video game is a new approach, which allowed us to standardise a test across a very large sample. We found no reliable evidence for any difference in spatial ability between left and right handers, across all countries. Moreover, that large data sample allowed us to confirm that factors like age, gender and education don't play a part in the relationship between hand preference and spatial ability.""
The users in the study downloaded and played Sea Hero Quest, a free app that measures spatial navigational ability and was originally designed to contribute to research on dementia. It asks participants to view a map featuring both their current position and their goal locations, and they are then asked to navigate a boat as quickly as possible towards goal locations in a specified order. Only participants reaching level 11 of the game were included.
Informed in-app consent was obtained from all participants. Left handers in the sample made up an average of 9.94% of the participants, with more males using their left hand compared with women, similar to what had previously been found in the general population.
Dr Fernandez-Velasco adds: ""We're still finding out so much about cognition, and although we've shown that large-scale spatial skills aren't affected by left and right handedness, perhaps further research will find some differences based on handedness when it comes to navigation styles, or to preferences for different types of environments""

","score: 16.263108399138552, grade_level: '16'","score: 18.03051687006461, grade_levels: ['college_graduate'], ages: [24, 100]",10.1098/rspb.2023.1514,"There is an active debate concerning the association of handedness and spatial ability. Past studies used small sample sizes. Determining the effect of handedness on spatial ability requires a large, cross-cultural sample of participants and a navigation task with real-world validity. Here, we overcome these challenges via the mobile app Sea Hero Quest. We analysed the navigation performance from 422 772 participants from 41 countries and found no reliable evidence for any difference in spatial ability between left- and right-handers across all countries. A small but growing gap in performance appears for participants over 64 years old, with left-handers outperforming right-handers. Further analysis, however, suggests that this gap is most likely due to selection bias. Overall, our study clarifies the factors associated with spatial ability and shows that left-handedness is not associated with either a benefit or a deficit in spatial ability."
"
The human mind does not like to make mistakes -- and makes time to avoid repeating them.

A new study from University of Iowa researchers shows how the human brain, in just one second, can distinguish between an outcome caused by human error and one in which the person is not directly to blame.
Moreover, the researchers found, in cases of human error, the brain takes additional time to catalog the error and inform the rest of the body about it to avoid repeating the miscue.
""The novel aspect about this study is the brain can very quickly distinguish whether an undesirable outcome is due to a (human) error, or due to something else,"" says Jan Wessel, professor in the Department of Psychological and Brain Sciences at Iowa and the study's corresponding author. ""If the brain realizes an error was the cause, it will then start additional processes to avoid further errors, which it won't do if the outcome wasn't due to its own action.""
The Iowa researchers learned about the brain's ability to separate human error from a non-self-inflicted error by asking 76 young adults to look at a cluster of arrows and choose the correct direction one specific arrow was pointing. Nearly every time the subjects responded -- almost always correctly, given the task's simplicity -- a triangle would appear on the screen. But every now and then, another symbol (an anchor, frog, helicopter, etc.) would appear on screen, meant to mimic a ""surprise"" or unexpected outcome, and, importantly, appearing even when the subject responded correctly and expected the triangle symbol.
The researchers measured at three different intervals (350, 1,700, and 3,000 milliseconds) how the brain responded to situations with the standard outcome (the triangle) and the surprise outcome (a different symbol).
What they found is that the brain can distinguish between the two outcomes after about one second (1,000 milliseconds).

If human error is the reason for the outcome, the brain remains active for an additional two to three seconds, the researchers found. That means the brain realizes a mistake was made, and essentially wants to learn from it.
""When it is something that has to do with my own action and I can do something about it, then the brain takes a few seconds to reconfigure the entire cognitive apparatus, the visual system, the motor system,"" says Wessel, who has a joint appointment in the Department of Neurology. ""It's as if the brain is taking a moment to fill in the rest of the body, the senses, the motor control, to tell the other working parts, 'Let's not do this again.'""
The researchers also measured brain waves through scalp electroencephalograms (EEGs) and observed ongoing neural activity that was unique to instances when human error occurred.
""Indeed, we found that while both errors and unexpected outcomes of correct actions led to comparable neural activity early on, only errors showed reliable, sustained brain activity more than a second after the response,"" Wessel says.
Previous research had shown the brain can recognize instances when human error has occurred, but there was debate about whether the brain's reaction to an outcome was the same regardless of whether the cause was human error or not.
""Some have argued that we don't actually have a genuine error detection system in the brain,"" Wessel notes.

But Wessel's research demonstrates that the brain does make an error/no error distinction and communicates information related to either outcome with the rest of the body.
""All in all, this shows that we do have genuine, error-specific systems in the human brain that detect our action errors that trigger adaptive responses, such as the strategic slowing of ongoing actions,"" Wessel says.
The study, ""Early action error processing is due to domain-general surprise while later processing is error-specific,"" was published online Oct. 13 in JNeurosci, a journal of the Society for Neuroscience.
The study's first author is Yoojeong Choo, a graduate student in Wessel's lab. A co-author is Alec Mather, an Iowa graduate who worked in Wessel's lab as an undergraduate and is now data science manager at Sony Music Entertainment.
The National Institutes of Health funded the research.

","score: 14.128168838526914, grade_level: '14'","score: 15.552847025495751, grade_levels: ['college_graduate'], ages: [24, 100]",10.1523/JNEUROSCI.1334-23.2023,"The ability to adapt behavior after erroneous actions is one of the key aspects of cognitive control. Error commission typically causes people to slow down their subsequent actions [post-error slowing (PES)]. Recent work has challenged the notion that PES reflects adaptive, controlled processing and instead suggests that it is a side effect of the surprising nature of errors. Indeed, human neuroimaging suggests that the brain networks involved in processing errors overlap with those processing error-unrelated surprise, calling into question whether there is a specific system for error processing in the brain at all. In the current study, we used EEG decoding and a novel behavioral paradigm to test whether there are indeed unique, error-specific processes that contribute to PES beyond domain-general surprise. Across two experiments in male and female humans (N= 76), we found that both errors and error-unrelated surprise were followed by slower responses when response–stimulus intervals were short. Furthermore, the early neural processes following error-specific and domain-general surprise showed significant cross-decoding. However, at longer intervals, which provided additional processing time, only errors were still followed by post-trial slowing. Furthermore, this error-specific PES effect was reflected in sustained neural activity that could be decoded from that associated with domain-general surprise, with the strongest contributions found at lateral frontal, occipital, and sensorimotor scalp sites. These findings suggest that errors and surprise initially share common processes, but that after additional processing time, unique, genuinely error-specific processes take over and contribute to behavioral adaptation. SIGNIFICANCE STATEMENTHumans typically slow their actions after errors (PES). Some suggest that PES is a side effect of the unexpected, surprising nature of errors, challenging the notion of a genuine error processing system in the human brain. Here, we used multivariate EEG decoding to identify behavioral and neural processes uniquely related to error processing. Action slowing occurred following both action errors and error-unrelated surprise when time to prepare the next response was short. However, when there was more time to react, only errors were followed by slowing, further reflected in sustained neural activity. This suggests that errors and surprise initially share common processing, but that after additional time, error-specific, adaptive processes take over."
"
Roughly two decades ago, a community-wide reckoning emerged concerning the credibility of published literature in the social-behavioral sciences, especially psychology. Several large scale studies attempted to reproduce previously published findings to no avail or to a much lesser magnitude, sending the credibility of the findings -- and future studies in social-behavioral sciences -- into question.

A handful of top experts in the field, however, set out to show that when best practices are employed, high replicability is possible. Over six years, researchers at labs from UC Santa Barbara, UC Berkeley, Stanford University and the University of Virginia discovered and replicated 16 novel findings with ostensibly gold standard best practices, including pre-registration, large sample sizes and replication fidelity. Their findings, published in Nature Human Behaviour, indeed suggest that with best practices, high replicability is achievable.
""It's an existence proof that we can set out to discover new findings and replicate them at a very high level,"" said UC Santa Barbara Distinguished Professor Jonathan Schooler, director of UCSB's META Lab and the Center for Mindfulness and Human Potential, and senior author of the paper. ""The major finding is that when you follow current best practices in conducting and replicating online social-behavioral studies, you can accomplish high and generally stable replication rates.""
Their study's replication findings were 97% the size of the original findings on average. By comparison, prior replication projects observed replication findings that were roughly 50%.
The paper's principal investigators were John Protzko of UCSB's META Lab and Central Connecticut State University (CCSU), Jon Krosnick of Stanford's Political Psychology Research Group, Leif Nelson at UC Berkeley's Haas School of Business and Brian Nosek, who is affiliated with the University of Virginia and is the executive director of the standalone Center for Open Science.
""There have been a lot of concerns over the past few years about the replicability of many sciences, but psychology was among the first fields to start systematically investigating the issue,"" said lead author Protzko, a research associate to Schooler's lab, where he was a postdoctoral scholar during the study. He is now an assistant professor of psychological science at CCSU. ""The question was whether past replication failures and declining effect sizes are inherently built into the assorted scientific domains that have observed them. For example, some have speculated that it is an inherent aspect of the scientific enterprise that newly discovered findings can become less replicable or smaller over time.""
The group decided to perform new studies using emerging best practices in open science -- and then to replicate them with an innovative design in which the researchers committed to replicating the initial confirmation studies regardless of outcome. Over the course of six years, research teams at each lab developed studies which were then replicated by all of the other labs.

In total, the coalition discovered 16 new phenomena and replicated each of them 4 times involving 120,000 participants. ""If you use best practices of large samples, pre-registration, open materials in the discovery of new science, and you run replications with as best fidelity to the original process as you can, you end up with a very highly replicable science,"" Protzko said of the findings.
One key innovation the study offered was that all of the participating labs agreed to replicate the initial confirmation studies regardless of their outcome. This removed the scientific community's customary bias of only publishing and replicating positive outcomes, which may have contributed to inflated initial assessments of effect sizes in the past. Furthermore, this approach enabled the researchers to observe several cases for which study designs that failed to produce significant findings in the original confirmation later attained reliable effects when replicated at other labs.
Across the board, the project revealed extremely high replicability rates of their social-behavioral findings, and no statistically significant evidence of decline over repeated replications. Given the sample sizes and effect sizes, the observed replicability rate of 86%, based on statistical significance, could not have been any higher, the researchers pointed out.
To test the novelty of their discoveries, they ran independent tests on people's predictions regarding the direction of the new findings and their likelihood of replicability. Several follow-up surveys in which naïve participants evaluated descriptions of both the new studies and those associated with previous replication projects, found no differences in their respective predictability. Thus, the replication success of these studies was not due to them discovering obvious results that would necessarily be expected to replicate. Indeed, many of the newly discovered findings have already been independently published in high quality journals.
""It would not be particularly interesting to discover that it is easy to replicate completely obvious findings,"" Schooler said. ""But our studies were comparable in their surprise factor to studies that have been difficult to replicate in the past. Untrained judges who were given summaries of the two conditions in each of our studies and a comparable set of two-condition studies from a prior replication effort found it similarly difficult to predict the direction of our findings relative to the earlier ones.""
Because each research lab developed its own studies, they came from a variety of social, behavioral and psychological fields such as marketing, political psychology, prejudice, and decision-making. They all involved human subjects and adhered to certain constraints, such as not using deception. ""We really built into the process that the individual labs would act independently,"" Protzko said. ""They would go about their sort of normal topics they were interested in and how they would run their studies.""
Collectively, their meta-scientific investigation provides evidence that low replicability and declining effects are not inevitable. Rigor enhancing practices can lead to very high replication rates, but exactly identifying which practices work best will take further study. This study's ""kitchen sink"" approach -- using multiple rigor-enhancing practices at once -- didn't isolate any individual practice's effect.

","score: 16.664693373576622, grade_level: '17'","score: 17.969907257511316, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41562-023-01749-9,"Failures to replicate evidence of new discoveries have forced scientists to ask whether this unreliability is due to suboptimal implementation of methods or whether presumptively optimal methods are not, in fact, optimal. This paper reports an investigation by four coordinated laboratories of the prospective replicability of 16 novel experimental findings using rigour-enhancing practices: confirmatory tests, large sample sizes, preregistration and methodological transparency. In contrast to past systematic replication efforts that reported replication rates averaging 50%, replication attempts here produced the expected effects with significance testing (P < 0.05) in 86% of attempts, slightly exceeding the maximum expected replicability based on observed effect sizes and sample sizes. When one lab attempted to replicate an effect discovered by another lab, the effect size in the replications was 97% that in the original study. This high replication rate justifies confidence in rigour-enhancing methods to increase the replicability of new discoveries."
"
Increasing workplace flexibility may lower employees' risk of cardiovascular disease, according to a new study led by Harvard T.H. Chan School of Public Health and Penn State University. In workplaces that implemented interventions designed to reduce conflict between employees' work and their personal/family lives, researchers observed that employees at higher baseline cardiometabolic risk, particularly older employees, experienced a reduction in their risk for cardiovascular disease equivalent to between five and 10 years of age-related cardiometabolic changes.

The study will be published on November 8 in The American Journal of Public Health. It is among the first studies to assess whether changes to the work environment can affect cardiometabolic risk.
""The study illustrates how working conditions are important social determinants of health,"" said co-lead author Lisa Berkman, Thomas D. Cabot Professor of Public Policy and of Epidemiology at Harvard Chan School and director of the Harvard Center for Population and Development Studies. ""When stressful workplace conditions and work-family conflict were mitigated, we saw a reduction in the risk of cardiovascular disease among more vulnerable employees, without any negative impact on their productivity. These findings could be particularly consequential for low- and middle-wage workers who traditionally have less control over their schedules and job demands and are subject to greater health inequities.""
As part of the Work, Family and Health Network, the researchers designed a workplace intervention meant to increase work-life balance: Supervisors were trained on strategies to show support for employees' personal and family lives alongside their job performances, and teams (supervisors and employees) attended hands-on trainings to identify new ways to increase employees' control over their schedules and tasks.
The researchers randomly assigned the intervention to work units/sites within two companies: an IT company, comprised of 555 participating employees, and a long-term care company, with 973 participating employees. The IT employees consisted of male and female high and moderate-salaried technical workers; the long-term care employees mostly consisted of female, low-wage direct caregivers. Other units/sites were not assigned the intervention and therefore conducted their business as usual.
These 1,528 employees across the experimental and control groups had their systolic blood pressure, body mass index, glycated hemoglobin, smoking status, HDL cholesterol, and total cholesterol recorded at the beginning of the study and again 12 months later. The researchers used this health information to calculate a cardiometabolic risk score (CRS) for each employee, with a higher score indicating a higher estimated risk of developing cardiovascular disease within the decade.
The study found that the workplace intervention did not have any significant overall effects on employees' CRS. However, the researchers did observe reductions in CRS specifically among those with a higher baseline CRS: Those employees of the IT company and of the long-term care company saw a reduction in their CRS equivalent to 5.5 and 10.3 years of age-related changes, respectively. Age also played a role: Employees older than 45 with a higher baseline CRS were likelier to see a reduction than their younger counterparts.
""The intervention was designed to change the culture of the workplace over time with the intention of reducing conflict between employees' work and personal lives and ultimately improving their health,"" said co-lead author Orfeu Buxton, professor of biobehavioral health and director of the Sleep, Health & Society Collaboratory at Penn State. ""Now we know such changes can improve employee health and should be more broadly implemented.""
Hayami Koga, postdoctoral fellow at the Harvard Center for Population and Development Studies, was also a co-author.
Funding for the study came from the National Institutes of Health and the Centers for Disease Control and Prevention: Eunice Kennedy Shriver National Institute of Child Health and Human Development (grants U01HD051217, U01HD051218, U01HD051256, U01HD051276); National Institute on Aging (grant U01AG027669); Office of Behavioral and Social Sciences Research; National Institute for Occupational Safety and Health (grants U01OH008788, U01HD059773); and the National Heart Lung and Blood Institute (grant R01-HL107240). Additional funding came from the University of Minnesota's College of Liberal Arts, McKnight Foundation, William T. Grant Foundation, Alfred P. Sloan Foundation, and the Administration for Children and Families.

","score: 18.50019878081103, grade_level: '19'","score: 21.075194805194805, grade_levels: ['college_graduate'], ages: [24, 100]",10.2105/AJPH.2023.307413,"Objectives. To examine whether workplace interventions to increase workplace flexibility and supervisor support and decrease work–family conflict can reduce cardiometabolic risk. Methods. We randomly assigned employees from information technology (n = 555) and long-term care (n = 973) industries in the United States to the Work, Family and Health Network intervention or usual practice (we collected the data 2009–2013). We calculated a validated cardiometabolic risk score (CRS) based on resting blood pressure, HbA1c (glycated hemoglobin), HDL (high-density lipoprotein) and total cholesterol, height and weight (body mass index), and tobacco consumption. We compared changes in baseline CRS to 12-month follow-up. Results. There was no significant main effect on CRS associated with the intervention in either industry. However, significant interaction effects revealed that the intervention improved CRS at the 12-month follow-up among intervention participants in both industries with a higher baseline CRS. Age also moderated intervention effects: older employees had significantly larger reductions in CRS at 12 months than did younger employees. Conclusions. The intervention benefited employee health by reducing CRS equivalent to 5 to 10 years of age-related changes for those with a higher baseline CRS and for older employees. Trial Registration. ClinicalTrials.gov Identifier: NCT02050204. (Am J Public Health. 2023;113(12):1322–1331. https://doi.org/10.2105/AJPH.2023.307413 )"
"
Nature has no shortage of patterns, from spots on leopards to stripes on zebras and hexagons on boxfish. But a full explanation for how these patterns form has remained elusive.

Now engineers at the University of Colorado Boulder have shown that the same physical process that helps remove dirt from laundry could play a role in how tropical fish get their colorful stripes and spots. Their findings were published Nov. 8 in the journal Science Advances.
""Many biological questions are fundamentally the same question: How do organisms develop complicated patterns and shapes when everything starts off from a spherical clump of cells,"" said Benjamin Alessio, the paper's first author and an undergraduate researcher in the Department of Chemical and Biological Engineering. ""Our work uses a simple physical and chemical mechanism to explain a complicated biological phenomenon.""
Biologists have previously shown that many animals evolved to have coat patterns to camouflage themselves or attract mates. While genes encode pattern information like the color of a leopard's spots, genetics alone do not explain where exactly the spots will develop, for example.
In 1952, before biologists discovered the double helix structure of DNA, Alan Turing, the mathematician who invented modern computing, proposed a bold theory of how animals got their patterns.
Turing hypothesized that as tissues develop, they produce chemical agents. These agents diffuse through tissue in a process similar to adding milk to coffee. Some of the agents react with each other, forming spots. Others inhibit the spread and reaction of the agents, forming space between spots. Turing's theory suggested that instead of complex genetic processes, this simple reaction-diffusion model could be enough to explain the basics of biological pattern formation.
""Surely Turing's mechanism can produce patterns, but diffusion doesn't yield sharp patterns,"" said corresponding author Ankur Gupta, an assistant professor in the Department of Chemical and Biological Engineering. For instance, when milk diffuses in coffee, it flows in all directions with a fuzzy outline.

When Alessio visited the Birch Aquarium in San Diego, he was impressed by the sharpness of the boxfish's intricate pattern: It's made of a purple dot surrounded by a distinct hexagonal yellow outline with thick black spacing in between. Turing's theory alone would not be able to explain the sharp outlines of these hexagons, he thought. But the pattern reminded Alessio of computer simulations he had been conducting, where particles do form sharply defined stripes. Alessio, a member of the Gupta research group, wondered if the process known as diffusiophoresis plays a role in nature's pattern formation.
Diffusiophoresis happens when a molecule moves through liquid in response to changes, such as differences in concentrations, and accelerates the movement of other types of molecules in the same environment. While it may seem like an obscure concept to non-scientists, it's actually how laundry gets clean.
One recent study showed that rinsing soap-soaked clothes in clean water removes the dirt faster than rinsing soap-soaked clothes in soapy water. This is because when soap diffuses out of the fabric into water with lower soap concentration, the movement of soap molecules draws out the dirt. When the clothes are put in soapy water, the lack of a difference in soap concentration causes the dirt to stay in place.
The movement of molecules during diffusiophoresis, as Gupta and Alessio observed in their simulations, always follows a clear trajectory and gives rise to patterns with sharp outlines.
To see if it may play a role in giving animals their vivid patterns, Gupta and Alessio ran a simulation of the purple and black hexagonal pattern seen on the ornate boxfish skin using only the Turing equations. The computer produced a picture of blurry purple dots with a faint black outline. Then the team modified the equations to incorporate diffusiophoresis. The result turned out to be much more similar to the bright and sharp bi-color hexagonal pattern seen on the fish.
The team's theory suggests that when chemical agents diffuse through tissue as Turing described, they also drag pigment-producing cells with them through diffusiophoresis -- just like soap pulls dirt out of laundry. These pigment cells form spots and stripes with a much sharper outline.
Decades after Turing proposed his seminal theory, scientists have used the mechanism to explain many other patterns in biology, such as the arrangement of hair follicles in mice and the ridges in the roof of the mouth of mammals.
Gupta hopes their study, and more research underway by his research group, can also improve the understanding of pattern formation, inspiring scientists to develop innovative materials and even medicines.
""Our findings emphasize diffusiophoresis may have been underappreciated in the field of pattern formation. This work not only has the potential for applications in the fields of engineering and materials science but also opens up the opportunity to investigate the role of diffusiophoresis in biological processes, such as embryo formation and tumor formation,"" Gupta said.

","score: 13.485280830280832, grade_level: '13'","score: 14.857967032967032, grade_levels: ['college_graduate'], ages: [24, 100]",10.1126/sciadv.adj2457,"Turing patterns are fundamental in biophysics, emerging from short-range activation and long-range inhibition processes. However, their paradigm is based on diffusive transport processes that yield patterns with shallower gradients than those observed in nature. A complete physical description of this discrepancy remains unknown. We propose a solution to this phenomenon by investigating the role of diffusiophoresis, which is the propulsion of colloids by a chemical gradient, in Turing patterns. Diffusiophoresis enables robust patterning of colloidal particles with substantially finer length scales than the accompanying chemical Turing patterns. A scaling analysis and a comparison to recent experiments indicate that chromatophores, ubiquitous in biological pattern formation, are likely diffusiophoretic and the colloidal Péclet number controls the pattern enhancement. This discovery suggests that important features of biological pattern formation can be explained with a universal mechanism that is quantified straightforwardly from the fundamental physics of colloids."
"
Scientists have unlocked the genetic basis underlying the remarkable variation in body size observed in song sparrows, one of North America's most familiar and beloved songbirds. This discovery also provides insights into this species' capacity to adapt to the challenges of climate change.

The study, published today in Nature Communications, used genomic sequencing to successfully pinpoint eight genetic variants, or DNA mutations, largely responsible for the nearly threefold difference in body size observed across the song sparrow range from Mexico to Alaska. For instance, song sparrows that live year-round in the Aleutian Islands can be up to three times larger than their counterparts in the coastal marshes of California.
Katherine Carbeck, the study's first author and a PhD candidate in the faculty of forestry, University of British Columbia, explains that body size varies predictably in many species that inhabit vastly different climatic conditions, aligning with ""Bergmann's rule"" which states that organisms in cooler climates tend to be larger as an adaptation to regulate body temperature.
""The existence of 'locally adapted' populations implies that natural selection has shaped the genetic makeup of song sparrow populations across their range, enabling individuals to survive and reproduce in drastically different climatic conditions,"" said Carbeck. ""However, the genetic mechanisms underlying Bergmann's rule have remained elusive until now.""
Whole-genome sequencing cracks the code
Carbeck and colleagues from the Cornell Lab of Ornithology, University of Alaska and Ouachita Baptist University used the power of whole-genome sequencing to decode the entire song sparrow genome and unlock its secrets.
They combed through genetic samples from the two largest song sparrow subspecies that live year-round in the Aleutian Islands, as well as two smaller subspecies: one that breeds in Alaska but migrates to warmer sites in winter, and one that lives year-round on the B.C. coast, where the Pacific Ocean maintains comparatively mild winter weather.

Their comparison of the larger and smaller-bodied subspecies revealed several candidate genes associated with body mass. By characterizing these candidates, they identified eight specific genetic variants closely linked to body mass -- aligning with Bergmann's rule.
Genetic diversity helps life adapt to climate change
The researchers suggest that revealing a genetic basis for Bergmann's rule helps us to understand how evolution, natural selection and climate have interacted throughout a species' history.
""Our results highlight the potential role habitat conservation plays in enabling the continued exchange of genes between populations -- which is important in the face of ongoing change,"" said Carbeck.
Dr. Jen Walsh, a study co-author and research associate at the Cornell Lab of Ornithology, added: ""From a genomic perspective, identifying a small number of candidate genes with an apparently large impact on variation in body size is really interesting. The magnificent range of phenotypic diversity seen in song sparrows suggest they offer exciting opportunities to identify genes underlying a host of well-known and generally accepted eco-geographic rules.""
Dr. Peter Arcese, a co-author and a professor in UBC's department of forest and conservation sciences, said the findings suggest a resilient future for these birds.
""Our findings imply that some, if not all, locally adapted song sparrow populations may continue to adapt to climate change, as long as we maintain habitat conditions that facilitate the movement of individuals and genes between populations,"" he said.

","score: 18.26947474747475, grade_level: '18'","score: 20.0867595959596, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41467-023-42786-2,"Ecogeographic rules denote spatial patterns in phenotype and environment that may reflect local adaptation as well as a species’ capacity to adapt to change. To identify genes underlying Bergmann’s Rule, which posits that spatial correlations of body mass and temperature reflect natural selection and local adaptation in endotherms, we compare 79 genomes from nine song sparrow (Melospiza melodia) subspecies that vary ~300% in body mass (17 − 50 g). Comparing large- and smaller-bodied subspecies revealed 9 candidate genes in three genomic regions associated with body mass. Further comparisons to the five smallest subspecies endemic to California revealed eight SNPs within four of the candidate genes (GARNL3, RALGPS1, ANGPTL2, and COL15A1) associated with body mass and varying as predicted by Bergmann’s Rule. Our results support the hypothesis that co-variation in environment, body mass and genotype reflect the influence of natural selection on local adaptation and a capacity for contemporary evolution in this diverse species."
"
Invoking a sense of guilt -- a common tool used by advertisers, fundraisers and overbearing parents everywhere -- can backfire if it explicitly holds a person responsible for another's suffering, a meta-analysis of studies revealed.

While guilt is widely used to try and persuade people to act, research has been mixed on its effectiveness in spurring behavior change. This analysis, published in the journal Frontiers in Psychology, found that overall guilt had only a small persuasive effect, which is in line with previous research.
However, researchers uncovered that guilt worked better when it was more ""existential,"" meaning it appealed to a person's general desire to better society rather than giving them direct responsibility for a specific problem, a tactic that might be seen as overly manipulative.
""Guilt can be effective, but it will not generate some magic outcome,"" said lead author Wei Peng, an assistant professor in Washington State University's Murrow College of Communication. ""The surprising finding from this meta-analysis is that making people feel they are responsible for misdeeds or transgressions is not actually effective. Practitioners may want to consider the many different factors that make guilt appeals more persuasive.""
For this study, researchers analyzed data from 26 studies involving more than 7,500 participants. In addition to the effect of responsibility, they found that guilt seems to work better when it is clear that the problem can be changed and possible actions to take are proposed.
Peng and his colleagues also found that guilt was more persuasive with certain subject matter: namely in environmental and educational issues. It was less effective in health communications. The authors noted that health topics may be complicated by the fact that the desired behavior can affect the individual as well as hold benefits for others, such as getting vaccinated for COVID-19.
This analysis also showed that guilt can be an effective motivator for action when it comes to distant and broader issues, such as people suffering after a natural disaster or from social injustice.
Guilt, like pride and shame, is thought to be a unique emotion to human beings, tied to high-level pursuits that go beyond meeting an individual's basic needs, Peng said, such as a person's role in creating a better group, country or overall human society. This may help explain why inducing guilty feelings in relation to more distant issues work better than ones that are very personal.
""When people want to use guilt in an appeal, it may be better to use it implicitly to try to make other people feel they should take on this responsibility, rather than say explicitly that they are responsible for what other people are suffering,"" Peng said.

","score: 15.445944444444446, grade_level: '15'","score: 16.517433333333337, grade_levels: ['college_graduate'], ages: [24, 100]",10.3389/fpsyg.2023.1201631,"Guilt appeals are widely used as a persuasive approach in various areas of practice. However, the strength and direction of the persuasive effects of guilt appeals are mixed, which could be influenced by theoretical and methodological factors. The present study is a comprehensive meta-analysis of 26 studies using a random-effects model to assess the persuasive effects of guilt appeals. In total, 127 effect sizes from seven types of persuasive outcomes (i.e., guilt, attitude, behavior, behavioral intention, non-guilt emotions, motivation, and cognition) were calculated based on 7,512 participants. The analysis showed a small effect size of guilt appeals [g= 0.19, 95% CI (0.10, 0.28)]. The effect of guilt appeals was moderated by the theoretical factors related to appraisal and coping of guilt arousal, including attributed responsibility, controllability and stability of the causal factors, the proximity of perceiver-victim relationship, recommendation of reparative behaviors, and different outcome types. The effect was also associated with methods used in different studies. Overall, the findings demonstrated the persuasive effects of guilt appeals, but theoretical and methodological factors should be considered in the design and testing of guilt appeals. We also discussed the practical implications of the findings."
"
Many parents experience stress, anxiety, and depressive symptoms throughout their lives, particularly during times of transition, such as pregnancy and children's entry into school. Studies have generally found that high levels of anxiety and depression in parents are linked to poorer behavioural and cognitive outcomes in children.

A team of researchers led by Tina Montreuil, Associate Professor in McGill's Department of Educational and Counselling Psychology and Scientist in the Child Health and Human Development Program at the Research Institute of the McGill University Health Centre (RI-MUHC), has found that slightly higher, but mild anxious or depressive symptoms in fathers were associated with fewer behavioural difficulties in the first years of elementary school and better scores on a standardized IQ test in their children. Their findings are published in Frontiers in Psychology.
""Our study shows that both mothers' and fathers' well-being are important to promote the cognitive-behavioural development of their children, and that they are potentially complementary,"" says Prof. Montreuil.
Linking fathers' mental health to children's development
While the role of mothers' stress, anxiety and depression on children's behavioural and cognitive development is well established, less is known about the connection between fathers' mental health and children's development.
The team of researchers examined if paternal anxiety and depressive symptoms, measured during their partner's pregnancy, and again six to eight years later, are associated with children's cognitive function and behaviour. They studied this association in a community sample, where parental levels of self-reported anxious and depressive symptoms were variable and typically less severe than among a clinically diagnosed population.
The first assessments, made during pregnancy and in infancy, included parental mental health and psychosocial measures, such as the parents' highest level of education, relationship satisfaction, and parenting perceptions. The ancillary study investigation was conducted at the critical age of six to eight years, when children are in the early elementary school years and expected to make increased use of their behavioural and cognitive skills.

""Our findings show that fathers' reported symptoms of anxiety and/or depression were not associated with worse behavioural and cognitive outcomes in their children, as previously found in other studies,"" says Sherri Lee Jones, first author of the study and Research Associate at Douglas Research Centre who was a Postdoctoral Fellow and Research Associate at the RI-MUHC during the study.
More specifically, the researchers found that slightly higher levels of depressive symptoms reported by fathers when their partner was pregnant were associated with fewer behavioural and emotional difficulties in their child at about six to eight years of age. This included children being able to sit still for long periods of time, infrequently losing their temper and having a good attention span, as reported by parents in questionnaires. In contrast, higher symptoms of anxiety and depression among mothers were associated with poorer childhood behavioural outcomes, both at birth and during middle childhood.
At the childhood assessment, slightly higher but still mild paternal anxious and depressive symptoms were both associated with slightly higher scores of cognitive functions in the 6-8 year old children. This was also in contrast to the patterns found among mothers.
Understanding parental influence
The researchers point out that their findings may not be generalizable to parents who are experiencing clinical levels of depression and anxiety, and that none of the factors they examined could explain the associations between the father's mental health symptoms and the child's outcomes.
""More studies are needed to understand the respective roles and the combined contribution of parents in child development,"" says Prof. Montreuil. ""Our findings, like others, point to the importance of coaching individuals transitioning into parenthood. They also highlight the importance of parental attunement. This term refers to the parent's ability to respond adaptively to their child signals, by attentively adjusting their response to the child's needs, in a given situation.""
""Since greater parental attunement is associated to child cognitive and social competencies, one potential explanation is that the fathers in our study sample may have shown greater attunement to their child to 'compensate' for environmental risk factors, such as maternal depressive or anxiety symptoms, or others known predictors,"" adds Prof. Montreuil.

","score: 18.175093795093797, grade_level: '18'","score: 21.125541125541126, grade_levels: ['college_graduate'], ages: [24, 100]",10.3389/fpsyg.2023.1218384,"Paternal mental health has been associated with adverse consequences on offspring psychosocial development, and family environmental factors may partly explain those associations. To clarify this, we need comprehensive prospective studies, particularly in middle-childhood when the child enters school and is expected to make use of behavioral and cognitive skills as part of their interactions and learning. Using data from a sub-sample of the prospective 3D birth cohort study comprised of mother-father-child triads, and a follow-up of the parents and the children at 6–8 years of age (n = 61; 36 boys, 25 girls), we examined whether paternal anxious and depressive symptoms measured during the pregnancy period (i.e., prenatally) or concurrently when the child was assessed at 6–8 years old were associated with children's cognition/behavior. In contrast to our hypotheses, we found that greater prenatal paternal depressive symptoms predicted fewer child behavioral difficulties; and that greater concurrent childhood paternal depression or anxiety symptoms were associated with higher child full-scale IQ, controlling for the equivalent maternal mental health assessment and parental education. Father parenting perception did not mediate these associations, nor were they moderated by maternal mental health at the concurrent assessment, or paternal ratings of marital relationship quality. These findings suggest that higher symptoms of paternal mental health symptoms are associated with fewer child behavioral difficulties and higher cognitive performance in middle childhood. Potential clinical implications and future research directions are discussed."
"
Anyone who's ever done a belly flop into a swimming pool knows it ends with a blunt-sounding splat, a big splash and a searing red sting. What most people don't know is why.

Daniel Harris does. The assistant professor in Brown University's School of Engineering says the physics behind the phenomenon aren't too complex. What happens -- and what makes it so painful, he explains -- is that the forces from the water surface put up a fierce resistance to the body suddenly going from air to water, which is often still.
""All of a sudden, the water has to accelerate to catch up to the speed of what's falling through the air,"" said Harris, who studies fluid mechanics. ""When this happens, that large reaction force is sent back to whatever's doing the impacting, leading to that signature slam.""
How and why this happens in fluid mechanics isn't just important for developing a prize-winning belly flop for competitions, or dolling out pool-party trivia on why belly flops hurt so much -- the understanding is critical to naval and marine engineering, which often have structures that need to survive high-impact air-to-water slamming forces. For that reason, the phenomenon has been studied thoroughly for the past century. But a research team led by Harris and Brown graduate student John Antolik found novel insights in a new study done in partnership with scholars at the Naval Undersea Warfare Center in Newport and Brigham Young University.
For the Journal of Fluid Mechanics study, the researchers set up a belly flop-like water experiment using a blunt cylinder but adding an important vibrating twist to it, which ultimately led them to counterintuitive findings.
""Most of the work that's been done in this space looks at rigid bodies slamming into the water, whose overall shape doesn't really change or move in response to the impact,"" Harris said. ""The questions that we start to get at are: 'What if the object that's impacting is flexible so that once it feels the force it can either change shape or deform? How does that change the physics and then, more importantly, the forces that are felt on these structures?""
To answer that, the researchers attached a soft ""nose"" to the body of their cylinder, referred to as an impactor, with a system of flexible springs.

The idea, Antolik explains, is that the springs -- which act in principle similar to the suspension of a car -- should help soften the impact by distributing the impact load over a longer period. This strategy has been floated as a potential solution for reducing sometimes catastrophic slamming impacts in air-to-water transitions, but few experiments have ever looked closely at the fundamental mechanics and physics involved.
For this experiment, the researchers dropped the cylinder repeatedly into still water and analyzed both the visual results and data from sensors embedded inside the cylinder.
This is where the unexpected happened.
The results show that while the strategy can be effective, surprisingly, it doesn't always soften the impact. In fact, contrary to conventional thinking, sometimes the more flexible system can increase the maximum impact force on the body as compared to a fully rigid structure.
This forced the researchers to dig deeper. Through extensive experiments and by developing a theoretical model, they found their answer. Depending on the height from which the impactor is dropped and how stiff the springs are, the body will not only feel the impact from the slam but it will also feel the vibrations of the structure as it enters the water, compounding the slamming force.
""The structure is vibrating back and forth due to the violent impact, so we were getting readings from both the impact of hitting the fluid and an oscillation because the structure is shaking itself,"" Harris said. ""If you don't time those right, you can basically make the situation worse.""
The researchers found the key was the springs: they have to be soft enough to gently absorb the impact without leading to more rapid vibrations that add to the overall force.

Working in Brown's Engineering Research Center, Antolik recorded the experiments using high-speed cameras and used an impact measurement tool called an accelerometer. ""The whole back corner gets a little bit wet when I'm doing the experiments,"" he joked.
The researchers are now looking at next steps in their research line, taking inspiration from diving birds.
""Biological studies of these birds have shown that they perform certain maneuvers as they enter the water to improve the conditions so they don't experience such high forces,"" Antolik said. ""What we're moving towards is trying to design what is essentially a robotic impactor that can perform some active maneuver during water entry to do the same for blunt objects.""
The study was supported by the Office of Naval Research and Naval Undersea Warfare Center.

","score: 12.50535230502344, grade_level: '13'","score: 13.916860443657036, grade_levels: ['college_graduate'], ages: [24, 100]",10.1017/jfm.2023.820,"When a blunt body impacts an air–water interface, large hydrodynamic forces often arise, a phenomenon many of us have unfortunately experienced in a failed dive or ‘belly flop’. Beyond assessing risk to biological divers, an understanding and methods for remediation of such slamming forces are critical to the design of numerous engineered naval and aerospace structures. Herein we systematically investigate the role of impactor elasticity on the resultant structural loads in perhaps the simplest possible scenario: the water entry of a simple harmonic oscillator. Contrary to conventional intuition, we find that ‘softening’ the impactor does not always reduce the peak impact force, but may also increase the force as compared with a fully rigid counterpart. Through our combined experimental and theoretical investigation, we demonstrate that the transition from force reduction to force amplification is delineated by a critical ‘hydroelastic’ factor that relates the hydrodynamic and elastic time scales of the problem."
"
World Rugby Chairman Sir Bill Beaumont has welcomed results from the largest ever studies into the forces experienced by rugby players. The results, which provide players and parents with greater clarity and confidence than ever before into the benefits and safety of rugby, are a first anywhere in world sport.

The Otago Community Head Impact Detection study (ORCHID) a joint project between World Rugby, Prevent Biometrics, New Zealand Rugby, Otago Rugby and the University of Otago, has published the first independent, peer-reviewed findings into community rugby following almost two years of trail-blazing research. The study measures over 17,000 separate head acceleration events across more than 300 players from senior rugby through to U13s level.
This work was followed by the Elite Extension of the ORCHID study in partnership with the Ulster University and Premiership Rugby. Further updates into the women's community game are currently being prepared for peer review and publication.
Both studies used smart mouthguard technology, supplied by Prevent Biometrics, to understand the forces on the head experienced by players both in matches and training situations. The mouthguards measure g-forces which are experienced for less time than it takes to blink, using technology independently verified both in research laboratories and on the field of play.
The ORCHID paper shows that in the men's community game: 86 per cent of forces measured are the same as or less than those experienced in other forms of exercise such as running, jumping or skipping 94 per cent of forces are lower than those previously measured on people riding a rollercoaster The large majority of events resulting in the highest measured forces are as a result of poor technique in the tackle and at the breakdownThe Elite Extension study also showed that: Most contact events in elite rugby do not result in any significant force to the head. Where low, medium and high force events do occur they are most common in tackles and carries, followed by rucks Both men's and women's forwards were more likely to experience force events than backsWorld Rugby has already used preliminary findings from the ORCHID study to inform trials of a lower tackle height in the community game. The international federation has also expanded and enhanced the range of training for players and coaches available for free online including the Tackle Ready and new Breakdown Ready programmes.

At the elite level, in a world first, World Rugby announced in October that smart mouthguards will be added to the Head Injury Assessment (HIA) protocols from January 2024.
Dr Melanie Bussey, Associate Professor in Biomechanics at the University of Otago said, ""Our ultimate goal as researchers is to make a meaningful impact through our work. Therefore, we are extremely pleased to see our work integrated into new strategies and guidelines designed to enhance player safety. We appreciate World Rugby's approach, which granted us the time to ensure robustness in our analysis and the autonomy to let the data speak for itself.""
""Looking ahead, we believe that Smart mouthguard technology holds immense potential for advancing player safety and performance analysis in rugby and beyond. Our research has opened doors to a wealth of insights, and we are committed to further exploring this innovative field. We envision continued collaboration with World Rugby and other stakeholders to harness the full potential of this technology, driving advancements that will benefit players and the sport as a whole. The journey has just begun, and we are excited to embark on it.""
Dr Gregory Tierney, Assistant Professor in Biomechanics at Ulster University said, ""These studies put in the groundwork so that we can now monitor player head impact exposure in rugby and develop strategies to ensure the game is played in the safest possible manner. Smart mouthguards can aid sideline medical decision making and it is exciting that our research has contributed to World Rugby implementing the technology into the Head Injury Assessment protocol.""
World Rugby Sir Bill Beaumont welcomed the world-leading study: ""Using the latest research and technology is at the heart of our six point plan to make rugby the most progressive sport in the world on player welfare. These studies are concrete proof that World Rugby us putting our time, energy and efforts in to back up our words and the insights gained are already helping us make evidence-led moves to make the sport even safer, we will never stand still on player welfare.
""I'd like to thank the players all across the world who took part in the study, what they have helped to shed light on will be invaluable in advancing player welfare in rugby at all levels. Using this data we can say with some certainty that community and elite level rugby are very much the same game, but played very differently.

World Rugby Chief Medical Officer Dr Eanna Falvey said ""It is encouraging to see that alongside our recent research into the health benefits of rugby, we now have the data that offers a more complete picture of what it is like to play our sport. These studies gives us the ability like never before to understand the causes of head impacts and accelerations and we will leave no stone unturned, making whatever changes may be needed to reduce large forces to the head in our game.""
New Zealand Rugby General Manager Community Rugby Steve Lancaster said:
""New Zealand Rugby has an absolute commitment to making the game as safe as possible and reducing the risk of injury to our participants at all levels. We believe that there are so many benefits to playing rugby and are committed to balancing those with the need to respond to the research and risks in an appropriate way.
""We're particularly proud that our New Zealand rugby community has been at the forefront of this research, and we are already seeing this contribution make a material difference through initiatives like the reduced tackle height in community rugby that has been positively received.""
The studies into come in the wake of recent World Rugby commissioned research which shows that playing rugby provided USD 1.5bn in preventative healthcare savings around the world, 30 per cent reductions in levels of childhood obesity, 15% reduction in heart disease and 33 per cent reduction in mental illness amongst adults.

","score: 17.077054007820134, grade_level: '17'","score: 19.143337609970672, grade_levels: ['college_graduate'], ages: [24, 100]",10.1007/s40279-023-01923-z,"The aim of this study was to examine the cumulative head acceleration event (HAE) exposure in male rugby players from the Under-13 (U13) to senior club level over 4 weeks of matches and training during the 2021 community rugby season. This prospective, observational cohort study involved 328 male rugby players. Players were representative of four playing grades: U13 (N = 60, age 12.5 ± 0.6 years), U15 (N = 100, age 14.8 ± 0.9 years), U19 (N = 78, age 16.9 ± 0.7 years) and Premier senior men (N = 97, age 22.5 ± 3.1 years). HAE exposure was tracked across 48 matches and 113 training sessions. HAEs were recorded using boil-and-bite instrumented mouthguards (iMGs). The study assessed the incidence and prevalence of HAEs by ages, playing positions, and session types (match or training). For all age grades, weekly match HAE incidence was highest at lower magnitudes (10–29 g). Proportionally, younger players experienced higher weekly incidence rates during training. The U19 players had 1.36 times the risk of high-magnitude (> 30 g) events during matches, while the U13 players had the lowest risk compared with all other grades. Tackles and rucks accounted for the largest HAE burden during matches, with forwards having 1.67 times the risk of > 30 g HAEs in rucks compared with backs. This study provides novel data on head accelerations during rugby matches and training. The findings have important implications for identifying populations at greatest risk of high cumulative and acute head acceleration. Findings may guide training load management and teaching of skill execution in high-risk activities, particularly for younger players who may be exposed to proportionally more contact during training and for older players during matches."
