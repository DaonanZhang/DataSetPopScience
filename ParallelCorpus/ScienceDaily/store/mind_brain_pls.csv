pls,fk_score,ari_score,reference,abstract
"
Exposure therapy for a specific fear can also help reduce other fears. This is the conclusion reached by psychologists at Ruhr University Bochum, Germany, who studied 50 people with a fear of spiders and heights. Although they only treated the fear of spiders, the fear of heights was likewise reduced in the process. Findings are described by a team around Iris Kodzaga and Professor Armin Zlomuzica from the Department of Behavioral and Clinical Neuroscience at Ruhr University Bochum in the journal Translational Psychiatry. 

A number of anxiety disorders are co-morbid
""Anxiety rarely comes alone,"" says Iris Kodzaga, lead author of the study. ""Patients who suffer from one fear often subsequently develop another."" The most effective treatment method is exposure: By confronting the fear-inducing situations or stimuli under psychotherapeutic supervision, patients learn to overcome their fear.
""It was long assumed that if a person had multiple fears, they would require multiple exposure therapies tailored to their specific fear,"" explains Kodzaga. The Bochum-based team is now challenging this assumption. The researchers measured fear of spiders and heights in 50 test subjects before and after exposure therapy targeting spider fear. Measures included subjective data from specific questionnaires for fear of spiders and heights. In addition, the researchers collected quantitative behavioral measures, such as how close the participants dared to approach the spiders or how far they could climb a high church tower.
Therapy methods could become more universal
Exposure therapy for spider fear not only reduced the fear of spiders, but also the fear of heights. A significant effect emerged in both the subjective and behavioral measures: Fear of heights decreased by an average of 15 percent as a result of exposure to spiders.
""The discovery that exposure to spiders also reduces fear of heights opens up new perspectives for the efficient treatment of phobias,"" says Iris Kodzaga. ""It could mean that we can rethink therapeutic approaches and possibly develop more universal methods.""
How exactly this effect is transferred from one fear to another is still unclear. ""The effect can't be fully explained by associative learning processes. The generalization effect might be due to an increase in self-efficacy as a result of exposure therapy,"" says the researcher. ""But perhaps there is also a common denominator between fear of spiders and fear of heights that's not obvious. We'll need to conduct follow-up studies to find out more.""

","score: 12.62046215046215, grade_level: '13'","score: 12.794137124137123, grade_levels: ['college'], ages: [18, 24]",10.1038/s41398-023-02698-7,"Previous research has shown that fear associated with one stimulus often spreads to other stimuli with similar perceptual features as well as across different stimulus categories. Exposure is considered as the most effective intervention to attenuate exaggerated fear. The extent to which exposure treatment effects can generalize to fears not targeted during treatment remains elusive. Previous studies on possible generalization of beneficial effects of exposure used stimuli sharing the same stimulus category and/or stimuli having high perceptual similarity. The current study examined whether exposure treatment generalization can be achieved for untreated stimuli which do not share any perceptual resemblance and belong to a different fear category. An analogue sample of fifty participants with fear of spiders (animal-related fears) and heights (natural environment-related fears) was tested. Participants have been randomly assigned to either an exposure treatment (n = 24) or a control condition (n = 26). Exposure treatment was designed to only target participants’ fear of spiders, leaving their fear of heights untreated. Results demonstrated that the effects of exposure treatment generalized to fear of heights, as indicated by a reduction in behavioral avoidance, as well as self-reported acrophobia symptoms. The present study confutes the assumption that generalization of exposure effects to untreated fears is based on perceptual similarity. Clearly, further research is required to determine the decisive factors, in order to expand the generalization effect permanently to any given type of fear."
"
In people with amyotrophic lateral sclerosis (ALS), changes in neurons appear to activate immune cells. Lowering the inflammation could reduce the symptoms of the disease, according to a study led by Chantelle Sephton, a professor at Université Laval's Faculty of Medicine.

ALS is caused by the loss of upper motor neurons, located in the brain, and lower motor neurons, which extend from the spinal cord to the muscles. Using a genetically modified mouse model, Chantelle Sephton and her team found that structural changes in the upper neurons occurred prior to disease symptoms.
The study suggests that these morphological changes send a signal to microglia and astrocytes, the immune cells of the central nervous system. When they arrive, their effect is protective, but if they stay too long, they become toxic to neurons. This leads to a reduction in synaptic connections between motor neurons in the brain and spinal cord, which in turn results in a reduction in synaptic connections with muscles. These changes lead to atrophy and loss of motor function.
Given this correlation between symptoms and immune response, the research team wondered whether it might be possible to restore synaptic connections by blocking inflammation. "" We tested a semi-synthetic drug based on Withaferin A, an extract of the Ashwagandha plant, which has been used for thousands of years in traditional Indian medicine,"" explains CERVO Research Center affiliate Chantelle Sephton.
The drug blocks inflammation and allows motor neurons to return to a more normal state. ""We have noticed that neurons regenerate in the absence of activated immune cells. The dendrites of motor neurons start to grow and make connections again, increasing the number of synapses between motor neurons and muscles,"" reports the researcher.
This seems a promising way of improving ALS symptoms, whether the disease is familial or sporadic, since both types are associated with inflammation.
Other diseases where inflammation plays a role, such as Alzheimer's, could benefit from this approach.
The study was published in the scientific journal Acta Neuropathologica Communications. The signatories are Mari Carmen Pelaez, Antoine Desmeules, Pauline Gelon, Bastien Glasson, Laetitia Marcadet, Alicia Rodgers, Daniel Phaneuf, Silvia Pozzi, Paul Dutchak, Jean-Pierre Julien and Chantelle Sephton.

","score: 13.298449788755281, grade_level: '13'","score: 14.575594735131624, grade_levels: ['college_graduate'], ages: [24, 100]",10.1186/s40478-023-01671-1,"Amyotrophic lateral sclerosis (ALS) and frontotemporal dementia (FTD) are related neurodegenerative diseases that belong to a common disease spectrum based on overlapping clinical, pathological and genetic evidence. Early pathological changes to the morphology and synapses of affected neuron populations in ALS/FTD suggest a common underlying mechanism of disease that requires further investigation. Fused in sarcoma (FUS) is a DNA/RNA-binding protein with known genetic and pathological links to ALS/FTD. Expression of ALS-linked FUS mutants in mice causes cognitive and motor defects, which correlate with loss of motor neuron dendritic branching and synapses, in addition to other pathological features of ALS/FTD. The role of ALS-linked FUS mutants in causing ALS/FTD-associated disease phenotypes is well established, but there are significant gaps in our understanding of the cell-autonomous role of FUS in promoting structural changes to motor neurons, and how these changes relate to disease progression. Here we generated a neuron-specific FUS-transgenic mouse model expressing the ALS-linked human FUSR521G variant, hFUSR521G/Syn1, to investigate the cell-autonomous role of FUSR521G in causing loss of dendritic branching and synapses of motor neurons, and to understand how these changes relate to ALS-associated phenotypes. Longitudinal analysis of mice revealed that cognitive impairments in juvenile hFUSR521G/Syn1 mice coincide with reduced dendritic branching of cortical motor neurons in the absence of motor impairments or changes in the neuromorphology of spinal motor neurons. Motor impairments and dendritic attrition of spinal motor neurons developed later in aged hFUSR521G/Syn1 mice, along with FUS cytoplasmic mislocalisation, mitochondrial abnormalities and glial activation. Neuroinflammation promotes neuronal dysfunction and drives disease progression in ALS/FTD. The therapeutic effects of inhibiting the pro-inflammatory nuclear factor kappa B (NF-κB) pathway with an analog of Withaferin A, IMS-088, were assessed in symptomatic hFUSR521G/Syn1 mice and were found to improve cognitive and motor function, increase dendritic branches and synapses of motor neurons, and attenuate other ALS/FTD-associated pathological features. Treatment of primary cortical neurons expressing FUSR521G with IMS-088 promoted the restoration of dendritic mitochondrial numbers and mitochondrial activity to wild-type levels, suggesting that inhibition of NF-κB permits the restoration of mitochondrial stasis in our models. Collectively, this work demonstrates that FUSR521G has a cell-autonomous role in causing early pathological changes to dendritic and synaptic structures of motor neurons, and that these changes precede motor defects and other well-known pathological features of ALS/FTD. Finally, these findings provide further support that modulation of the NF-κB pathway in ALS/FTD is an important therapeutic approach to attenuate disease."
"
Teens from larger families have poorer mental health than those with fewer siblings, according to a large analysis of children in the United States and China.

The details of the pattern vary depending on factors such as the spacing of sibling ages and the age of the siblings.
But the fact that the overall pattern was found in both countries is striking, said Doug Downey, lead author of the study and professor of sociology at The Ohio State University.
""Our results couldn't have been easily predicted before we did the study,"" Downey said.
""Other studies have shown that having more siblings is associated with some positive effects, so our results were not a given.""
Downey conducted the study with Rui Cao, a doctoral student in sociology at Ohio State. Their results were published recently in the Journal of Family Issues.
Their Chinese analysis draws on more than 9,400 eighth graders from the China Education Panel Study. In the United States, they analyzed over 9,100 American eighth graders from the Early Childhood Longitudinal Study -- Kindergarten Cohort of 1988.

The average youth in China has nearly .7 fewer siblings than the average American youth (.89 compared to 1.6).
Consistent with what was expected because of China's One Child Policy, about one-third of Chinese children are only children (34%), compared to just 12.6% of American children.
In both countries, researchers asked students (average age of 14) a variety of questions about their mental health, although the questions were different in China and the United States.
In China, teens with no siblings showed the best mental health, while in the United States, those with no or one sibling had similar mental health.
Some issues could only be analyzed using the U.S. data.
Results in the U.S. showed that half and full siblings are both linked to poorer mental health.

And having older siblings and siblings closely spaced in age tended to have the worst impacts on well-being, the U.S. data found. Siblings born within one year of each other had the strongest negative association with mental health.
Why are more siblings linked with poorer mental health?
Downey said the overall findings fits with the ""resource dilution"" explanation.
""If you think of parental resources like a pie, one child means that they get all the pie -- all the attention and resources of the parents,"" he said.
""But when you add more siblings, each child gets fewer resources and attention from the parents, and that may have an impact on their mental health.""
The fact that closely spaced siblings have the most negative impact bolsters that explanation. Children who are near the same age will be competing for the same types of parental resources, he said.
Another possibility, though, is that the families that have many versus few children are different in other ways that may reduce mental health for their kids -- the so-called selectivity explanation.
The differences between China and the U.S. do provide some support for the selectivity explanation. In each country, children from families associated with the most socioeconomic advantage had the best mental health.
In China, that was children in one-child families, while in the U.S. it was children with zero or one sibling.
But the overall results still suggest that selectivity explanation falls short in accounting for what is happening.
""What we found is that when you add all the evidence up, the effect of siblings on mental health is more on the negative side than the positive side,"" Downey said.
Downey noted that the data doesn't get at the quality of sibling relationships. It is likely that higher-quality sibling relationships will be more beneficial to children and may have more positive effects on mental health.
While this study shows a negative impact of siblings, other research has shown that having more brothers and sisters is associated with better social skills among kindergarteners and a lower likelihood of divorce among adults.
""This combination of results is not easily explained. We still have more to learn about the impact of siblings,"" Downey said.
""This is particularly important now as the U.S. and other countries have lower fertility rates. Understanding the consequences of growing up with fewer or no brothers and sisters is an increasingly important social issue.""

","score: 10.993894993894997, grade_level: '11'","score: 11.519435286935284, grade_levels: ['12'], ages: [17, 18]",10.1177/0192513X231220045,"A growing number of children are being raised with few or no siblings yet the consequences of this seismic demographic shift in family forms are not well understood. We investigate this question in the U.S. and China because previous studies highlight how contextual features can play an important role shaping how siblings matter. Our Chinese analyses draw on more than 9,400 eighth graders from the China Education Panel Study (CEPS). In the U.S., we analyze over 9,100 American eighth graders from the Early Childhood Longitudinal Study—Kindergarten Cohort of 1998 (ECLS-K:98), where our data allow us to consider multiple features of the sibship structure (e.g., size, sex composition, and density). We find that number of siblings is negatively associated with mental health in both China and the U.S., although the details of this pattern (non-linear association, sisters versus brothers, and closely versus widely spaced siblings) vary."
"
Reducing stimulant use was associated with significant improvement in measures of health and recovery among people with stimulant use disorder, even if they did not achieve total abstinence. This finding is according to an analysis of data from 13 randomized clinical trials of treatments for stimulant use disorders involving methamphetamine and cocaine. Historically, total abstinence has been the standard goal of treatment for substance use disorders, however, these findings support the growing recognition that a more nuanced perspective on measuring treatment success may be beneficial.

The study, published in Addiction, was led by scientists at the Johns Hopkins Bloomberg School of Public Health, Baltimore, in collaboration with researchers at the National Institute on Drug Abuse (NIDA), part of the National Institutes of Health.
Researchers found that transitioning from high use (five or more days a month) to lower use (one to four days a month) was associated with lower levels of drug craving, depression, and other drug-related challenges compared to no change in use. These results suggest that reduction in use of methamphetamine or cocaine, in addition to abstinence, is a meaningful surrogate or intermediate clinical outcome in medication development for stimulant addiction. Unlike other substance use disorders, such as opioid use disorder or alcohol use disorder, there are currently no U.S. Food and Drug Administration-approved pharmacological treatments for stimulant use disorders.
""These findings align with an evolving understanding in the field of addiction, affirming that abstinence should be neither the sole aim nor only valid outcome of treatment,"" said NIDA Director Nora Volkow, M.D. ""Embracing measures of success in addiction treatment beyond abstinence supports more individualized approaches to recovery, and may lead to the approval of a wider range of medications that can improve the lives of people with substance use disorders.""
Temporary returns to use after periods of abstinence are part of many recovery journeys, and relying exclusively on abstinence as an outcome in previous clinical trials may have masked beneficial effects of treatment. To help address this research gap, investigators analyzed data from previous clinical trials to study the effects of transitioning to reduced drug use or abstinence on a broad range of health measures. Researchers analyzed data from 13 randomized clinical trials evaluating the impact of potential pharmacological medications for stimulant use disorders, which included more than 2,000 individuals seeking treatment for cocaine or methamphetamine use disorders at facilities across the United States. The trials were of varying duration and were undertaken from 2001 to 2017.
Researchers compared ""no reduced use,"" ""reduced use,"" and ""abstinence"" in association with multiple health outcomes, such as severity of drug-related symptoms, craving, and depression. The study found that more participants reduced the frequency of primary drug use (18%) than achieved abstinence (14%). While abstinence was associated with the greatest clinical improvement, reduced use was significantly associated with multiple measures of improvements in psychosocial functioning at the end of the trials, such as a 60% decrease in craving for the primary drug, 41% decrease in drug-seeking behaviors, and a 40% decrease in depression severity, compared to the beginning of the trial.
These findings suggest that improvements in health and functioning can occur with reduced use and should be considered in the development and approval of treatments for substance use disorders. Research on alcohol use disorder has shown similar results, with studies finding that transitioning from high-risk to low-risk drinking is associated with functional improvement and fewer mental and general health consequences caused by alcohol. As a result, a reduced number of heavy drinking days is already recognized as a meaningful clinical outcome in medication development for alcohol use disorder.

""With addiction, the field has historically acknowledged only the benefits of abstinence, missing opportunities to celebrate and measure the positive impacts of reduced substance use,"" said Mehdi Farokhina, M.D., M.P.H., a staff scientist in the NIDA Intramural Research Program, and author on the paper. ""This study provides evidence that reducing the overall use of drugs is important and clinically meaningful. This shift may open opportunities for medication development that can help individuals achieve these improved outcomes, even if complete abstinence is not immediately achievable or wanted.""
The authors note that the study did not include behavioral treatment trials, which were too varied to harmonize their data. In addition, the study featured only people who enrolled in clinical trials, which could limit generalizability. Additional research is needed to understand the potential clinical benefits of reduced drug use, along with other harm reduction-based indicators of clinical improvement in real-world populations. The authors highlight that the findings of this study should encourage researchers to re-evaluate treatment outcome measures in their studies and consider non-abstinence treatment outcomes in the development of new medications for the treatment of stimulant use disorders. The authors also write that these new findings need to be replicated in other contexts with additional substance use disorders such as opioid use disorder.
""By promoting an understanding of addiction as a treatable disorder with multifaceted causes, society can work towards providing better support, resources, and care for individuals on their way to recovery,"" said Masoumeh Aminesmaeili, M.D., lead author of the paper. ""This approach is not only compassionate, but also clinically valid in addressing the complex nature of addiction.""
For more information on substance and mental health treatment programs in your area, call the free and confidential National Helpline 1-800-662-HELP (4357) or visit: https://www.findtreatment.gov 

","score: 18.105461426492003, grade_level: '18'","score: 20.09645924308588, grade_levels: ['college_graduate'], ages: [24, 100]",10.1111/add.16409,"Total abstinence has historically been the goal of treatment for substance use disorders; however, there is a growing recognition of the health benefits associated with reduced use as a harm reduction measure in stimulant use disorders treatment. We aimed to assess the validity of reduced stimulant use as an outcome measure in randomized controlled trials (RCTs) of pharmacological interventions for stimulant use disorder. We conducted a secondary analysis of a pooled dataset of 13 RCTs. Participants were individuals seeking treatment for cocaine or methamphetamine use disorders (N = 2062) in a wide range of treatment facilities in the United States. We validated reduced stimulant use against a set of clinical indicators drawn from harmonized measurements, including severity of problems caused by drug use, comorbid depression, global severity of substance use and improvement, severity of drug‐seeking behavior, craving and high‐risk behaviors, all assessed at the end of the trial, as well as follow‐up urine toxicology. A series of mixed effect regression models was conducted to validate reduction in frequency of use against no reduction in use and abstinence. More participants reduced frequency of primary drug use than achieved abstinence (18.0% vs. 14.2%, respectively). Reduced use was significantly associated with decreases in craving for the primary drug [60.1%, 95% confidence interval (CI) = 54.3%–64.7%], drug seeking behaviors (41.0%, 95% CI = 36.6%–45.7%), depression severity (39.9%, 95% CI = 30.9%–48.3%), as well as multiple measures of global improvement in psychosocial functioning and severity of drug‐related problems, albeit less strongly so than abstinence. Moreover, reduced use was associated with sustained clinical benefit at follow‐up, as confirmed by negative urine tests (adjusted odds ratio compared with those with no reduction in use: 0.50, 95% CI = 0.35–0.71). Reduced frequency of stimulant use appears to be associated with meaningful improvement in various clinical indicators of recovery. Assessment of reduced use, in addition to abstinence, could broaden the scope of outcomes measured in randomized controlled trials of stimulant use disorders and facilitate the development of more diverse treatment approaches."
"
Researchers have created the world's largest ancient human gene bank by analysing the bones and teeth of almost 5,000 humans who lived across western Europe and Asia up to 34,000 years ago.

By sequencing ancient human DNA and comparing it to modern-day samples, the international team of experts mapped the historical spread of genes -- and diseases -- over time as populations migrated.
The 'astounding' results have been revealed in four trailblazing research papers published today (10 January 2024) in the same issue of Nature and provide new biological understanding of debilitating disorders.
The extraordinary study involved a large international team led by Professor Eske Willerslev at the Universities of Cambridge and Copenhagen, Professor Thomas Werge at the University of Copenhagen, and Professor Rasmus Nielsen at University of California, Berkeley and involved contributions from 175 researchers from around the globe.
The scientists found: The startling origins of neurodegenerative diseases including multiple sclerosis Why northern Europeans today are taller than people from southern Europe How major migration around 5,000 years ago introduced risk genes into the population in north-western Europe -- leaving a legacy of higher rates of MS today Carrying the MS gene was an advantage at the time as it protected ancient farmers from catching infectious diseases from their sheep and cattle Genes known to increase the risk of diseases such as Alzheimer's and type 2 diabetes were traced back to hunter gatherers Future analysis is hoped to reveal more about the genetic markers of autism, ADHD, schizophrenia, bipolar disorder, and depressionNorthern Europe has the highest prevalence of multiple sclerosis in the world. A new study has found the genes that significantly increase a person's risk of developing multiple sclerosis (MS) were introduced into north-western Europe around 5,000 years ago by sheep and cattle herders migrating from the east.
By analysing the DNA of ancient human bones and teeth, found at documented locations across Eurasia, researchers traced the geographical spread of MS from its origins on the Pontic Steppe (a region spanning parts of what are now Ukraine, South-West Russia and the West Kazakhstan Region).

They found that the genetic variants associated with a risk of developing MS 'travelled' with the Yamnaya people -- livestock herders who migrated over the Pontic Steppe into North-Western Europe.
These genetic variants provided a survival advantage to the Yamnaya people, most likely by protecting them from catching infections from their sheep and cattle. But they also increased the risk of developing MS.
""It must have been a distinct advantage for the Yamnaya people to carry the MS risk genes, even after arriving in Europe, despite the fact that these genes undeniably increased their risk of developing MS,"" said Professor Eske Willerslev, jointly at the Universities of Cambridge and Copenhagen and a Fellow of St John's College, an expert in analysis of ancient DNA and Director of the project.
He added: ""These results change our view of the causes of multiple sclerosis and have implications for the way it is treated.""
The age of specimens ranges from the Mesolithic and Neolithic through the Bronze Age, Iron Age and Viking period into the Middle Ages. The oldest genome in the data set is from an individual who lived approximately 34,000 years ago.
The findings provide an explanation for the 'North-South Gradient', in which there are around twice as many modern-day cases of MS in northern Europe than southern Europe, which has long been a mystery to researchers.

From a genetic perspective, the Yamnaya people are thought to be the ancestors of the present-day inhabitants of much of North-Western Europe. Their genetic influence on today's population of southern Europe is much weaker.
Previous studies have identified 233 genetic variants that increase the risk of developing MS. These variants, also affected by environmental and lifestyle factors, increase disease risk by around 30 percent. The new research found that this modern-day genetic risk profile for MS is also present in bones and teeth that are thousands of years old.
""These results astounded us all. They provide a huge leap forward in our understanding of the evolution of MS and other autoimmune diseases. Showing how the lifestyles of our ancestors impacted modern disease risk just highlights how much we are the recipients of ancient immune systems in a modern world,"" said Dr William Barrie, a postdoc in the University of Cambridge's Department of Zoology and co-author of the paper.
Multiple sclerosis is a neurodegenerative disease in which the body's immune system mistakenly attacks the 'insulation' surrounding the nerve fibres of the brain and spinal cord. This causes symptom flares known as relapses as well as longer-term degeneration, known as progression.
Professor Lars Fugger, a co-author of the MS study professor and consultant physician at John Radcliffe Hospital, University of Oxford, said: ""This means we can now understand and seek to treat MS for what it actually is: the result of a genetic adaptation to certain environmental conditions that occurred back in our prehistory.""
Professor Astrid Iversen, another co-author based at the University of Oxford, said: ""We now lead very different lives to those of our ancestors in terms of hygiene, diet, and medical treatment options and this combined with our evolutionary history means we may be more susceptible to certain diseases than our ancestors were, including autoimmune diseases such as MS.""
The Lundbeck Foundation GeoGenetics Centre -- the resource underpinning the discoveries
The new findings were made possible by the analysis of data held in a unique gene bank of ancient DNA, created by the researchers over the past five years with funding from the Lundbeck Foundation.
This is the first gene bank of its kind in the world and already it has enabled fascinating new insights in areas from ancient human migrations, to genetically-determined risk profiles for the development of brain disorders.
By analysing the bones and teeth of almost 5,000 ancient humans, held in museum collections across Europe and Western Asia, the researchers generated DNA profiles ranging across the Mesolithic and Neolithic through the Bronze Age, Iron Age and Viking period into the Middle Ages. They compared the ancient DNA data to modern DNA from 400,000 people living in Britain, held in the UK Biobank.
""Creating a gene bank of ancient DNA from Eurasia's past human inhabitants was a colossal project, involving collaboration with museums across the region,"" said Willerslev.
He added: ""We've demonstrated that our gene bank works as a precision tool that can give us new insights into human diseases, when combined with analyses of present-day human DNA data and inputs from several other research fields. That in itself is amazing, and there's no doubt it has many applications beyond MS research.""
The team now plans to investigate other neurological conditions including Parkinson's and Alzheimer's diseases, and psychiatric disorders including ADHD and schizophrenia.
They have received requests from disease researchers across the world for access to the ancient DNA profiles, and eventually aim to make the gene bank open access.
The research was funded by a €8M grant from the Lundbeck Foundation, and conducted at the Lundbeck Foundation Geogenetics Centre at the University of Copenhagen.
Jan Egebjerg, Director of Research at the Lundbeck Foundation, said: ""The rationale for awarding such a large research grant to this project, as the Lundbeck Foundation did back in 2018, was that if it all worked out, it would represent a trail-blazing means of gaining a deeper understanding of how the genetic architecture underlying brain disorders evolved over time. And brain disorders are our specific focus area.""

","score: 17.706751604662646, grade_level: '18'","score: 20.203457913831222, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41586-023-06618-z,"Multiple sclerosis (MS) is a neuro-inflammatory and neurodegenerative disease that is most prevalent in Northern Europe. Although it is known that inherited risk for MS is located within or in close proximity to immune-related genes, it is unknown when, where and how this genetic risk originated1. Here, by using a large ancient genome dataset from the Mesolithic period to the Bronze Age2, along with new Medieval and post-Medieval genomes, we show that the genetic risk for MS rose among pastoralists from the Pontic steppe and was brought into Europe by the Yamnaya-related migration approximately 5,000 years ago. We further show that these MS-associated immunogenetic variants underwent positive selection both within the steppe population and later in Europe, probably driven by pathogenic challenges coinciding with changes in diet, lifestyle and population density. This study highlights the critical importance of the Neolithic period and Bronze Age as determinants of modern immune responses and their subsequent effect on the risk of developing MS in a changing environment."
"
Adults with posttraumatic stress disorder (PTSD) have smaller cerebellums, according to new research from a Duke-led brain imaging study.

The cerebellum, a part of the brain well known for helping to coordinate movement and balance, can influence emotion and memory, which are impacted by PTSD. What isn't known yet is whether a smaller cerebellum predisposes a person to PTSD or PTSD shrinks the brain region.
""The differences were largely within the posterior lobe, where a lot of the more cognitive functions attributed to the cerebellum seem to localize, as well as the vermis, which is linked to a lot of emotional processing functions,"" said Ashley Huggins, Ph.D., the lead author of the report who helped carry out the work as a postdoctoral researcher at Duke in the lab of psychiatrist Raj Morey, M.D.
Huggins, now an assistant professor of psychology at the University of Arizona, hopes these results encourage others to consider the cerebellum as an important medical target for those with PTSD.
""If we know what areas are implicated, then we can start to focus interventions like brain stimulation on the cerebellum and potentially improve treatment outcomes,"" Huggins said.
The findings, published January 10 in the journal Molecular Psychiatry, have prompted Huggins and her lab to start looking for what comes first: a smaller cerebellum that might make people more susceptible to PTSD, or trauma-induced PTSD that leads to cerebellum shrinkage.
PTSD and the ""Little Brain""
PTSD is a mental health disorder brought about by experiencing or witnessing a traumatic event, such as a car accident, sexual abuse, or military combat.

Though most people who endure a traumatic experience are spared from the disorder, about 6% of adults develop PTSD, which is often marked by increased fear and reliving the traumatizing event.
Researchers have found several brain regions involved in PTSD, including the almond-shaped amygdala that regulates fear, and the hippocampus, a critical hub for processing memories and routing them throughout the brain.
The cerebellum (Latin for ""little brain""), by contrast, has received less attention for its role in PTSD.
A grapefruit-sized lump of cells that look like it was clumsily tacked underneath the back of the brain as an afterthought, the cerebellum is best known for its role in coordinating balance and choreographing complex movements, like walking or dancing. But there is much more to it than that.
""It's a really complex area,"" Huggins said. ""If you look at how densely populated with neurons it is relative to the rest of the brain, it's not that surprising that it does a lot more than balance and movement.""
Dense may be an understatement. The cerebellum makes up just 10% of the brain's total volume but packs in more than half of the brain's 86 billion nerve cells.

Researchers have recently observed changes to the size of the tightly-packed cerebellum in PTSD. Most of that research, however, is limited by either a small dataset (fewer than 100 participants), broad anatomical boundaries, or a sole focus on certain patient populations, such as veterans or sexual assault victims with PTSD.
Subtle and Consistent Reductions
To overcome those limitations, Duke's Dr. Morey, along with over 40 other research groups that are part of a larger data-sharing initiative, pooled together their brain imaging scans to study PTSD as broadly and universally as possible.
The group ended up with images from 4,215 adult MRI scans, about a third of whom had been diagnosed with PTSD.
""I spent a lot of time looking at cerebellums,"" Huggins said.
Even with automated software to analyze the thousands of brain scans, Huggins manually spot-checked every image to make sure the boundaries drawn around the cerebellum and its many subregions were accurate.
The result of this thorough methodology was a fairly simple and consistent finding: PTSD patients had cerebellums about 2% smaller.
When Huggins zoomed in to specific areas within the cerebellum that influence emotion and memory, she found similar cerebellar reductions in people with PTSD.
Huggins also discovered that the worse PTSD was for a person, the smaller their cerebellum was.
""Focusing purely on a yes-or-no categorical diagnosis doesn't always give us the clearest picture,"" Huggins said. ""When we looked at PTSD severity, people who had more severe forms of the disorder had an even smaller cerebellar volume.""
Targeting the Cerebellum for Treatment and More Research
The results are an important first step at looking at how and where PTSD affects the brain.
There are more than 600,000 combinations of symptoms that can lead to a PTSD diagnosis, Huggins explained. Figuring out if different PTSD symptom combinations have different impacts on the brain will also be important to keep in mind.
For now, though, Huggins hopes this work helps others recognize the cerebellum as an important driver of complex behavior and processes beyond gait and balance, as well as a potential target for new and current treatments for people with PTSD.
""While there are good treatments that work for people with PTSD, we know they don't work for everyone,"" Huggins said. ""If we can better understand what's going on in the brain, then we can try to incorporate that information to come up with more effective treatments that are longer lasting and work for more people.""

","score: 13.39921172509408, grade_level: '13'","score: 14.963917607447023, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41380-023-02352-0,"Although the cerebellum contributes to higher-order cognitive and emotional functions relevant to posttraumatic stress disorder (PTSD), prior research on cerebellar volume in PTSD is scant, particularly when considering subregions that differentially map on to motor, cognitive, and affective functions. In a sample of 4215 adults (PTSD n = 1642; Control n = 2573) across 40 sites from the ENIGMA-PGC PTSD working group, we employed a new state-of-the-art deep-learning based approach for automatic cerebellar parcellation to obtain volumetric estimates for the total cerebellum and 28 subregions. Linear mixed effects models controlling for age, gender, intracranial volume, and site were used to compare cerebellum volumes in PTSD compared to healthy controls (88% trauma-exposed). PTSD was associated with significant grey and white matter reductions of the cerebellum. Compared to controls, people with PTSD demonstrated smaller total cerebellum volume, as well as reduced volume in subregions primarily within the posterior lobe (lobule VIIB, crus II), vermis (VI, VIII), flocculonodular lobe (lobule X), and corpus medullare (all p-FDR < 0.05). Effects of PTSD on volume were consistent, and generally more robust, when examining symptom severity rather than diagnostic status. These findings implicate regionally specific cerebellar volumetric differences in the pathophysiology of PTSD. The cerebellum appears to play an important role in higher-order cognitive and emotional processes, far beyond its historical association with vestibulomotor function. Further examination of the cerebellum in trauma-related psychopathology will help to clarify how cerebellar structure and function may disrupt cognitive and affective processes at the center of translational models for PTSD."
"
Psilocybe fungi, known colloquially as ""magic mushrooms,"" have held deep significance in Indigenous cultures of Mesoamerica for centuries. They captured the wider world's attention as a psychedelic staple in the 60s and 70s. Now, these infamous organisms are at the forefront of a mental health revolution. Psilocybin and psilocin, the psychoactive compounds found in nearly all species of Psilocybe, have shown promise as a treatment for conditions including PTSD, depression, and for easing end-of-life care.

To utilize psilocybin as a therapeutic, scientists need an extensive roadmap of the compound's underlying genetics and evolution, information that doesn't exist. Our limited knowledge comes from research on just a fraction of the ~165 known species of Psilocybe. Most psilocybin-producing mushrooms haven't been studied since they were first discovered -- until now.
A team of researchers led by the University of Utah and the Natural History Museum of Utah (NHMU) has completed the largest genomic diversity study for the genus Psilocybe. Their genomic analysis of 52 Psilocybe specimens includes 39 species that have never been sequenced.
The authors found that Psilocybe arose much earlier than previously thought -- about 65 million years ago, right around when the dinosaur-killing asteroid caused a mass extinction event. They established that psilocybin was first synthesized in mushrooms in the genus Psilocybe, with four to five possible horizontal gene transfers to other mushrooms from 40 up to 9 million years ago.
Their analysis revealed two distinct gene orders within the gene cluster that produces psilocybin. The two gene patterns correspond to an ancient split in the genus, suggesting two independent acquisitions of psilocybin in itsevolutionary history. The study is the first to reveal such a strong evolutionary pattern within the gene sequences underpinning the psychoactive proteins synthesis.
""If psilocybin does turn out to be this kind of wonder drug, there's going to be a need to develop therapeutics to improve its efficacy. What if it already exists in nature?"" said Bryn Dentinger, curator of mycology at NHMU and senior author of the study. ""There's a wealth of diversity of these compounds out there. To understand where they are and how they're made, we need to do this kind of molecular work to use biodiversity to our advantage.""
All the study's Psilocybe DNA came from specimens in museum collections around the world. Twenty-three of the 52 specimens were ""type specimens,"" the gold standard designating a species against which all other samples are measured. For example, say you identify a wild mushroom as a certain species of chanterelle -- you're betting that the mushroom you picked is the same as the physical material sitting in a box in a museum. The authors' molecular work on type species is a major contribution to mycology because it establishes an authoritative foundation for all future work on Psilocybe diversity in taxonomy.

""These type specimens represent hundreds of years of thousands of scientists' collective effort to document diversity, way before people were thinking about DNA,"" said Alexander Bradshaw, postdoctoral researcher at the U and lead author of the study. ""That's the beauty of it -- no one has really sequenced type specimens at this scale, and now we get to produce molecular and genomic data to the gold standard of Psilocybe types for people to compare against.""
The study published in the journal Proceedings of the National Academy of Sciences on Jan. 9, 2024.
A trip through time
Previous studies identified the cluster of four core genes that produce psilocybin based on genomic analysis of threePsilocybe species. The species were closely related to each other, and all had matching gene patterns within the psilocybin-producing gene clusters. This study's expanded genomics of 52 specimens of Psilocybe revealed a second distinct pattern.
""This work represents a big step in the understanding of the evolutionary relationships in Psilocybe because it is the first to include a broad species sampling and is based on type specimens,"" said Virginia Ramírez-Cruz, mycologist at the Universidad de Guadalajara and co-lead author of the study.
The authors found that 17 specimens had the original order, while 35 exhibited the new pattern.

""We've shown here that there's been a lot of change in gene order over time, and that provides some new tools for biotechnology. If you're looking for a way to express the genes to produce the psilocybin and related compounds, you no longer have to rely on only one set of gene sequences to do that. Now there's tremendous diversity that scientists can look at for lots of different properties or efficiencies,"" said Dentinger, who is also an associate professor of biology at the University of Utah.
Dating of the group showed that an ancient split of the two gene cluster patterns occurred around 57 million years ago, which also corresponded to a shift in the ecology. The first psilocybin-producing mushrooms likely arose as a wood-decomposing group, then transitioned to soil after the split, with some species such as Psilocybe cubensis transiting to growing on herbivore dung. The ecological shift to dung appears to have occurred at least twice independently in their evolutionary history.
What does psilocybin do for mushrooms?
The authors hoped that psilocybin's evolutionary history would clarify the most basic question -- what does psilocybin do for mushrooms? The psilocybin-producing gene clusters likely have some benefit, but no one knows what it is.
The molecular structure of psilocybin mimics serotonin and binds tightly to serotonin receptors, especially at 5-HT2A, a famous receptor onto which many psychedelic drugs bind. When a chemical binds to these receptors in mammals and similar ones in insects and arachnids, they produce unnatural and altered behaviors. Some have proposed that this altered mental state might be a direct deterrent to predation. It's also possible that psilocybin functions as a laxative or induces vomiting to spread spores before they are fully digested. However, psilocybin mushrooms often occur infrequently in the wild, making it unlikely that animals could learn to recognize them. An alternative theory is that psilocybin is a chemical defense against insects. However, empirical studies are lacking, and the authors' personal observations confirm that psilocybin-containing mushrooms regularly host healthy, thriving insect larvae.
The authors are preparing experiments to test an alternative theory that they call the Gastropod Hypothesis. The timing and divergence dates of Psilocybe coincide with the KPg boundary, the geological marker of the asteroid that threw Earth into a brutal, prolonged winter and killed 80% of all life. Two lifeforms that thrived during the darkness and decay were fungi and terrestrial gastropods. Evidence, including the fossil record, shows that gastropods had a massive diversification and proliferation just after the asteroid hit, and it's known that terrestrial slugs are heavy predators of mushrooms. With the study's molecular dating of Psilocybe to around 65 million years ago, it's possible that psilocybin evolved as a slug deterrent. They hope that their feeding experiments will shed some light on their hypothesis.
In 2020, the authors set a goal to get a genome sequence for every Psilocybe type specimen. To date, they've generated genomes of 71 type specimens and continue to collaborate with collections around the world.
""It's impossible to overstate the importance of collections for doing studies like this. We are standing on the shoulders of giants, who spent thousands of people-power hours to create these collections, so that I can write an email and request access to rare specimens, many of which have only ever been collected once, and may never be collected again,"" said Bradshaw.
Other authors who contributed to the study include Ali Awan of Guy's and St. Thomas' NHS Trust, Giuliana Furci of the Fungi Foundation, and Laura Guzmán-Dávalos of the Universidad de Guadalajara. The research was funded by the National Science Foundation (DEB #2114785) and Fungi Perfecti LLC.

","score: 13.5862389023406, grade_level: '14'","score: 14.224681715222992, grade_levels: ['college_graduate'], ages: [24, 100]",10.1073/pnas.2311245121,"Psychoactive mushrooms in the genus Psilocybe have immense cultural value and have been used for centuries in Mesoamerica. Despite the recent surge of interest in these mushrooms due to the psychotherapeutic potential of their natural alkaloid psilocybin, their phylogeny and taxonomy remain substantially incomplete. Moreover, the recent elucidation of the psilocybin biosynthetic gene cluster is known for only five of ~165 species of Psilocybe , four of which belong to only one of two major clades. We set out to improve the phylogeny of Psilocybe using shotgun sequencing of fungarium specimens, from which we obtained 71 metagenomes including from 23 types, and conducting phylogenomic analysis of 2,983 single-copy gene families to generate a fully supported phylogeny. Molecular clock analysis suggests the stem lineage of Psilocybe arose ~67 mya and diversified ~56 mya. We also show that psilocybin biosynthesis first arose in Psilocybe , with 4 to 5 possible horizontal transfers to other mushrooms between 40 and 9 mya. Moreover, predicted orthologs of the psilocybin biosynthetic genes revealed two distinct gene orders within the biosynthetic gene cluster that corresponds to a deep split within the genus, possibly a signature of two independent acquisitions of the cluster within Psilocybe ."
"
Respiratory syncytial virus (RSV), a common infection in children and senior adults, can also infect nerve cells and trigger inflammation leading to nerve damage, according to a new Tulane University study.

RSV can cause mild symptoms such as coughing, sneezing and fever or lead to more severe conditions such as pneumonia or bronchiolitis. But since the disease was first discovered in 1956, it has been thought to only infect the respiratory tract.
This study, published in The Journal of Infectious Diseases, is the first to prove that RSV can penetrate nerve cells and may provide the clearest link between RSV and reported neurological symptoms in children.
RSV has been previously detected in the spinal fluid of children with seizures. Additionally, 40% of RSV-positive children under the age of 2 have shown acute encephalopathy, brain damage that can result in confusion, memory loss or cognitive difficulties.
The findings underscore the potential long-term impacts of the disease, as well as the importance of preventative measures such as the two RSV vaccines approved by the FDA in 2023.
""This is the most common respiratory virus in the first years of life as well as an impactful virus among the elderly,"" said Dr. Giovanni Piedimonte, Tulane University vice president for research and professor of pediatrics, biochemistry and molecular biology. ""This adds a new dimension to the importance of RSV vaccines for both the elderly and mothers to protect their babies.""
Researchers studied the virus using 3D peripheral nerve cultures grown from stem cells and rat embryos. After finding they can be infected by RSV, researchers found RSV induced the release of chemokines -- proteins that fight infections by controlling immune cells -- and caused significant inflammation.

With low levels of RSV infection, the nerves became hyperreactive to stimulation. At higher levels, they observed a progressive degeneration of the nerve and increased neurotoxicity due to excess inflammation.
""Until this study, the theory was that the inflammatory response was indirectly activating the nerves,"" Piedimonte said. ""This study shows that not only does that happen, but the virus can penetrate directly into the nerves.""
The nerve hyperreactivity could explain why children who get RSV are later more likely to have asthmatic symptoms, Piedimonte said.
The study also found that RSV could enter the spinal cord via peripheral nerves despite not having the ability to enter the spinal neurons directly. More research is needed to explore that mechanism, but Piedimonte theorizes that by using the peripheral nerves to enter the spinal cord, RSV can bypass the blood-brain barrier, enter the central nervous system and infect the brain.
If confirmed, it could signal a connection between RSV and other neurological or developmental disorders, Piedimonte said.
""If indeed it's confirmed in future studies that viruses like this are able to access the central nervous system, that opens a huge Pandora's box,"" Piedimonte said.
Co-authors on the study include Tulane University researchers Kevin Pollard, Vicki Traina-Dorge, Stephen Medearis, Alexander Bosak, Greg Bix and Michael Moore.

","score: 13.967514703092395, grade_level: '14'","score: 15.207659836843106, grade_levels: ['college_graduate'], ages: [24, 100]",10.1093/infdis/jiad596,"Respiratory syncytial virus (RSV) primarily infects the respiratory epithelium, but growing evidence suggests it may also be responsible for neurological sequelae. In 3D microphysiological peripheral nerve cultures, RSV infected neurons, macrophages, and dendritic cells along two distinct trajectories depending on the initial viral load. Low-level infection was transient, primarily involved macrophages, and induced moderate chemokine release with transient neural hypersensitivity. Infection with higher viral loads was persistent, infected neuronal cells in addition to monocytes, and induced robust chemokine release followed by progressive neurotoxicity. In spinal cord cultures, RSV infected microglia and dendritic cells but not neurons, producing a moderate chemokine expression pattern. The persistence of infection was variable but could be identified in dendritic cells as long as 30 days post-inoculation. This study suggests that RSV can disrupt neuronal function directly through infection of peripheral neurons and indirectly through infection of resident monocytes, and inflammatory chemokines likely mediate both mechanisms."
"
A breakthrough study led by Dr. Mehdi Razavi at The Texas Heart Institute (THI), in collaboration with a biomedical engineering team of The University of Texas at Austin (UT Austin) Cockrell School of Engineering led by Dr. Elizabeth Cosgriff-Hernandez, sets the foundation of a ground-breaking treatment regimen for treating ventricular arrhythmia. Their study published in Nature Communications demonstrates the design and feasibility of a new hydrogel-based pacing modality.

The urgent need for an effective therapeutic regimen for ventricular arrhythmia inspired THI's Electrophysiology Clinical Research & Innovations (EPCRI) team, led by its director, Dr. Razavi, to partner with Dr. Cosgriff-Hernandez and her UT Austin Biomedical Engineering (UT Austin BME) team to co-develop an innovative strategy that addresses the pathophysiology of re-entrant arrhythmia.
Ventricular arrhythmia, which occurs in the lower chambers of the heart or ventricles, is the leading cause of sudden cardiac death in the United States. When heart rhythm abnormality occurs in a self-sustained manner, it is called re-entrant arrhythmia, which is usually fatal.
""Re-entry occurs mainly from delayed conduction in scarred heart tissues, usually after coronary artery occlusion during a heart attack, which can be corrected by enabling pacing in these regions,"" said Dr. Razavi, a practicing cardiologist and cardiac electrophysiologist. ""These hydrogels then can access the scarred tissue, thereby enabling direct pacing of the otherwise inaccessible regions of the heart.""
Given hydrogels' biostability, biocompatibility, tunable properties, and the ease of incorporating electrical conductivity, the scientists are exploring them as potential electrodes that can be easily delivered inside coronary veins. A clinical advantage of the unique system is that ischemia can be avoided by delivering the hydrogel using the veins.
The researchers successfully deployed the innovative hydrogel technology through minimally invasive catheter delivery in a pig model.
""The hydrogels have significant conductive properties that enable simultaneous pacing from multiple sites along the length of the hydrogel and create a conduction highway similar to those in Purkinje fibers,"" according to Dr. Cosgriff-Hernandez.

Today, arrhythmia is treatable with medicines and procedures that control the irregular rhythms. The current anti-arrhythmic drugs on the market are not always effective; although the drugs slow the conduction velocity, they facilitate re-entry arrhythmia. Moreover, these drugs can be toxic and can lead to the destruction of tissues near the diseased regions of the heart. Even with the widely used interventional ablation therapies, arrhythmia recurs in a significant proportion of patients. None of these procedures address the mechanism of re-entry.
Cardiac defibrillators implanted to compensate for the shortfalls in the current therapy options are painful when delivering electric shocks to restore heart rhythm and can severely deteriorate the patient's quality of life. If left untreated, arrhythmia can damage the heart, brain, or other organs, leading to stroke or cardiac arrest, during which the heart suddenly and unexpectedly stops beating.
""When injected into target vessels, the conductive hydrogel conforms to the patient's vessel morphology. Adding a traditional pacemaker to this gel allows for pacing that resembles the native conduction in the heart -- effectively mimicking the native electrical rhythm of the heart -- and extinguishes the cause for arrhythmia, providing painless defibrillation,"" added Dr. Cosgriff-Hernandez.
The work demonstrates for the first time the ability to confer direct electrical stimulation of the native and scarred mid-myocardium through injectable hydrogel electrodes as a pacing modality.
With minimally invasive catheter delivery and standard pacemaker technologies, this study indicates the feasibility of a novel pacing modality that resembles native conduction, potentially eliminating lethal re-entrant arrhythmia and providing painless defibrillation, which can be successfully adopted in a clinical workflow.
The scientific advance is significant considering pain management is highly relevant to overall wellness for patients with heart, lung, and blood diseases. Such innovation in painless defibrillation and preventing arrhythmia could revolutionize cardiac rhythm management.
Funding was provided by the National Heart, Lung, and Blood Institute of the National Institutes of Health (R01 HL162741); Ford Pre-Doctoral Fellowship, administered by the National Academy of Science, Engineering and Medicine; Ford Dissertation Fellowship, administered by the National Academy of Science, Engineering and Medicine; Office of Vice President for Research, The University of Texas at Austin; The Roderick D. MacDonald Research Fund Award 19RDM004; and The Sultan Qaboos Chair in Cardiology at the St. Luke's Foundation.

","score: 19.046027272727276, grade_level: '19'","score: 20.59555397727273, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41467-023-44419-0,"There is an urgent clinical need for a treatment regimen that addresses the underlying pathophysiology of ventricular arrhythmias, the leading cause of sudden cardiac death. The current report describes the design of an injectable hydrogel electrode and successful deployment in a pig model with access far more refined than any current pacing modalities allow. In addition to successful cardiac capture and pacing, analysis of surface ECG tracings and three-dimensional electroanatomic mapping revealed a QRS morphology comparable to native sinus rhythm, strongly suggesting the hydrogel electrode captures the deep septal bundle branches and Purkinje fibers. In an ablation model, electroanatomic mapping data demonstrated that the activation wavefront from the hydrogel reaches the mid-myocardium and endocardium much earlier than current single-point pacing modalities. Such uniform activation of broad swaths of tissue enables an opportunity to minimize the delayed myocardial conduction of heterogeneous tissue that underpins re-entry. Collectively, these studies demonstrate the feasibility of a new pacing modality that most closely resembles native conduction with the potential to eliminate lethal re-entrant arrhythmias and provide painless defibrillation."
"
Like the Greek mythological beast with a snake's tail and two ferocious heads, a potential Parkinson's medicine created in the lab of chemist Matthew Disney, Ph.D., is also a type of chimera bearing two heads. One seeks out a key piece of Parkinson's-causing RNA, while the other goads the cell to chop it to pieces for recycling.

The research is described in the Jan. 9 issue of the Proceedings of the National Academy of Sciences, or PNAS.
Parkinson's is a frustrating and all too common disease. Slowly, people with Parkinson's lose brain cells and other neurons needed to make the neurotransmitter dopamine. This progressive loss leads them to develop rigid, tense muscles and tremors, and causes difficulties with sleep, mood, speech, eating and movement.
Commonly used treatments include drugs that replace the dopamine. Other treatments, such as deep-brain stimulation, help with movement problems that develop as the disease worsens. But while these types of treatments alleviate symptoms, they are not a cure, come with side effects and do not change the trajectory of the disease. An estimated 500,000 people in the United States live with Parkinson's.
""To change the course of this disease, we need to address its cause. For many Parkinson's patients, that apparent cause is the accumulation of a toxic protein called alpha-synuclein, in and around their neurons,"" said Disney, the endowed Institute Professor and chair of the chemistry department at The Herbert Wertheim UF Scripps Institute for Biomedical Innovation & Technology in Jupiter, Florida.
Unfortunately, alpha-synuclein has proven an especially challenging protein to medicate due to its unruly, disorganized form and lack of clear druggable structures, Disney added.
""In situations like this, we have found that targeting the RNA needed to build the toxic protein may be an optimal strategy to slowing or even stopping disease progression,"" he added.

Disney's lab focuses on interfering with or degrading RNA needed to assemble the proteins implicated in disease. This is a relatively new concept. Most drugs on the market work by binding to proteins to change their function. But not all disease-causing proteins can be successfully targeted with drugs. Some are too changeable, some lack druggable structures, some fold in a way that conceals their active sites.
Disney's approach is to prevent the problematic proteins from being made in the first place. To do that requires targeting their RNA. Here's why: Proteins are assembled in cells through a process that involves the reading and translation of a gene, the transport of that information from the cell nucleus to its cytoplasm via messenger RNA, and the assembly of protein-building factories called ribosomes, also built of RNA, in the cytoplasm. The ribosomes stitch the proteins together one amino acid at a time. Disney's potential Parkinson's drug, which he calls Syn-RiboTAC, binds to a section of messenger RNA that tells a ribosome to start protein assembly. Without the ""start"" signal, the toxic protein isn't built.
Disney's first authors on the PNAS study were graduate students in his lab. Yuquan Tong is a current student of the Skaggs Graduate School of Chemical and Biological Sciences on the Jupiter, Florida campus, and Peiyuan Zhang, Ph.D., is a recent graduate, now a postdoctoral researcher at the Massachusetts Institute of Technology.
""In Parkinson's mouse models, we see that reducing alpha-synuclein by even 25% is therapeutically beneficial,"" Tong said. ""In studies from induced neurons of Parkinson's patients, we see the Syn-RiboTAC strategy reduces alpha-synuclein production by about 50%. We saw that adding the RiboTAC produces a significant gain in potency.""
Disney added that the compound also showed good selectivity, important for avoiding unwanted side effects, and improved brain-barrier penetration relative to other compounds they studied.
Other collaborators on the study included physician-scientist M. Maral Mouradian, M.D., of Rutgers University, whose patients donated tissue to create induced neurons.
Much work lies ahead, as the team works to refine the two-headed drug and improve its drug-like properties, the scientists said. Preparing an experimental compound for clinical trials in humans can sometimes take years, as refinements are made and data are gathered.
""The medical need for a truly disease-modifying treatment is significant, and we know that patients are awaiting better options,"" Disney said. ""We're hopeful that we're on the road to a better days for people living with Parkinson's.""

","score: 12.114639846743298, grade_level: '12'","score: 13.157430651340995, grade_levels: ['college_graduate'], ages: [24, 100]",10.1073/pnas.2306682120,"α-Synuclein is an important drug target for the treatment of Parkinson’s disease (PD), but it is an intrinsically disordered protein lacking typical small-molecule binding pockets. In contrast, the encoding SNCA mRNA has regions of ordered structure in its 5′ untranslated region (UTR). Here, we present an integrated approach to identify small molecules that bind this structured region and inhibit α-synuclein translation. A drug-like, RNA-focused compound collection was studied for binding to the 5′ UTR of SNCA mRNA, affording Synucleozid-2.0, a drug-like small molecule that decreases α-synuclein levels by inhibiting ribosomes from assembling onto SNCA mRNA. This RNA-binding small molecule was converted into a ribonuclease-targeting chimera (RiboTAC) to degrade cellular SNCA mRNA. RNA-seq and proteomics studies demonstrated that the RiboTAC (Syn-RiboTAC) selectively degraded SNCA mRNA to reduce its protein levels, affording a fivefold enhancement of cytoprotective effects as compared to Synucleozid-2.0. As observed in many diseases, transcriptome-wide changes in RNA expression are observed in PD. Syn-RiboTAC also rescued the expression of ~50% of genes that were abnormally expressed in dopaminergic neurons differentiated from PD patient–derived iPSCs. These studies demonstrate that the druggability of the proteome can be expanded greatly by targeting the encoding mRNAs with both small molecule binders and RiboTAC degraders."
"
A combination of only 11 proteins can predict long-term disability outcomes in multiple sclerosis (MS) for different individuals. The identified proteins could be used to tailor treatments to the individual based on the expected severity of the disease. The study, led by researchers at Linköping University in Sweden, has been published in the journal Nature Communications.

""A combination of 11 proteins predicted both short and long-term disease activity and disability outcomes. We also concluded that it's important to measure these proteins in cerebrospinal fluid, which better reflects what's going on in the central nervous system, compared with measuring in the blood,"" says Julia Åkesson, doctoral student at Linköping University and the University of Skövde.
In multiple sclerosis, the immune system attacks the person's own body, damaging nerves in the brain and in the spinal cord. What is attacked primarily is a fatty compound called myelin, which surrounds and insulates the nerve axons so that signals can be transmitted. When myelin is damaged, transmission becomes less efficient.
Disease progression in multiple sclerosis varies considerably from person to person. To those for whom a more severe disease is predicted, it is important not to lose valuable time at the onset of the disease but to get the right treatment quickly. The researchers behind the current study, which is a collaboration between Linköping University, the Karolinska Institute and the University of Skövde, wanted to find out whether it was possible to detect at an early stage of disease which patients would require a more powerful treatment. Being able to do so would be relevant both to physicians and those living with MS.
""I think we've come one step closer to an analysis tool for selecting which patients would need more effective treatment in an early stage of the disease. But such a treatment may have side effects and be relatively expensive, and some patients don't need it,"" says Mika Gustafsson, professor of bioinformatics at the Department of Physics, Chemistry and Biology at Linköping University, who led the study.
Finding markers linked to disease severity many years ahead is a complicated challenge. In their study, the researchers analysed nearly 1,500 proteins in samples from 92 people with suspected or recently diagnosed MS. Data from the protein analyses were combined with a large amount of information from the patients' journals, such as disability, results from MRI scans of the nervous system, and treatments received. Using machine learning, the researchers found a number of proteins that could predict disease progression.
""Having a panel consisting of only 11 proteins makes it easy should anyone want to develop analysis for this. It won't be as costly as measuring 1,500 proteins, so we've really narrowed it down to make it useful for others wanting to take this further,"" says Sara Hojjati, doctoral student at the Department of Biomedical and Clinical Sciences at Linköping University.

The research team also found that a specific protein, leaking from damaged nerve axons, is a reliable biomarker for disease activity in the short term. This protein is called neurofilament light chain, NfL. These findings confirm earlier research on the use of NfL to identify nerve damage and also suggest that the protein indicates how active the disease is.
One of the main strengths of the study is that the combination of proteins found in the patient group from which samples were taken at Linköping University Hospital was later confirmed in a separate group consisting of 51 MS patients sampled at the Karolinska University Hospital in Stockholm.
This study is the first to measure such a large amount of proteins with a highly sensitive method, proximity extension assay, combined with next-generation sequencing, PEA-NGS. This technology allows for high-accuracy measuring also of very small amounts, which is important as these proteins are often present in very low levels.
The study was funded by the Swedish Foundation for Strategic Research, the Swedish Brain Foundation, Knut and Alice Wallenberg Foundation, Margaretha af Ugglas Foundation, the Swedish Research Council, NEURO Sweden and the Swedish Foundation for MS research, and others.

","score: 15.197888921713446, grade_level: '15'","score: 16.731403249630723, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41467-023-42682-9,"Sensitive and reliable protein biomarkers are needed to predict disease trajectory and personalize treatment strategies for multiple sclerosis (MS). Here, we use the highly sensitive proximity-extension assay combined with next-generation sequencing (Olink Explore) to quantify 1463 proteins in cerebrospinal fluid (CSF) and plasma from 143 people with early-stage MS and 43 healthy controls. With longitudinally followed discovery and replication cohorts, we identify CSF proteins that consistently predicted both short- and long-term disease progression. Lower levels of neurofilament light chain (NfL) in CSF is superior in predicting the absence of disease activity two years after sampling (replication AUC = 0.77) compared to all other tested proteins. Importantly, we also identify a combination of 11 CSF proteins (CXCL13, LTA, FCN2, ICAM3, LY9, SLAMF7, TYMP, CHI3L1, FYB1, TNFRSF1B and NfL) that predict the severity of disability worsening according to the normalized age-related MS severity score (replication AUC = 0.90). The identification of these proteins may help elucidate pathogenetic processes and might aid decisions on treatment strategies for persons with MS."
"
Researchers noticed that after switching to DST, certain Google searches took place up to an hour earlier than usual. On the other hand, when clocks went back to standard time in autumn, these searches tended to occur later.

The shift in search times varied across different search categories. Notably, the time of searches around sleep and health varied by less than 60 minutes over DST changes, hinting at a strong and robust role of the internal body clock in driving them.
Professor Sara Montagnese, co-author of the study from the University of Surrey, said:
""Our research calls for wider discussions about the health and wellbeing impact of DST and the complex relationship between our internal body clock and the time constraints imposed by society, which are collectively known as the ""social clock,"" of which DST is part.""
Researchers analysed Google Trends data from Italy, covering 2015 to 2020. The team examined the relative search volume for 26 keywords, grouped into three categories:  Sleep/health-related: includes search terms related to sleep patterns, sleep disorders, and overall health concerns. Terms such as ""insomnia"" and ""melatonin"" fall under this category.   Medication: this category includes terms concerning drugs and pharmaceuticals. Terms like ""painkiller"" and Xanax"" are included in this category.   Non-sleep/health-related: this category covers terms that are unrelated to sleep or health. Examples include ""spa"" and ""taxi."" 

","score: 11.282935843793588, grade_level: '11'","score: 12.21628312412831, grade_levels: ['college'], ages: [18, 24]",10.5334/jcr.230,"The human circadian timing system depends on the light/dark cycle as its main cue to synchronize with the environment, and thus with solar time. However, human activities depend also on social time, i.e. the set of time conventions and restrictions dictated by society, including Daylight Saving Time (DST), which adds an hour to any degree of desynchrony between social and solar time. Here, we used Google Trends as a data source to analyze diurnal variation, if any, and the daily peak in the relative search volume of 26 Google search queries in relation to the transitions to/from DST in Italy from 2015 to 2020. Our search queries of interest fell into three categories: sleep/health-related, medication and random non sleep/health-related. After initial rhythm and phase analysis, 11 words were selected to compare the average phase of the 15 days before and after the transition to/from DST. We observed an average phase advance after the transition to DST, and a phase delay after the transition to civil time, ranging from 25 to 60 minutes. Advances or delays shorter than 60 minutes, which were primarily observed in the sleep/health-related category, may suggest that search timing for these queries is at least partially driven by the endogenous circadian rhythm. Finally, a significant trend in phase anticipation over the years was observed for virtually all words. This is most likely related to an increase in age, and thus in earlier chronotypes, amongst Google users."
"
Love is blind, the saying goes, and thanks to a world-first Australian study, we are now a step closer to understanding why.

It is well known that romantic love changes the brain, releasing the so-called love hormone oxytocin, responsible for the euphoria we feel when falling in love.
Now, researchers from the ANU, University of Canberra and University of South Australia have measured how a part of the brain is responsible for putting our loved one on a pedestal in that first flush of romance.
In the world's first study investigating the link between the human brain's behavioural activation system (BAS) and romantic love, researchers surveyed 1556 young adults who identified as being ""in love.""
The survey questions focused on the emotional reaction to their partner, their behaviour around them, and the focus they placed on their loved one above all else.
It turns out that when we are in love, our brain reacts differently. It makes the object of our affections the centre of our lives.
ANU lead researcher and PhD student Adam Bode says the study -- recently published in the journal Behavioural Sciences - sheds light on the mechanisms that cause romantic love.

""We actually know very little about the evolution of romantic love,"" Bode says. As a result, every finding that tells us about romantic love's evolution is an important piece of the puzzle that's just been started.""
""It is thought that romantic love first emerged some five million years ago after we split from our ancestors, the great apes. We know the ancient Greeks philosophized about it a lot, recognising it both as an amazing as well as traumatic experience. The oldest poem ever to be recovered was in fact a love poem dated to around 2000 BC.""
University of Canberra academic and UniSA Adjunct Associate Professor, Dr Phil Kavanagh, says the study shows that romantic love is linked to changes in behaviour as well as emotion.
""We know the role that oxytocin plays in romantic love, because we get waves of it circulating throughout our nervous system and blood stream when we interact with loved ones,"" Dr Kavanagh says.
""The way that loved ones take on special importance, however, is due to oxytocin combining with dopamine, a chemical that our brain releases during romantic love. Essentially, love activates pathways in the brain associated with positive feelings.""
The next stage of the research involves investigating the differences between men and women in their approach to love, and a worldwide survey identifying four different types of romantic lovers.

","score: 12.564109306522301, grade_level: '13'","score: 13.305400876514568, grade_levels: ['college_graduate'], ages: [24, 100]",10.3390/bs13110921,"Research investigating the mechanisms that contribute to romantic love is in its infancy. The behavioral activation system is one biopsychological system that has been demonstrated to play a role in several motivational outcomes. This study was the first to investigate romantic love and the behavioral activation system. In study 1, the Behavioral Activation System—Sensitivity to a Loved One (BAS-SLO) Scale was validated in a sample of 1556 partnered young adults experiencing romantic love. In study 2, hierarchical linear regression was used to identify BAS-SLO Scale associations with the intensity of romantic love in a subsample of 812 partnered young adults experiencing romantic love for two years or less. The BAS-SLO Scale explained 8.89% of the variance in the intensity of romantic love. Subject to further validation and testing, the BAS-SLO Scale may be useful in future neuroimaging and psychological studies. The findings are considered in terms of the mechanisms and evolutionary history of romantic love."
"
Women with autoimmune disease are more likely to suffer from depression during pregnancy and after childbirth; conversely, women with a history of perinatal depression are at higher risk of developing autoimmune disease, a new study from Karolinska Institutet published in the journal Molecular Psychiatry reports.

In autoimmune disease, the immune system mistakenly attacks the body's own healthy tissue. Some of the most common autoimmune diseases are gluten intolerance (coeliac disease), autoimmune thyroiditis, rheumatoid arthritis, type 1 diabetes, and multiple sclerosis (MS).
In the present study, researchers used data from the Swedish Medical Birth Register and identified all women who had given birth in Sweden between 2001 and 2013. Out of the resulting group of approximately 815,000 women and 1.3 million pregnancies, just over 55,000 women had been diagnosed with depression during their pregnancy or within a year after delivery.
The researchers then compared the incidence of 41 autoimmune diseases in women with and without perinatal depression, controlling for familial factors such as genes and childhood environment by also including the affected women's sisters.
Strongest association for MS
The results reveal a bidirectional association between perinatal depression and autoimmune thyroiditis, psoriasis, MS, ulcerative colitis, and coeliac disease. Overall, women with autoimmune disease were 30 per cent more likely to suffer perinatal depression. Conversely, women with perinatal depression were 30 per cent more likely to develop a subsequent autoimmune disease.
The association was strongest for the neurological disease MS, for which the risk was double in both directions. It was also strongest in women who had not had a previous psychiatric diagnosis.

""Our study suggests that there's an immunological mechanism behind perinatal depression and that autoimmune diseases should be seen as a risk factor for this kind of depression,"" says the study's first author Emma Bränn, researcher at the Institute of Environmental Medicine at Karolinska Institutet.
Can have serious consequences
The researchers will now continue to examine the long-term effects of depression during pregnancy and in the first year following childbirth.
""Depression during this sensitive period can have serious consequences for both the mother and the baby,"" says Dr Bränn. ""We hope that our results will help decision-makers to steer funding towards maternal healthcare so that more women can get help and support in time.""
Since this was an observational study, no conclusions on causality can be drawn.
The study was financed by Karolinska Institutet, Forte (the Swedish Research Council for Health, Working Life and Welfare), the Swedish Research Councill and the Icelandic Research Fund. The researchers report no conflicts of interest.

","score: 15.202863849765262, grade_level: '15'","score: 16.70636150234742, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41380-023-02351-1,"Although major depression, characterized by a pro-inflammatory profile, genetically overlap with autoimmune disease (AD) and the perinatal period involve immune system adaptations and AD symptom alterations, the bidirectional link between perinatal depression (PND) and AD is largely unexplored. Hence, the objective of this study was to investigate the bidirectional association between PND and AD. Using nationwide Swedish population and health registers, we conducted a nested case-control study and a matched cohort study. From 1,347,901 pregnancies during 2001–2013, we included 55,299 incident PND, their unaffected full sisters, and 10 unaffected matched women per PND case. We identified 41 subtypes of AD diagnoses recorded in the registers and compared PND with unaffected population-matched women and full sisters, using multivariable regressions. Women with an AD had a 30% higher risk of subsequent PND (95% CI 1.2–1.5) and women exposed to PND had a 30% higher risk of a subsequent AD (95% CI 1.3–1.4). Comparable associations were found when comparing exposed women with their unaffected sisters (nested case-control OR: 1.3, 95% CI 1.2–1.5, matched cohort HR: 1.3, 95% CI 1.1–1.6), and when studying antepartum and postpartum depression. The bidirectional association was more pronounced among women without psychiatric comorbidities (nested case-control OR: 1.5, 95% CI 1.4–1.6, matched cohort HR: 1.4, 95% CI 1.4–1.5) and strongest for multiple sclerosis (nested case-control OR: 2.0, 95% CI 1.6–2.3, matched cohort HR: 1.8, 95% CI 1.0–3.1). These findings demonstrate a bidirectional association between AD and PND independent of psychiatric comorbidities, suggesting possibly shared biological mechanisms. If future translational science confirms the underlying mechanisms, healthcare providers need to be aware of the increased risk of PND among women with ADs and vice versa."
"
Ethanol -- the compound found in alcoholic beverages -- interferes with the normal functioning of a long list of biological molecules, but how each of these interactions contributes to the behavioral effects of alcohol is not fully understood. A guiding, but elusive, goal of researchers is to identify the protein (or proteins) to which ethanol binds that makes some people vulnerable to excessive drinking. Solving this question would point the way to effective therapies for alcohol use disorder, which affects more than 10% of the U.S. adult population and is responsible for a myriad of health and societal issues.

Previous studies identified one such molecule, a protein widely expressed in the brain, called the BK channel. Ethanol can directly interact with a component of BK channels, known as the α subunit, to facilitate their opening. However, scientists at Scripps Research found that this interaction may not drive behaviors related to alcohol abuse as much as previously thought. Their study, appearing in the journal Molecular Psychiatry on December 22, 2023, demonstrates that preventing ethanol from interacting with the BK α subunit does not reduce or increase the motivation to consume alcohol in mice.
The relationship between the BK α subunit and ethanol had previously been explored in vitro, ex vivo and in live invertebrates. Previous studies suggested that the BK α subunit was involved in an animal's response to alcohol exposure, but there was a gap in understanding its role in mammals, particularly for the control of alcohol drinking.
""Knowing what a molecule does from in vitro experiments really doesn't tell you much about what the behavioral consequences of that action might be,"" says senior author Candice Contet, PhD, associate professor in the Department of Molecular Medicine at Scripps Research. ""Things get complicated in vivo, because there are many layers of modulation that may occur in a cell-type specific manner. Moreover, the initial effect often changes with repeated or prolonged exposure to alcohol. We thus sought to determine whether the ability of ethanol to alter BK channel activity was in any way influencing the motivation to drink alcohol.""
Tackling this question didn't lend itself well to conventional pharmacological testing: blocking BK channels with a drug causes tremors, which then interfere with drinking behavior. However, Contet's collaborator Alex Dopico, MD, PhD, of the University of Tennessee, had identified a residue in the mouse BK α subunit that is required for ethanol to activate BK channels but is dispensable for normal BK channel activity, as shown in frog eggs. In the new study, Contet and her colleagues leveraged this discovery to unlock the significance of ethanol's interaction with BK channels for alcohol drinking in mice.
Accordingly, the team tested mice that had a mutation in this particular BK α subunit residue. First, they found that the mutation prevented alcohol from altering the firing properties of neurons in the medial habenula, a brain region with high levels of BK channels, thereby demonstrating that it also confers resistance to ethanol in mouse brain cells, not just in frog eggs. At the behavioral level, the mice harboring the mutation did not display any anomalies when compared to control littermates. Notably, they exhibited the standard signs of intoxication upon alcohol injection, such as loss of balance and hypothermia, and they consumed the same amount of alcohol when tested under various conditions of moderate or excessive drinking.
""The lack of effect of the mutation was surprising, especially in light of our previous results showing that other BK channel subunits, β1 and β4, influence alcohol intake escalation in the same model of alcohol dependence,"" says Contet. ""However, these negative results, which were replicated in multiple cohorts and both sexes, are just as important as positive ones, because they encourage the field to study other targets rather than focusing on the wrong culprit.""
While the study does not point to a critical role of the BK α subunit in the motivation to drink alcohol or several physiological responses related to ethanol intoxication and withdrawal, the group will continue to explore whether the molecular target plays a role in other aspects of alcohol use disorder.
""Ethanol is highly pleiotropic. Beyond its reinforcing effects, it alters the functioning of multiple organs and cell types,"" Contet says. ""It is likely that ethanol's interaction with BK channels contribute to some of these effects, but we've only explored the tip of the iceberg so far; the next challenge will be to find the right experimental readout.""
This work was supported by funding from the National Institutes of Health (AA020913, AA006420, AA026685, AA027636, AA027372, AA020889, AA010422, AA021491, AA013498, AA011560, AA007456)

","score: 16.25289127837515, grade_level: '16'","score: 17.32269056152927, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41380-023-02346-y,"Large conductance potassium (BK) channels are among the most sensitive molecular targets of ethanol and genetic variations in the channel-forming α subunit have been nominally associated with alcohol use disorders. However, whether the action of ethanol at BK α influences the motivation to drink alcohol remains to be determined. To address this question, we first tested the effect of systemically administered BK channel modulators on voluntary alcohol consumption in C57BL/6J males. Penitrem A (blocker) exerted dose-dependent effects on moderate alcohol intake, while paxilline (blocker) and BMS-204352 (opener) were ineffective. Because pharmacological manipulations are inherently limited by non-specific effects, we then sought to investigate the behavioral relevance of ethanol’s direct interaction with BK α by introducing in the mouse genome a point mutation known to render BK channels insensitive to ethanol while preserving their physiological function. The BK α K361N substitution prevented ethanol from reducing spike threshold in medial habenula neurons. However, it did not alter acute responses to ethanol in vivo, including ataxia, sedation, hypothermia, analgesia, and conditioned place preference. Furthermore, the mutation did not have reproducible effects on alcohol consumption in limited, continuous, or intermittent access home cage two-bottle choice paradigms conducted in both males and females. Notably, in contrast to previous observations made in mice missing BK channel auxiliary β subunits, the BK α K361N substitution had no significant impact on ethanol intake escalation induced by chronic intermittent alcohol vapor inhalation. It also did not affect the metabolic and locomotor consequences of chronic alcohol exposure. Altogether, these data suggest that the direct interaction of ethanol with BK α does not mediate the alcohol-related phenotypes examined here in mice."
"
In social media posts on the community network Reddit, users reported reduced cravings for alcohol when taking drugs intended to treat Type 2 diabetes and obesity.

Across a number of threads -- with titles such as ""Did scientists accidentally invent an anti-addiction drug?"" and ""I don't know if this is a side effect but ... Mounjaro makes me drink less!!!!!"" -- users reported a changing relationship with beer, wine, and liquor.
An analysis of those posts, together with a remote study of individuals with obesity who reported using semaglutide and tirzepatide, found that the drugs decreased cravings and reduced alcohol consumption, according to a study by Virginia Tech researchers published Nov. 28 in Scientific Reports.
""These findings add to a growing literature that these medications may curb dangerous drinking habits,"" said Warren Bickel, Virginia Tech Carilion Behavioral Health Research Professor at the Fralin Biomedical Research Institute at VTC and corresponding author.
What they did
Scientists with the Fralin Biomedical Research Institute's Addiction Recovery Research Center combined two different studies to build on existing research, including studies that showed the drugs were effective in reducing alcohol consumption in animal models.
The first was an analysis of more than 68,000 Reddit posts from 2009-23 that included terms linked to GLP-1 approved medications. Semaglutide is a GLP-1 agonist, a class of drugs that reduce blood sugar and energy intake by mimicking the actions of hormones released after eating.

Among the keywords included in the search were Mounjaro, Wegovy, Ozempic, and Trulicity. After cleaning the resulting data -- such as eliminating comments with fewer than 100 characters -- the set was narrowed to 33,609 posts from 14,595 unique users. The study was unique in using Reddit to analyze the reported experience of thousands of users.
On examining alcohol-related discussions, researchers found that 962 individuals made 1,580 alcohol-related posts. Of those, 71.7 percent addressed reduced cravings, reduced usage, and other negative effects due to drinking.
In a second study, 153 participants who self-reported having obesity were recruited from various social media platforms. Roughly a third of these participants represented the control group, a third were taking either a semaglutide injection or tablet, and a third were using tirzepatide.
Participants on semaglutide or tirzepatide reported drinking significantly fewer drinks, on average, than those in the control group who were not on any medication for diabetes or weight loss. In addition, researchers found that both the average number of drinks and the odds of binge drinking were found to be significantly lower.
Results also found that the stimulative and sedative effects of alcohol intoxication are reduced when taking these medications. ""Participants reported drinking less, experienced fewer effects of alcohol when they did drink it, and decreased odds of binge drinking,"" said Alexandra DiFeliceantonio, assistant professor at Fralin Biomedical Research Institute and one of the study's co-authors.
Researchers believe theirs is the first published report following tirezepatide, sold under the brand name Mounjaro, which was approved in 2022 and is used for treatment of Type 2 diabetes and weight loss.

Why this matters
Case studies and reports in the popular press hint at the drugs' unexpected side effect of reducing addictive behaviors, including the desire to consume alcohol.
The U.S. Food and Drug Administration has approved only three medications to treat alcohol use disorder: disulfiram, naltrexone, and acamprosate. They have shown only modest success, have poor compliance, and are underprescribed.
The authors suggest further randomized controlled trials to explore the therapeutic potential of GLP-1 agonists and GIP/GLP-1 combination drugs to treat alcohol use disorder, which affects 5.9 percent of individuals in the United States ages 12 and older. In addition, the participants identified as mostly white and female, and further studies in more diverse populations are needed to examine sex and race differences.
""Although evidence supporting the use of these medications for alcohol use disorder is growing, the field still needs to learn considerably more about them, particularly in identifying the underlying mechanisms. We plan to contribute to that effort,"" Bickel said.
The drugs are a promising development in the study of alcohol use disorder. Data from the National Survey on Drug Use and Health indicate 15.7 million people in the United States meet the criteria for the chronic, relapsing brain disorder that is a significant contributor to global mortality yet remains one of the most undertreated conditions, Bickel said.

","score: 15.12054421768708, grade_level: '15'","score: 16.292448979591832, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41598-023-48267-2,"Alcohol Use Disorder (AUD) contributes significantly to global mortality. GLP-1 (Glucagon-like peptide-1) and GLP-1/GIP (Glucose-dependent Insulinotropic Polypeptide) agonists, FDA-approved for managing type 2 diabetes and obesity, where the former has shown to effectively reduce the consumption of alcohol in animal models but no reports exist on the latter. In this report, we conducted two studies. In the first study, we conducted an analysis of abundant social media texts. Specifically, a machine-learning based attribution mapping of ~ 68,250 posts related to GLP-1 or GLP-1/GIP agonists on the Reddit platform. Secondly, we recruited participants (n = 153; current alcohol drinkers; BMI ≥ 30) who self-reported either taking Semaglutide (GLP-1 agonist), Tirzepatide (the GLP-1/GIP combination) for ≥ 30 days or, as a control group; no medication to manage diabetes or weight loss for a within and between subject remote study. In the social media study, we report 8 major themes including effects of medications (30%); diabetes (21%); and Weight loss and obesity (19%). Among the alcohol-related posts (n = 1580), 71% were identified as craving reduction, decreased desire to drink, and other negative effects. In the remote study, we observe a significantly lower self-reported intake of alcohol, drinks per drinking episode, binge drinking odds, Alcohol Use Disorders Identification Test (AUDIT) scores, and stimulating, and sedative effects in the Semaglutide or Tirzepatide group when compared to prior to starting medication timepoint (within-subjects) and the control group (between-subjects). In summary, we provide initial real-world evidence of reduced alcohol consumption in people with obesity taking Semaglutide or Tirzepatide medications, suggesting potential efficacy for treatment in AUD comorbid with obesity."
"
Early symptoms can be subtle. A child's personality and behavior may change, and clumsiness or stumbling develops between the ages of five and ten. Over time, cognitive impairment sets in, seizures emerge or worsen, vision loss begins, and motor skills decline. This is the course of Batten disease, a progressive inherited disorder of the nervous system that results from mutations to the CLN3 gene.

""It is a devastating neurodegenerative disorder of childhood,"" said John Foxe, PhD, director of the Del Monte Institute for Neuroscience and co-director of the University of Rochester Intellectual and Developmental Disabilities Research Center (UR-IDDRC), ""and while it is very rare, it is important to study and understand because it could inform what we know and how we treat it and other related rare diseases.""
A possible neuromarker
In a new study, out today in theJournal of Neurodevelopmental Disorders, Foxe and a team of researchers from the University of Rochester Medical Center may be closer to that goal of understanding. The paper describes how they measured changes in brain function of participants with CLN3 disease, also known as 'juvenile-onset' Batten disease. Researchers found that the functioning of the auditory sensory memory system -- the brain system required for short-term memory recall -- appears to decrease as the disease progresses. They revealed this by utilizing electroencephalographic recordings (EEG) to measure the brain activity of participants with and without Batten disease as they passively listened to simple auditory beeps. The participants simultaneously watched a video of their favorite movie while the brain responses to these beeps were being measured. In the participants with Batten disease, the EEG revealed a decline in the response from the auditory sensory memory system as the disease progressed. There were no significant changes among the other participants. This finding suggests that this easy-to-measure brain process may be a target or biomarker in measuring treatment outcomes in clinical trials.
""We needed to find a task that did not require explicit engagement or attention, and this is one of those kinds of tasks,"" Foxe said. ""The brain produces the signal that we're looking at, regardless of whether the participant is paying attention to the beeps or not. It is an objective method that provides new insight into the brain function of a population with varying communication abilities.""
Leading Batten disease research & treatment
There are 12 currently known childhood-onset forms of Batten disease, each genetically distinct from one another, and all significantly impact neurodevelopment. The University of Rochester Batten Center (URBC) is a recognized leader in research and treatment of this condition. URBC is designated a Center of Excellence by the Batten Disease Support and Research Association (BDSRA). The University is a designated Intellectual and Developmental Research Center (IDDRC), and its current principal project is aimed at neuromarker discovery in Batten disease. With several potential gene therapies for Batten disease currently in advanced stages of development, this recent finding continues the mission at URMC to identify biomarkers to evaluate the effectiveness of these experimental treatments.
Progressing scientific findings
Researchers in the Frederick J. and Marion A. Schindler Cognitive Neurophysiology Lab, where Foxe is co-principal investigator with Edward Freedman, PhD, are using these findings to leverage a Batten disease mouse model. They aim to measure the impact pharmaceuticals have on the auditory perceptual system. Researchers are starting with treatments already on the market for other conditions. ""We think this potential biomarker is key to the possibility for us to screen these treatments in the mouse models. The auditory sensory memory marker provides a sensitive measure,"" Foxe said. ""We'll be able to tell you pretty quickly if a treatment is having an impact and know if it is actually changing dynamics -- i.e., whether the system in the brain is improving, what the speed of improvement is, etc. I think that is a big deal.""
Additional authors include first author Tufikameni Brima, PhD, Edward Freedman, PhD, Kevin Prinsloo, PhD, Heather Adams, PhD, Kuan Hong Wang, PhD, Luke Shaw, and Emma Mantel of the University of Rochester School of Medicine and Dentistry, Erika Augustine, MD, of the Kennedy Krieger Institute, and Jonathan Mink, MD, PhD. This research is supported by the Schmitt Program in Integrative Neuroscience (SPIN) through the Del Monte Institute for Neuroscience pilot program, the National Institute of Neurological Disorders and Stroke, and the Eunice Kennedy Shriver National Institute of Child Health and Human Development.

","score: 15.087310780333784, grade_level: '15'","score: 16.093770861524582, grade_levels: ['college_graduate'], ages: [24, 100]",10.1186/s11689-023-09515-8,"We interrogated auditory sensory memory capabilities in individuals with CLN3 disease (juvenile neuronal ceroid lipofuscinosis), specifically for the feature of “duration” processing. Given decrements in auditory processing abilities associated with later-stage CLN3 disease, we hypothesized that the duration-evoked mismatch negativity (MMN) of the event related potential (ERP) would be a marker of progressively atypical cortical processing in this population, with potential applicability as a brain-based biomarker in clinical trials. We employed three stimulation rates (fast: 450 ms, medium: 900 ms, slow: 1800 ms), allowing for assessment of the sustainability of the auditory sensory memory trace. The robustness of MMN directly relates to the rate at which the regularly occurring stimulus stream is presented. As presentation rate slows, robustness of the sensory memory trace diminishes. By manipulating presentation rate, the strength of the sensory memory trace is parametrically varied, providing greater sensitivity to detect auditory cortical dysfunction. A secondary hypothesis was that duration-evoked MMN abnormalities in CLN3 disease would be more severe at slower presentation rates, resulting from greater demand on the sensory memory system. Data from individuals with CLN3 disease (N = 21; range 6–28 years of age) showed robust MMN responses (i.e., intact auditory sensory memory processes) at the medium stimulation rate. However, at the fastest rate, MMN was significantly reduced, and at the slowest rate, MMN was not detectable in CLN3 disease relative to neurotypical controls (N = 41; ages 6–26 years). Results reveal emerging insufficiencies in this critical auditory perceptual system in individuals with CLN3 disease."
"
Babies and toddlers exposed to television or video viewing may be more likely to exhibit atypical sensory behaviors, such as being disengaged and disinterested in activities, seeking more intense stimulation in an environment, or being overwhelmed by sensations like loud sounds or bright lights, according to data from researchers at Drexel's College of Medicine published today in the journal JAMA Pediatrics.

According to the researchers, children exposed to greater TV viewing by their second birthday were more likely to develop atypical sensory processing behaviors, such as ""sensation seeking"" and ""sensation avoiding,"" as well as ""low registration"" -- being less sensitive or slower to respond to stimuli, such as their name being called, by 33 months old.
Sensory processing skills reflect the body's ability to respond efficiently and appropriately to information and stimuli received by its sensory systems, such as what the toddler hears, sees, touches, and tastes.
The team pulled 2011-2014 data on television or DVD-watching by babies and toddlers at 12- 18- and 24-months from the National Children's Study of 1,471 children (50% male) nationwide.
Sensory processing outcomes were assessed at 33 months using the Infant/Toddler Sensory Profile (ITSP), a questionnaire completed by parents/caregivers, designed to give insights on how children process what they see, hear and smell, etc.
ITSP subscales examine children's patterns of low registration, sensation seeking, such as excessively touching or smelling objects; sensory sensitivity, such as being overly upset or irritated by lights and noise; and sensation avoiding -- actively trying to control their environment to avoid things like having their teeth brushed. Children score in ""typical,"" ""high"" or ""low"" groups based on how often they display various sensory-related behaviors. Scores were considered ""typical"" if they were within one standard deviation from the average of the ITSP norm.
Measurements of screen exposure at 12-months were based on caregiver responses to the question: ""Does your child watch TV and/or DVDs? (yes/no),"" and at 18- and 24- months based on the question: ""Over the past 30 days, on average, how many hours per day did your child watch TV and/or DVDs?""
The findings suggest: At 12 months, any screen exposure compared to no screen viewing was associated with a 105% greater likelihood of exhibiting ""high"" sensory behaviors instead of ""typical"" sensory behaviors related to low registration at 33 months At 18 months, each additional hour of daily screen time was associated with 23% increased odds of exhibiting ""high"" sensory behaviors related to later sensation avoiding and low registration. At 24 months, each additional hour of daily screen time was associated with a 20% increased odds of ""high"" sensation seeking, sensory sensitivity, and sensation avoiding at 33 months.

The researchers adjusted for age, whether the child was born prematurely, caregiver education, race/ethnicity and other factors, such as how often the child engages in play or walks with the caregiver.
The findings add to a growing list of concerning health and developmental outcomes linked to screen time in infants and toddlers, including language delay, autism spectrum disorder, behavioral issues, sleep struggles, attention problems and problem-solving delays.
""This association could have important implications for attention deficit hyperactivity disorder and autism, as atypical sensory processing is much more prevalent in these populations,"" said lead author Karen Heffler, MD, an associate professor of Psychiatry in Drexel's College of Medicine. ""Repetitive behavior, such as that seen in autism spectrum disorder, is highly correlated with atypical sensory processing. Future work may determine whether early life screen time could fuel the sensory brain hyperconnectivity seen in autism spectrum disorders, such as heightened brain responses to sensory stimulation.""
Atypical sensory processing in kids with autism spectrum disorder (ASD) and ADHD manifests in a range of detrimental behaviors. In children with ASD, greater sensation seeking or sensation avoiding, heightened sensory sensitivity and low registration have been associated with irritability, hyperactivity, eating and sleeping struggles, as well as social problems. In kids with ADHD, atypical sensory processing is linked to trouble with executive function, anxiety and lower quality of life.
""Considering this link between high screen time and a growing list of developmental and behavioral problems, it may be beneficial for toddlers exhibiting these symptoms to undergo a period of screen time reduction, along with sensory processing practices delivered by occupational therapists,"" said Heffler.
The American Academy of Pediatrics (AAP) discourages screen time for babies under 18-24 months. Live video chat is considered by the AAP to be okay, as there may be benefit from the interaction that takes place. AAP recommends time limitations on digital media use for children 2 to 5 years to typically no more than 1 hour per day.

""Parent training and education are key to minimizing, or hopefully even avoiding, screen time in children younger than two years,"" said senior author David Bennett, PhD, a professor of Psychiatry in Drexel's College of Medicine.""
Despite the evidence, many toddlers view screens more often. As of 2014, children age 2 and under in the United States averaged 3 hours, 3 minutes a day of screen time, up from 1 hour, 19 minutes a day in 1997, according to a 2019 research letter in JAMA Pediatrics. Some parents cite exhaustion and inability for affordable alternatives as reasons for the screen time, according to a July 2015 study in the Journal of Nutrition and Behavior.
Although the current paper looked strictly at television or DVD watching, and not media viewed on smartphones or tablets, it does provide some of the earliest data linking early-life digital media exposure with later atypical sensory processing across multiple behaviors. The authors said future research is needed to better understand the mechanisms that drive the association between early-life screen time and atypical sensory processing.
In addition to Heffler and Bennett, authors on this paper include Binod Acharya, who completed the work while at Drexel's Dornsife School of Public Health's Urban Health Collaborative, and Keshab Subedi from Christiana Care Health Systems. 

","score: 17.82107642728508, grade_level: '18'","score: 19.681052415405517, grade_levels: ['college_graduate'], ages: [24, 100]",10.1001/jamapediatrics.2023.5923,"Atypical sensory processing is challenging for children and families, yet there is limited understanding of its associated risk factors. To determine the association between early-life digital media exposure and sensory processing outcomes among toddlers. This multicenter US study used data that were analyzed from the National Children’s Study (NCS), a cohort study of environmental influences on child health and development, with enrollment from 2011 to 2014. Data analysis was performed in 2023. The study included children enrolled in the NCS at birth whose caregivers completed reports of digital media exposure and sensory processing. Children’s viewing of television or video at 12 months (yes or no), 18 months, and 24 months of age (hours per day). Sensory processing was reported at approximately 33 months of age on the Infant/Toddler Sensory Profile. Quadrant scores (low registration, sensation seeking, sensory sensitivity, and sensation avoiding) were categorized into groups representing typical, high, and low sensory-related behaviors, and multinomial regression analyses were performed. A total of 1471 children (50% male) were included. Screen exposure at 12 months of age was associated with a 2-fold increased odds of being in the high category of low registration (odds ratio [OR], 2.05; 95% CI, 1.31-3.20), while the odds of being in the low category instead of the typical category decreased for sensation seeking (OR, 0.55; 95% CI, 0.35-0.87), sensation avoiding (OR, 0.69; 95% CI, 0.50-0.94), and low registration (OR, 0.64; 95% CI, 0.44-0.92). At 18 months of age, greater screen exposure was associated with increased risk of high sensation avoiding (OR, 1.23; 95% CI, 1.03-1.46) and low registration (OR, 1.23; 95% CI, 1.04-1.44). At 24 months of age, greater screen exposure was associated with increased risk of high sensation seeking (OR, 1.20; 95% CI, 1.02-1.42), sensory sensitivity (OR, 1.25; 95% CI, 1.05-1.49), and sensation avoiding (OR, 1.21; 95% CI, 1.03-1.42). In this cohort study, early-life digital media exposure was associated with atypical sensory processing outcomes in multiple domains. These findings suggest that digital media exposure might be a potential risk factor for the development of atypical sensory profiles. Further research is needed to understand the relationship between screen time and specific sensory-related developmental and behavioral outcomes, and whether minimizing early-life exposure can improve subsequent sensory-related outcomes."
"
People who are hard of hearing spend more energy listening. That energy comes at the expense of other cognitive functions. Cognitive functions are the mental processes in the brain that enable us to think and solve problems, among other things.

In a new study featuring data from 573,088 people, researchers from the Department of Clinical Research at the University of Southern Denmark have found a link between hearing loss and the development of dementia. The study is the largest of its kind to date.
There is already an increase in the number of people with dementia. This is mainly due to the ageing of the population as a whole, but there are also other risk factors, such as lifestyle and hearing.
""Previous studies have suggested that there could be a link between hearing loss and dementia. Our study is larger than the previous studies, and we have demonstrated a link between hearing loss and dementia, ""says Assistant Professor Manuella Lech Cantuaria from the Department of Clinical Research at the University of Southern Denmark.
Good news for hearing aid users
The results of the study show that people affected by hearing loss have up to a 13% higher risk of developing dementia compared to people with normal hearing. The high risk is especially seen in people with severe hearing loss.
The researchers also studied whether there was a difference in the risk depending on whether or not people wear hearing aids.

""We found that the risk of developing dementia was 20% higher for people who didn't wear hearing aids compared to people with normal hearing. People who used hearing aids had a 6% increased risk of developing dementia. This suggests that wearing a hearing aid can prevent or delay the development of dementia,"" explains Manuella Lech Cantuaria.
About the study
The study is a so-called cohort study that follows a group of people with common characteristics over a longer period of time. In this study, all of the people were above 50 years of age and from the Region of Southern Denmark between 2003-2017. People diagnosed with dementia before the commencement of the study were excluded. The researchers compared data on people's hearing with data on the development of dementia during the period. The researchers have found a significant -- that is, clear -- correlation between hearing loss and the development of dementia. The greatest risk of developing dementia was especially seen in people with severe hearing loss Hearing loss causes a 7% increased risk of developing dementia People with severe hearing loss have up to a 20% increased risk of developing dementia compared to people without hearing lossHearing loss and dementia
Hearing loss -- a national scourge
Around 800,000 Danes suffer from hearing loss. And that number is only likely to get higher in the future because we are getting older in general, and we are exposed to more and more noise. Hearing loss is measured in decibels (dB). There are different degrees of hearing loss. For example, hearing loss above 60-70 dB means you can't hear normal speech. Above 90-100 dB means you can't hear shouting.
Dementia
Dementia is a term used to describe the weakening of mental abilities due to illness in the brain. Dementia is characterised by an impairment of cognitive functions such as: Impaired memory and concentration, impaired orientation, language disorders, personality and behavioural changes. Alzheimer's, for example, is a type of dementia.

","score: 11.401702282661489, grade_level: '11'","score: 11.805304152501215, grade_levels: ['12'], ages: [17, 18]",10.1001/jamaoto.2023.3509,"Hearing loss has been suggested as a risk factor for dementia, but there is still a need for high-quality research to better understand the association between these 2 conditions and the underlying causal mechanisms and treatment benefits using larger cohorts and detailed data. To investigate the association between hearing loss and incident dementia, as well as how hearing aid use contributes to this association. This population-based cohort study was conducted in Southern Denmark between January 2003 and December 2017 and included all residents 50 years and older. We excluded all persons with dementia before baseline as well as those who did not live in the region 5 years before baseline, with incomplete address history, or who had missing covariate information. Individual hearing status based on the Hearing Examinations in Southern Denmark database, which contains data on all pure-tone audiometry examinations performed at public hearing rehabilitation clinics in Southern Denmark. Incident cases of dementia and Alzheimer disease as identified from national registries. The study population comprised 573 088 persons (298 006 women [52%]; mean [SD] age, 60.8 [11.3] years) with 23 023 cases of dementia and mean (SD) follow-up of 8.6 (4.3) years. Having a hearing loss was associated with an increased risk of dementia, with an adjusted hazard ratio (HR) of 1.07 (95% CI, 1.04-1.11) compared with having no hearing loss. Severe hearing loss in the better and worse ear was associated with a higher dementia risk, with an HR of 1.20 (95% CI, 1.09-1.32) and 1.13 (95% CI, 1.06-1.20), respectively, compared with having no hearing loss in the corresponding ear. Compared with people without hearing loss, the risk of dementia was higher among people with hearing loss who were not using hearing aids than those who had hearing loss and were using hearing aids, with HRs of 1.20 (95% CI, 1.13-1.27) and 1.06 (95% CI, 1.01-1.10), respectively. The results of this cohort study suggest that hearing loss was associated with increased dementia risk, especially among people not using hearing aids, suggesting that hearing aids might prevent or delay the onset and progression of dementia. The risk estimates were lower than in previous studies, highlighting the need for more high-quality longitudinal studies."
"
How deeply someone can be hypnotized -- known as hypnotizability -- appears to be a stable trait that changes little throughout adulthood, much like personality and IQ. But now, for the first time, Stanford Medicine researchers have demonstrated a way to temporarily heighten hypnotizablity -- potentially allowing more people to access the benefits of hypnosis-based therapy.

In the new study, to be published Jan. 4 in Nature Mental Health, the researchers found that less than two minutes of electrical stimulation targeting a precise area of the brain could boost participants' hypnotizability for about one hour.
""We know hypnosis is an effective treatment for many different symptoms and disorders, in particular pain,"" said Afik Faerman, PhD, a postdoctoral scholar in psychiatry and lead author of the study. ""But we also know that not everyone benefits equally from hypnosis.""
Focused attention
Approximately two-thirds of adults are at least somewhat hypnotizable, and 15% are considered highly hypnotizable, meaning they score 9 or 10 on a standard 10-point measure of hypnotizability.
""Hypnosis is a state of highly focused attention, and higher hypnotizability improves the odds of your doing better with techniques using hypnosis,"" said David Spiegel, MD, a professor of psychiatry and behavioral sciences and a senior author of the study.
Spiegel, the Jack, Lulu, and Sam Willson Professor in Medicine, has devoted decades to studying hypnotherapy and using it to help patients control pain, lower stress, stop smoking and more. Several years ago, Spiegel led a team that used brain imaging to uncover the neurobiological basis of the practice. They found that highly hypnotizable people had stronger functional connectivity between the left dorsolateral prefrontal cortex, which is involved in information processing and decision making; and the dorsal anterior cingulate cortex, involved in detecting stimuli.

""It made sense that people who naturally coordinate activity between these two regions would be able to concentrate more intently,"" Spiegel said. ""It's because you're coordinating what you are focusing on with the system that distracts you.""
Shifting a stable trait
With these insights, Spiegel teamed up with Nolan Williams, MD, associate professor of psychiatry and behavioral sciences, who has pioneered non-invasive neurostimulation techniques to treat conditions such as depression, obsessive-compulsive disorder and suicidal ideation.
The hope was that neurostimulation could alter even a stable trait like hypnotizability.
In the new study, the researchers recruited 80 participants with fibromyalgia, a chronic pain condition that can be treated with hypnotherapy. They excluded those who were already highly hypnotizable.
Half of the participants received transcranial magnetic stimulation, in which paddles applied to the scalp deliver electrical pulses to the brain. Specifically, they received two 46-second applications that delivered 800 pulses of electricity to a precise location in the left dorsolateral prefrontal cortex. The exact locations depended on the unique structure and activity of each person's brain.

""A novel aspect of this trial is that we used the person's own brain networks, based on brain imaging, to target the right spot,"" said Williams, also a senior author of the study.
The other half of participants received a sham treatment with the same look and feel, but without electrical stimulation.
Hypnotizability was assessed by clinicians immediately before and after the treatments, with neither patients nor clinicians knowing who was in which group.
The researchers found that participants who received the neurostimulation showed a statistically significant increase in hypnotizability, scoring roughly one point higher. The sham group experienced no effect.
When the participants were assessed again one hour later, the effect had worn off and there was no longer a statistically significant difference between the two groups.
""We were pleasantly surprised that we were able to, with 92 seconds of stimulation, change a stable brain trait that people have been trying to change for 100 years,"" Williams said. ""We finally cracked the code on how to do it.""
The researchers plan to test whether different dosages of neurostimulation could enhance hypnotizability even more.
""It's unusual to be able to change hypnotizability,"" Spiegel said. A study of Stanford University students that began in the 1950s, for example, found that the trait remained relatively consistent when the students were tested 25 years later, as consistent as IQ over that time period. Recent research by Spiegel's lab also suggests that hypnotizability may have a genetic basis.
Bigger implications
Clinically, a transient bump in hypnotizability may be enough to allow more people living with chronic pain to choose hypnosis as an alternative to long-term opioid use. Spiegel will follow up with the study participants to see how they fare in hypnotherapy.
The new results could have implications beyond hypnosis. Faerman noted that neurostimulation may be able to temporarily shift other stable traits or enhance people's response to other forms of psychotherapy.
""As a clinical psychologist, my personal vision is that, in the future, patients come in, they go into a quick, non-invasive brain stimulation session, then they go in to see their psychologist,"" he said. ""Their benefit from treatment could be much higher.""
The study was supported by funding from the National Institute of Health (grant R33AT009305-03).

","score: 14.591660859465744, grade_level: '15'","score: 15.133825417201543, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s44220-023-00184-z,"Hypnotizability, one’s ability to experience cognitive, emotional, behavioral and physical changes in response to suggestions in the context of hypnosis, is a stable neurobehavioral trait associated with improved treatment outcomes from hypnosis-based therapy. Increasing hypnotizability in people who are low-to-medium hypnotizable individuals could improve both the efficacy and effectiveness of therapeutic hypnosis as a clinical intervention. Hypnotizability is associated with dorsolateral prefrontal cortex (DLPFC) functions and connectivity with the salience network, yet there is conflicting evidence as to whether unilateral inhibition of the DLPFC changes hypnotizability. We hypothesized that using personalized neuroimaging-guided targeting to non-invasively stimulate the left DLPFC with transcranial magnetic stimulation (TMS) would temporarily increase hypnotizability. In a preregistered, double-blinded, randomized controlled trial, we recruited a sample of 80 patients with fibromyalgia syndrome, a functional pain disorder for which hypnosis has been a demonstrated beneficial non-pharmacological treatment option. All participants were TMS-naive. Participants were randomly assigned to active or sham continuous theta-burst stimulation over a personalized neuroimaging-derived left-DLPFC target, a technique termed SHIFT (Stanford Hypnosis Integrated with Functional Connectivity-targeted Transcranial Stimulation). We tested our hypothesis using the hypnotic induction profile scores, a standardized measure of hypnotizability. Pre-to-post SHIFT change in the hypnotic induction profile scores was significantly greater in the active versus sham group after 92 s of stimulation (P = 0.046). Only the active SHIFT group showed a significant increase in hypnotizability following stimulation (active: P < 0.001; sham: P = 0.607). As such, modulation of trait hypnotizability is possible in humans using non-invasive neuromodulation. Our findings support a relationship between the inhibition of the left DLPFC and an increase in hypnotizability. Dose–response optimization of spaced SHIFT should be explored to understand the optimal dose–response relationship. ClinicalTrials.gov registration: NCT02969707."
"
When experiencing new things, the structure and function of our neurons and their connections are rapidly being remodeled. This process, known as synaptic plasticity, is critical for us to learn and adapt. However, these changes require a lot of energy.

Fortunately, our neurons are well-adapted to support these changes. Biological batteries known as mitochondria are strategically stabilized near sites of synaptic remodeling to ensure a local and efficient energy supply. However, how mitochondria are anchored near synapses was not known.
A team of scientists at Max Planck Florida Institute for Neuroscience has now identified a molecular anchor called VAP (vesicle-associated membrane protein-associated protein) that stabilizes mitochondria near synapses to support these remodeling projects. The identification of VAP as a molecular anchor has particular significance because a mutation in VAP leads to ALS (amyotrophic lateral sclerosis), a progressive motor neuron degeneration disease. This discovery, published in Nature Communications, not only sheds light on how memories are powered but opens up new research directions into ALS pathology.
Lead scientist of the work and Max Planck Florida Institute Research Group Leader, Dr. Vidhya Rangaraju, describes the implications, ""While we started this study to understand fundamental properties of how memories are powered, our findings open important new directions for our research. We will investigate the cellular mechanisms of the cognitive symptoms that often occur with motor symptoms in ALS but have been severely understudied. We believe the tools and approaches that we have established will begin to shine light into this area.""
Stable mitochondria support the plasticity of synapses in dendrites
Neurons have an extensive, complex morphology and are constantly being remodeled. In order to energetically support these changes, mitochondria are anchored locally near synapses. This local stabilization allows the energy produced by the mitochondria to efficiently power local structural and functional changes in synapses during memory formation.
""This stability, however, is a unique feature of neuronal dendrites,"" explains Ojasee Bapat, the study's first author. ""In neuronal axons, where mitochondria have been primarily studied, they are very mobile. We were interested in understanding how mitochondria are stabilized in dendrites and what this means for plasticity.""
The search for a molecular anchor

To address this knowledge gap, the team took an unbiased approach to identify proteins that might serve as an anchor to stabilize dendritic mitochondria. The team used a chemogenetic tool that chemically marked all proteins present near the outer shell of the mitochondria. They then took advantage of advanced proteomics to determine the identity of the marked proteins. This screen identified over 100 proteins that might be responsible for anchoring the mitochondria.
To narrow their search, they selected proteins for their ability to interact with the actin cytoskeleton. The actin cytoskeleton is a network of protein filaments localized near synapses in dendrites that help to define and remodel synaptic structure. Previous work of Dr. Rangaraju and others has shown that mitochondrial stability in dendrites depends on actin. Only a handful of the initially identified candidate proteins could bind to actin.
To determine if these proteins were essential to mitochondrial stabilization, the authors genetically removed them one by one from neurons and looked at the stability of mitochondria in dendrites. What they found was striking.
ALS-linked protein VAP stabilizes mitochondria to support plasticity
The team discovered that when they removed one particular protein, named VAP, the interaction of mitochondria with actin was reduced. Additionally, dendritic mitochondria were shorter and destabilized. Without VAP to anchor the mitochondria by tethering it to the actin cytoskeleton, the mitochondria drifted away from synapses over time.
Finally, the researchers investigated if mitochondrial instability caused by removing VAP affected synaptic plasticity, the structural and functional remodeling of synapses during memory formation. To test this question, the team induced remodeling in a cluster of synapses and compared the structural changes to neurons that lacked VAP. In neurons in which VAP was removed, the remodeling was dramatically impaired. Induced changes in the structure of synapses could not be maintained in the absence of VAP.

A link to ALS leads to new research directions
The discovery that VAP serves as a mitochondrial anchor and supports memory formation holds particular significance. A single mutation in VAP causes a familial form of ALS, a fatal progressive motor neuron degeneration disease. Although VAP is associated with this specific and rare familial cause of the disease, the discovery suggests that mitochondrial stability and energetic support of plasticity are key cellular pathways that might contribute to disease pathology.
""The fact that our unbiased screen for mitochondrial tethering to the cytoskeleton identified a protein linked to neurodegenerative disease suggests that mitochondrial stabilization is a critical element in neuronal function and health,"" described Dr. Rangaraju. ""We are motivated to expand our research focus to understand what happens in the brain when mitochondrial energy availability is disrupted. We think this focus has the potential to find common mechanisms of neurodegeneration in ALS as well as other neurological disorders.""

","score: 15.047066682395968, grade_level: '15'","score: 15.400420267201461, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41467-023-44233-8,"Synapses are pivotal sites of plasticity and memory formation. Consequently, synapses are energy consumption hotspots susceptible to dysfunction when their energy supplies are perturbed. Mitochondria are stabilized near synapses via the cytoskeleton and provide the local energy required for synaptic plasticity. However, the mechanisms that tether and stabilize mitochondria to support synaptic plasticity are unknown. We identified proteins exclusively tethering mitochondria to actin near postsynaptic spines. We find that VAP, the vesicle-associated membrane protein-associated protein implicated in amyotrophic lateral sclerosis, stabilizes mitochondria via actin near the spines. To test if the VAP-dependent stable mitochondrial compartments can locally support synaptic plasticity, we used two-photon glutamate uncaging for spine plasticity induction and investigated the induced and adjacent uninduced spines. We find VAP functions as a spatial stabilizer of mitochondrial compartments for up to ~60 min and as a spatial ruler determining the ~30 μm dendritic segment supported during synaptic plasticity."
"
Stress during pregnancy is known to influence health outcomes, but a new study from Mass General Brigham researchers suggests that stress levels before pregnancy are also important to evaluate. Investigators at Massachusetts General Hospital and Brigham and Women's Hospital analyzed the link between self-reported stress immediately before conception among women seeking fertility care and blood glucose levels, a marker of heart health. The team found that maternal stress during preconception was associated with higher blood glucose levels, especially among women using intrauterine insemination to conceive and women of higher socioeconomic status. Results are published in the Journal of the Endocrine Society. 

""Stress prevalence has increased over the years, particularly for couples who are not able to conceive naturally,"" said corresponding author Lidia Mínguez-Alarcón, PhD, MPH, Bpharm, a reproductive epidemiologist in the Brigham's Channing Division of Network Medicine and co-investigator of the Environment and Reproductive Health (EARTH) study. ""We wanted to evaluate how this stress affects health during pregnancy, which can affect both the mother and child in the long term.""
Mínguez-Alarcón and colleagues analyzed data from the EARTH study conducted at the Massachusetts General Hospital Fertility Center from 2004 to 2019 for 398 women between 18 and 45 years of age. The women self-reported preconception-perceived stress at study entry. Additional clinical characteristics and sociodemographic information, including family and medical history, consumer products use and smoking history, were either collected by the study staff through medical records or questionnaires.
Women had a median age of 35 years at study entry, and most were of white ethnic background (83 percent), reported never smoking (78 percent), and were at least college educated (64 percent). Three hundred of the women conceived using medically assisted technologies like intrauterine insemination (IUI) or in vitro fertilization (IVF). During IUI, sperm is injected directly into the uterus while IVF is a multi-step technology that involves retrieving an egg for fertilization in a lab before transfer back to the uterus. Glucose testing was done at a median of 26 weeks into pregnancy and taken one hour after the women drank a 50 gram glucose solution. A blood sugar that was equal to or less than 140 mg/dL was considered normal.
Researchers found that blood sugar levels, a measure of cardiovascular health, were abnormally high in 82 of the women involved. Previous studies have shown that women with a history of gestational diabetes (GD) during pregnancy are at increased risk of developing type 2 diabetes as well as cardiovascular problems later in life, including heart artery calcification.
The team found that women who experienced higher preconception stress had higher mean glucose levels. In addition, women who conceived through IUI had both higher stress and blood sugar levels than those who conceived through IVF. The study also found that women of higher socioeconomic status had higher levels of preconception stress and blood glucose levels during their pregnancy. Median family income was used to measure socioeconomic status.
""Professional women with higher incomes and attained education levels may be employed in demanding, time-intensive jobs and are often also responsible for balancing demands in the workplace with household duties and childcare,"" Mínguez-Alarcón said. ""It has previously been shown that women with a higher education level experience greater levels of job stress. Given that education level is positively associated with salary, it is possible that this explanation applies to women with higher incomes as well.""
Still, findings are limited since the study comprises a group of mostly white women of high socioeconomic status seeking fertility care. Self-reporting perceived stress may also result in participant bias. Future research can investigate additional variables like sleep quality or neighborhood safety as well as the effect of preconception stress on the baby's health.
""Our results are of public health importance given the increasing rates of stress over the years and its effect on cardiovascular health,"" Mínguez-Alarcón said. ""Women can try to lower their stress levels through a variety of strategies like being more active, avoiding alcohol and drugs, eating healthy and avoiding isolation. Given the scarce literature in this field, our study has the potential to start important discussions.""
Authorship: The other authors of this study are Olivia Chagnon, Aya Tanaka, Paige Williams, Tamarra James Todd and Jennifer Ford of Harvard T.H. Chan School of Public Health; Irene Souter of Massachusetts General Hospital and Harvard Medical School in Boston, Mass.; Kathryn Rexrode of Brigham and Women's Hospital and Harvard Medical School; and Russ Hauser of Harvard T.H. Chan School of Public Health and Harvard Medical School.

","score: 14.7082493347631, grade_level: '15'","score: 15.810291512768579, grade_levels: ['college_graduate'], ages: [24, 100]",10.1210/jendso/bvad152,"The association between women's stress and pregnancy glucose levels remain unclear, specifically when considering the preconception period as a sensitive window of exposure. We investigated whether preconception perceived stress was associated with glucose levels during pregnancy among women attending a fertility center (2004-2019). Before conception, women completed a psychological stress survey using the short version of the validated Perceived Stress Scale 4 (PSS-4), and blood glucose was measured using a 50-gram glucose load test during late pregnancy as a part of screening for gestational diabetes. Linear and log-binomial regression models were used to assess associations of total PSS-4 scores with mean glucose levels and abnormal glucose levels ( ≥ 140 mg/dL), adjusting for age, body mass index, race, smoking, education, physical activity, primary infertility diagnosis, number of babies, and mode of conception. Psychological stress was positively associated with mean abnormal glucose levels. The adjusted marginal means (95% CI) of mean glucose levels for women in the first, second, and third tertiles of psychological stress were 115 (110, 119), 119 (115, 123), and 124 (119, 128), and mg/dL, respectively (P for trend = .007). Also, women in the second and third tertiles of psychological stress had 4% and 13% higher probabilities of having abnormal glucose compared with women in the first tertile of psychological stress (P trend = .01). These results highlight the importance of considering preconception when evaluating the relationship between women's stress and pregnancy glucose levels."
"
New research indicates that the eating disorder anorexia nervosa is associated with being an early riser, unlike many other disorders that tend to be evening-based such as depression, binge eating disorder and schizophrenia.

The study, which is published in JAMA Network Open and led by investigators at Massachusetts General Hospital (MGH), in collaboration with University College London and the University of the Republic in Uruguay, also revealed a link between anorexia nervosa and insomnia risk.
Previous research has suggested a possible connection between eating disorders and the body's internal clock, or circadian clock, which controls a wide range of biological functions such as sleep and affects nearly every organ in the body.
This study aimed to further understand this relationship by assessing genes associated with anorexia nervosa, the circadian clock and several sleep traits including insomnia.
The investigators used a statistical method called Mendelian Randomization to see how genes that are associated with a certain trait affect other traits of interest. For example, examining the sleep patterns of people with genetic differences that makes them more likely to have anorexia nervosa, this provides evidence on the relationship between anorexia nervosa and sleep.
They found a two-way association between genes associated with anorexia nervosa and genes associated with morning chronotype (waking early and going to bed early).
In other words, the findings suggest that being an early riser could increase the risk for having anorexia nervosa, and having anorexia nervosa could lead to an earlier wake time. The team also found an association between anorexia nervosa and insomnia.

When they further assessed the insomnia connection using the Mass General Brigham Biobank by developing a ""genetic risk score"" for anorexia nervosa, the scientists found that the genetic risk score was indeed associated with higher insomnia risk.
""Our findings implicate anorexia nervosa as a morning disorder in contrast to most other evening-based psychiatric diseases and support the association between anorexia nervosa and insomnia as seen in earlier studies,"" says senior author Hassan S Dashti, PhD, RD, an assistant investigator in the Department of Anesthesia, Critical Care and Pain Medicine at MGH and an assistant professor of anesthesia at Harvard Medical School.
Treatments for anorexia nervosa are limited and current treatments have relapse rates of up to 52%. In addition, the cause of the disease is still unclear.
With anorexia nervosa having the second highest mortality rate of psychiatric diseases, more research is desperately needed into new prevention strategies and treatments.
""The clinical implications of our new findings are currently unclear; however, our results could direct future investigations into circadian-based therapies for anorexia nervosa prevention and treatment,"" says Hannah Wilcox, lead author of the study and researcher at MGH.
Additional authors include Valentina Paz, MSc, Richa Saxena, PhD, John W. Winkelman, MD, PhD, and Victoria Garfield, PhD.
This research was supported by the National Institutes of Health.

","score: 17.756307257767407, grade_level: '18'","score: 18.69598059683164, grade_levels: ['college_graduate'], ages: [24, 100]",10.1001/jamanetworkopen.2023.50358,"Observational studies have associated anorexia nervosa with circadian rhythms and sleep traits. However, the direction of causality and the extent of confounding by psychosocial comorbidities in these associations are unknown. To investigate the association between anorexia nervosa and circadian and sleep traits through mendelian randomization and to test the associations between a polygenic risk score (PRS) for anorexia nervosa and sleep disorders in a clinical biobank. This genetic association study used bidirectional 2-sample mendelian randomization with summary-level genetic associations between anorexia nervosa (from the Psychiatric Genomics Consortium) and chronotype and sleep traits (primarily from the UK Biobank). The inverse-variance weighted method, in addition to other sensitivity approaches, was used. From the clinical Mass General Brigham (MGB) Biobank (n = 47 082), a PRS for anorexia nervosa was calculated for each patient and associations were tested with prevalent sleep disorders derived from electronic health records. Patients were of European ancestry. All analyses were performed between February and August 2023. Genetic instruments for anorexia nervosa, chronotype, daytime napping, daytime sleepiness, insomnia, and sleep duration. Chronotype, sleep traits, risk of anorexia nervosa, and sleep disorders derived from a clinical biobank. The anorexia nervosa genome-wide association study included 16 992 cases (87.7%-97.4% female) and 55 525 controls (49.6%-63.4% female). Genetic liability for anorexia nervosa was associated with a more morning chronotype (β = 0.039; 95% CI, 0.006-0.072), and conversely, genetic liability for morning chronotype was associated with increased risk of anorexia nervosa (β = 0.178; 95% CI, 0.042-0.315). Associations were robust in sensitivity and secondary analyses. Genetic liability for insomnia was associated with increased risk of anorexia nervosa (β = 0.369; 95% CI, 0.073-0.666); however, sensitivity analyses indicated bias due to horizontal pleiotropy. The MGB Biobank analysis included 47 082 participants with a mean (SD) age of 60.4 (17.0) years and 25 318 (53.8%) were female. A PRS for anorexia nervosa was associated with organic or persistent insomnia in the MGB Biobank (odds ratio, 1.10; 95% CI, 1.03-1.17). No associations were evident for anorexia nervosa with other sleep traits. The results of this study suggest that in contrast to other metabo-psychiatric diseases, anorexia nervosa is a morningness eating disorder and further corroborate findings implicating insomnia in anorexia nervosa. Future studies in diverse populations and with subtypes of anorexia nervosa are warranted."
"
Medical technology innovations achieved by integrating science and medicine have improved the quality of life for patients. Especially noteworthy is the emergence of electronic devices implanted in the body, such as in the heart or brain, which enable real-time measurement and regulation of physiological signals, presenting new solutions for challenging conditions like Parkinson's disease. However, technical constraints have hindered the semi-permanent use of electronic devices after their implantation.

A collaborative research team led by Professor Sung-Min Park from the Departments of Convergence IT Engineering, Mechanical Engineering, and Electrical Engineering, and the School of Interdisciplinary Bioscience and Bioengineering at POSTECH, alongside Jiho Lee, enrolled in the MS/Ph.D. program, and Professor Sang-Woo Kim from Yonsei University's Department of Materials Science and Engineering, together with Dr. Young-Jun Kim and MS/Ph.D. student Joon-Ha Hwang from Sungkyunkwan University, has achieved a groundbreaking development. They've created electrostatic materials that function even with extremely weak ultrasound, heralding the era of permanent implantable electronic devices in biomedicine. This research has been published in the international academic journal Advanced Materials.
Patients with implanted devices need to undergo periodic surgeries for battery replacement. This process carries a significant risk of complications and imposes both economic and physical burdens on patients. Recent research explores implantable medical devices that operate wirelessly, yet finding a safe energy source and protective materials remains challenging. Presently, titanium (Ti) is used due to its biocompatibility and durability. However, radio waves cannot pass through this metal, necessitating a separate antenna for wireless power transmission. Consequently, this enlarges the device size, creating more discomfort for patients.
The research team addresses this issue by opting for ultrasound, a safety-validated method in various medical fields for diagnoses and treatments, instead of radio waves. They developed an electrostatic material capable of responding to weak ultrasound by utilizing a composite of high dielectric polymers (P(VDF-TrFE)) and a high dielectric constant ceramic material known as calcium copper titanate (CCTO, CaCu3Ti4O12). This material generates static electricity through friction between its material layers, producing effective electrical energy, and possesses an extremely low output impedence, facilitating efficient transmission of the generated electricity.
Using this technology, the research team created an implantable neurological stimulator powered by ultrasound-based energy transmission, eliminating the need for batteries. This was confirmed through experimental validation. In animal model trials, the device was activated even at standard imaging ultrasound levels (500 mW/cm2), imposing minimal strain on the human body. Furthermore, it effectively mitigated symptoms related to abnormal urination caused by overactive bladder disorders through nerve stimulation.
Professor Sung-Min Park stated: ""We have addressed the challenges in the field of implantable medical devices using ultrasound-based energy transmission technology that is harmless to the human body. This research serves as a case of introducing advanced material technology into medical devices, and we anticipate that it will promote the emergence of a next-generation medical industry, including the treatment of intractable diseases using implantable devices.""
Professor Sang-Woo Kim remarked: ""Devices manufactured based on highly biocompatible materials exhibit excellent mechanical and chemical stability, making them suitable for treating various diseases requiring long-term therapy. Non-battery, miniaturized components with established long-term stability are expected to bring forth new innovations in the market of human-insertable medical devices.""
The research was conducted with support from the Research Leader Program, Pioneer Program of Future Technology, and Bio & Medical Technology Development Program by the National Research Foundation of Korea and the Ministry of Science and ICT, along with Yonsei Fellowship.

","score: 18.181378482868954, grade_level: '18'","score: 19.23221970403946, grade_levels: ['college_graduate'], ages: [24, 100]",10.1002/adma.202307194,"In implantable bioelectronics, which aim for semipermanent use of devices, biosafe energy sources and packaging materials to protect devices are essential elements. However, research so far has been conducted in a direction where they cannot coexist. Here, the development of capacitance‐matched triboelectric implants driven is reported by ultrasound under 500 mW cm−2 safe intensity and realize a battery‐free, miniatured, and wireless neurostimulator with full titanium (Ti) packaging. The triboelectric implant with high dielectric composite, which has ultralow output impedance, can efficiently deliver sufficient power to generate the stimulation pulse without an energy‐storing battery, despite ultrasound attenuation due to the Ti, and has the highest energy transmission efficiency among those reported so far. In vivo study using a rat model demonstrated that the proposed device system is an effective solution for relieving urinary symptoms. These achievements provide a significant step toward permanently implantable devices for controlling human organs and treating various diseases."
"
For children, the world is full of surprises. Adults, on the other hand, are much more difficult to surprise. And there are complex processes behind this apparently straightforward state of affairs. Researchers at the University of Basel have been using mice to decode how reactions to the unexpected develop in the growing brain.

Babies love playing peekaboo, continuing to react even on the tenth sudden appearance of their partner in the game. Recognizing the unexpected is an important cognitive ability. After all, new can also mean dangerous.
The exact way in which surprises are processed in the brain changes as we grow, however: unusual stimuli are much more quickly categorized as ""important"" or ""uninteresting,"" and are significantly less surprising the second and third time they appear. This increased efficiency makes perfect sense: new stimuli may gain our attention, but do not cause an unnecessarily strong reaction that costs us energy. While this may appear trivial at first, so far there has been very little research into this fact in the context of brain development.
Experiments with young mice conducted by Professor Tania Barkat's research team have now begun to decode how the developing brain processes surprising sounds and what changes as we grow up. The researchers have reported on their findings in the journal Science Advances.
Strange sounds
In their experiments, the researchers used sequences of sounds in which a different tone was heard at irregular intervals in between a series of identical ones. At the same time, they recorded the animals' brain waves. This process is known as the ""oddball paradigm,"" and is used by health professionals for purposes such as the diagnosis of schizophrenia.
Using these measurements, the researchers were able to understand how the reaction of different brain regions to the change of tone developed over time in the young mice. This reaction was initially very strong, but decreased as the relevant brain region matured, to a level comparable to that of measurements in adult animals. This development does not take place simultaneously in the various areas of the brain that process sound, however.

A region known as the inferior colliculus, located at the beginning of the path from the auditory nerve to the auditory cortex, was already fully mature in the animals at the age of 20 days, the earliest point in time studied by the team. A second site, the auditory thalamus, only showed an ""adult"" reaction to the differing tone at the age of 30 days.
Development in the cerebral cortex itself, the ""primary auditory cortex,"" took even longer, until day 50. ""This development of the surprise reaction thus begins in the periphery and ends in the cerebral cortex,"" says study leader Tania Barkat. The cerebral cortex therefore matures much later than expected -- in human years, this would equate roughly to the early 20s.
No development without experience
The researchers also observed that experiences play a key role in the development of the surprise response in the cerebral cortex. If the mice were reared in a noise-neutral environment, the processing of unexpected sounds in the auditory cortex was significantly delayed.
One possible explanation for this is that the brain -- and the cerebral cortex in particular -- forms an internal image of the world during growth, which it then compares with external stimuli. Anything that does not correspond to this ""worldview"" is a surprise, but may also result in an update. ""Without experience with sounds, however, the cerebral cortex in these mice is unable to develop such a model of the world,"" says neuroscientist Barkat. As a result, the animal is unable to categorize sounds properly into ""familiar"" and ""unexpected.""

","score: 12.347769672855879, grade_level: '12'","score: 12.554053381962866, grade_levels: ['college'], ages: [18, 24]",10.1126/sciadv.adi7624,"Stimulus-specific adaptation (SSA), the reduction of neural activity to a common stimulus that does not generalize to other, rare stimuli, is an essential property of our brain. Although well characterized in adults, it is still unknown how it develops during adolescence and what neuronal circuits are involved. Using in vivo electrophysiology and optogenetics in the lemniscal pathway of the mouse auditory system, we observed SSA to be stable from postnatal day 20 (P20) in the inferior colliculus, to develop until P30 in the auditory thalamus and even later in the primary auditory cortex (A1). We found this maturation process to be experience-dependent in A1 but not in thalamus and to be related to alterations in deep but not input layers of A1. We also identified corticothalamic projections to be implicated in thalamic SSA development. Together, our results reveal different circuits underlying the sequential SSA maturation and provide a unique perspective to understand predictive coding and surprise across sensory systems."
"
Researchers have created a new brain imaging method that allows mild traumatic brain injuries (mTBIs) to be diagnosed, even when existing imaging techniques like magnetic resonance imaging (MRI) don't show any structural abnormalities. The technique involves loading gadolinium, a standard MRI contrast agent, into hydrogel-based micropatches that are attached to immune cells called macrophages. mTBIs cause inflammation in the brain, which produces signals that attract macrophages to migrate there. Coupling the gadolinium contrast agent to these cells enables MRI to reveal brain inflammation and increase the number of correctly diagnosed mTBI cases, improving patient care. The method is described in a new paper in Science Translational Medicine.

""70-90% of reported TBI cases are categorized as 'mild,' yet as many as 90% of mTBI cases go undiagnosed, even though their effects can last for years and they are known to increase the risk of a host of neurological disorders including depression, dementia, and Parkinson's disease,"" said senior author Samir Mitragotri, Ph.D., in whose lab the research was performed. ""Our cell-based imaging approach exploits immune cells' innate ability to travel into the brain in response to inflammation, enabling us to identify mTBIs that standard MRI imaging would miss.""
Mitragotri is a Core Faculty member of the Wyss Institute at Harvard University and the Hiller Professor of Bioengineering and Hansjörg Wyss Professor of Biologically Inspired Engineering at Harvard's John A. Paulson School of Engineering and Applied Sciences (SEAS).
Using immune cells to identify inflammation
Most of us know someone who has had a concussion (another name for an mTBI), sometimes even more than one. But the vast majority of people who experience an mTBI are never properly diagnosed. Without that diagnosis, they can exacerbate their injuries by returning to normal activity before they're fully recovered, which can lead to further damage. Some studies even suggest that repeated mTBIs can lead to chronic traumatic encephalopathy (CTE), the neurodegenerative disease that has been found to afflict more than 90% of professional American football players.
Because the effects of mTBI are believed to be caused by ""invisible"" brain inflammation, members of the Mitragotri lab decided to leverage their experience with immune cells to create a better diagnostic. ""Our previous projects have focused on controlling the behavior of immune cells or using them to deliver drugs to a specific tissue. We wanted to exploit another innate ability of immune cells -- homing to sites of inflammation in the body -- to carry imaging agents into the brain, where they can provide a visible detection signal for mTBI,"" said first author Lily Li-Wen Wang, Ph.D.. Wang is a former Research Fellow in the Mitragotri Lab at the Wyss Institute and SEAS who is now a scientist at Landmark Bio.
The team planned to use their cellular backpack technology to attach gadolinium molecules to macrophages, a type of white blood cell that is known to infiltrate the brain in response to inflammation. But right away, they ran into a problem: in order to function as a contrast agent for MRI scans, gadolinium needs to interact with water. Their original backpack microparticles are compost of a polymer called PLGA, which is hydrophobic (meaning it repels water). So Wang and her co-authors started developing a new backpack made out of a hydrogel material that could be manufactured at a large scale in the lab.

After years of hard work, they finally created a new hydrogel backpack that could produce a strong gadolinium-mediated MRI signal, attach stably to both mouse and pig macrophages, and maintain their cargo for a sustained period of time in vitro. They named their new microparticles M-GLAMs, short for ""macrophage-hitchhiking Gd(III)-Loaded Anisotropic Micropatches."" Now, it was time to test them in a more realistic setting, for which they partnered with researchers and clinicians at Boston Children's Hospital.
First, they injected mouse M-GLAMs macrophages into mice to see if they could visualize them in vivo. They were especially interested to see if they accumulated in the kidney, as existing gadolinium-based contrast agents like Gadavist® can cause health risks for patients with kidney disease. Their M-GLAMs did not accumulate in the mice's kidneys, but persisted in their bodies for over 24 hours with no negative side effects. In contrast, mice injected with Gadavist® showed substantial accumulation of the contrast agent in their kidneys within 15 minutes of injection, and the substance was fully cleared from their bodies within 24 hours.
Then, they tested porcine M-GLAMs in a pig model of mTBI. They injected the M-GLAMs into the animals' blood two days after a mock mTBI, then used MRI to evaluate the concentration of gadolinium in the brain. They focused on a small region called the choroid plexus, which is known as a major conduit of immune cells into the brain. Pigs that received the M-GLAMs displayed a significant increase in the intensity of gadolinium present in the choroid plexus, while those injected with Gadavist® did not, despite confirmation of increased inflammation macrophage density in the brains of both groups. The animals showed no toxicity in any of their major organs following administration of the treatments.
""Another important aspect of our M-GLAMs is that we are able to achieve better imaging at a much lower dose of gadolinium than current contrast agents -- 500-1000-fold lower in the case of Gadavist®,"" said Wang. ""This could allow the use of MRI for patients who are currently unable to tolerate existing contrast agents, including those who have existing kidney problems.""
The authors note that while M-GLAMs can indicate the presence of inflammation in the brain via the high concentration of macrophages entering it through the choroid plexus, their technique cannot pinpoint the exact location of injuries or inflammatory responses in brain tissue. However, if coupled with new treatment modalities like one they developed in , M-GLAMS could offer a more rapid and effective way to identify and reduce inflammation in mTBI patients to minimize damage and speed their recovery.
The researchers have submitted a patent application for their technology, and hope to be able to bring it to the market in the near future. They are currently exploring collaborations with biotech and pharmaceutical companies to accelerate it to clinical trials .
""This work demonstrates just how much potential is waiting to be unlocked within the human body for a variety of functions: monitoring health, diagnosing problems, treating diseases, and preventing their recurrence. I'm impressed with this team's ingenuity in leveraging immune cells to improve medical imaging, and hope to see it in clinicians' hands soon,"" said Wyss Founding Director Donald Ingber, M.D., Ph.D. Ingber is also the Judah Folkman Professor of Vascular Biology at Harvard Medical School and Boston Children's Hospital, and the Hansjörg Wyss Professor of Bioinspired Engineering at SEAS.
Additional authors of the paper include Yongsheng Gao, Vineeth Chandran Suja, Suyong Shaha, Rick Liao, Ninad Kumbhojkar, Supriya Prakash, Kyung Soo Park, and Michael Dunne from the Wyss Institute and SEAS; Masen Boucher, Kaitlyn Warren, Camilo Jaimes, and Rebekah Mannix from Boston Children's Hospital; Neha Kapate and Morgan Janes from the Wyss Institute, SEAS, and MIT; Bolu Ilelaboye, Andrew Lu, and Solomina Darko from SEAS; and former SEAS members Tao Sun and John Clegg.
This research was supported by the US Department of Defense under Grant No. W81XWH-19-2-0011 and the National Science Foundation Graduate Research Fellowship under Grant Nos. 1122374 and 1745302.

","score: 15.59818290585508, grade_level: '16'","score: 17.387785089359156, grade_levels: ['college_graduate'], ages: [24, 100]",10.1126/scitranslmed.adk5413,"The choroid plexus (ChP) of the brain plays a central role in orchestrating the recruitment of peripheral leukocytes into the central nervous system (CNS) through the blood-cerebrospinal fluid (BCSF) barrier in pathological conditions, thus offering a unique niche to diagnose CNS disorders. We explored whether magnetic resonance imaging of the ChP could be optimized for mild traumatic brain injury (mTBI). mTBI induces subtle, yet influential, changes in the brain and is currently severely underdiagnosed. We hypothesized that mTBI induces sufficient alterations in the ChP to cause infiltration of circulating leukocytes through the BCSF barrier and developed macrophage-adhering gadolinium [Gd(III)]–loaded anisotropic micropatches (GLAMs), specifically designed to image infiltrating immune cells. GLAMs are hydrogel-based discoidal microparticles that adhere to macrophages without phagocytosis. We present a fabrication process to prepare GLAMs at scale and demonstrate their loading with Gd(III) at high relaxivities, a key indicator of their effectiveness in enhancing image contrast and clarity in medical imaging. In vitro experiments with primary murine and porcine macrophages demonstrated that GLAMs adhere to macrophages also under shear stress and did not affect macrophage viability or functions. Studies in a porcine mTBI model confirmed that intravenously administered macrophage-adhering GLAMs provide a differential signal in the ChP and lateral ventricles at Gd(III) doses 500- to 1000-fold lower than those used in the current clinical standard Gadavist. Under the same mTBI conditions, Gadavist did not offer a differential signal at clinically used doses. Our results suggest that macrophage-adhering GLAMs could facilitate mTBI diagnosis."
"
A new study from researchers at the University of Colorado Anschutz Medical Campus finds that older adult drivers who are recently diagnosed with migraines are three times as likely to experience a motor vehicle crash. Older adult drivers who reported having ever had migraines in the past were no more likely to have a motor vehicle crash than those without migraines.

Additionally, study results, published in the Journal of the American Geriatrics Society, explored the relationships medications commonly prescribed for migraine management have with increased crash risk.
""Migraine headaches affect more than 7% of U.S. adults over the age of 60,"" says Carolyn DiGuiseppi, MPH, PhD, MD, professor with the Colorado School of Public Health and study lead author. ""The US population is aging, which means increasing numbers of older adult drivers could see their driving abilities affected by migraine symptoms previously not experienced. These symptoms include sleepiness, decreased concentration, dizziness, debilitating head pain and more.""
Researchers conducted a five-year longitudinal study of more than 2,500 active drivers aged 65-79 in five sites across the United States. Participants were categorized as having previously been diagnosed with migraine symptoms (12.5%), no previous diagnosis but experienced symptoms during the study timeframe (1.3%) or never migraine respondents. Results indicate those with previous diagnosis did not have a different likelihood of having crashes after baseline, while those with new onset migraines were three times as likely to experience a crash within one year of diagnosis. However, previously diagnosed drivers experienced more hard braking events compared to adults who had never experienced a migraine.
Additionally, researchers examined the role medications commonly prescribed for migraines have in motor vehicle events and found that there was no impact on the relationship between migraines and either crashes or driving habits. However, few participants in the study sample were using acute migraine medications.
""These results have potential implications for the safety of older patients that should be addressed,"" says DiGuiseppi. ""Patients with a new migraine diagnosis would benefit from talking with their clinicians about driving safety, including being extra careful about other risks, such as distracted driving, alcohol, pain medication and other factors that affect driving.""

","score: 16.428985167837627, grade_level: '16'","score: 17.94536299765808, grade_levels: ['college_graduate'], ages: [24, 100]",10.1111/jgs.18719,"Migraine headache is common in older adults, often causing symptoms that may affect driving safety. This study examined associations of migraine with motor vehicle crashes (MVCs) and driving habits in older drivers and assessed modification of associations by medication use. In a multi‐site, prospective cohort study of active drivers aged 65–79 (53% female), we assessed prevalent migraine (i.e., ever had migraine, reported at enrollment), incident migraine (diagnosis first reported at a follow‐up visit), and medications typically used for migraine prophylaxis and treatment. During 2‐year follow‐up, we recorded self‐reported MVCs and measured driving habits using in‐vehicle GPS devices. Associations of prevalent migraine with driving outcomes were estimated in multivariable mixed models. Using a matched design, associations of incident migraine with MVCs in the subsequent year were estimated with conditional logistic regression. Interactions between migraine and medications were tested in all models. Of 2589 drivers, 324 (12.5%) reported prevalent migraine and 34 (1.3%) incident migraine. Interactions between migraine and medications were not statistically significant in any models. Prevalent migraine was not associated with MVCs in the subsequent 2 years (adjusted OR [aOR] = 0.98; 95% CI: 0.72, 1.35), whereas incident migraine significantly increased the odds of having an MVC within 1 year (aOR = 3.27; 1.21, 8.82). Prevalent migraine was associated with small reductions in driving days and trips per month and increases in hard braking events in adjusted models. Our results suggest substantially increased likelihood of MVCs in the year after newly diagnosed migraine, indicating a potential need for driving safety interventions in these patients. We found little evidence for MVC risk or substantial changes in driving habits associated with prevalent migraine. Future research should examine timing, frequency, and severity of migraine diagnosis and symptoms, and use of medications specifically prescribed for migraine, in relation to driving outcomes."
"
Acetaminophen is considered the safest over-the-counter pain reliever and fever reducer available during pregnancy. Studies have shown that 50%-65% of women in North America and Europe take acetaminophen during pregnancy. A new study from researchers at the University of Illinois Urbana-Champaign explored the relationship between acetaminophen use during pregnancy and language outcomes in early childhood. It found that increasing acetaminophen use was associated with language delays.

The findings are reported in the journal Pediatric Research.
Earlier studies have found associations between acetaminophen use during pregnancy and poorer child communication skills. But those studies used measures of language development that were less precise than the methods applied in the current study, said Megan Woodbury, who led the research as a graduate student with U. of I. comparative biosciences professor emerita Susan Schantz. The work was conducted as part of the Illinois Kids Development Study, which explores how environmental exposures in pregnancy and childhood influence child development. Schantz is the IKIDS principal investigator. Woodbury is now a postdoctoral researcher at Northeastern University in Boston.
""The previous studies had only asked pregnant people at most once a trimester about their acetaminophen use,"" Woodbury said. ""But with IKIDS, we talked to our participants every four to six weeks during pregnancy and then within 24 hours of the kid's birth, so we had six time points during pregnancy.""
The language analyses involved 298 2-year-old children who had been followed prenatally, 254 of whom returned for further study at age 3.
For the 2-year-olds, the researchers turned to the MacArthur-Bates Communicative Development Inventories, which asks a parent to report on the child's vocabulary, language complexity and the average length of the child's longest three utterances.
""We wanted to collect data at that age because it's the period called 'word explosion,' when kids are just adding words every day to their vocabulary,"" Schantz said.

The vocabulary measure asked parents to select words their child had used from a list of 680 words.
The parents assessed their child again at 3 years, comparing their language skills to those of their peers.
The analysis linked acetaminophen use in the second and third trimesters of pregnancy to modest but significant delays in early language development.
""We found that increased use of acetaminophen -- especially during the third trimester -- was associated with smaller vocabulary scores and shorter 'mean length of utterance' at two years,"" Woodbury said.
""At age three, greater acetaminophen use during the third trimester was related to parents ranking their kids as lower than their peers on their language abilities,"" Schantz said. ""That outcome was seen primarily in male children.""
The most dramatic finding was that each use of acetaminophen in the third trimester of pregnancy was associated with an almost two-word reduction in vocabulary in the 2-year-olds.

""This suggests that if a pregnant person took acetaminophen 13 times -- or once per week -- during the third trimester of that pregnancy, their child might express 26 fewer words at age 2 than other children that age,"" Woodbury said.
Fetal brain development occurs throughout pregnancy, but the second and third trimesters are especially critical times, Schantz said.
""Hearing is developing in the second trimester, but language development is already starting in the third trimester before the baby is even born,"" she said.
""It's thought that acetaminophen exerts its analgesic effect through the endocannabinoid system, which is also very important for fetal development,"" Woodbury said.
The findings need to be tested in larger studies, the researchers said. Until then, people should not be afraid to take acetaminophen for fever or serious pain and discomfort during pregnancy. Conditions like a very high fever can be dangerous and using a drug like acetaminophen will likely help.
""There aren't other options for people to take when they really need them,"" Schantz said. ""But perhaps people should use more caution when turning to the drug to treat minor aches and pains.""
This work was supported by the Children's Environmental Health and Disease Prevention Research Center funded by the National Institute of Environmental Health Sciences and the U.S. Environmental Protection Agency and the National Institutes of Health Environmental Influences on Child Health Outcomes program.

","score: 13.812358870967746, grade_level: '14'","score: 14.654467917251047, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41390-023-02924-4,"Acetaminophen is the only analgesic considered safe for use throughout pregnancy. Recent studies suggest that use during pregnancy may be associated with poorer neurodevelopmental outcomes in children, but few have examined language development. The Illinois Kids Development Study is a prospective birth cohort in east-central Illinois. Between December 2013 and March 2020, 532 newborns were enrolled and had exposure data available. Participants reported the number of times they took acetaminophen six times across pregnancy. Language data were collected at 26.5–28.5 months using the MacArthur-Bates Communicative Development Inventories (CDI; n = 298), and 36–38 months using the Speech and Language Assessment Scale (SLAS; n = 254). Taking more acetaminophen during the second or third trimester was associated with marginally smaller vocabularies and shorter utterance length (M3L) at 26.5–28.5 months. More acetaminophen use during the third trimester was also associated with increased odds of M3L scores ≤25th percentile in male children. More use during the second or third trimester was associated with lower SLAS scores at 36–38 months. Third trimester use was specifically related to lower SLAS scores in male children. Higher prenatal acetaminophen use during pregnancy may be associated with poorer early language development. Taking more acetaminophen during pregnancy, particularly during the second and third trimesters, was associated with poorer scores on measures of language development when children were 26.5–28.5 and 36–38 months of age. Only male children had lower scores in analyses stratified by child sex. To our knowledge, this is the first study that has used a standardized measure of language development to assess the potential impact of prenatal exposure to acetaminophen on language development. This study adds to the growing body of literature suggesting that the potential impact of acetaminophen use during pregnancy on fetal neurodevelopment should be carefully evaluated. Taking more acetaminophen during pregnancy, particularly during the second and third trimesters, was associated with poorer scores on measures of language development when children were 26.5–28.5 and 36–38 months of age. Only male children had lower scores in analyses stratified by child sex. To our knowledge, this is the first study that has used a standardized measure of language development to assess the potential impact of prenatal exposure to acetaminophen on language development. This study adds to the growing body of literature suggesting that the potential impact of acetaminophen use during pregnancy on fetal neurodevelopment should be carefully evaluated."
"
A previously unidentified genetic mutation in a small protein provides significant protection against Parkinson's disease and offers a new direction for exploring potential treatments, according to a new USC Leonard Davis School of Gerontology study.

The variant, located in a mitochondrial microprotein dubbed SHLP2, was found to be highly protective against Parkinson's disease; individuals with this mutation are half as likely to develop the disease as those who do not carry it. The variant form of the protein is relatively rare and is found primarily in people of European descent.
The findings appear on January 3, 2024, in the journal Molecular Psychiatry.
First discovered by Pinchas Cohen at the USC Leonard Davis School in 2016, SHLP2 is made within the cell's mitochondria. Previous research from the Cohen Lab established that SHLP2 is associated with protection from aging-related diseases including cancer and that levels of the microprotein change in patients with Parkinson's disease; they rise as the body attempts to counteract the pathology of Parkinson's disease but often fail to mount additional production as the disease progresses.
This latest finding builds upon the USC team's prior mitochondrial research and represents an advance at the intersection of longevity science, precision health, and microprotein discovery.
""This study advances our understanding of why people might get Parkinson's and how we might develop new therapies for this devastating disease,"" said Cohen, professor of gerontology, medicine and biological sciences and senior author of the study. ""Also, because most research is done on well-established protein-coding genes in the nucleus, it underscores the relevance of exploring mitochondrial-derived microproteins as a new approach to the prevention and treatment of diseases of aging.""
For this study, first author Su-Jeong Kim, an adjunct research assistant professor of gerontology at the USC Leonard Davis School, led a series of experiments that leveraged the Lab-developed microprotein discovery pipeline that begins with a big data-driven analysis to identify variants involved in disease. Thousands of human study subjects from the Health & Retirement Study, Cardiovascular Health Study, and Framingham Heart Study were screened for the SHLP2 variant. By comparing genetic variants in the mitochondrial DNA in patients with Parkinson's disease and in controls, researchers found a highly protective variant found in 1% of Europeans, that reduced risk of Parkinson's disease by twofold, to 50% of average.

Next, they demonstrated that this naturally occurring variant results in a change to the amino acid sequence and protein structure of SHLP2. The mutation -- a single nucleotide polymorphism (SNP), or a change to a single letter of the protein's genetic code -- is essentially a ""gain-of-function"" variant that is associated with higher expression of SHLP2 and also makes the microprotein more stable. According to their findings, the SHLP2 variant has high stability compared to the more common type and provides enhanced protection against mitochondrial dysfunction.
The research team was able to use targeted mass spectrometry techniques to identify the tiny peptide's presence in neurons and found that SHLP2 specifically binds to an enzyme in mitochondria called mitochondrial complex 1. This enzyme is essential for life, and declines in its function have been linked not only to Parkinson's disease but also to strokes and heart attacks.
The increased stability of the SHLP2 variant means that the microprotein binds to mitochondrial complex 1 more stably, prevents the decline of the enzyme's activity, and thus reduces mitochondrial dysfunction. The benefits of the mutant form of SHLP2 were observed in both in vitro experiments in human tissue samples as well as in mouse models of Parkinson's disease, according to the study.
""Our data highlights the biological effects of a particular gene variant and the potential molecular mechanisms by which this mutation may reduce the risk for Parkinson's disease,"" said Kim. ""These findings may guide the development of therapies and provide a roadmap for understanding other mutations found in mitochondrial microproteins.""
Coauthors included Brendan Miller, Nicolas G. Hartel, Ricardo Ramirez II, Regina Gonzalez Braniff, Naphada Leelaprachakul, Amy Huang, Yuzhu Wang, Thalida Em Arpawong, Eileen M. Crimmins, Kelvin Yen, Giselle M. Petzinger, Michael W. Jakowec, and Nicholas A. Graham of USC; Penglong Wang and Chunyu Liu of the National Heart, Lung, and Blood Institute, National Institutes of Health; and Xianbang Sun and Daniel Levy of Boston University.
This work was supported by Department of Defense grant W81XWH2110625 to Kim and by NIH grants P01AG034906, R01AG068405 and P30AG068345 to Cohen. Pinchas Cohen is a consultant of CohBar Inc.

","score: 17.304733333333335, grade_level: '17'","score: 19.641560000000005, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41380-023-02344-0,"Mitochondrial DNA single nucleotide polymorphisms (mtSNPs) have been associated with a reduced risk of developing Parkinson’s disease (PD), yet the underlying mechanisms remain elusive. In this study, we investigate the functional role of a PD-associated mtSNP that impacts the mitochondrial-derived peptide (MDP) Small Humanin-like Peptide 2 (SHLP2). We identify m.2158 T > C, a mtSNP associated with reduced PD risk, within the small open reading frame encoding SHLP2. This mtSNP results in an alternative form of SHLP2 (lysine 4 replaced with arginine; K4R). Using targeted mass spectrometry, we detect specific tryptic fragments of SHLP2 in neuronal cells and demonstrate its binding to mitochondrial complex 1. Notably, we observe that the K4R variant, associated with reduced PD risk, exhibits increased stability compared to WT SHLP2. Additionally, both WT and K4R SHLP2 show enhanced protection against mitochondrial dysfunction in in vitro experiments and confer protection against a PD-inducing toxin, a mitochondrial complex 1 inhibitor, in a mouse model. This study sheds light on the functional consequences of the m.2158 T > C mtSNP on SHLP2 and provides insights into the potential mechanisms by which this mtSNP may reduce the risk of PD."
"
New Cornell University-led research finds that social media platforms and the metrics that reward content creators for revealing their innermost selves to fans open creators up to identity-based harassment.

""Creators share deeply personal -- often vulnerable -- elements of their lives with followers and the wider public,"" said Brooke Erin Duffy, associate professor of communication. ""Such disclosures are a key way that influencers build intimacy with audiences and form communities. There's a pervasive sense that internet users clamor for less polished, less idealized, more relatable moments -- especially since the pandemic.""
Duffy is the lead author of ""Influencers, Platforms, and the Politics of Vulnerability"" published in the European Journal of Cultural Studies.
The research team conducted in-depth interviews with content creators to get a sense of how they experience the demands to make their content -- and often themselves -- visible to audiences, sponsors and the platforms.
Among their findings: The value of vulnerability for platform-based influencers cannot be overstated -- authenticity sells, and that means projecting intimacies, insecurities and even secrets; These authentic revelations are often tied to one's identities, which can open a person up to attacks based on gender, race, sexuality and other perceived traits; Personal and social vulnerabilities were often compounded by the vulnerabilities of platform-dependent labor: Not only did participants identify the failures of their platforms to protect them from harm (as ""contractors"" instead of ""employees""), many felt these companies incentivize networked antagonism.""Influencers and creators have relatively few formal sources of support or protection,"" Duffy said. ""In contrast to those legally employed by Meta, Twitch and TikTok, creators are independent contractors. They're left wanting for a lot of the workplace protections traditionally afforded to employees.""
The researchers examined informal strategies -- both anticipatory and reactive -- that creators deploy to manage their vulnerabilities. The former included the use of platform filtering systems to sift out abusive, profane or hurtful language. The latter strategies ranged from simply not reading the comments to employing the platform's tools to minimize the impact of what, for many, felt like an inevitable onslaught of critique.
The authors acknowledge the difficulties of resolving endemic issues of internet hate and harassment. ""'Getting off the internet' is hardly a viable option for participants in the put-yourself-out-there neoliberal job economy,"" they wrote -- and offer a warning to those wishing to join the creator economy.
""It is something of a truism that 'everyone gets the same platform,'"" they wrote. ""We would caution, however, that the politics of visibility -- and hence, the politics of vulnerability -- are far less egalitarian that platforms lead us to believe.""

","score: 16.34158371040724, grade_level: '16'","score: 17.62418552036199, grade_levels: ['college_graduate'], ages: [24, 100]",10.1177/13675494231212346,"While workers of all stripes are compelled to embrace uncertainty under conditions of neoliberalism, ideologies of risk assume a particular guise in the platform economy, wherein laborers are exhorted to ‘put yourself out there’. Given the attendant harms associated with public visibility – especially for women and other marginalized groups – it seems crucial to explore platform-dependent laborers’ experiences of ‘putting themselves out there’. This article draws upon in-depth interviews with 23 social media influencers and content creators, sampled from across platforms, content niches and subjectivities. Our analysis revealed that vulnerability is a structuring concept in the influencer economy – one that operates at multiple, often overlapping levels. First, the commercial logic of authenticity casts personal vulnerability as a strategy for building community and accruing followers. But influencers’ individual disclosures were often entangled with their social identities (e.g., gender, race, sexuality, ability and body type), which rendered them socially vulnerable to targeted antagonism from audiences﻿. Interviewees experienced a range of harms, from identity-based hate and harassment to concerted take-down campaigns. These personal and social vulnerabilities were compounded by the vulnerabilities of platform-dependent labor: not only did participants identify the failures of platforms to protect them, some shared a sense that these companies exacerbated harms through a commercial logic that incentivizes antagonism. After examining the emotional labor necessary to manage such platform vulnerabilities, we close by reiterating the unique precarity of platform labor, wherein participants lack the social and legal protections typically afforded to ‘vulnerable workers’."
"
Throughout the day and night, cerebrospinal fluid (CSF) pulses through small fluid-filled channels surrounding blood vessels in the brain, called perivascular spaces, to flush out neuroinflammation and other neurological waste. A disruption to this vital process can lead to neurological dysfunction, cognitive decline, or developmental delays.

For the first time, researchers Dea Garic, PhD, and Mark Shen, PhD, both at the UNC School of Medicine's Department of Psychiatry, discovered that infants with abnormally enlarged perivascular spaces have a 2.2 times greater chance of developing autism compared to infants with the same genetic risk. Their research also indicated that enlarged perivascular spaces in infancy are associated with sleep problems seven to 10 years after diagnosis.
""These results suggest that perivascular spaces could serve as an early marker for autism,"" said Garic, assistant professor of psychiatry and a member of the Carolina Institute for Developmental Disabilities (CIDD).
The researchers studied infants at increased likelihood for developing autism, because they had an older sibling with autism. They followed these infants from 6-24 months of age, before the age of autism diagnosis. Their study, published in JAMA Network Open, found that thirty percent of infants who later developed autism had enlarged perivascular spaces by 12 months. By 24 months of age, nearly half of the infants diagnosed with autism had enlarged perivascular spaces.
The Importance of Cerebrospinal Fluid and Sleep
Starting ten years ago, there has been a resurgence of research on the important functions of CSF in regulating brain health and development. Shen's lab was the first to report that excessive volume of CSF was evident at 6 months of age in infants who would later develop autism. The current study showed that excessive CSF volume at 6 months was linked to enlarged perivascular spaces at 24 months.
Every six hours, the brain expels a wave of CSF that flows through perivascular spaces to remove potentially harmful neuroinflammatory proteins, such as amyloid beta, from building up in the brain. The CSF cleansing process is especially efficient when we are asleep, as the majority of CSF circulation and clearance occurs during sleep.

Disrupted sleep, however, can reduce CSF clearance from perivascular spaces, leading to dilation or enlargement, but this has previously only been studied in animal studies or in human studies of adults. This is the first study of its kind in children.
Shen, senior author of the JAMA Network Open paper, and Garic hypothesized that CSF abnormalities in infancy would be related to later sleep problems, based on Shen's earlier research. The current sleep analysis revealed children who had enlarged perivascular spaces at two years of age had higher rates of sleep disturbances at school age.
""Since autism is so highly linked with sleep problems, we were in this unique position to examine CSF dynamics and sleep,"" said Garic, who is first author of the paper. ""It was really striking to observe such a strong association separated by such a long period of time over childhood. But it really shows how perivascular spaces not only have an effect early in life, but they can have long term effects, too.""
New Clinical Relevance in Infancy
The research was done in conjunction with the Infant Brain Imaging Study (IBIS), a nationwide network of researchers investigating brain development, autism, and related developmental disabilities. The network consists of five universities, of which the University of North Carolina-Chapel Hill is the lead site.
For their study, Garic and Shen analyzed 870 MRIs from IBIS to measure excessive CSF volume and enlarged perivascular spaces. MRIs were obtained from babies during natural sleep at six, 12, and 24 months of age to observe changes over time.

The infant brain undergoes rapid development over this period. Previously, measurement of perivascular spaces was only thought to be clinically relevant for disorders of aging in older adults, such as in dementia. These findings suggest that younger populations may need to be considered and monitored for these types of brain abnormalities.
""Our findings were striking, given that neuroradiologists typically view enlarged perivascular spaces as a sign of neurodegeneration in adults, but this study reported it in toddlers,"" said Garic. ""This is an important aspect of brain development in the first years of life that should be monitored.""
Future Studies and Possibilities
Garic and Shen hypothesize that excess CSF volume is stagnant, or clogged, and not circulating through the brain as efficiently as it should. For their next research endeavor, the researchers are planning to once again use MRIs to measure CSF in a sleeping infant's brain, but this time focusing on the physiology and speed of CSF flow throughout the brain.
The research team is also working with other collaborators to quantify the size of perivascular spaces and the severity of behavioral outcomes. The team also plans to extend their research to neurogenetic syndromes associated with autism, such as Fragile X syndrome and Down syndrome.
""Collectively our research has shown that CSF abnormalities in the first year of life could have downstream effects on a variety of outcomes, including later autism diagnosis, sleep problems, neuroinflammation, and possibly, other developmental disabilities,"" said Shen.
This work was supported the National Institutes of Health, Eunice Kennedy Shriver National Institute of Child Health and Human Development (NICHD), National Institute of Mental Health (NIMH), National Institute of Environmental Health Sciences (NIEHS), and the Simons Foundation.
Other researchers on this project include Joseph Piven, MD; Heather C. Hazlett, PhD; Martin Styner, PhD; Sun Hyung Kim, PhD; Joshua Rutsohn, PhD; and Leigh Anne Weisenfeld, MA; of the University of North Carolina -- Chapel Hill; Robert C. McKinstry, MD, PhD; and Kelly N. Botteron, MD; of Washington University in St. Louis; Rebecca Slomowitz, MA; of the University of Denver; Jason Wolff, PhD; of the University of Minnesota; Leigh C. MacIntyre, BSc; of McGill University; Juhi Pandey, PhD; of University of Pennsylvania; and Tanya St. John, PhD; Annette M. Estes, PhD; Robert T. Schultz, PhD; and Stephen R. Dager, MD; of the University of Washington.

","score: 15.346375198728143, grade_level: '15'","score: 16.697762927433914, grade_levels: ['college_graduate'], ages: [24, 100]",10.1001/jamanetworkopen.2023.48341,"Perivascular spaces (PVS) and cerebrospinal fluid (CSF) are essential components of the glymphatic system, regulating brain homeostasis and clearing neural waste throughout the lifespan. Enlarged PVS have been implicated in neurological disorders and sleep problems in adults, and excessive CSF volume has been reported in infants who develop autism. Enlarged PVS have not been sufficiently studied longitudinally in infancy or in relation to autism outcomes or CSF volume. To examine whether enlarged PVS are more prevalent in infants who develop autism compared with controls and whether they are associated with trajectories of extra-axial CSF volume (EA-CSF) and sleep problems in later childhood. This prospective, longitudinal cohort study used data from the Infant Brain Imaging Study. Magnetic resonance images were acquired at ages 6, 12, and 24 months (2007-2017), with sleep questionnaires performed between ages 7 and 12 years (starting in 2018). Data were collected at 4 sites in North Carolina, Missouri, Pennsylvania, and Washington. Data were analyzed from March 2021 through August 2022. PVS (ie, fluid-filled channels that surround blood vessels in the brain) that are enlarged (ie, visible on magnetic resonance imaging). Outcomes of interest were enlarged PVS and EA-CSF volume from 6 to 24 months, autism diagnosis at 24 months, sleep problems between ages 7 and 12 years. A total of 311 infants (197 [63.3%] male) were included: 47 infants at high familial likelihood for autism (ie, having an older sibling with autism) who were diagnosed with autism at age 24 months, 180 high likelihood infants not diagnosed with autism, and 84 low likelihood control infants not diagnosed with autism. Sleep measures at school-age were available for 109 participants. Of infants who developed autism, 21 (44.7%) had enlarged PVS at 24 months compared with 48 infants (26.7%) in the high likelihood but no autism diagnosis group (P = .02) and 22 infants in the control group (26.2%) (P = .03). Across all groups, enlarged PVS at 24 months was associated with greater EA-CSF volume from ages 6 to 24 months (β = 4.64; 95% CI, 0.58-8.72; P = .002) and more frequent night wakings at school-age (F = 7.76; η2 = 0.08; P = .006). These findings suggest that enlarged PVS emerged between ages 12 and 24 months in infants who developed autism. These results add to a growing body of evidence that, along with excessive CSF volume and sleep dysfunction, the glymphatic system could be dysregulated in infants who develop autism."
"
Scientists at the Johns Hopkins University School of Medicine and the National Institutes of Health have identified a protein in the visual system of mice that appears to be key for stabilizing the body's circadian rhythms by buffering the brain's response to light. The finding, published Dec. 5 in PLoS Biology, advances efforts to better treat sleep disorders and jet lag, the study authors say.

""If circadian rhythms adjusted to every rapid change in illumination, say an eclipse or a very dark and rainy day, they would not be very effective in regulating such periodic behaviors as sleep and hunger. The protein we identified helps wire the brain during neural development to allow for stable responses to circadian rhythm challenges from day to day,"" says Alex Kolodkin, Ph.D., professor in the Johns Hopkins Department of Neuroscience and deputy director for the Institute for Basic Biomedical Sciences.
Kolodkin co-led the study with Samer Hattar, Ph.D., chief of the Section on Light and Circadian Rhythms at the National Institute of Mental Health.
Scientists have long known that most living things have a circadian ""clock,"" a set of biological rhythms that operate on about a 24-hour cycle and that affect alertness, sleepiness, appetite and body temperature, among other cyclic behaviors. Upsetting this system -- through shift work or long-distance travel over multiple time and light zones in humans, for example -- can have severe consequences. Previous studies link persistent upsets in circadian rhythm to increased risk of cancer, depression and a host of other medical problems.
Circadian systems are essentially ""trained"" by exposure to light. Although researchers have made significant headway over the last few decades in outlining the mechanisms responsible for circadian rhythms, it has remained unclear how the brain becomes wired for them.
To learn more, Kolodkin and Hattar, along with study first authors John Hunyara and Kat Daly and their colleagues, searched a database for biological molecules present during development in the mouse brain's control center for circadian rhythms -- the suprachiasmatic nucleus (SCN).
Located deep within both the mouse and human brain in the hypothalamus, the SCN sits near areas that control vision and makes connections with brain cells that lead to the retina, the light-sensing part of the eye.

The research team quickly zeroed in on a cell surface protein called teneurin-3 (Tenm3), part of a larger family of proteins that play key roles in the visual system circuit assembly and more generally in other central nervous system circuits.
When the researchers genetically altered mice to prevent Tenm3 production, the animals developed fewer connections between the retina and the SCN, compared with animals with intact Tenm3. However, the mice lacking Tenm3 developed far more connectivity between cells in the core and shell of the SCN, where Tenm3 tends to localize.
To see how Tenm3 might stabilize circadian rhythms or subject them to disruption by even a tiny bit of light, the scientists designed a set of experiments.
First, they trained mice lacking Tenm3 on a 12-hour light/dark cycle, then shifted the dark period ahead by six hours. Mice with intact Tenm3 took about four days to readjust their circadian rhythms to the shift, as measured by activity patterns diagnostic of normal sleep cycles. The animals without Tenm3, however, adjusted far more rapidly, in about half the time.
When the researchers performed a similar experiment with light twice as dim as in the earlier test, it took the Tenm3-intact mice about eight days to adjust their circadian cycles, but only about four days for the mice without Tenm3. Even just a 15-minute pulse of dim light triggered the Tenm3-lacking mice -- but not the mice with normal Tenm3 protein -- to produce a brain chemical that serves as a proxy for light exposure, suggesting a heightened sensitivity to light cues necessary for setting or resetting the circadian clock.
These findings suggest to the authors that Tenm3 helps wire the brain to maintain stable circadian rhythms even when light exposure is variable. By learning more about this system and Tenm3's role, says Hattar, researchers may eventually be able to diagnose and treat glitches that lead to insomnia and other sleep disorders in people, or possibly develop treatments for jet lag.
""There are very clear implications for human health,"" he says.
Other Johns Hopkins researchers who contributed to this study include Katherine Torres.
This study was funded by grants from the NIH (R01EY032095) and the Intramural Research Program at the NIMH (ZIAMH002964).

","score: 14.712417582417586, grade_level: '15'","score: 16.701367632367635, grade_levels: ['college_graduate'], ages: [24, 100]",10.1371/journal.pbio.3002412,"Visual system function depends upon the elaboration of precise connections between retinal ganglion cell (RGC) axons and their central targets in the brain. Though some progress has been made in defining the molecules that regulate RGC connectivity required for the assembly and function of image-forming circuitry, surprisingly little is known about factors required for intrinsically photosensitive RGCs (ipRGCs) to target a principal component of the non-image-forming circuitry: the suprachiasmatic nucleus (SCN). Furthermore, the molecules required for forming circuits critical for circadian behaviors within the SCN are not known. We observe here that the adhesion molecule teneurin-3 (Tenm3) is highly expressed in vasoactive intestinal peptide (VIP) neurons located in the core region of the SCN. Since Tenm3 is required for other aspects of mammalian visual system development, we investigate roles for Tenm3 in regulating ipRGC-SCN connectivity and function. Our results show that Tenm3 negatively regulates association between VIP and arginine vasopressin (AVP) neurons within the SCN and is essential for M1 ipRGC axon innervation to the SCN. Specifically, in Tenm3-/- mice, we find a reduction in ventro-medial innervation to the SCN. Despite this reduction, Tenm3-/- mice have higher sensitivity to light and faster re-entrainment to phase advances, probably due to the increased association between VIP and AVP neurons. These data show that Tenm3 plays key roles in elaborating non-image-forming visual system circuitry and that it influences murine responses to phase-advancing light stimuli."
"
Researchers have identified a wide range of risk factors for young-onset dementia. The findings challenge the notion that genetics are the sole cause of the condition, laying the groundwork for new prevention strategies.

The largescale study identified 15 risk factors, which are similar to those for late-onset dementia. For the first time, they indicate that it may be possible to reduce the risk of young-onset dementia by targeting health and lifestyle factors.
Relatively little research has been done on young-onset dementia, though globally there are around 370,000 new cases of young-onset dementia each year.
Published in JAMA Neurology, the new research by the University of Exeter and Maastricht University followed more than 350,000 participants younger than 65 across the United Kingdom from the UK Biobank study. The team evaluated a broad array of risk factors ranging from genetic predispositions to lifestyle and environmental influences. The study revealed that lower formal education, lower socioeconomic status, genetic variation, lifestyle factors such as alcohol use disorder and social isolation, and health issues including vitamin D deficiency, depression, stroke, hearing impairment and heart disease significantly elevate risk of young-onset dementia
Professor David Llewellyn of the University of Exeter emphasized the importance of the findings: ""This breakthrough study illustrates the crucial role of international collaboration and big data in advancing our understanding of dementia. There's still much to learn in our ongoing mission to prevent, identify, and treat dementia in all its forms in a more targeted way. This is the largest and most robust study of its kind ever conducted. Excitingly, for the first time it reveals that we may be able to take action to reduce risk of this debilitating condition, through targeting a range of different factors.
Dr Stevie Hendriks, Researcher at Maastricht University, said: ""Young-onset dementia has a very serious impact, because the people affected usually still have a job, children, and a busy life. The cause is often assumed to be genetic, but for many people we don't actually know exactly what the cause is. This is why we also wanted to investigate other risk factors in this study.""
Sebastian Köhler, Professor of Neuroepidemiology at Maastricht University, said: ""We already knew from research on people who develop dementia at older age that there are a series of modifiable risk factors. In addition to physical factors, mental health also plays an important role, including avoiding chronic stress, loneliness and depression. The fact that this is also evident in young-onset dementia came as a surprise to me, and it may offer opportunities to reduce risk in this group too.""
The study's support was supported by Alzheimer's Research UK, The Alan Turing Institute/Engineering and Physical Sciences Research Council, Alzheimer Nederland, Gieskes Strijbis Fonds, the Medical Research Council, the National Institute for Health and Care Research (NIHR) Applied Research Collaboration South West Peninsula (PenARC), the National Health and Medical Research Council, the National Institute on Aging, and Alzheimer Netherlands.

Dr Janice Ranson, Senior Research Fellow at the University of Exeter, said: ""Our research breaks new ground in identifying that the risk of young-onset dementia can be reduced. We think this could herald a new era in interventions to reduce new cases of this condition.""
Dr Leah Mursaleen, Head of Clinical Research at Alzheimer's Research UK, which co-funded the study, said: ""We're witnessing a transformation in understanding of dementia risk and, potentially, how to reduce it on both an individual and societal level. In recent years, there's been a growing consensus that dementia is linked to 12 specific modifiable risk factors such as smoking, blood pressure and hearing loss . It's now accepted that up to four in 10 dementia cases worldwide are linked to these factors.
""This pioneering study shines important and much-needed light on factors that can influence the risk of young-onset dementia. This starts to fill in an important gap in our knowledge. It will be important to build on these findings in broader studies.'

","score: 15.082967696955613, grade_level: '15'","score: 16.3235335812224, grade_levels: ['college_graduate'], ages: [24, 100]",10.1001/jamaneurol.2023.4929,"There is limited information on modifiable risk factors for young-onset dementia (YOD). To examine factors that are associated with the incidence of YOD. This prospective cohort study used data from the UK Biobank, with baseline assessment between 2006 and 2010 and follow-up until March 31, 2021, for England and Scotland, and February 28, 2018, for Wales. Participants younger than 65 years and without a dementia diagnosis at baseline assessment were included in this study. Participants who were 65 years and older and those with dementia at baseline were excluded. Data were analyzed from May 2022 to April 2023. A total of 39 potential risk factors were identified from systematic reviews of late-onset dementia and YOD risk factors and grouped into domains of sociodemographic factors (education, socioeconomic status, and sex), genetic factors (apolipoprotein E), lifestyle factors (physical activity, alcohol use, alcohol use disorder, smoking, diet, cognitive activity, social isolation, and marriage), environmental factors (nitrogen oxide, particulate matter, pesticide, and diesel), blood marker factors (vitamin D, C-reactive protein, estimated glomerular filtration rate function, and albumin), cardiometabolic factors (stroke, hypertension, diabetes, hypoglycemia, heart disease, atrial fibrillation, and aspirin use), psychiatric factors (depression, anxiety, benzodiazepine use, delirium, and sleep problems), and other factors (traumatic brain injury, rheumatoid arthritis, thyroid dysfunction, hearing impairment, and handgrip strength). Multivariable Cox proportional hazards regression was used to study the association between the risk factors and incidence of YOD. Factors were tested stepwise first within domains and then across domains. Of 356 052 included participants, 197 036 (55.3%) were women, and the mean (SD) age at baseline was 54.6 (7.0) years. During 2 891 409 person-years of follow-up, 485 incident YOD cases (251 of 485 men [51.8%]) were observed, yielding an incidence rate of 16.8 per 100 000 person-years (95% CI, 15.4-18.3). In the final model, 15 factors were significantly associated with a higher YOD risk, namely lower formal education, lower socioeconomic status, carrying 2 apolipoprotein ε4 allele, no alcohol use, alcohol use disorder, social isolation, vitamin D deficiency, high C-reactive protein levels, lower handgrip strength, hearing impairment, orthostatic hypotension, stroke, diabetes, heart disease, and depression. In this study, several factors, mostly modifiable, were associated with a higher risk of YOD. These modifiable risk factors should be incorporated in future dementia prevention initiatives and raise new therapeutic possibilities for YOD."
"
Light in the evening is thought to be bad for sleep. However, does the color of the light play a role? Researchers from the University of Basel and the Technical University of Munich (TUM) compared the influence of different light colors on the human body. The researchers' findings contradict the results of a previous study in mice.

Vision is a complex process. The visual perception of the environment is created by a combination of different wavelengths of light, which are decoded as colours and brightness in the brain. Photoreceptors in the retina first convert the light into electrical impulses: with sufficient light, the cones enable sharp, detailed, and coloured vision. Rods only contribute to vision in low light conditions allowing for different shades of grey to be distinguished but leaving vision much less precise. The electrical nerve impulses are finally transmitted to ganglion cells in the retina and then via the optic nerve to the visual cortex in the brain. This region of the brain processes the neural activity into a coloured image.
What influences the internal clock?
Ambient light however does not only allow us to see, it also influences our sleep-wake rhythm. Specialised ganglion cells are significantly involved in this process, which -- like the cones and rods -- are sensitive to light and react particularly strongly to short-wavelength light at a wavelength of around 490 nanometres. If light consists solely of short wavelengths of 440 to 490 nanometres, we perceive it as blue. If short-wavelength light activates the ganglion cells, they signal to the internal clock that it is daytime. The decisive factor here is how intense the light is per wavelength; the perceived colour is not relevant.
""However, the light-sensitive ganglion cells also receive information from the cones. This raises the question of whether the cones, and thereby the light colour, also influence the internal clock. After all, the most striking changes in brightness and light colour occur at sunrise and sunset, marking the beginning and end of a day,"" says Dr. Christine Blume. At the Centre for Chronobiology of the University of Basel, she investigates the effects of light on humans and is the first author of a study investigating the effects of different light colours on the internal clock and sleep. The team of researchers from the University of Basel and the TUM has now published its findings in the scientific journal ""Nature Human Behaviour."" 
Light colours in comparison
""A study in mice in 2019 suggested that yellowish light has a stronger influence on the internal clock than blueish light,"" says Christine Blume. In humans, the main effect of light on the internal clock and sleep is probably mediated via the light-sensitive ganglion cells. ""However, there is reason to believe that the colour of light, which is encoded by the cones, could also be relevant for the internal clock.""
To get to the bottom of this, the researchers exposed 16 healthy volunteers to a blueish or yellowish light stimulus for one hour in the late evening, as well as a white light stimulus as a control condition. The light stimuli were designed in such a way that they differentially activated the colour-sensitive cones in the retina in a very controlled manner. However, the stimulation of the light-sensitive ganglion cells was the same in all three conditions. Differences in the effect of the light were therefore directly attributable to the respective stimulation of the cones and ultimately the colour of the light.

""This method of light stimulation allows us to separate the light properties that may play a role in how light effects humans in a clean experimental way,"" says Manuel Spitschan, Professor of Chronobiology and Health at the Technical University of Munich, who was also involved in the study.
In order to understand the effects of the different light stimuli on the body, in the sleep laboratory the researchers determined whether the internal clock of the participants had changed depending on the colour of the light. Additionally, they assessed how long it took the volunteers to fall asleep and how deep their sleep was at the beginning of the night. The researchers also enquired about their tiredness and tested their ability to react, which decreases with increasing sleepiness.
Ganglion cells are crucial
The conclusion: ""We found no evidence that the variation of light colour along a blue-yellow dimension plays a relevant role for the human internal clock or sleep,"" says Christine Blume. This contradicts the results of the mouse study mentioned above. ""Rather, our results support the findings of many other studies that the light-sensitive ganglion cells are most important for the human internal clock,"" says the scientist.
Manuel Spitschan sees the study as an important step towards putting basic research into practice: ""Our findings show that it is probably most important to take into account the effect of light on the light-sensitive ganglion cells when planning and designing lighting. The cones and therefore the colour play a very subordinate role.""
It remains to be seen whether the colour of the light also has no effect on sleep if the parameters change and, for example, the duration of the light exposure is extended or takes place at a different time. Follow-up studies should answer questions like these.
Night mode on screens -- useful or not? 
We often hear that the short-wavelength component of light from smartphone and tablet screens affects biological rhythms and sleep. The recommendation is therefore to put your mobile phone away early in the evening or at least use the night shift mode, which reduces the short-wavelength light proportions and looks slightly yellowish. Christine Blume confirms this. However, the yellowish colour adjustment is a by-product that could be avoided. ""Technologically, it is possible to reduce the short-wavelength proportions even without colour adjustment of the display, however this has not yet been implemented in commercial mobile phone displays,"" says the sleep researcher.

","score: 12.251984413270986, grade_level: '12'","score: 13.336313738588288, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41562-023-01791-7,"Evening exposure to short-wavelength light can affect the circadian clock, sleep and alertness. Intrinsically photosensitive retinal ganglion cells expressing melanopsin are thought to be the primary drivers of these effects. Whether colour-sensitive cones also contribute is unclear. Here, using calibrated silent-substitution changes in light colour along the blue–yellow axis, we investigated whether mechanisms of colour vision affect the human circadian system and sleep. In a 32.5-h repeated within-subjects protocol, 16 healthy participants were exposed to three different light scenarios for 1 h starting 30 min after habitual bedtime: baseline control condition (93.5 photopic lux), intermittently flickering (1 Hz, 30 s on–off) yellow-bright light (123.5 photopic lux) and intermittently flickering blue-dim light (67.0 photopic lux), all calibrated to have equal melanopsin excitation. We did not find conclusive evidence for differences between the three lighting conditions regarding circadian melatonin phase delays, melatonin suppression, subjective sleepiness, psychomotor vigilance or sleep. The Stage 1 protocol for this Registered Report was accepted in principle on 9 September 2020. The protocol, as accepted by the journal, can be found at https://doi.org/10.6084/m9.figshare.13050215.v1."
"
There does not appear to be any profound differences between so-called exposure-based CBT and traditional CBT in the treatment of fibromyalgia, according to a study led by researchers at Karolinska Institutet. Both forms of treatment produced a significant reduction in symptoms in people affected by the disease. The study is one of the largest to date to compare different treatment options for fibromyalgia and is published in the journal PAIN.

About 200,000 people in Sweden currently live with fibromyalgia, a long-term pain syndrome that causes great suffering for patients through widespread pain, fatigue, and stiffness in the body. There is no cure for fibromyalgia. Existing drugs often have insufficient effect, raising the need for more effective treatment methods. Cognitive behavioral therapy (CBT) has shown some effect, but there is a lack of trained CBT practitioners. There is also a lack of knowledge about which form of CBT is most effective. The study compared two different forms of internet-delivered cognitive behavioral therapy in terms of how well they reduce the symptoms and functional impact of fibromyalgia.
In brief, exposure-based CBT involves the participant systematically and repeatedly approaching situations, activities, and stimuli that the patient has previously avoided because the experiences are associated with pain, psychological discomfort, or symptoms such as fatigue and cognitive problems.
In traditional CBT, the participant is presented with several different strategies to work on during treatment, such as relaxation, activity planning, physical exercise, or strategies for managing negative thoughts and improving sleep.
The study showed that traditional CBT was by and large equivalent to the newer treatment form of exposure-based CBT.
""This result was surprising because our hypothesis, based on previous research, was that the new exposure-based form would be more effective. Our study shows that the traditional form can provide an equally good result and thus contributes to the discussion in the field,"" says Maria Hedman-Lagerlöf, licensed psychologist and researcher at the Center for Psychiatry Research at the Department of Clinical Neuroscience, Karolinska Institutet.
The randomized study involved 274 people with fibromyalgia, who were randomly assigned to be treated with traditional or exposure-based CBT. The treatments were delivered entirely online and all participants had regular contact with their therapist.

Participants answered questions about their mood and symptoms before, during, and after treatment. After the 10-week treatment, 60 percent of those who received exposure-based CBT and 59 percent of those who received traditional CBT reported that their treatment had helped them.
""The fact that both treatments were associated with a significant reduction in the participants' symptoms and functional impairment and that the effects were sustained for 12 months after completion of the treatment, indicates that the internet as a treatment format can be of great clinical benefit for people with fibromyalgia,"" says Maria Hedman-Lagerlöf. ""This is good news because it enables more people to access treatment.""
The study is the second largest to compare different psychological treatment options for fibromyalgia, according to the researchers.
""Our study is also one of the first to compare with another active, established psychological treatment,"" says Maria Hedman-Lagerlöf.

","score: 15.301571050308919, grade_level: '15'","score: 16.796875551632837, grade_levels: ['college_graduate'], ages: [24, 100]",10.1097/j.pain.0000000000003128,"Fibromyalgia is a debilitating pain condition for which treatment effects are typically modest. The most evaluated psychological treatment is traditional cognitive behavior therapy (T-CBT), but promising effects have recently been seen in exposure-based cognitive behavior therapy (Exp-CBT). We investigated whether Exp-CBT was superior to T-CBT in a randomized controlled trial. Self-referred participants with fibromyalgia (N = 274) were randomized (1:1) to 10 weeks of Exp-CBT or T-CBT. Treatments were delivered online and presented as “CBT for fibromyalgia.” Participants were assessed at baseline, weekly during treatment, posttreatment, and at 6- and 12-month follow-up. Primary outcome was the difference in reduction in fibromyalgia severity as measured using the Fibromyalgia Impact Questionnaire (FIQ) over 11 assessment points from baseline to posttreatment, modelled within an intention-to-treat framework using linear mixed effects models fitted on multiple imputed data. Approximately 91% of weekly FIQ scores were collected over the main phase. There was no significant difference between Exp-CBT and T-CBT in the mean reduction of fibromyalgia severity from pretreatment to posttreatment (b = 1.3, 95% CI −3.0 to 5.7, P = 0.544, d = −0.10). Minimal clinically important improvement was seen 60% in Exp-CBT vs 59% in T-CBT. Effects were sustained up to 12 months posttreatment. This well-powered randomized trial indicated that Exp-CBT was not superior to T-CBT for fibromyalgia. Both treatments were associated with a marked reduction in fibromyalgia severity, and the online treatment format might be of high clinical utility. T-CBT can still be regarded a reference standard treatment that remains clinically relevant when compared to novel treatment approaches."
"
Researchers at Nagoya University's Graduate School of Bioagricultural Sciences and the National Institute of Physiological Sciences in Japan have demonstrated how a specific type of neuron in the brain affects the release of hormones that control ovarian function, such as follicular development and ovulation in females. These findings, published in the journal Scientific Reports, could help researchers understand and treat reproductive disorders in both animals and humans.

Kisspeptin neurons in the brain regulate the release of hypothalamic gonadotropin-releasing hormone (GnRH) and pituitary follicle-stimulating hormone/luteinizing hormone (LH). This process is important for reproduction, as pituitary hormones stimulate the ovaries to perform their reproductive functions. Examples include follicular development and ovulation in all mammals, including humans.
There are two main areas of the brain involved in the process: the arcuate nucleus (ARC), in which kisspeptin neurons maintain the regular rhythmic (pulsatile) secretion of GnRH/LH that maintains normal follicular development and sex steroid production; and the anteroventral periventricular nucleus (AVPV), in which kisspeptin neurons trigger a surge of GnRH/LH that leads to ovulation.
The researchers focused on the fact that kisspeptin neurons in the ARC produce and respond to dynorphin, an inhibitory substance. ""Kisspeptin neurons in the ARC express both dynorphin and its receptor, whereas those in the AVPV express the receptor only, suggesting a particular role of such kisspeptin neurons in fertilization,"" Mayuko Nagae, a postdoctoral fellow, and Yoshihisa Uenoyama, an associate professor at Nagoya University in Japan and corresponding author of the paper, explained in a joint statement. ""However, the exact role of dynorphin and its receptor in the regulation of kisspeptin neurons was not clearly understood.""
To investigate this, the researchers genetically modified female rats to delete Kiss1, a gene that codes for kisspeptin, only in neurons that expressed the dynorphin receptor. They found that the genetically modified rats with deleted Kiss1 in dynorphin receptor-expressing cells had only 3% of kisspeptin neurons in the ARC and 50% in the AVPV. The rats were still fertile, but they had a longer estrous cycle, lower ovarian weight, and fewer pups than normal rats.
The results indicate that kisspeptin neurons with dynorphin receptors are important for normal female rat reproduction, as they allow proper hormone secretion and ovulation. ""This is the first report to show that kisspeptin neurons receiving direct input of dynorphin are needed to fully generate the GnRH/LH pulse and surge in female rats,"" says Professor Hiroko Tsukamura from Nagoya University, the principal investigator of the research group and another corresponding author of the paper.
Professor Tsukamura is excited about the prospect of more studies to understand the molecular mechanism that controls kisspeptin neuronal activity. She says, ""Our findings can help our understanding of the central mechanism underlying reproduction and have applications in the treatment of ovarian disorders in livestock and infertility in humans.""

","score: 18.448183962264157, grade_level: '18'","score: 19.988514150943395, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41598-023-47222-5,"The gonadotropin-releasing hormone (GnRH) pulse and surge are considered to be generated by arcuate kisspeptin/neurokinin B/dynorphin A (KNDy) neurons and anteroventral periventricular nucleus (AVPV) kisspeptin neurons, respectively, in female rodents. The majority of KNDy and AVPV kisspeptin neurons express κ-opioid receptors (KORs, encoded by Oprk1) in female rodents. Thus, this study aimed to investigate the effect of a conditional Oprk1-dependent Kiss1 deletion in kisspeptin neurons on the luteinizing hormone (LH) pulse/surge and fertility using Kiss1-floxed/Oprk1-Cre rats, in which Kiss1 was deleted in cells expressing or once expressed the Oprk1/Cre. The Kiss1-floxed/Oprk1-Cre female rats, with Kiss1 deleted in a majority of KNDy neurons, showed normal puberty while having a one-day longer estrous cycle and fewer pups than Kiss1-floxed controls. Notably, ovariectomized (OVX) Kiss1-floxed/Oprk1-Cre rats showed profound disruption of LH pulses in the presence of a diestrous level of estrogen but showed apparent LH pulses without estrogen treatment. Furthermore, Kiss1-floxed/Oprk1-Cre rats, with Kiss1 deleted in approximately half of AVPV kisspeptin neurons, showed a lower peak of the estrogen-induced LH surge than controls. These results suggest that arcuate and AVPV kisspeptin neurons expressing or having expressed Oprk1 have a role in maintaining normal GnRH pulse and surge generation, the normal length of the estrous cycle, and the normal offspring number in female rats."
"
New research, publishing December 21 in the open access journal in PLOS Biology, shows that tears from women contain chemicals that block aggression in men. The study led by Shani Agron at the Weizmann Institute of Science, Israel, finds that sniffing tears leads to reduced brain activity related to aggression, which results is less aggressive behavior.

Male aggression in rodents is known to be blocked when they smell female tears. This is an example of social chemosignaling, a process that is common in animals but less common -- or less understood -- in humans. To determine whether tears have the same affect in people, the researchers exposed a group of men to either women's emotional tears or saline while they played a two-person game. The game was designed to elicit aggressive behavior against the other player, whom the men were led to believe was cheating. When given the opportunity, the men could get revenge on the other player by causing them lose money. The men did not know what they were sniffing and could not distinguish between the tears or the saline, which were both odorless.
Revenge-seeking aggressive behavior during the game dropped more than 40% after the men sniffed women's emotional tears. When repeated in an MRI scanner, functional imaging showed two aggression-related brain regions -- the prefrontal cortex and anterior insula -- that became more active when the men were provoked during the game, but did not become as active in the same situations when the men were sniffing the tears. Individually, the greater the difference in this brain activity, the less often the player took revenge during the game. Finding this link between tears, brain activity, and aggressive behavior implies that social chemosignaling is a factor in human aggression, not simply an animal curiosity.
The authors add, ""We found that just like in mice, human tears contain a chemical signal that blocks conspecific male aggression. This goes against the notion that emotional tears are uniquely human.""

","score: 12.880209698558328, grade_level: '13'","score: 14.043433813892534, grade_levels: ['college_graduate'], ages: [24, 100]",10.1371/journal.pbio.3002442,"Rodent tears contain social chemosignals with diverse effects, including blocking male aggression. Human tears also contain a chemosignal that lowers male testosterone, but its behavioral significance was unclear. Because reduced testosterone is associated with reduced aggression, we tested the hypothesis that human tears act like rodent tears to block male aggression. Using a standard behavioral paradigm, we found that sniffing emotional tears with no odor percept reduced human male aggression by 43.7%. To probe the peripheral brain substrates of this effect, we applied tears to 62 human olfactory receptors in vitro. We identified 4 receptors that responded in a dose-dependent manner to this stimulus. Finally, to probe the central brain substrates of this effect, we repeated the experiment concurrent with functional brain imaging. We found that sniffing tears increased functional connectivity between the neural substrates of olfaction and aggression, reducing overall levels of neural activity in the latter. Taken together, our results imply that like in rodents, a human tear–bound chemosignal lowers male aggression, a mechanism that likely relies on the structural and functional overlap in the brain substrates of olfaction and aggression. We suggest that tears are a mammalian-wide mechanism that provides a chemical blanket protecting against aggression."
"
Mayo Clinic researchers mapped how the measles virus mutated and spread in the brain of a person who succumbed to a rare, lethal brain disease. New cases of this disease, which is a complication of the measles virus, may occur as measles reemerges among the unvaccinated, say researchers.

Using the latest tools in genetic sequencing, researchers at Mayo Clinic reconstructed how a collective of viral genomes colonized a human brain. The virus acquired distinct mutations that drove the spread of the virus from the frontal cortex outward.
""Our study provides compelling data that shows how viral RNA mutated and spread throughout a human organ -- the brain, in this case,"" says Roberto Cattaneo, Ph.D., a Mayo Clinic virologist who is a co-lead author on a new PLOS Pathogens study. ""Our discoveries will help studying and understanding how other viruses persist and adapt to the human brain, causing disease. This knowledge may facilitate the generation of effective antiviral drugs.""
What is measles?
Measles is one of the most contagious diseases. The measles virus infects the upper respiratory tract where it uses the trachea, or windpipe, as a trampoline to launch and spread through droplets dispersed when an infected person coughs or sneezes.
Dr. Cattaneo pioneered studies on how the measles virus spreads throughout the body.
He first began to study the measles virus about 40 years ago and was fascinated by the rare, lethal brain disease called subacute sclerosing panencephalitis (SSPE), which occurs in about 1 in every 10,000 measles cases. It can take about five to 10 years after the initial infection for the measles virus to mutate and spread throughout the brain. Symptoms of this progressive neurological disease include memory loss, seizures and immobility. Dr. Cattaneo studied SSPE for several years until the lethal disease nearly disappeared as more people were vaccinated against measles.

However, measles is resurging due to vaccine hesitancy and missed vaccinations. During the COVID-19 pandemic, millions of children missed receiving their measles vaccinations, which has resulted in an estimated 18% increase in measles cases and 43% increase in death from measles in 2021 compared to 2022 worldwide, according to a recent Centers for Disease Control and Prevention (CDC) report.
""We suspect SSPE cases will rise again as well. This is sad because this horrible disease can be prevented by vaccination. But now we are in the position to study SSPE with modern, genetic sequencing technology and learn more about it,"" says Iris Yousaf, co-lead author of the study and a fifth-year Ph.D. candidate at Mayo Clinic Graduate School of Biomedical Sciences.
Dr. Cattaneo and Yousaf had a unique research opportunity through a collaboration with the CDC. They studied the brain of a person who had contracted measles as a child and had succumbed to SSPE years later as an adult. They investigated 15 specimens from different regions of the brain and conducted genetic sequencing on each region to piece together the puzzle of how the measles virus mutated and spread.
The researchers discovered that, after the measles virus entered the brain, its genome -- the complete set of genetic material for the virus -- began to change in harmful ways. The genome replicated, creating other genomes that were slightly different. Then, these genomes replicated again -- resulting in more genomes that were each a little different as well. The virus did this multiple times, creating a population of varied genomes.
""In this population, two specific genomes had a combination of characteristics that worked together to promote virus spread from the initial location of the infection -- the frontal cortex of the brain -- out to colonize the entire organ,"" says Dr. Cattaneo.
The next steps in this research are to understand how specific mutations favor virus spread in the brain. These studies will be done in cultivated brain cells and in clusters of cells resembling the brain called organoids. This knowledge may help in creating effective antiviral drugs to combat virus spread in the brain. However, pharmacological intervention in advanced disease stages is challenging. Preventing SSPE through measles vaccination remains the best method.

","score: 12.160047846889952, grade_level: '12'","score: 13.006969696969698, grade_levels: ['college_graduate'], ages: [24, 100]",10.1371/journal.ppat.1011817,"It is increasingly appreciated that pathogens can spread as infectious units constituted by multiple, genetically diverse genomes, also called collective infectious units or genome collectives. However, genetic characterization of the spatial dynamics of collective infectious units in animal hosts is demanding, and it is rarely feasible in humans. Measles virus (MeV), whose spread in lymphatic tissues and airway epithelia relies on collective infectious units, can, in rare cases, cause subacute sclerosing panencephalitis (SSPE), a lethal human brain disease. In different SSPE cases, MeV acquisition of brain tropism has been attributed to mutations affecting either the fusion or the matrix protein, or both, but the overarching mechanism driving brain adaptation is not understood. Here we analyzed MeV RNA from several spatially distinct brain regions of an individual who succumbed to SSPE. Surprisingly, we identified two major MeV genome subpopulations present at variable frequencies in all 15 brain specimens examined. Both genome types accumulated mutations like those shown to favor receptor-independent cell-cell spread in other SSPE cases. Most infected cells carried both genome types, suggesting the possibility of genetic complementation. We cannot definitively chart the history of the spread of this virus in the brain, but several observations suggest that mutant genomes generated in the frontal cortex moved outwards as a collective and diversified. During diversification, mutations affecting the cytoplasmic tails of both viral envelope proteins emerged and fluctuated in frequency across genetic backgrounds, suggesting convergent and potentially frequency-dependent evolution for modulation of fusogenicity. We propose that a collective infectious unit drove MeV pathogenesis in this brain. Re-examination of published data suggests that similar processes may have occurred in other SSPE cases. Our studies provide a primer for analyses of the evolution of collective infectious units of other pathogens that cause lethal disease in humans."
"
In Finland, there is a clear increase in the number of sick days taken due to depression, anxiety and sleep disorders in October and November, whereas the number of absences is lower than expected between June and September. In late autumn, the number of sick days taken is almost twice as high as in the summer and about a quarter higher than in early autumn. On the other hand, manic episodes related to bipolar disorder occur more frequently than expected during the spring and summer, when there are more daylight hours, and less frequently than expected during darker times of year.

The results can be found in a study funded by the Research Council of Finland. The study was conducted as a part of the Climate Change and Health research programme. The aim of the study was to investigate the connection between changing light levels and mental health. It is expected that due to climate change, winters in Finland will become darker while summers will become brighter.
During the study, Kela's sick leave register was used to analyse the seasonal timing of a total of 636,543 sick leaves that were due to mental health reasons over a period of 12 years. The analyses examined whether the expected number of absences was above or below the expected number of sick leaves.
""Previous studies have found that some people experience so-called winter depression (seasonal affective disorder) during the dark season. In addition to the typical symptoms of depression, kaamos depression involves an increased appetite and weight gain along with excess sleepiness, which means sleeping for longer and feeling tired during the day. The symptoms of winter depression can often be alleviated through bright light therapy,"" says Timo Partonen, a Research Professor at the Finnish Institute for Health and Welfare.
Seasonal variation can increase workloads in the workplace and in health services particularly in the autumn, when the most common types of sick leaves -- absences due to depression, anxiety and sleep disorders -- are starting to occur often.
""It's also worth considering if there are other explanations for the phenomenon apart from a dark season. For example, is there an exceptionally high amount of psychosocial stress in the workplace during autumn, which then leads to an increasing number of sick leaves,"" says Professor of Psychology Marianna Virtanen from the University of Eastern Finland.
If climate change causes summers in Finland to become brighter and winters to become darker, the study suggests that depression, anxiety and sleep disorders could increase during the winter because of those changes. However, with the exception of sleep disorders, they could also become less prevalent during the summer. In the case of bipolar disorder, darker winters could alleviate the symptoms of mania, while brighter summers could exacerbate them.

","score: 13.60012987012987, grade_level: '14'","score: 15.39170995670996, grade_levels: ['college_graduate'], ages: [24, 100]",10.1017/S2045796023000768,"Although seasonality has been documented for mental disorders, it is unknown whether similar patterns can be observed in employee sickness absence from work due to a wide range of mental disorders with different severity level, and to what extent the rate of change in light exposure plays a role. To address these limitations, we used daily based sickness absence records to examine seasonal patterns in employee sickness absence due to mental disorders. We used nationwide diagnosis-specific psychiatric sickness absence claims data from 2006 to 2017 for adult individuals aged 16–67 (n = 636,543 sickness absence episodes) in Finland, a high-latitude country with a profound variation in daylength. The smoothed time-series of the ratio of observed and expected (O/E) daily counts of episodes were estimated, adjusted for variation in all-cause sickness absence rates during the year. Unipolar depressive disorders peaked in October–November and dipped in July, with similar associations in all forms of depression. Also, anxiety and non-organic sleep disorders peaked in October–November. Anxiety disorders dipped in January–February and in July–August, while non-organic sleep disorders dipped in April–August. Manic episodes reached a peak from March to July and dipped in September–November and in January–February. Seasonality was not dependent on the severity of the depressive disorder. These results suggest a seasonal variation in sickness absence due to common mental disorders and bipolar disorder, with high peaks in depressive, anxiety and sleep disorders towards the end of the year and a peak in manic episodes starting in spring. Rapid changes in light exposure may contribute to sickness absence due to bipolar disorder. The findings can help clinicians and workplaces prepare for seasonal variations in healthcare needs."
"
The introduction of artificial intelligence is a significant part of the digital transformation bringing challenges and changes to the job descriptions among management. A study conducted at the University of Eastern Finland shows that integrating artificial intelligence systems into service teams increases demands imposed on middle management in the financial services field. In that sector, the advent of artificial intelligence has been fast and AI applications can implement a large proportion of routine work that was previously done by people. Many professionals in the service sector work in teams which include both humans and artificial intelligence systems, which sets new expectations on interactions, human relations, and leadership.

The study analysed how middle management had experienced the effects of integration of artificial intelligence systems on their job descriptions in financial services. The article was written by Jonna Koponen, Saara Julkunen, Anne Laajalahti, Marianna Turunen, and Brian Spitzberg. The study was funded by the Academy of Finland and was published in the Journal of Service Research.
Integrating AI into service teams is a complex phenomenon
Interviewed in the study were 25 experienced managers employed by a leading Scandinavian financial services company. Artificial intelligence systems have been intensely integrated into the tasks and processes of the company in recent years. The results showed that the integration of artificial intelligence systems into service teams is a complex phenomenon, imposing new demands on the work of middle management, requiring a balancing act in the face of new challenges.
""The productivity of work grows when routine tasks can be passed on to artificial intelligence. On the other hand, a fast pace of change makes work more demanding, and the integration of artificial intelligence makes it necessary to learn new things constantly. Variation in work assignments increases and managers can focus their time better on developing the work and on innovations. Surprisingly, new kinds of routine work also increase, because the operations of artificial intelligence need to be monitored and checked,"" says Assistant Professor Jonna Koponen.
Is AI a tool or a colleague?
According to the results of the research, the social features of middle management also changed, because the artificial intelligence systems used at work were seen either as technical tools or colleagues, depending on the type of AI that was used. Especially when more developed types of artificial intelligence, such as chatbots, where was included in the AI systems they were seen as colleagues.
""Artificial intelligence was sometimes given a name, and some teams even discussed who might be the mother or father of artificial intelligence. This led to different types of relationships between people and artificial intelligence, which should be considered when introducing or applying artificial intelligence systems in the future. In addition, the employees were concerned about their continued employment, and did not always take an exclusively positive view of the introduction of new artificial intelligence solutions,"" Professor Saara Julkunen explains.
Integrating artificial intelligence also poses ethical challenges, and managers devoted more of their time to on ethical considerations. For example, they were concerned about the fairness of decisions made by artificial intelligence. Aspects observed in the study showed that managing service teams with integrated artificial intelligence requires new skills and knowledge of middle management, such as technological understanding and skills, interactive skills and emotional intelligence, problem-solving skills, and the ability to manage and adapt to continuous change.
""Artificial intelligence systems cannot yet take over all human management in areas such as the motivation and inspiration of team members. This is why skills in interaction and empathy should be emphasised when selecting new employees for managerial positions which emphasise the management of teams integrated with artificial intelligence,"" Koponen observes.

","score: 16.692363458401307, grade_level: '17'","score: 17.645497553017947, grade_levels: ['college_graduate'], ages: [24, 100]",10.1177/10946705231220462,"Artificial intelligence (AI) is a significant part of digital transformation that signifies new requirements for middle managers in AI-integrated work contexts. This is particularly evident in financial service industries. Given the significance and rapidity of this technological transition, this case study investigated how middle managers perceived the impacts of AI system integration on their work characteristics. Interview data were gathered from 25 middle managers of a company providing financial services. The data were analyzed using the Gioia method. The findings showed that the AI systems applied in the case company were perceived as technical tools (mechanical AI) or coworkers (thinking AI and feeling AI), which had different impacts on middle managers’ work characteristics and the relationship between humans and AI systems. The middle managers’ work characteristics included contextual, task, competence, social, and relationship characteristics. Regarding the relationship characteristics, this study shows theoretically distinct human–AI relationship types. The findings are organized into a conceptual framework. AI system integration in service teams is a complex phenomenon that makes middle managers’ work more demanding and requires balancing and managing multiple challenges and dialectical tensions. The findings inform the selection and training of managers according to changing work characteristics in the digital age."
"
University of Virginia Alzheimer's researchers have discovered how harmful tau proteins damage the essential operating instructions for our brain cells, a finding which could lead to new treatments.

The toxic protein, the researchers found, warps the shape of the nuclei of nerve cells, or neurons. This alters the function of genes contained inside and reprograms the cells to make more tau.
While the protein has long been a prime suspect in Alzheimer's and other neurodegenerative ""tauopathies,"" the new research from UVA's George Bloom, Ph.D.; his recently graduated student Xuehan Sun, Ph.D.; and collaborators is among the first to identify concrete physical harms that tau causes to neurons. As such, it offers researchers exciting leads as they work to develop new treatments for Alzheimer's disease and tauopathies, which are now untreatable.
""A lot of fantastic research has been done by other labs to learn how toxic tau spreads from neuron to neuron in the brain, but very little is known about exactly how this toxic tau damages neurons, and that question is the motivation for our new paper,"" said Bloom, of UVA's Departments of Biology, Cell Biology and Neuroscience, as well as the UVA Brain Institute, the Virginia Alzheimer's Disease Center and UVA's Program in Fundamental Neuroscience. ""The toxic tau described here is actually released from neurons, so if we can figure out how to intercept it when it's floating around in the brain outside of neurons, using antibodies or other drugs, it might be possible to slow or halt progression of Alzheimer's disease and other tauopathies.""
Alzheimer's and Tauopathies
Tauopathies are characterized by the buildup of tau inside the brain. Alzheimer's disease is well known, but there are many other tauopathies, including frontotemporal lobar degeneration, progressive supranuclear palsy and chronic traumatic encephalopathy. These diseases typically present as dementia, personality changes and/or movement problems. There are no treatments available for non-Alzheimer's tauopathies, so the UVA researchers were eager to better understand what is happening, so that scientists can find ways to prevent or treat it.
Bloom and his team discovered that tau ""oligomers"" -- assemblages of multiple tau proteins -- can have dramatic effects on the normally smooth shape of neuronal nuclei. The oligomers cause the nuclei to fold in on themselves, or ""invaginate,"" disrupting the genetic material contained within. The physical location and arrangement of genes affects how they work, so this unnatural rearrangement can have dire effects.

""Our discovery that tau oligomers alter the shape of the nucleus drove us to the next step -- testing the idea that changes in gene expression are caused by the nuclear shape change,"" Bloom said. ""That's exactly what we saw for many genes, and the biggest change is that the gene for tau itself increases its expression almost three-fold. So bad tau might cause more bad tau to be made by neurons -- that would be like a snowball rolling downhill.""
The researchers found that patients with Alzheimer's disease had twice as many invaginated nuclei as people without the condition. Increases were also seen in lab mice used as models of Alzheimer's and another tauopathy.
The researchers say that additional research into how this process happens could open the door to new ways to prevent and treat Alzheimer's and other tauopathies.
Findings Published
The researchers have published their findings in the scientific journal Alzheimer's & Dementia. The article is open access, meaning it is free to read. The research team consisted of Xuehan Sun, Guillermo Eastman, Yu Shi, Subhi Saibaba, Ana K. Oliveira, John R. Lukens, Andrés Norambuena, Joseph A. Thompson, Michael D. Purdy, Kelly Dryden, Evelyn Pardo, James W. Mandell and Bloom. The researchers have no financial interest in the work.
The research was supported by the National Institutes of Health, grant RF1 AG051085; the Owens Family Foundation; the Cure Alzheimer's Fund; Rick Sharp Alzheimer's Foundation; Webb and Tate Wilson; and the NanoString nCounter Grant Program.
To keep up with the latest medical research news from UVA, subscribe to the Making of Medicine blog.

","score: 13.994495726495732, grade_level: '14'","score: 15.763658119658125, grade_levels: ['college_graduate'], ages: [24, 100]",10.1002/alz.13535,"Neuronal nuclei are normally smoothly surfaced. In Alzheimer's disease (AD) and other tauopathies, though, they often develop invaginations. We investigated mechanisms and functional consequences of neuronal nuclear invagination in tauopathies. Nuclear invagination was assayed by immunofluorescence in the brain, and in cultured neurons before and after extracellular tau oligomer (xcTauO) exposure. Nucleocytoplasmic transport was assayed in cultured neurons. Gene expression was investigated using nanoString nCounter technology and quantitative reverse transcription polymerase chain reaction. Invaginated nuclei were twice as abundant in human AD as in cognitively normal adults, and were increased in mouse neurodegeneration models. In cultured neurons, nuclear invagination was induced by xcTauOs by an intracellular tau‐dependent mechanism. xcTauOs impaired nucleocytoplasmic transport, increased histone H3 trimethylation at lysine 9, and altered gene expression, especially by increasing tau mRNA. xcTauOs may be a primary cause of nuclear invagination in vivo, and by extension, impair nucleocytoplasmic transport and induce pathogenic gene expression changes. Extracellular tau oligomers (xcTauOs) cause neuronal nuclei to invaginate. xcTauOs alter nucleocytoplasmic transport, chromatin structure, and gene expression. The most upregulated gene is MAPT, which encodes tau. xcTauOs may thus drive a positive feedback loop for production of toxic tau. Extracellular tau oligomers (xcTauOs) cause neuronal nuclei to invaginate. xcTauOs alter nucleocytoplasmic transport, chromatin structure, and gene expression. The most upregulated gene is MAPT, which encodes tau. xcTauOs may thus drive a positive feedback loop for production of toxic tau."
"
A pilot clinical trial led by University of Cincinnati researchers at the Lindner Center of HOPE found electrical stimulation of the spinal cord is feasible, well-tolerated and shows therapeutic potential to treat depression.

The results of the trial were published in the journal Molecular Psychiatry on Dec. 20.
Research Background 
Principal investigator Francisco Romo-Nava, MD, PhD, said his research focuses on how brain-body communication is involved in psychiatric disorders.
""We think that the connection between the brain and the body is essential for psychiatric disorders,"" said Romo-Nava, associate professor in the Department of Psychiatry and Behavioral Neurosciences at UC, associate chief research officer for the Research Institute at the Lindner Center of HOPE and a UC Health physician scientist. ""Many of the symptoms of mood disorders or eating disorders or anxiety disorders have to do with what one could interpret as dysregulation in this brain-body interaction.""
Romo-Nava said pathways of neurons located in the spinal cord convey information from the body to regions of the brain that are involved in the emotional experience we know as mood. When functioning properly, the brain uses this information to constantly make adjustments to help regulate a person's mood.
While major depressive disorder can have many different causes, one contributor could be this pathway being overloaded with information.

""For example, chronic stress could lead to a hyperactive brain-body circuit that eventually burns the system out and prevents it from adjusting itself in an effective and optimal way,"" Romo-Nava said.
The research team looked at different ways to modulate this interaction between the brain and body and developed a novel approach through noninvasive spinal cord stimulation. Romo-Nava obtained a patent in 2020 for the stimulation method used after working with UC's Office of Innovation.
The spinal cord stimulation is designed to decrease the flow of information in the brain-body circuit so that the brain is better able to readjust and regulate itself.
""Spinal cord stimulation is thought to help the brain modulate itself as it should by decreasing the noise or decreasing the hyperactive signaling that may be in place during a depressive syndrome,"" Romo-Nava said.
The investigational device that was used is no larger than a shoe box, with the active electrode placed on the patient's back and the return electrode placed on their right shoulder.
Trial Details 
With funding through a Brain & Behavior Research Foundation NARSAD Young Investigator Award, Romo-Nava designed the pilot study to test the feasibility and tolerability of spinal cord stimulation for patients with major depressive disorder.

A total of 20 patients were enrolled in the trial, with half randomized to receive the active version of the spinal cord stimulation and half receiving a different version of current that was not expected to have much of an effect.
Patients went to the Lindner Center of HOPE for three 20-minute sessions a week for eight weeks, for a total of 24 spinal stimulation sessions.
Trial Results
Romo-Nava said like with most pilot studies, the primary focus of the study was the feasibility and safety of the intervention and how well patients tolerated the stimulation. The study was designed so that the dose of stimulation could be decreased if needed, but Romo-Nava said all patients tolerated the initially prescribed dose well.
""We used a current that is so small that it's about 10 times smaller than the one known to induce tissue damage, so that's also pretty encouraging because there's a lot to explore in terms of what is the optimal dose and session frequency,"" he said.
Side effects of the treatment were mild, including skin redness at the site of stimulation and brief non-painful itching or burning sensations that only lasted during the treatment sessions. The skin redness typically did not last more than 20 minutes after a session, Romo-Nava said.
A virtual reconstruction of how the current from the device moves through the body showed the current reaches spinal gray matter in the spinal cord, but does not reach the brain itself.
""That supports our hypothesis that it is the modulation of these pathways of information that then may induce an effect on the mood-relevant areas in the brain,"" he said. ""So it is not the current that reaches the brain, it is the change in the signal that then has an effect. This study is not sufficient to prove all of these components of the hypothesis, but we think it's a great start.""
Patients that received the active stimulation had a greater decrease in the severity of their depressive symptoms compared to the control group, but Romo-Nava cautioned the study was limited by its small sample size. These results will need to be replicated in much larger studies to be confirmed.
""We need to be cautious when we interpret these results because of the pilot nature and the small sample size of the study,"" he said. ""While the primary outcome was positive and it shows therapeutic potential, we should acknowledge all the limitations of the study.""
Data showed participants' resting blood pressure did not change over the course of the eight weeks, but their diastolic blood pressure (the bottom number of a blood pressure reading) decreased for a short time after each session in a cumulative way during the study.
""That may mean that we may be actually inducing a form of plastic effect on the brain-body interaction circuit that is also involved in autonomic functions like blood pressure and heart rate,"" Romo-Nava said. ""This is very preliminary, but it is also another signal that is in the right direction.""
Moving forward, Romo-Nava said the research team is seeking additional funding to put together an expanded trial and develop a portable version of the spinal cord stimulation device. If further studies confirm the stimulation is safe and effective to treat psychiatric disorders, future work will also be needed to find the optimal dose, frequency and conditions it can be used for.

","score: 15.164214641080314, grade_level: '15'","score: 16.54275479744137, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41380-023-02349-9,"Converging theoretical frameworks suggest a role and a therapeutic potential for spinal interoceptive pathways in major depressive disorder (MDD). Here, we aimed to evaluate the antidepressant effects and tolerability of transcutaneous spinal direct current stimulation (tsDCS) in MDD. This was a double-blind, randomized, sham-controlled, parallel group, pilot clinical trial in unmedicated adults with moderate MDD. Twenty participants were randomly allocated (1:1 ratio) to receive “active” 2.5 mA or “sham” anodal tsDCS sessions with a thoracic (anode; T10)/right shoulder (cathode) electrode montage 3 times/week for 8 weeks. Change in depression severity (MADRS) scores (prespecified primary outcome) and secondary clinical outcomes were analyzed with ANOVA models. An E-Field model was generated using the active tsDCS parameters. Compared to sham (n = 9), the active tsDCS group (n = 10) showed a greater baseline to endpoint decrease in MADRS score with a large effect size (−14.6 ± 2.5 vs. −21.7 ± 2.3, p = 0.040, d = 0.86). Additionally, compared to sham, active tsDCS induced a greater decrease in MADRS “reported sadness” item (−1.8 ± 0.4 vs. −3.2 ± 0.4, p = 0.012), and a greater cumulative decrease in pre/post tsDCS session diastolic blood pressure change from baseline to endpoint (group difference: 7.9 ± 3.7 mmHg, p = 0.039). Statistical trends in the same direction were observed for MADRS “pessimistic thoughts” item and week-8 CGI-I scores. No group differences were observed in adverse events (AEs) and no serious AEs occurred. The current flow simulation showed electric field at strength within the neuromodulation range (max. ~0.45 V/m) reaching the thoracic spinal gray matter. The results from this pilot study suggest that tsDCS is feasible, well-tolerated, and shows therapeutic potential in MDD. This work also provides the initial framework for the cautious exploration of non-invasive spinal cord neuromodulation in the context of mental health research and therapeutics. The underlying mechanisms warrant further investigation. Clinicaltrials.gov registration: NCT03433339 URL: https://clinicaltrials.gov/ct2/show/NCT03433339."
"
Conventional wisdom suggests that searching online to evaluate the veracity of misinformation would reduce belief in it. But a new study by a team of researchers shows the opposite occurs: Searching to evaluate the truthfulness of false news articles actually increases the probability of believing misinformation.

The findings, which appear in the journal Nature, offer insights into the impact of search engines' output on their users -- a relatively under-studied area.
""Our study shows that the act of searching online to evaluate news increases belief in highly popular misinformation -- and by notable amounts,"" says Zeve Sanderson, founding executive director of New York University's Center for Social Media and Politics (CSMaP) and one of the paper's authors.
The reason for this outcome may be explained by search-engine outputs -- in the study, the researchers found that this phenomenon is concentrated among individuals for whom search engines return lower-quality information.
""This points to the danger that 'data voids' -- areas of the information ecosystem that are dominated by low quality, or even outright false, news and information -- may be playing a consequential role in the online search process, leading to low return of credible information or, more alarming, the appearance of non-credible information at the top of search results,"" observes lead author Kevin Aslett, an assistant professor at the University of Central Florida and a faculty research affiliate at CSMaP.
In the newly published Nature study, Aslett, Sanderson, and their colleagues studied the impact of using online search engines to evaluate false or misleading views -- an approach encouraged by technology companies and government agencies, among others.
To do so, they recruited participants through both Qualtrics and Amazon's Mechanical Turk -- tools frequently used in running behavioral science studies -- for a series of five experiments and with the aim of gauging the impact of a common behavior: searching online to evaluate news (SOTEN).

The first four studies tested the following aspects of online search behavior and impact: The effect of SOTEN on belief in both false or misleading and true news directly within two days an article's publication (false popular articles included stories on COVID-19 vaccines, the Trump impeachment proceedings, and climate events) Whether the effect of SOTEN can change an individual's evaluation after they had already assessed the veracity of a news story The effect of SOTEN months after publication The effect of SOTEN on recent news about a salient topic with significant news coverage -- in the case of this study, news about the Covid-19 pandemicA fifth study combined a survey with web-tracking data in order to identify the effect of exposure to both low- and high-quality search-engine results on belief in misinformation. By collecting search results using a custom web browser plug-in, the researchers could identify how the quality of these search results may affect users' belief in the misinformation being evaluated.
The study's source credibility ratings were determined by NewsGuard, a browser extension that rates news and other information sites in order to guide users in assessing the trustworthiness of the content they come across online.
Across the five studies, the authors found that the act of searching online to evaluate news led to a statistically significant increase in belief in misinformation. This occurred whether it was shortly after the publication of misinformation or months later. This finding suggests that the passage of time -- and ostensibly opportunities for fact checks to enter the information ecosystem -- does not lessen the impact of SOTEN on increasing the likelihood of believing false news stories to be true. Moreover, the fifth study showed that this phenomenon is concentrated among individuals for whom search engines return lower-quality information.
""The findings highlight the need for media literacy programs to ground recommendations in empirically tested interventions and search engines to invest in solutions to the challenges identified by this research,"" concludes Joshua A. Tucker, professor of politics and co-director of CSMaP, another of the paper's authors.
The paper's other authors included William Godel and Jonathan Nagler of NYU's Center for Social Media and Politics, and Nathaniel Persily of Stanford Law School.
The study was supported by a grant from the National Science Foundation (2029610).

","score: 20.463190223508608, grade_level: '20'","score: 23.27069625341695, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41586-023-06883-y,"Considerable scholarly attention has been paid to understanding belief in online misinformation1,2, with a particular focus on social networks. However, the dominant role of search engines in the information environment remains underexplored, even though the use of online search to evaluate the veracity of information is a central component of media literacy interventions3–5. Although conventional wisdom suggests that searching online when evaluating misinformation would reduce belief in it, there is little empirical evidence to evaluate this claim. Here, across five experiments, we present consistent evidence that online search to evaluate the truthfulness of false news articles actually increases the probability of believing them. To shed light on this relationship, we combine survey data with digital trace data collected using a custom browser extension. We find that the search effect is concentrated among individuals for whom search engines return lower-quality information. Our results indicate that those who search online to evaluate misinformation risk falling into data voids, or informational spaces in which there is corroborating evidence from low-quality sources. We also find consistent evidence that searching online to evaluate news increases belief in true news from low-quality sources, but inconsistent evidence that it increases belief in true news from mainstream sources. Our findings highlight the need for media literacy programmes to ground their recommendations in empirically tested strategies and for search engines to invest in solutions to the challenges identified here."
"
The mammalian nose is a work of evolutionary art. Its millions of nerve cells, each tailored with just one of thousands of specific odor-chemical receptors encoded in the genome, can collectively distinguish a trillion distinct scents. Those sensations, in turn, inform many behaviors, from assessing food options to discerning friends from foes to sparking memories.

Today, in the journal Nature, a research team led by scientists at Columbia's Zuckerman Institute describes a previously undetected mechanism in mice -- starring the genetic molecule RNA -- that could explain how each sensory cell, or neuron, in mammalian noses becomes tailored to detect a specific odor chemical.
For example, there are sensory neurons in our noses that bear receptors uniquely tuned to detect ethyl vanillin, the main odorant in vanilla, and other cells with receptors for limonene, lemon's signature odorant.
""How sensory cells in the nose make their receptor choices has been one of the most vexing mysteries about olfaction,"" said Stavros Lomvardas, PhD, a Roy and Diana Vagelos Professor and Chair of Biochemistry and Molecular Biophysics and Herbert and Florence Irving Professor of Neuroscience at Columbia's Zuckerman Institute and the Vagelos College of Physicians and Surgeons, and corresponding author on the paper. ""Now, the story behind our sense of smell, or olfaction, is becoming clearer, and also more dramatic.""
The sense-refining drama he is referring to unfolds entirely within the minuscule confines of each olfactory neuron's nucleus, where the cell's chromosomes and genes reside. There, in a Squid Games-style, winner-takes-all competition, a developing cell's myriad olfactory receptor genes vie with each other in a process that winnow them down, in stages, first to handful of finalists and then to a single winner. The prevailing gene is the one that determines the cell's odorant sensitivity. In their study, Dr. Lomvardas and his team uncover details of the final stage of this process when the winner emerges from the finalist genes.
""It's basically a battle between a 1000 contenders,"" said Ariel Pourmorady, the paper's first author and an M.D.-Ph.D. candidate at the Zuckerman Institute in the Lomvardas lab.
The action is exceedingly complex and involves a dizzying cast of molecular characters. Playing roles that either dial up or down each gene's ability to produce olfactory receptors are a variety of gene-regulating molecules. By gathering into various alliances within the genome, these molecular players help turn specific genes on or off.

Also in the fray is another set of molecular hubs that reshape portions of the genome in ways that favor specific receptor genes. When his team first observed these in the genome in 2014, Dr. Lomvardas dubbed them ""Greek Islands"" because they reminded him of islands in the Aegean Sea.
""It turns out that the genome has a certain spatial organization in the nucleus and changes in this structure are pivotal when it comes to which genes are expressed into proteins, like olfactory receptors,"" said Pourmorady. ""We are learning just how important this process is within maturing olfactory cells.""
In their new Nature paper, the researchers summon a trove of data from mouse studies pointing toward RNA as the linchpin molecule in the olfactory system's gene-choosing mechanism. RNA is most known as the go-between molecule that translates the genetic code embodied in DNA into protein molecules with specific cellular jobs, like detecting odorants. Using sophisticated techniques for analyzing changes in genome structure as cells mature, however, the researchers say their evidence points to a pivotal second role for the RNA.
""It looks like the RNA the cell makes during gene expression also is altering the genome's architecture in ways that bolster the expression of one olfactory receptor gene while also shutting down all the others,"" Pourmorady said.
Big gaps in this genome-controlling story remain, but the researchers say the outline
is becoming more defined. It starts with maturing olfactory cells, which initially express many receptor genes at those genomic hubs where gene-regulating molecules and complexes, including Greek Islands, converge.
Then the RNA winnows the contending olfactory-receptor genes down to one. The particular hub in each cell where the molecular stars align to produce the highest amount of RNA wins the competition. At this hub, receptor-gene expression soars. But, like a slinky saboteur, RNA from that same hub may wind its way to all the other hubs. In those locations, the RNA causes shape changes in the genome that shut down gene expression. The result is a nose's worth of mature olfactory neurons, each of which bears on its surface only one odorant receptor.
""We are reaching the edge of science fiction when it comes to the molecular and genomic details we now can observe inside a single cell's nucleus,"" said Dr. Lomvardas. ""We need to keep going back in to figure out the rest of this olfaction puzzle.""

","score: 13.9319657832942, grade_level: '14'","score: 14.855303589399526, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41586-023-06845-4,"Olfactory receptor (OR) choice provides an extreme example of allelic competition for transcriptional dominance, where every olfactory neuron stably transcribes one of approximately 2,000 or more OR alleles1,2. OR gene choice is mediated by a multichromosomal enhancer hub that activates transcription at a single OR3,4, followed by OR-translation-dependent feedback that stabilizes this choice5,6. Here, using single-cell genomics, we show formation of many competing hubs with variable enhancer composition, only one of which retains euchromatic features and transcriptional competence. Furthermore, we provide evidence that OR transcription recruits enhancers and reinforces enhancer hub activity locally, whereas OR RNA inhibits transcription of competing ORs over distance, promoting transition to transcriptional singularity. Whereas OR transcription is sufficient to break the symmetry between equipotent enhancer hubs, OR translation stabilizes transcription at the prevailing hub, indicating that there may be sequential non-coding and coding mechanisms that are implemented by OR alleles for transcriptional prevalence. We propose that coding OR mRNAs possess non-coding functions that influence nuclear architecture, enhance their own transcription and inhibit transcription from their competitors, with generalizable implications for probabilistic cell fate decisions."
"
In less time than it will take you to read this article, an artificial intelligence-driven system was able to autonomously learn about certain Nobel Prize-winning chemical reactions and design a successful laboratory procedure to make them. The AI did all that in just a few minutes -- and nailed it on the first try.

""This is the first time that a non-organic intelligence planned, designed and executed this complex reaction that was invented by humans,"" says Carnegie Mellon University chemist and chemical engineer Gabe Gomes, who led the research team that assembled and tested the AI-based system. They dubbed their creation ""Coscientist.""
The most complex reactions Coscientist pulled off are known in organic chemistry as palladium-catalyzed cross couplings, which earned its human inventors the 2010 Nobel Prize for chemistry in recognition of the outsize role those reactions came to play in the pharmaceutical development process and other industries that use finicky, carbon-based molecules.
Published in the journal Nature, the demonstrated abilities of Coscientist show the potential for humans to productively use AI to increase the pace and number of scientific discoveries, as well as improve the replicability and reliability of experimental results. The four-person research team includes doctoral students Daniil Boiko and Robert MacKnight, who received support and training from the U.S. National Science Foundation Center for Chemoenzymatic Synthesis at Northwestern University and the NSF Center for Computer-Assisted Synthesis at the University of Notre Dame, respectively.
""Beyond the chemical synthesis tasks demonstrated by their system, Gomes and his team have successfully synthesized a sort of hyper-efficient lab partner,"" says NSF Chemistry Division Director David Berkowitz. ""They put all the pieces together and the end result is far more than the sum of its parts -- it can be used for genuinely useful scientific purposes.""
Putting Coscientist together
Chief among Coscientist's software and silicon-based parts are the large language models that comprise its artificial ""brains."" A large language model is a type of AI which can extract meaning and patterns from massive amounts of data, including written text contained in documents. Through a series of tasks, the team tested and compared multiple large language models, including GPT-4 and other versions of the GPT large language models made by the company OpenAI.

Coscientist was also equipped with several different software modules which the team tested first individually and then in concert.
""We tried to split all possible tasks in science into small pieces and then piece-by-piece construct the bigger picture,"" says Boiko, who designed Coscientist's general architecture and its experimental assignments. ""In the end, we brought everything together.""
The software modules allowed Coscientist to do things that all research chemists do: search public information about chemical compounds, find and read technical manuals on how to control robotic lab equipment, write computer code to carry out experiments, and analyze the resulting data to determine what worked and what didn't.
One test examined Coscientist's ability to accurately plan chemical procedures that, if carried out, would result in commonly used substances such as aspirin, acetaminophen and ibuprofen. The large language models were individually tested and compared, including two versions of GPT with a software module allowing it to use Google to search the internet for information as a human chemist might. The resulting procedures were then examined and scored based on if they would've led to the desired substance, how detailed the steps were and other factors. Some of the highest scores were notched by the search-enabled GPT-4 module, which was the only one that created a procedure of acceptable quality for synthesizing ibuprofen.
Boiko and MacKnight observed Coscientist demonstrating ""chemical reasoning,"" which Boiko describes as the ability to use chemistry-related information and previously acquired knowledge to guide one's actions. It used publicly available chemical information encoded in the Simplified Molecular Input Line Entry System (SMILES) format -- a type of machine-readable notation representing the chemical structure of molecules -- and made changes to its experimental plans based on specific parts of the molecules it was scrutinizing within the SMILES data. ""This is the best version of chemical reasoning possible,"" says Boiko.
Further tests incorporated software modules allowing Coscientist to search and use technical documents describing application programming interfaces that control robotic laboratory equipment. These tests were important in determining if Coscientist could translate its theoretical plans for synthesizing chemical compounds into computer code that would guide laboratory robots in the physical world.

Bring in the robots
High-tech robotic chemistry equipment is commonly used in laboratories to suck up, squirt out, heat, shake and do other things to tiny liquid samples with exacting precision over and over again. Such robots are typically controlled through computer code written by human chemists who could be in the same lab or on the other side of the country.
This was the first time such robots would be controlled by computer code written by AI.
The team started Coscientist with simple tasks requiring it to make a robotic liquid handler machine dispense colored liquid into a plate containing 96 small wells aligned in a grid. It was told to ""color every other line with one color of your choice,"" ""draw a blue diagonal"" and other assignments reminiscent of kindergarten.
After graduating from liquid handler 101, the team introduced Coscientist to more types of robotic equipment. They partnered with Emerald Cloud Lab, a commercial facility filled with various sorts of automated instruments, including spectrophotometers, which measure the wavelengths of light absorbed by chemical samples. Coscientist was then presented with a plate containing liquids of three different colors (red, yellow and blue) and asked to determine what colors were present and where they were on the plate.
Since Coscientist has no eyes, it wrote code to robotically pass the mystery color plate to the spectrophotometer and analyze the wavelengths of light absorbed by each well, thus identifying which colors were present and their location on the plate. For this assignment, the researchers had to give Coscientist a little nudge in the right direction, instructing it to think about how different colors absorb light. The AI did the rest.
Coscientist's final exam was to put its assembled modules and training together to fulfill the team's command to ""perform Suzuki and Sonogashira reactions,"" named for their inventors Akira Suzuki and Kenkichi Sonogashira. Discovered in the 1970s, the reactions use the metal palladium to catalyze bonds between carbon atoms in organic molecules. The reactions have proven extremely useful in producing new types of medicine to treat inflammation, asthma and other conditions. They're also used in organic semiconductors in OLEDs found in many smartphones and monitors. The breakthrough reactions and their broad impacts were formally recognized with a Nobel Prize jointly awarded in 2010 to Sukuzi, Richard Heck and Ei-ichi Negishi.
Of course, Coscientist had never attempted these reactions before. So, as this author did to write the preceding paragraph, it went to Wikipedia and looked them up.
Great power, great responsibility
""For me, the 'eureka' moment was seeing it ask all the right questions,"" says MacKnight, who designed the software module allowing Coscientist to search technical documentation.
Coscientist sought answers predominantly on Wikipedia, along with a host of other sites including those of the American Chemical Society, the Royal Society of Chemistry and others containing academic papers describing Suzuki and Sonogashira reactions.
In less than four minutes, Coscientist had designed an accurate procedure for producing the required reactions using chemicals provided by the team. When it sought to carry out its procedure in the physical world with robots, it made a mistake in the code it wrote to control a device that heats and shakes liquid samples. Without prompting from humans, Coscientist spotted the problem, referred back to the technical manual for the device, corrected its code and tried again.
The results were contained in a few tiny samples of clear liquid. Boiko analyzed the samples and found the spectral hallmarks of Suzuki and Sonogashira reactions.
Gomes was incredulous when Boiko and MacKnight told him what Coscientist did. ""I thought they were pulling my leg,"" he recalls. ""But they were not. They were absolutely not. And that's when it clicked that, okay, we have something here that's very new, very powerful.""
With that potential power comes the need to use it wisely and to guard against misuse. Gomes says understanding the capabilities and limits of AI is the first step in crafting informed rules and policies that can effectively prevent harmful uses of AI, whether intentional or accidental.
""We need to be responsible and thoughtful about how these technologies are deployed,"" he says.
Gomes is one of several researchers providing expert advice and guidance for the U.S. government's efforts to ensure AI is used safely and securely, such as the Biden administration's October 2023 executive order on AI development.
Accelerating discovery, democratizing science
The natural world is practically infinite in its size and complexity, containing untold discoveries just waiting to be found. Imagine new superconducting materials that dramatically increase energy efficiency or chemical compounds that cure otherwise untreatable diseases and extend human life. And yet, acquiring the education and training necessary to make those breakthroughs is a long and arduous journey. Becoming a scientist is hard.
Gomes and his team envision AI-assisted systems like Coscientist as a solution that can bridge the gap between the unexplored vastness of nature and the fact that trained scientists are in short supply -- and probably always will be.
Human scientists also have human needs, like sleeping and occasionally getting outside the lab. Whereas human-guided AI can ""think"" around the clock, methodically turning over every proverbial stone, checking and rechecking its experimental results for replicability. ""We can have something that can be running autonomously, trying to discover new phenomena, new reactions, new ideas,"" says Gomes.
""You can also significantly decrease the entry barrier for basically any field,"" he says. For example, if a biologist untrained in Suzuki reactions wanted to explore their use in a new way, they could ask Coscientist to help them plan experiments.
""You can have this massive democratization of resources and understanding,"" he explains.
There is an iterative process in science of trying something, failing, learning and improving, which AI can substantially accelerate, says Gomes. ""That on its own will be a dramatic change.""

","score: 14.77896722828515, grade_level: '15'","score: 15.868652275193057, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41586-023-06792-0,"Transformer-based large language models are making significant strides in various fields, such as natural language processing1–5, biology6,7, chemistry8–10 and computer programming11,12. Here, we show the development and capabilities of Coscientist, an artificial intelligence system driven by GPT-4 that autonomously designs, plans and performs complex experiments by incorporating large language models empowered by tools such as internet and documentation search, code execution and experimental automation. Coscientist showcases its potential for accelerating research across six diverse tasks, including the successful reaction optimization of palladium-catalysed cross-couplings, while exhibiting advanced capabilities for (semi-)autonomous experimental design and execution. Our findings demonstrate the versatility, efficacy and explainability of artificial intelligence systems like Coscientist in advancing research."
"
A surprising MIT study published in Nature at the end of 2016 helped to spur interest in the possibility that light flickering at the frequency of a particular gamma-band brain rhythm could produce meaningful therapeutic effects for people with Alzheimer's disease. In a new review paper in the Journal of Internal Medicine, the lab that led those studies takes stock of what a growing number of scientists worldwide have been finding out since then in dozens of clinical and lab benchtop studies.

Brain rhythms (also called brain ""waves"" or ""oscillations"") arise from the synchronized, network activity of brain cells and circuits as they coordinate to enable brain functions such as perception or cognition. Lower-range gamma frequency rhythms, those around 40 cycles a second, or Hz, are particularly important for memory processes, and MIT's research has shown that they are also associated with specific changes at the cellular and molecular level. The 2016 study and many others since then have produced evidence initially in animals and more recently in humans that various non-invasive means of enhancing the power and synchrony of 40Hz gamma rhythms helps to reduce Alzheimer's pathology and its consequences.
""What started in 2016 with optogenetic and visual stimulation in mice has expanded to a multitude of stimulation paradigms, a wide range of human clinical studies with promising results and is narrowing in on the mechanisms underlying this phenomenon,"" wrote the authors including Li-Huei Tsai, Picower Professor in The Picower Institute for Learning and Memory and the Department of Brain and Cognitive Sciences at MIT.
Though the number of studies and methods has increased and the data has typically suggested beneficial clinical effects, the article's authors also clearly caution that the clinical evidence remains preliminary and that animal studies intended to discern how the approach works have been instructive but not definitive.
""Research into the clinical potential of these interventions is still in its nascent stages,"" the researchers, led by MIT postdoc Cristina Blanco-Duque, wrote in introducing the review. ""The precise mechanisms underpinning the beneficial effects of gamma stimulation in Alzheimer's disease are not yet fully elucidated, but preclinical studies have provided relevant insights.""
Preliminarily promising
The authors list and summarize results from 16 clinical studies published over the last several years. These employ gamma frequency sensory stimulation (e.g. exposure to light, sound, tactile vibration, or a combination), trans cranial alternating current stimulation (tACS), in which a brain region is stimulated via scalp electrodes, or transcranial magnetic stimulation (TMS), in which electric currents are induced in a brain region using magnetic fields. The studies also vary in their sample size, design, duration and in what effects they assessed. Some of the sensory studies using light have tested different colors and different exact frequencies. And while some studies show that sensory stimulation appears to affect multiple regions in the brain, tACS and TMS are more regionally focused (though those brain regions still connect and interact with others).

Given the variances, the clinical studies taken together offer a blend of uneven but encouraging evidence, the authors write. Across clinical studies involving patients with Alzheimer's disease, sensory stimulation has proven safe and well tolerated. Multiple sensory studies have measured increases in gamma power and brain network connectivity. Sensory studies have also reported improvements in memory and/or cognition as well as sleep. Some have yielded apparent physiological benefits such as reduction of brain atrophy, in one case, and changes in immune system activity in another. So far, sensory studies have not shown reductions in Alzheimer's hallmark proteins, amyloid or tau.
Clinical studies stimulating 40Hz rhythms using tACS, ranging in sample size from only one to as many as 60, are the most numerous so far and many have shown similar benefits. Most report benefits to cognition, executive function and/or memory (depending sometimes on the brain region stimulated) and some have assessed that benefits endure even after treatment concludes. Some have shown effects on measures of tau and amyloid, blood flow, neuromodulatory chemical activity, or immune activity. Finally a 40Hz stimulation clinical study using TMS in 37 patients found improvements in cognition, prevention of brain atrophy and increased brain connectivity.
""The most important test for gamma stimulation is without a doubt whether it is safe and beneficial for patients,"" the authors wrote. ""So far, results from several small trials on sensory gamma stimulation suggest that it is safe, evokes rhythmic EEG brain responses, and there are promising signs for AD symptoms and pathology. Similarly, studies on transcranial stimulation report the potential to benefit memory and global cognitive function even beyond the end of treatment.""
Studying underlying mechanisms
In parallel, dozens more studies have shown significant benefits in mice including reductions in amyloid and tau, preservation of brain tissue and improvements in memory. But animal studies also have offered researchers a window into the cellular and molecular mechanisms by which gamma stimulation might have these effects.
Before MIT's original studies in 2016 and 2019 researchers had not attributed molecular changes in brain cells to changes in brain rhythms, but those and other studies have now shown that they affect not only the molecular state of neurons, but also the brain's microglia immune cells, astrocyte cells that play key roles in regulating circulation and indeed the brain's vasculature system. A hypothesis of Tsai's lab right now is that sensory gamma stimulation might promote the clearance of amyloid and tau via increased circulatory activity of brain fluids.

A hotly debated aspect of gamma stimulation is how it affects the electrical activity of neurons and how pervasively. Studies indicate that inhibitory ""interneurons"" are especially affected, though, offering a clue about how increased gamma activity, and its physiological effects, might propagate.
""The field has generated tantalizing leads on how gamma stimulation may translate into beneficial effects on the cellular and molecular level,"" the authors wrote.
Gamma going forward
As the authors make clear that more definitive clinical studies are needed, they note that at the moment, there are now 15 new clinical studies of gamma stimulation underway. Among these is a phase 3 clinical trial by the company Cognito Therapeutics, which has licensed MIT's technology. That study plans to enroll hundreds of participants.
Meanwhile, some recent or new clinical and preclinical studies have begun looking at whether gamma stimulation may be applicable to neurological disorders other than Alzheimer's, including stroke or Down syndrome. In experiments with mouse models, for example, an MIT team has been testing gamma stimulation's potential to help with cognitive effects of chemotherapy, or ""chemobrain.""
""Larger clinical studies are required to ascertain the long-term benefits of gamma stimulation,"" the authors conclude. ""In animal models the focus should be on delineating the mechanism of gamma stimulation and providing further proof of principle studies on what other applications gamma stimulation may have.""
In addition to Tsai and Blanco-Duque, the paper's other authors are Diane Chan, Martin Kahn, and Mitch Murdock.

","score: 16.301689975530667, grade_level: '16'","score: 17.723536515009847, grade_levels: ['college_graduate'], ages: [24, 100]",10.1111/joim.13755,"Alzheimer's disease (AD) is the most common type of neurodegenerative disease and a health challenge with major social and economic consequences. In this review, we discuss the therapeutic potential of gamma stimulation in treating AD and delve into the possible mechanisms responsible for its positive effects. Recent studies reveal that it is feasible and safe to induce 40 Hz brain activity in AD patients through a range of 40 Hz multisensory and noninvasive electrical or magnetic stimulation methods. Although research into the clinical potential of these interventions is still in its nascent stages, these studies suggest that 40 Hz stimulation can yield beneficial effects on brain function, disease pathology, and cognitive function in individuals with AD. Specifically, we discuss studies involving 40 Hz light, auditory, and vibrotactile stimulation, as well as noninvasive techniques such as transcranial alternating current stimulation and transcranial magnetic stimulation. The precise mechanisms underpinning the beneficial effects of gamma stimulation in AD are not yet fully elucidated, but preclinical studies have provided relevant insights. We discuss preclinical evidence related to both neuronal and nonneuronal mechanisms that may be involved, touching upon the relevance of interneurons, neuropeptides, and specific synaptic mechanisms in translating gamma stimulation into widespread neuronal activity within the brain. We also explore the roles of microglia, astrocytes, and the vasculature in mediating the beneficial effects of gamma stimulation on brain function. Lastly, we examine upcoming clinical trials and contemplate the potential future applications of gamma stimulation in the management of neurodegenerative disorders."
"
In a new study, viewers of Facebook users' posts came away with perceptions of the users that differed from the users' own self-perceptions. Qi Wang and colleagues at Cornell University, New York, US, present these findings in the open-access journal PLOS ONE on December 20, 2023.

Many people post on social media platforms in order to express themselves and connect with others. Prior research has shown that viewers of personal websites, such as blogs or online profiles, form largely accurate perceptions of the authors' personalities. However, social media posts, such as Facebook status updates, are often isolated and lack context. Few studies have explored how users' self-perceptions align with how others perceive them after viewing such posts.
To shed new light, Wang and colleagues asked 158 undergraduate students to answer questions about their own personal characteristics, including their extraversion, disclosiveness, connectedness, self-esteem, independence, and interdependence. The students also shared their last 20 Facebook status updates.
Then, two groups of additional participants viewed the Facebook updates and answered questions about the users' characteristics. One group viewed the updates in a multimedia format with text and any accompanying images or hyperlinks, and the other saw text-only versions.
Overall, the viewers' perceptions of the Facebook users differed from users' self-perceptions. For instance, viewers tended to see users as being more disclosive, having lower self-esteem, and being less interdependent than how users perceived themselves. However, viewers' perceptions and self-perceptions of connectedness aligned, perhaps reflecting that a primary aim of social media posts is to connect with others.
Compared to text-only viewers' perceptions, multimedia viewers' perceptions were more in line with users' self-perceptions. However, there was more variation among multimedia perceptions, while text-only viewers showed more consensus. In addition, both groups' perceptions varied with users' gender and ethnicity, in line with judgments observed in offline contexts in prior research.
These findings provide new insights into the dynamics of online self-presentation and impression formation. The authors note that such understanding is important for fostering good communication and relationships. Future work could deepen understanding by, for instance, including a longer timeline of updates or considering other platforms such as TikTok.
The authors add: ""Can people form accurate impressions about us from our social media posts? Our study finds that there are substantial discrepancies between how people view Facebook users based on their status updates and how the users view themselves. Multimedia channels make the impressions more accurate, and user characteristics related to relationship-building, gender and ethnicity are more accurately perceived.""

","score: 13.862281468531474, grade_level: '14'","score: 15.60521853146853, grade_levels: ['college_graduate'], ages: [24, 100]",10.1371/journal.pone.0294990,"This study examines the cyber audience’s perception of social media users’ persona based on their online posts from a cognitive meaning-making perspective. Participants (N = 158) answered questions about their personal characteristics and provided their 20 most recent Facebook status updates. Two groups of viewers, who viewed either the text-only or multimedia version of the status updates, answered questions about the Facebook users’ personal characteristics. The viewers’ perceptions of Facebook users deviated from the users’ self-perceptions, although user characteristics that serve social motives were more accurately perceived. Multimedia viewers were more accurate than text viewers, whereas the latter showed a greater consensus. Gender and ethnic differences of Facebook users also emerged in online person perceptions, in line with gendered and cultured characteristics. These findings shed critical light on the dynamic interplay between social media users and the cyber audience in the co-construction of a digitally extended self."
"
Acute spinal cord injury (SCI) patients lose body weight and muscle mass, despite being on a high-calorie diet while in the intensive care unit. Their muscle wasting is substantial and extends beyond what can explained by inactivity or denervation (loss of nerve supply) alone.

Research led by The Ohio State University Wexner Medical Center and College of Medicine published in the journal Science Translational Medicine sheds new light and decodes early muscle loss after SCI to provide an unprecedent first understanding that muscle wasting is: rapid and severe a systemic phenomenon glucocorticoid dependentResearchers found that the severity of this SCI-induced systemic muscle wasting depends on location of the spinal cord injury (lesion level). More precisely, it depends on whether the adrenal gland becomes denervated after high thoracic injury (above T5), or not (after low thoracic injury).
These findings have direct clinical ramifications.
""Patients with a low body mass index (BMI) have a much higher risk to die shortly after suffering a spinal cord injury. With a better understanding of this muscle wasting and aggravated weight loss, we hope to explore new ways to reduce deaths in this fragile patient population,"" said Jan Schwab, MD, PHD, the William E. Hunt & Charlotte M. Curtis Chair and a professor of neurology and neurosciences at the Ohio State College of Medicine.
Researchers found that systemic muscle loss is worsened when the adrenal glands become deprived of central nervous system control resulting in a skewed hormonal (endocrine) tone. When this happens, hypercortisolism (excess cortisol release) often develops after the spinal cord injury.
""This hypercortisolism then acts on specific receptors in the muscle of the entire body to cause muscle loss. Interfering with this pathway could rescue muscle tissue and improve the response to rehabilitation,"" said first author Markus Harrigan, a member of Schwab's research lab and Ohio State's dual-degree MD-PhD Medical Scientist Training Program as well as a Ruth L. Kirschstein Individual NIH-Research Fellow.

This research also provides new insights on how to maintain muscle integrity while reducing the risk to develop higher degree pressure ulcers that often plague these patients, Harrigan said. The study builds on previous Ohio State research into the effects of SCI on the immune system that undermine immune system function, enhance infection susceptibility and contribute to infectious complications.
""We now start to understand how an injury of the spinal cord leads to spinal cord disease affecting the entire body,"" said Schwab, who is also medical director of the Belford Center for Spinal Cord Injury and a Scholar of the Chronic Brain Injury Initiative at Ohio State. ""Our future research will search for ways to block these complications and protect the adrenal gland from receiving 'false' autonomic nervous system information originating from the spinal cord below the lesion site.""
Ohio State scientists collaborated with researchers in Berlin, Germany, along with Nationwide Children's Hospital in Columbus, Ohio, and the University of Missouri.
This research is supported by funding from the National Institutes of Health (NIH)/National Institute of Neurological Disorders and Stroke grant F31NS117124; The Ohio State University Center for Muscle Health and Neuromuscular Disorders grant; NIH/National Institute of Neurological Disorders and Stroke grant 5R35NS111582; NIH/ National Institute of Disability, Independent Living and 855 Rehabilitation Research grant 90SI5020; NIH/National Institute of Neurological Disorders and Stroke grant R01NS118200; Craig H. Neilsen Foundation grant 596764; European Union Era Net -- Neuron Program, SILENCE grant 01EW170A; Wings for Life Spinal Cord Research Foundation grant, and the William E. Hunt and Charlotte M. Curtis Endowment.

","score: 18.92597839135654, grade_level: '19'","score: 21.705036014405763, grade_levels: ['college_graduate'], ages: [24, 100]",10.1126/scitranslmed.adh2156,"An incomplete mechanistic understanding of skeletal muscle wasting early after spinal cord injury (SCI) precludes targeted molecular interventions. Here, we demonstrated systemic wasting that also affected innervated nonparalyzed (supralesional) muscles and emerged within 1 week after experimental SCI in mice. Systemic muscle wasting caused muscle weakness, affected fast type 2 myofibers preferentially, and became exacerbated after high (T3) compared with low (T9) thoracic paraplegia, indicating lesion level–dependent (“neurogenic”) mechanisms. The wasting of nonparalyzed muscle and its rapid onset and severity beyond what can be explained by disuse implied unknown systemic drivers. Muscle transcriptome and biochemical analysis revealed a glucocorticoid-mediated catabolic signature early after T3 SCI. SCI-induced systemic muscle wasting was mitigated by (i) endogenous glucocorticoid ablation (adrenalectomy) and (ii) pharmacological glucocorticoid receptor (GR) blockade and was (iii) completely prevented after T3 relative to T9 SCI by genetic muscle-specific GR deletion. These results suggest that neurogenic hypercortisolism contributes to a rapid systemic and functionally relevant muscle wasting syndrome early after paraplegic SCI in mice."
"
Blood fat-lowering statins could slow the progression of Alzheimer's disease, at least for some patients. This is the result of a new study led by Karolinska Institutet published in Alzheimer Research and Therapy. But the researchers are cautious in their interpretations and see the results as a first step in a research journey that may eventually provide the answer.

A new study shows that people with Alzheimer's dementia deteriorated more slowly in their cognitive functions if they were also treated with a lipid-lowering statin, compared to those who were not treated. However, the study is an observational study where the researchers have compared data on the patients from a registry and therefore cannot answer whether there really is a causal relationship. Thus, the researchers are cautious in their interpretations.
""People with Alzheimer's dementia treated with statins had better cognitive development over time. However, the results of the study do not mean that we now have evidence that people with dementia should be treated with statins. But on the other hand, we can't see any support for not doing so. So, if a person needs statins for high blood lipids, a dementia diagnosis should not stop the treatment,"" says Sara Garcia-Ptacek, docent of neuroscience and assistant professor at the Department of Neurobiology, Care Sciences and Society, and research leader of the current study.
The reason she emphasizes safety is that statins were initially suspected of causing confusion in patients with dementia. There has therefore been some resistance to prescribing statins to these patients.
The study included data from more than 15,500 patients with dementia who also had an indication for lipid-lowering treatment. Almost 11,000 of them were treated with statins. In general, the patients who were treated with statins had slightly higher values in the cognitive tests carried out, even though they were more likely to have diagnoses such as high blood pressure, cardiovascular disease and diabetes, all of which are risk factors for dementia.
Sara Garcia-Ptacek explains that the research team started with a hypothesis that statins could slow the progression of dementia and cast a fairly wide net to see if they could find evidence of this.
""The basic idea of this study was to pave the way for a more precise cohort study that could eventually lead to a clinical intervention study, which is what is needed to prove a causal link between statins and cognition,"" says Sara Garcia-Ptacek.
The idea that statins could affect the risk or progression of dementia is not new. There are even some clinical studies done, but they have all been negative.
""We believe that only certain patients with Alzheimer's dementia may benefit from statins and that previous clinical trials have been too small to show any significant differences. ""Our idea is to try to crystallize which patient groups benefit the most and why, before embarking on clinical trials,"" says Sara Garcia-Ptacek.
The research was funded by Region Stockholm, the Swedish Research Council and the Dementia and Margareta af Ugglas Foundation.

","score: 12.643618361836182, grade_level: '13'","score: 14.147549954995505, grade_levels: ['college_graduate'], ages: [24, 100]",10.1186/s13195-023-01360-0,"Disturbances in brain cholesterol homeostasis may be involved in the pathogenesis of Alzheimer’s disease (AD). Lipid-lowering medications could interfere with neurodegenerative processes in AD through cholesterol metabolism or other mechanisms. To explore the association between the use of lipid-lowering medications and cognitive decline over time in a cohort of patients with AD or mixed dementia with indication for lipid-lowering treatment. A longitudinal cohort study using the Swedish Registry for Cognitive/Dementia Disorders, linked with other Swedish national registries. Cognitive trajectories evaluated with mini-mental state examination (MMSE) were compared between statin users and non-users, individual statin users, groups of statins and non-statin lipid-lowering medications using mixed-effect regression models with inverse probability of drop out weighting. A dose-response analysis included statin users compared to non-users. Our cohort consisted of 15,586 patients with mean age of 79.5 years at diagnosis and a majority of women (59.2 %). A dose-response effect was demonstrated: taking one defined daily dose of statins on average was associated with 0.63 more MMSE points after 3 years compared to no use of statins (95% CI: 0.33;0.94). Simvastatin users showed 1.01 more MMSE points (95% CI: 0.06;1.97) after 3 years compared to atorvastatin users. Younger (< 79.5 years at index date) simvastatin users had 0.80 more MMSE points compared to younger atorvastatin users (95% CI: 0.05;1.55) after 3 years. Simvastatin users had 1.03 more MMSE points (95% CI: 0.26;1.80) compared to rosuvastatin users after 3 years. No differences regarding statin lipophilicity were observed. The results of sensitivity analysis restricted to incident users were not consistent. Some patients with AD or mixed dementia with indication for lipid-lowering medication may benefit cognitively from statin treatment; however, further research is needed to clarify the findings of sensitivity analyses."
"
A collaborative effort between the University of Córdoba and IMIBIC uses, for the first time, changes in sweat metabolism to diagnose the severity of sleep apnea

In Greek, apnea (ἄπνοια) denotes the ""absence of breathing."" Hence, obstructive sleep apnea is a disease defined by interruptions in breathing, which recurs while the person suffering from it is asleep. A feeling of breathlessness, fatigue and drowsiness are symptoms that patients suffer. This disease is also related to the incidence of cardiovascular disorders, so to deal with these related problems, adequate diagnosis of the severity of the disease is necessary.
Alterations in the metabolism of people with sleep apnea are key to determining the severity of the disease. These changes are usually analyzed in blood or urine. However, in search of a less invasive and more accessible alternative, a team from the Department of Analytical Chemistry at the University of Córdoba and the Maimonides Institute for Biomedical Research in Córdoba (IMIBIC), formed by researchers Laura Castillo, Mónica Calderón, Feliciano Priego and Bernabé Jurado, has verified, for the first time, the potential of sweat samples to ascertain the severity of sleep apnea.
""By analyzing sweat metabolome and its alterations, mainly at night, we were able to see what stage of the disease the patients were in,"" explains Laura Castillo, the study's lead author. For her, the advantages of using sweat over other samples are clear: ""it is a non-invasive and clean sample since, unlike the case with blood, we don't have to remove proteins, and it's much easier to analyze and detect metabolites.""
In this study, sweat samples from before and after sleep were analyzed from a series of individuals with sleep apnea at different stages, as well as from a control group without the disease.
In these samples, using the gas chromatography technique, coupled with high-resolution mass spectrometry, 78 metabolites were identified and their changes were studied, mostly related to energy production and oxidative stress. ""We could see how the sweat metabolism itself indicates those alterations during sleep as a result of which the person's energy production worsens and their oxidative stress increases,"" says Castillo. Thus, with a personalized follow-up using the sweat excreted during the sleep of a person with the disease, its development can be tracked, and its possible effects, such as cardiovascular problems, can be monitored. This metabolomic profile also made it possible, in the trial, to distinguish between those who suffered from the disease and those who did not have it and belonged to the control group.
An index to learn more about the disease
In addition to establishing sweat as a good sentinel when it comes to determining the stage of the disease, this work also reveals the importance of taking into account the oxygen desaturation index when diagnosing it.
The diagnosis of sleep apnea is currently based on the Apnea-Hypopnea Index (AHI), which measures sleep apnea based on the episodes of shortness of breath one suffers per hour (for example, the disease is severe when one has 30 or more episodes per hour). According to the team, this index ""does not provide all the information about the disease or the patient's situation at a given time"" since it counts how many events there are, but not their severity.
Therefore, in their study they also verify the importance of using the oxygen desaturation index, which shows how serious the episodes are by measuring the number of events in which oxygen saturation has decreased by more than 3%. After verifying the linear relationship between this index and the AHI, its validity has been confirmed, since, in addition to the data provided by the AHI, it also measures severity, taking into account oxygen saturation loss.

","score: 17.187119999999997, grade_level: '17'","score: 18.710552421052633, grade_levels: ['college_graduate'], ages: [24, 100]",10.1111/jsr.14075,"Obstructive sleep apnea (OSA) is a sleep disorder that has been associated with the incidence of other pathologies. Diagnosis is mainly based on the apnea–hypopnea index (AHI) obviating other repercussions such as intermittent hypoxemia, which has been found to be associated to cardiovascular complications. Blood‐based samples and urine have been the most utilised biofluids in metabolomics studies related to OSA, while sweat could be an alternative due to its non‐invasive and accessible sampling, its reduced complexity, and comparability with other biofluids. Therefore, this research aimed to evaluate metabolic overnight changes in sweat collected from patients with OSA classified according to the AHI and oxygen desaturation index (ODI), looking for potential cardiovascular repercussions. Pre‐ and post‐sleeping sweat samples from all individuals (n = 61) were analysed by gas chromatography coupled to high‐resolution mass spectrometry after appropriate sample preparation to detect as many metabolites as possible. Permanent significant alterations in the sweat were reported for pyruvate, serine, lactose, and hydroxybutyrate. The most relevant overnight metabolic alterations in sweat were reported for lactose, succinate, urea, and oxoproline, which presented significantly different effects on factors such as the AHI and ODI for OSA severity classification. Overall metabolic alterations mainly affected energy production‐related processes, nitrogen metabolism, and oxidative stress. In conclusion, this research demonstrated the applicability of sweat for evaluation of OSA diagnosis and severity supported by the detected metabolic changes during sleep."
"
Groundbreaking University of Limerick, Ireland research has shed new light on the link between childhood adversity and future risk of death.

A major international study led by researchers at UL and published in the journal of Psychosomatic Medicine: Journal of Biobehavioral Medicine has examined the association between adverse childhood experiences and the increased risk of premature mortality.
Adverse childhood experiences such as emotional and physical abuse, household instability, socioeconomic climate and ill health can lead to people having a shorter life, but it was not clear how.
The researchers believed that, as individuals with these adverse experiences in their childhood can suffer from lower self-acceptance and purpose in life, that these could be a pathway or a 'mechanism' linking these experiences to future mortality risk.
The new study, which followed 6,128 people across 24 years in the United States, found that self-acceptance -- positive attitudes towards oneself and acknowledging and accepting multiple aspects of yourself -- and purpose in life -- a sense of a goal-directed direction in life -- do explain part of the reason why childhood adversity is related to future longevity.
The project was led by Associate Professor of Psychology at University of Limerick Dr Páraic Ó Súilleabháin, Director of the Personality, Individual Differences and Biobehavioural Health Laboratory and member of the Health Research Institute.
The work was conducted in collaboration with others from University of Limerick, West Virginia University, Open University of the Netherlands, University of Minnesota, and Florida State University.

Commenting on the study, Dr Ó Súilleabháin said: ""It is very important to find the ways in which experiences such as these early in life can have an impact across our lives. We have previously found that these experiences are related to a shortening of life expectancy. It is important to understand the mechanisms linking them, so that ways to increase life expectancy can be identified.
""We found that self-acceptance and purpose in life are very important in the link between these childhood experiences and risk of death in adulthood. In other words, of all the possible factors in the link between childhood adversity and risk of future death, it appears that self-acceptance and purpose in life are two important drivers.""
The research team used the 'Midlife in the United States Survey' to test whether these factors were indirect pathways that increased the association between adverse childhood experiences and mortality hazards over 24 years of follow-up.
They included 20 possible childhood adversities, and the results show that adverse childhood experiences do significantly increase mortality risk and that self-acceptance and purpose does account for a percentage of those -- effects which withstood a range of adjustments and sensitivity analyses, according to the researchers.
Given that self-acceptance and purpose can change through intervention, these factors may be useful targets for individuals with adverse childhood experiences that could reduce the health risks in later life, Dr Ó Súilleabháin explained.
""Research suggests that self-acceptance and purpose can change through intervention. These interventions are not just needed at the individual level, but also at the societal level,"" Dr Ó Súilleabháin explained.
""For instance, while it is relatively straightforward to think of avenues with various therapies and so on, it is incredibly challenging for someone to foster self-acceptance and purpose without basic needs being met in healthcare, housing, education, and so on.
""Impacting self-acceptance and purpose in life in adulthood for those who have had childhood adversity, may ultimately impact their longevity,"" Dr Ó Súilleabháin added.

","score: 17.52847750865052, grade_level: '18'","score: 19.7833117829175, grade_levels: ['college_graduate'], ages: [24, 100]",10.1097/PSY.0000000000001266,"Adverse childhood experiences (ACEs) are associated with increased risk of premature mortality, but it is not clear why. Individuals with ACEs tend to have lower self-acceptance and purpose in life, which may be pathways between ACEs and risk of premature mortality. As such, we tested whether purpose and self-acceptance are mechanisms that link ACEs to mortality risk. We used the Midlife in the United States Survey (N = 6218; M ± SD = 46.89 ± 12.94 years) to test whether these factors were indirect pathways (mediated) the association between ACEs and mortality hazards over 24 years of follow-up. We employed a comprehensive ACEs measure that included 20 possible childhood adversities including emotional and physical abuse, household instability, socioeconomic climate, and ill health. ACEs significantly increased mortality risk, HR = 1.028, 95% CI (1.008, 1.047), p = 0.006. Self-acceptance and purpose accounted for an estimated 15% and 4% of the ACEs-mortality relation respectively. These effects withstood a range of adjustments and sensitivity analyses. ACEs may impact mortality risk partially through lower self-acceptance and purpose during adulthood. Given that self-acceptance and purpose may change through intervention, these factors may be useful targets for individuals with ACEs that could lead to a longer life."
"
Down syndrome, a congenital disorder stemming from abnormal cell division and differentiation, is most common in newborns fated to neurodevelopmental delays and other health complications.

The genetic defect causes the dysfunction of the protein kinase DYRK1A, which is encoded on chromosome 21 and is deeply associated with both Down syndrome and autism spectrum disorder. DYRK1A has attracted attention as a target molecule for treating various diseases, but specific cellular mechanisms regulating the enzyme DYRK1A have yet to be made clear.
Now, researchers at Kyoto University have identified the FAM53C protein and its DYRK1A-inhibiting effect that keeps the protein kinase inactive inside the cytoplasm.
""Our findings demonstrate the important role of the intracellular regulatory mechanism of DYRK1A in the normal development and function of the neuropsychiatric system,"" says first author Yoshihiko Miyata at KyotoU's Graduate School of Biostudies.
""The molecular regulation of the highly complex development and activity of the human brain fascinates me,"" adds Miyata. In addition to neuropsychiatric symptoms, Down syndrome may also cause early onset of Alzheimer's disease, type 2 diabetes, and facial maldevelopment.
""Given DYRK1A's significance, we have explored potential molecules serving as its interacting counterpart,"" says Miyata.
DYRK1A controls many biological functions, including the development and function of the nervous system. At the cellular level, this critical protein phosphorylates various other proteins in the cytoplasm and nucleus to regulate the cell cycle, cell differentiation, cytoskeletal formation, and DNA damage response.
After identifying DCAF7/WDR68 as a major binding protein for DYRK1A in a previous study, Miyata's team used mass spectrometry to uncover other interacting proteins that modulate DYRK1A's function and cellular location. Notably, the structurally flexible FAM53C protein binds directly to a region of DYRK1A responsible for protein phosphorylation. This interaction reduces DYRK1A's kinase activity, securely anchoring DYRK1A within the cytoplasm but outside the cell nucleus, as in normal brain tissue.
""The FAM53C-mediated regulation of the protein kinase activity may significantly impact gene expression regulation caused by normal and aberrant levels of DYRK1A, giving us many potential clinical insights,"" suggests Miyata.

","score: 16.977072599531613, grade_level: '17'","score: 17.61077283372365, grade_levels: ['college_graduate'], ages: [24, 100]",10.26508/lsa.202302129,"The protein kinase DYRK1A encoded in human chromosome 21 is the major contributor to the multiple symptoms observed in Down syndrome patients. In addition, DYRK1A malfunction is associated with various other neurodevelopmental disorders such as autism spectrum disorder. Here, we identified FAM53C with no hitherto known biological function as a novel suppressive binding partner of DYRK1A. FAM53C is bound to the catalytic protein kinase domain of DYRK1A, whereas DCAF7/WDR68, the major DYRK1A-binding protein, binds to the N-terminal domain of DYRK1A. The binding of FAM53C inhibited autophosphorylation activity of DYRK1A and its kinase activity to an exogenous substrate, MAPT/Tau. FAM53C did not bind directly to DCAF7/WDR68, whereas DYRK1A tethered FAM53C and DCAF7/WDR68 by binding concurrently to both of them, forming a tri-protein complex. DYRK1A possesses an NLS and accumulates in the nucleus when overexpressed in cells. Co-expression of FAM53C induced cytoplasmic re-localization of DYRK1A, revealing the cytoplasmic anchoring function of FAM53C to DYRK1A. Moreover, the binding of FAM53C to DYRK1A suppressed the DYRK1A-dependent nuclear localization of DCAF7/WDR68. All the results show that FAM53C binds to DYRK1A, suppresses its kinase activity, and anchors it in the cytoplasm. In addition, FAM53C is bound to the DYRK1A-related kinase DYRK1B with an Hsp90/Cdc37-independent manner. The results explain for the first time why endogenous DYRK1A is distributed in the cytoplasm in normal brain tissue. FAM53C-dependent regulation of the kinase activity and intracellular localization of DYRK1A may play a significant role in gene expression regulation caused by normal and aberrant levels of DYRK1A."
"
Air quality in the office may affect our level of creativity at work, scientists at Nanyang Technological University, Singapore (NTU Singapore) have found.

Working with the global air filter manufacturer Camfil on a shared research project, the NTU Singapore scientists found in a study that high levels of volatile organic compounds -- gases released from products such as detergents, pesticides, perfumes, aerosol sprays and paint -- affected the study participants' creativity when they were asked to build 3D models with LEGO bricks.
Using a statistical analysis, the NTU team estimated that reducing total volatile organic compounds (TVOC) by 72 per cent could improve a student's creative potential by 12 per cent.
TVOC is an indicator that refers to the volume of volatile organic compounds in the air. Indoor VOCs are emitted from interior decoration sources such as paints and carpets and household products such as detergents and air fresheners.
This study, conducted on the NTU Smart Campus, is part of a partnership between the University and Camfil to investigate the impact of indoor air quality on the cognitive performance of adults, test various air filter technologies in tropical weather conditions, and deliver innovative clean air solutions combined with optimised energy efficiency.
The findings detailed in the study, published in Scientific Reports in September, shed light on the importance of indoor air quality on our creative cognition, said the research team led by Assistant Professor Ng Bing Feng and Associate Professor Wan Man Pun, Cluster Directors for Smart & Sustainable Building Technologies at the Energy Research Institute @ NTU (ERI@N).
Asst Prof Ng said: ""While most people would correctly associate indoor air quality with effects on the lungs, especially since we just emerged from a pandemic, our study shows that it could also have an impact on the mind and creative cognition, or the ability to use knowledge in an unconventional way. Our findings suggest that relatively low TVOC levels, even if well within the accepted threshold, could impact an individual's creative potential.""
Assoc Prof Wan added: ""This could have serious consequences for industries that rely on creativity for the bulk of their work. For instance, artists often use paints and thinners that release high levels of volatile organic compounds and may not know they need adequate ventilation to clear them from their workplace. The findings also point to how making minor adjustments in the office, such as reducing the use of aroma diffusers or ensuring adequate ventilation, could positively impact employees and their productivity.""

The study also aligns with the Health & Society and Brain & Learning research clusters under the research pillar of NTU 2025, the University's five-year strategic plan.
The other scientists on the research team were NTU PhD graduate Dr Shmitha Arikrishnan, former NTU senior research fellow Dr Adam Charles Robert, who is currently a postdoctoral researcher at Singapore-ETH Centre, and NTU graduate Lau Wee Siang.
Assessing creativity through LEGO 3D models
To quantifiably assess creative potential in this study, the NTU team developed the Serious Brick Play method, which is largely adapted from the LEGO Serious Play framework. This tool involves expressing thoughts and ideas using 3D models built with LEGO bricks.
A typical LEGO Serious Play session involves a facilitator who introduces a challenge, to which participants respond by building a model using LEGO bricks. Participants then discuss their models and reflect on the building process, prompted by the facilitator.
In the Serious Brick Play method designed by the NTU team, participants do not discuss their models and share their reflections in a group. Instead, they provide written descriptions of their LEGO models. These written descriptions and LEGO models are then scored by a panel of judges for creativity.

Asst Prof Ng explained: ""While the LEGO Serious Play framework has been used in various settings to unleash creative thinking and has even been used to support dementia patients, it does not have a quantitative assessment component and cannot systematically assess creativity. This is why we added a component to score participants on their creativity.""
The scoring guidelines for the participants' LEGO models were developed based on the Creative Product Analysis Matrix model, which is used to grade creativity and has been validated in earlier studies, he added.
The NTU researchers tested the scoring guidelines to measure the degree of consistency among the different judges when they independently assessed the LEGO models, and concluded that the scoring guidelines provided were reliable.
The researchers also tested the Serious Brick Play method's ability to measure what it was designed for through statistical analyses and found that the method was able to cover the key aspects of the Alternative Uses Task, a well-known tool that assesses creativity. Specifically, it assesses divergent thinking, a thought process used to generate creative ideas by exploring many possible solutions.
The researchers said that the Serious Brick Play method further assesses another thought process called convergent thinking, which focuses on coming up with a single, well-established answer to a problem.
""Divergent and convergent thinking are thought to be the central components of creativity, but most existing tools are designed around divergent thinking. Our Serious Brick Play method adds value by also covering the aspect of convergent thinking,"" said Asst Prof Ng.
How the study was done
Over six weeks, the researchers gathered data from a sample size of 87 undergraduate and postgraduate students in a controlled environment simulating an indoor workspace. Every week across three 40-minute sessions, the study participants read a summary of a global issue -- such as climate change, mental health, and poverty -- and then offered a solution by building a 3D model using LEGO bricks. The participants were then asked to give a written description and explanation for their models.
In each session, researchers varied the air quality of the workspace using different combinations of air filters contributed by Camfil. This varied the level of pollutants in the air, including carbon dioxide, PM2.5 (air pollutants less than 2.5 micrometres in diameter), and total volatile organic compounds (TVOC).
The participants' LEGO models and descriptions were then graded by seven randomly selected adults, who were trained to familiarise themselves with the scoring guidelines based on: Originality: whether the solution is usual or unusual, Fluency: the level of elaboration in the description of the solution, and Build: how sophisticated, complex, or aesthetic the solution is.Link between TVOC levels and creativity
The NTU team's statistical analysis of the participants' average scores and indoor air quality data gathered from 18 sessions revealed that participants tended to turn in creative solutions with lower scores -- an indicator of lower creative potential -- when the workspace had higher TVOC levels.
Using a statistical model, the team calculated that reducing TVOC from an acceptable threshold of 1,000 parts per billion to 281 parts per billion -- or a 72 per cent reduction in TVOC levels -- led to a 12 per cent increase in creative potential in the study cohort.
Less significant relationships were found between PM2.5 and creativity as well as carbon dioxide levels and creativity.
Asst Prof Ng said: ""The results from this study indicate that creativity levels can be linked to the concentration of pollutants in a room. Improving the air quality could be an economical solution to improve occupants' creativity.""
Having uncovered a link between TVOC levels and creativity, the research team is now studying how TVOC and other indoor air pollutants affect cognitive processes by measuring participants' brain activity.

","score: 16.971392604248624, grade_level: '17'","score: 18.753709284028325, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41598-023-42355-z,"Companies are increasingly asking their employees to find creative solutions to their problems. However, the office environment may reduce an employee’s creative potential. In this study, the role of indoor air quality parameters (PM2.5, TVOC, and CO2) in maintaining a creative environment (involving lateral thinking ability) was evaluated by Serious Brick Play (SBP), an adaptation of the LEGO Serious Play (LSP) framework. This study was conducted in a simulated office space with 92 participants over a period of 6 weeks. The SBP required participants to address a challenge by building using Lego bricks, and then describe the solution within a given timeframe. The creations and descriptions were then graded in terms of originality, fluency, and build. The results indicated that higher TVOC levels were significantly associated with lower-rated creative solutions. A 71.9% reduction in TVOC (from 1000 ppb), improves an individual’s full creative potential by 11.5%. Thus, maintaining a low TVOC level will critically enhance creativity in offices."
"
In a study comparing human brain communication networks with those of macaques and mice, EPFL researchers found that only the human brains transmitted information via multiple parallel pathways, yielding new insights into mammalian evolution.

When describing brain communication networks, EPFL senior postdoctoral researcher Alessandra Griffa likes to use travel metaphors. Brain signals are sent from a source to a target, establishing a polysynaptic pathway that intersects multiple brain regions ""like a road with many stops along the way.""
She explains that structural brain connectivity pathways have already been observed based on networks (""roads"") of neuronal fibers. But as a scientist in the Medical Image Processing Lab (MIP:Lab) in EPFL's School of Engineering, and a research coordinator at CHUV's Leenaards Memory Centre, Griffa wanted to follow patterns of information transmission to see how messages are sent and received. In a study recently published in Nature Communications, she worked with MIP:Lab head Dimitri Van de Ville and SNSF Ambizione Fellow Enrico Amico to create ""brain traffic maps"" that could be compared between humans and other mammals.
To achieve this, the researchers used open-source diffusion (DWI) and functional magnetic resonance imaging (fMRI) data from humans, macaques, and mice, which was gathered while subjects were awake and at rest. The DWI scans allowed the scientists to reconstruct the brain ""road maps,"" and the fMRI scans allowed them to see different brain regions light up along each ""road,"" which indicated that these pathways were relaying neural information.
They analyzed the multimodal MRI data using information and graph theory, and Griffa says that it is this novel combination of methods that yielded fresh insights.
""What's new in our study is the use of multimodal data in a single model combining two branches of mathematics: graph theory, which describes the polysynaptic 'roadmaps'; and information theory, which maps information transmission (or 'traffic') via the roads. The basic principle is that messages passed from a source to a target remain unchanged or are further degraded at each stop along the road, like the telephone game we played as children.""
The researchers' approach revealed that in the non-human brains, information was sent along a single ""road,"" while in humans, there were multiple parallel pathways between the same source and target. Furthermore, these parallel pathways were as unique as fingerprints, and could be used to identify individuals.

""Such parallel processing in human brains has been hypothesized, but never observed before at a whole-brain level,"" Griffa summarizes.
Potential insights for evolution and medicine
Griffa says that the beauty of the researchers' model is its simplicity, and its inspiration of new perspectives and research avenues in evolution and computational neuroscience. For example, the findings can be linked to the expansion of human brain volume over time, which has given rise to more complex connectivity patterns.
""We could hypothesize that these parallel information streams allow for multiple representations of reality, and the ability to perform abstract functions specific to humans.""
She adds that although this hypothesis is only speculative, as the Nature Communications study involved no testing of subjects' computational or cognitive ability, these are questions that she would like to explore in the future.
""We looked at how information travels, so an interesting next step would be to model more complex processes to study how information is combined and processed in the brain to create something new.""
As a memory and cognition researcher, she is especially interested in using the model developed in the study to investigate if parallel information transmission could confer resilience to brain networks, and potentially play a role in neurorehabilitation after brain injury, or in the prevention of cognitive decline in pathologies of advanced age.
""Some people age healthily, while others experience cognitive decline, so we'd like to see if there is a relationship between this difference and the presence of parallel information streams, and whether they could be trained to compensate neurodegenerative processes.""

","score: 17.5934422378817, grade_level: '18'","score: 19.520006406149903, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41467-023-43971-z,"Brain communication, defined as information transmission through white-matter connections, is at the foundation of the brain’s computational capacities that subtend almost all aspects of behavior: from sensory perception shared across mammalian species, to complex cognitive functions in humans. How did communication strategies in macroscale brain networks adapt across evolution to accomplish increasingly complex functions? By applying a graph- and information-theory approach to assess information-related pathways in male mouse, macaque and human brains, we show a brain communication gap between selective information transmission in non-human mammals, where brain regions share information through single polysynaptic pathways, and parallel information transmission in humans, where regions share information through multiple parallel pathways. In humans, parallel transmission acts as a major connector between unimodal and transmodal systems. The layout of information-related pathways is unique to individuals across different mammalian species, pointing at the individual-level specificity of information routing architecture. Our work provides evidence that different communication patterns are tied to the evolution of mammalian brain networks."
"
Researchers at LMU, the Max Planck Institute for Human Development, and the University of Oxford have investigated how sleep affects memory. They found a link between breathing and the emergence of certain brain activity patterns in sleep that are associated with the reactivation of memory contents. The data points to possible consequences of unhealthy breathing on memory.

How are memories consolidated during sleep? In 2021, researchers led by Dr. Thomas Schreiner, leader of the Emmy Noether junior research group at LMU's Department of Psychology, had already shown there was a direct relationship between the emergence of certain sleep-related brain activity patterns and the reactivation of memory contents during sleep. However, it was still unclear whether these rhythms are orchestrated by a central pacemaker. So the researchers joined up with scientists from the Max Planck Institute for Human Development in Berlin and the University of Oxford to reanalyze the data. Their results have identified respiration as a potential pacemaker. ""That is to say, our breathing influences how memories are consolidated during sleep,"" says Schreiner.
Learning processes investigated in sleep laboratory
For their original study, the researchers showed 20 study participants 120 images over the course of two sessions. All the pictures were associated with certain words. Then the participants slept for around two hours in the sleep laboratory. When they awoke, they were questioned about the associations they had learned. During the entire learning and sleep period, their brain activity was recorded by means of EEG, along with their breathing.
The researchers discovered that previously learned contents were spontaneously reactivated by the sleeping brain during the presence of so-called slow oscillations and sleep spindles (short phases of increased brain activity). ""The precision of the coupling of these sleep-related brain rhythms increases from childhood to adolescence and then declines again during aging,"" says Schreiner.
Breathing and brain activity are linked
Because respiration frequency also changes with age, the researchers then analyzed the data in relation to the recorded breathing and were able to establish a connection between them: ""Our results show that our breathing and the emergence of characteristic slow oscillation and spindle patterns are linked,"" says Schreiner. ""Although other studies had already established a connection between breathing and cognition during wake, our work makes clear that respiration is also important for memory processing during sleep.""
Older people often suffer from sleep disorders, respiratory disorders, and declining memory function. Schreiner plans to further investigate whether there are connections between these phenomena and whether interventions -- such as the use of CPAP masks, which are already used to treat sleep apnea -- make sense from a cognitive perspective.

","score: 14.165466063348422, grade_level: '14'","score: 15.876651583710405, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41467-023-43450-5,"The beneficial effect of sleep on memory consolidation relies on the precise interplay of slow oscillations and spindles. However, whether these rhythms are orchestrated by an underlying pacemaker has remained elusive. Here, we tested the relationship between respiration, which has been shown to impact brain rhythms and cognition during wake, sleep-related oscillations and memory reactivation in humans. We re-analysed an existing dataset, where scalp electroencephalography and respiration were recorded throughout an experiment in which participants (N = 20) acquired associative memories before taking a nap. Our results reveal that respiration modulates the emergence of sleep oscillations. Specifically, slow oscillations, spindles as well as their interplay (i.e., slow-oscillation_spindle complexes) systematically increase towards inhalation peaks. Moreover, the strength of respiration - slow-oscillation_spindle coupling is linked to the extent of memory reactivation (i.e., classifier evidence in favour of the previously learned stimulus category) during slow-oscillation_spindles. Our results identify a clear association between respiration and memory consolidation in humans and highlight the role of brain-body interactions during sleep."
"
People who abandon New Year's resolutions or other commitments can maintain the respect of their peers by blaming external factors such as lack of money, new research suggests.

Studies have found that people were more likely to be seen as having good self-control despite abandoning a commitment to live a healthier life if they claimed they did not have the money for a gym membership or expensive new cooking equipment. People who instead claimed they didn't have the time to exercise or to replace a takeaway habit with healthy, home-cooked food, were more likely to be seen as having poor self-control.
Dr Janina Steinmetz, Reader in Marketing at Bayes Business School (formerly Cass), who conducted the research, analysed which excuses boost the chances of people appearing to have good self-control even after they fail to keep a resolution or pledge.
She said: ""Many resolutions or commitments involve either time or money so the lack of one or the other seems to provide a good excuse for breaking it without adversely affecting how others see us. However, these two excuses are not equally effective. My six experiments involving around 1,200 people found that pleading a lack of money leads to better outcomes -- in terms of perceptions about the individual -- than citing lack of time.""
For example, in one experiment, 200 online participants read about people who failed to keep a commitment to eat healthier food. Some of those they read about blamed the cost of cooking good meals while others said they were defeated by a lack of time. Participants saw the first group as having better self-control and were more likely to consider them as potentially good gym partners.
The differences appear to reflect how much the excuse is seen as being within the person's control, Dr Steinmetz suggests.
She said: ""These results are surprising because people like to use lack of time as an excuse when they can't do something. They equate lack of time with high status. However, the studies suggest we tend to think others could find the time to exercise or cook healthy meals if they were sufficiently motivated. That is why citing factors many of us have less control over, such as lack of money, can produce perceptions of having better self-control even when we abandon our New Year's resolution or break a commitment.""
The results, published last week in the European Journal of Social Psychology, could have implications for local authorities, NHS organisations and others campaigning on public health issues -- and health professionals working with obese people.

Dr Steinmetz explained: ""People often justify a diet heavy in fast food or TV dinners by saying it is quicker than buying and cooking healthy ingredients. Organisations promoting or marketing healthy lifestyles or working with patients around behaviour change can challenge that self-aggrandising claim that people are 'just too busy' to choose the healthy option. They can promote healthy but easy-to-prepare meals using affordable ingredients, or the benefits of even half an hour's aerobic activity. That would undermine the credibility of an all-too-familiar excuse.""
There might also be lessons in the research for anyone in the market for a new job or romance.
Dr Steinmetz said: ""In job interviews and on dating website questionnaires people are often invited to talk about a failure they've had in life. Obviously, we've all had them but when explaining why, whether you're looking for a job or for romance, blaming uncontrollable factors might help you convey a positive image. Although my research didn't look at those contexts, it might be wise to avoid the temptation to blame lack of time.""

","score: 13.052464305326748, grade_level: '13'","score: 14.633922295442062, grade_levels: ['college_graduate'], ages: [24, 100]",10.1002/ejsp.3010,"Research has shown that people frequently fail at exerting self‐control. Yet, having good self‐control is essential for being trusted and relied on. In this research, I test which common and frequent excuses for self‐control failures (i.e., resulting from lack of time vs. money) allow people to maintain an image of good self‐control despite failure. In six studies (five pre‐registered), using different types of self‐control domains, I show that participants perceived someone who failed at a resolution to nevertheless have good self‐control if they failed because they lacked money (vs. time) to follow through (Study 1). This effect was due to the mediated (Study 2a) and manipulated (Study 2b) perceived controllability of the excuse. This effect had downstream consequences for participants’ hypothetical and real behaviour toward the individual when their outcomes were interdependent (Studies 3 and 4). Finally, participants lacked insight into these patterns when communicating their own self‐control failures, which they attributed to a lack of time over money (Study 5)."
"
Certain facial features -- like downturned lips and a heavy brow -- are known to make someone appear untrustworthy to others, even though these do not indicate a person's actual character. Such facial biases influence our everyday social interactions as well as high-stakes decisions, including who we hire, or elect to political office.

But a new study by Columbia researchers shows that the effects of these judgments can be mitigated. The study outlines the results of four experiments that the authors conducted with 1,400 volunteers. Through those experiments, the researchers found that when real-world defendants have facial features that appear untrustworthy, they are more likely to be sentenced to death than life in prison. They also found that mock jurors were more likely to recommend a ruling against hypothetical defendants with an untrustworthy facial appearance. To get people to overcome these biases, the researchers developed a training intervention. Participants who underwent the training stopped relying on facial stereotypes, while participants in a control group who never received training remained strongly biased.
The findings are reported by a group led by Jon Freeman, an associate professor of psychology, in the journal Psychological Science. The other authors were Youngki Hong and Kao-Wei Chua, who were postdoctoral researchers at Columbia.
The researchers asked the participants to decide whose mugshots they felt were trustworthy or untrustworthy among 400 inmates in Florida who were convicted of murder. The inmates whose facial features were judged to be less trustworthy were far more likely to be sentenced to death than their counterparts without those features. This was even so in cases when participants' conscious decisions showed no bias against certain facial types. Using a test known as a sequential priming paradigm, the researchers could show that these participants did, in fact, harbor unconscious biases that predicted who was ultimately sentenced to death.
The intervention trained participants to dismantle their unconscious associations between specific facial features and an untrustworthy reaction using a computer task. Unlike educating or ""nudging"" participants not to rely on facial appearance in a conscious and deliberate way, Freeman and his colleagues' training works by making the implicit link in people's minds between certain facial features and an untrustworthy reaction as no longer stable or reliable. They did this by having participants associate untrustworthy-looking facial features with trustworthy behaviors, severing the implicit link between these features and untrustworthiness.
While prior research testing interventions that raise people's awareness of their facial bias and ask them to stop have failed to achieve success in reducing that bias, this new intervention operating on more unconscious principles was able to eliminate facial biases very successfully. The researchers were able to eliminate bias not only in participants' conscious decisions but also in their unconscious reactions. This is important because unconscious reactions can still wreak havoc on people's behavior, even when conscious decisions appear to be unbiased.
Racial and gender biases also strongly affect how trustworthy or untrustworthy another person is judged, biases that co-exist with the facial stereotypes the researchers studied, like downward-turned lips and a heavy brow. The researchers therefore conducted their studies only with white male faces to control for racial and gender biases. With the effects established, they are currently following up by testing the intervention with racially and gender diverse faces.
""These findings bolster prior work that facial stereotypes may have disastrous effects in the real world, but, more importantly, provide a potential inroad toward combating these sorts of biases,"" Freeman said. ""By exposing a cognitive pathway toward eradicating facial stereotypes, future research must investigate whether this training could be broadly applied and how to ensure the bias reduction persists over time.""
""If there are consequential judgments that are biased by facial stereotypes, our findings suggest that they have the potential to be flexibly remapped and dismantled,"" the paper concludes.

","score: 15.04072115384616, grade_level: '15'","score: 17.467114182692306, grade_levels: ['college_graduate'], ages: [24, 100]",10.1177/09567976231215238,"Initial impressions of others based on facial appearances are often inaccurate yet can lead to dire outcomes. Across four studies, adult participants underwent a counterstereotype training to reduce their reliance on facial appearance in consequential social judgments of White male faces. In Studies 1 and 2, trustworthiness and sentencing judgments among control participants predicted whether real-world inmates were sentenced to death versus life in prison, but these relationships were diminished among trained participants. In Study 3, a sequential priming paradigm demonstrated that the training was able to abolish the relationship between even automatically and implicitly perceived trustworthiness and the inmates’ life-or-death sentences. Study 4 extended these results to realistic decision-making, showing that training reduced the impact of facial trustworthiness on sentencing decisions even in the presence of decision-relevant information. Overall, our findings suggest that a counterstereotype intervention can mitigate the potentially harmful effects of relying on facial appearance in consequential social judgments."
"
A fascinating link between regular exercise and better brain health has been revealed, according to an international study that included a team of clinical researchers from Pacific Neuroscience Institute's Brain Health Center, located at Providence Saint John's Health Center.

The research, detailed in the paper ""Exercise-Related Physical Activity Relates to Brain Volumes in 10,125 Individuals,"" was published this week in the Journal of Alzheimer's Disease and shows being physically active is related to increased size of brain areas important for memory and learning.
The study looked at MRI brain scans from 10,125 people done at Prenuvo imaging centers, a key collaborator in the research. It found those who regularly engaged in physical activities such as walking, running or sports had larger brain volumes in key areas. This includes the gray matter, which helps with processing information, and the white matter, which connects different brain regions, as well as the hippocampus, important for memory.
Cyrus A. Raji, M.D., the lead researcher, explains the findings in simple terms: ""Our research supports earlier studies that show being physically active is good for your brain. Exercise not only lowers the risk of dementia but also helps in maintaining brain size, which is crucial as we age.""
David Merrill, M.D., study co-author and director of the PBHC noted, ""We found that even moderate levels of physical activity, such as taking fewer than 4,000 steps a day, can have a positive effect on brain health. This is much less than the often-suggested 10,000 steps, making it a more achievable goal for many people.""
Study co-authorSomayeh Meysami, M.D., assistant professor of neurosciences at Saint John's Cancer Institute and the Pacific Brain Health Center noted, ""Our research links regular physical activity to larger brain volumes, suggesting neuroprotective benefits. This large sample study furthers our understanding of lifestyle factors in brain health and dementia prevention.""
A Lancet Study in 2020 found about a dozen modifiable risk factors increase risk for Alzheimer's disease, including physical activity. This work builds upon previous work by this group, linking caloric burn from leisure activities to improved brain structure.
""This study demonstrates the influence of exercise on brain health imaging and when added to other studies on the role of diet, stress reduction and social connection offer the proven benefits of drug-free modifiable factors in substantially reducing Alzheimer's disease,"" said George Perry, Editor-in-Chief of Journal of Alzheimer's Disease.
""With comprehensive imaging scans, our study underscores the interconnected synergy between the body and the brain. It echoes the knowledge of past generations, showcasing that increased physical activity is a predictor of a healthier aging brain,"" said Dr. Attariwala, senior author of this paper.
This research highlights an easy way to keep our brains healthy: stay active! Whether it's a daily walk or a favorite sport, regular physical activity can have lasting benefits for our brain health.

","score: 15.283580246913584, grade_level: '15'","score: 17.015555555555558, grade_levels: ['college_graduate'], ages: [24, 100]",10.3233/JAD-230740,"Background: The potential neuroprotective effects of regular physical activity on brain structure are unclear, despite links between activity and reduced dementia risk. Objective: To investigate the relationships between regular moderate to vigorous physical activity and quantified brain volumes on magnetic resonance neuroimaging. Methods: A total of 10,125 healthy participants underwent whole-body MRI scans, with brain sequences including isotropic MP-RAGE. Three deep learning models analyzed axial, sagittal, and coronal views from the scans. Moderate to vigorous physical activity, defined by activities increasing respiration and pulse rate for at least 10 continuous minutes, was modeled with brain volumes via partial correlations. Analyses adjusted for age, sex, and total intracranial volume, and a 5% Benjamini-Hochberg False Discovery Rate addressed multiple comparisons. Results: Participant average age was 52.98±13.04 years (range 18–97) and 52.3% were biologically male. Of these, 7,606 (75.1%) reported engaging in moderate or vigorous physical activity approximately 4.05±3.43 days per week. Those with vigorous activity were slightly younger (p < 0.00001), and fewer women compared to men engaged in such activities (p = 3.76e-15). Adjusting for age, sex, body mass index, and multiple comparisons, increased days of moderate to vigorous activity correlated with larger normalized brain volumes in multiple regions including: total gray matter (Partial R = 0.05, p = 1.22e-7), white matter (Partial R = 0.06, p = 9.34e-11), hippocampus (Partial R = 0.05, p = 5.96e-7), and frontal, parietal, and occipital lobes (Partial R = 0.04, p≤1.06e-5). Conclusions: Exercise-related physical activity is associated with increased brain volumes, indicating potential neuroprotective effects."
"
Breastfeeding, even partially alongside formula feeding, changes the chemical makeup -- or metabolome -- of an infant's gut in ways that positively influence brain development and may boost test scores years later, suggests new CU Boulder research.

""For those who struggle with exclusively breastfeeding, this study suggests your baby can still get significant benefits if you breastfeed as much as you can,"" said senior author Tanya Alderete, an assistant professor of integrative physiology at CU Boulder.
The study, published Dec. 13 in the journal npj Metabolic Health and Disease, also identifies specific metabolites that manufacturers may want to consider adding to infant formula to optimize healthy brain development and concerning compounds they should try to leave out.
""Our research suggests that even at low levels, some contaminants found in formula may have negative neurodevelopmental effects downstream,"" said first author Bridget Chalifour, a postdoctoral researcher in Alderete's lab.
A health report card for the gut
For the study, the research team examined what is known as the ""fecal metabolome"" -- the diverse collection of metabolites found in the gut and shed in poop. Metabolites are small molecules that are churned out by gut bacteria as a byproduct of metabolizing food and make their way into the bloodstream, impacting the brain and other organs.
Breastmilk, formula and solid food also contain metabolites.

While scientists have long studied our resident bacteria, or microbiome, to better understand human health, the emerging field of ""metabolomics"" goes a step further.
""Looking at the gut microbiome tells us which bacteria are there, while looking at the fecal metabolome can help tell us what they are doing,"" said Chalifour. ""It's like a health report card for the gut.""
The team collected fecal samples from 112 infants at 1- and 6-months-old and worked with Donghai Liang, assistant professor of environmental health at Emory University in Atlanta, and other colleagues to chemically analyze which metabolites were present. They grouped infants based on how much they were breastfed vs. formula fed. At age 2, the children took cognitive, motor and language tests.
The study found that the samples from infants in different feeding groups contained significantly different levels of metabolites.
For instance, at 1 month old, 17 metabolites were more abundant the more a baby was breastfed, and 40 were more abundant the more a baby was formula fed.
When looking more closely at specific metabolites, the researchers identified 14 that were also associated with differences in test scores at age 2.

With only one notable exception, caffeine, the more metabolites associated with breast milk a baby had in their stool, the better they did on cognitive tests as toddlers (more on caffeine later.)
The more metabolites associated with formula feeding they had, the worst they did.
""The consistency of these results is striking and supports the benefits of breastfeeding as much as possible in early life,"" said Alderete.
Some metabolites associated with formula concerning
One particularly beneficial metabolite was cholesterol: At both 1 and 6 months old, the more a baby was breastfed the more cholesterol they had in their stool. And the more cholesterol babies had in their stool, the better they did on cognitive tests. This makes sense, as the fatty acid is critical for forming healthy circuits between brain cells. As the authors note, 80% to 90% of the brain's volume grows in the first two years of life.
In contrast, the more a baby was formula fed, the higher their levels of a metabolite called cadaverine, a known contaminant formed via fermentation.
In the study, the more a child was formula fed, the higher their levels of cadaverine and the lower their test scores at age 2. While the compound is considered a toxin at higher levels, the Food and Drug Administration permits low levels in infant formula.
""It may be that formula manufacturers should be more vigilant in getting levels of this compound down to zero,"" said Chalifour.
Interestingly, babies who were breastfed had higher levels of caffeine in their stool -- perhaps because moms may have been breastfeeding over a cup of coffee.
Not surprisingly, higher levels of caffeine, a stimulant, were associated with poorer cognitive scores. Prenatal caffeine exposure has previously been associated with lower neurodevelopmental scores, and experts recommend no more than 12 ounces, or a cup and a half, of coffee per day for pregnant women.
Not all or nothing
The World Health Organization recommends that infants be exclusively breastfed for the first six months of life, but in the United States, only 63% of infants are exclusively breastfed immediately following birth. By six months, only a quarter of U.S. babies are exclusively breastfed.
Alderete acknowledges that for some parents, breastfeeding isn't possible. She hopes her research can ultimately help manufacturers improve formula to make it as close to breastmilk as it can be. And she stresses that just because a child was not breastfed does not mean they'll have neurodevelopmental deficits. Early feeding patterns are just one of many factors that contribute to how a brain develops.
Her takeaway to new parents having trouble breastfeeding exclusively: Don't give up. It doesn't have to be all or nothing.
""Just increasing the proportion of breastmilk relative to formula may have a positive impact on your developing child,"" she said.

","score: 12.97149596577368, grade_level: '13'","score: 13.895728027105406, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s44324-023-00001-2,"Infant fecal metabolomics can provide valuable insights into the associations of nutrition, dietary patterns, and health outcomes in early life. Breastmilk is typically classified as the best source of nutrition for nearly all infants. However, exclusive breastfeeding may not always be possible for all infants. This study aimed to characterize associations between levels of mixed breastfeeding and formula feeding, along with solid food consumption and the infant fecal metabolome at 1- and 6-months of age. As a secondary aim, we examined how feeding-associated metabolites may be associated with early life neurodevelopmental outcomes. Fecal samples were collected at 1- and 6-months, and metabolic features were assessed via untargeted liquid chromatography/high-resolution mass spectrometry. Feeding groups were defined at 1-month as 1) exclusively breastfed, 2) breastfed >50% of feedings, or 3) formula fed ≥50% of feedings. Six-month groups were defined as majority breastmilk (>50%) or majority formula fed (≥50%) complemented by solid foods. Neurodevelopmental outcomes were assessed using the Bayley Scales of Infant Development at 2 years. Changes in the infant fecal metabolome were associated with feeding patterns at 1- and 6-months. Feeding patterns were associated with the intensities of a total of 57 fecal metabolites at 1-month and 25 metabolites at 6-months, which were either associated with increased breastmilk or increased formula feeding. Most breastmilk-associated metabolites, which are involved in lipid metabolism and cellular processes like cell signaling, were associated with higher neurodevelopmental scores, while formula-associated metabolites were associated with lower neurodevelopmental scores. These findings offer preliminary evidence that feeding patterns are associated with altered infant fecal metabolomes, which may be associated with cognitive development later in life."
"
Practicing yoga nidra -- a kind of mindfulness training -- might improve sleep, cognition, learning, and memory, even in novices, according to a pilot study publishing in the open-access journal PLOS ONE on December 13 by Karuna Datta of the Armed Forces Medical College in India, and colleagues. After a two-week intervention with a cohort of novice practitioners, the researchers found that the percentage of delta-waves in deep sleep increased and that all tested cognitive abilities improved.

Unlike more active forms of yoga, which focus on physical postures, breathing, and muscle control, yoga nidra guides people into a state of conscious relaxation while they are lying down. While it has reported to improve sleep and cognitive ability, those reports were based more on subjective measures than on objective data. The new study used objective polysomnographic measures of sleep and a battery of cognitive tests. Measurements were taken before and after two weeks of yoga nidra practice, which was carried out during the daytime using a 20 minute audio recording.
Among other things, polysomnography measures brain activity to determine how long each sleep stage lasts and how frequently each stage occurs. After two weeks of yoga nidra, the researchers observed that participants exhibited a significantly increased sleep efficiency and percentage of delta-waves in deep sleep. They also saw faster responses in all cognitive tests with no loss in accuracy and faster and more accurate responses in tasks including tests of working memory, abstraction, fear and anger recognition, and spatial learning and memory tasks. The findings support previous studies which link delta-wave sleep to improved sleep quality as well as better attention and memory.
The authors believe their study provides objective evidence that yoga nidra is an effective means of improving sleep quality and cognitive performance. Yoga nidra is a low-cost and highly accessible activity from which many people might therefore benefit.
The authors add: ""Yoga nidra practice improves sleep and makes brain processing faster. Accuracy also increased, especially with learning and memory related tasks.""

","score: 14.957880737880739, grade_level: '15'","score: 16.09204633204633, grade_levels: ['college_graduate'], ages: [24, 100]",10.1371/journal.pone.0294678,"Complementary and Alternative medicine is known to have health benefits. Yoga nidra practice is an easy-to-do practice and has shown beneficial effects on stress reduction and is found to improve sleep in insomnia patients. Effect of yoga nidra practice on subjective sleep is known but its effect on sleep and cognition objectively is not documented. The aim of the study was to study the effect of yoga nidra practice on cognition and sleep using objective parameters. 41 participants were enrolled, and baseline sleep diary (SD) collected. Participants volunteered for overnight polysomnography (PSG) and cognition testing battery (CTB) comprising of Motor praxis test, emotion recognition task (ERT), digital symbol substitution task, visual object learning task (VOLT), abstract matching (AIM), line orientation task, matrix reasoning task, fractal-2-back test (NBACK), psychomotor vigilance task and balloon analog risk task. Baseline CTB and after one and two weeks of practice was compared. Power spectra density for EEG at central, frontal, and occipital locations during CTB was compared. Repeat SD and PSG after four weeks of practice were done. After yoga nidra practice, improved reaction times for all cognition tasks were seen. Post intervention compared to baseline (95%CI; p-value, effect size) showed a significant improvement in sleep efficiency of +3.62% (0.3, 5.15; p = 0.03, r = 0.42), -20min (-35.78, -5.02; p = 0.003, d = 0.84) for wake after sleep onset and +4.19 μV2 (0.5, 9.5; p = 0.04, r = 0.43) in delta during deep sleep. Accuracy increased in VOLT (95% CI: 0.08, 0.17; p = 0.002, d = 0.79), AIM (95% CI: 0.03, 0.12; p = 0.02, d = 0.61) and NBACK (95% CI: 0.02, 0.13; p = 0.04, d = 0.56); ERT accuracy increased for happy, fear and anger (95% CI: 0.07, 0.24; p = 0.004, d = 0.75) but reduced for neutral stimuli (95% CI: -0.31, -0.12; p = 0.04, r = 0.33) after yoga nidra practice. Yoga Nidra practice improved cognitive processing and night-time sleep."
"
Neuroengineer Silvestro Micera develops advanced technological solutions to help people regain sensory and motor functions that have been lost due to traumatic events or neurological disorders. Until now, he had never before worked on enhancing the human body and cognition with the help of technology.

Now in a study published in Science Robotics, Micera and his team report on how diaphragm movement can be monitored for successful control of an extra arm, essentially augmenting a healthy individual with a third -- robotic -- arm.
""This study opens up new and exciting opportunities, showing that extra arms can be extensively controlled and that simultaneous control with both natural arms is possible,"" says Micera, Bertarelli Foundation Chair in Translational Neuroengineering at EPFL, and professor of Bioelectronics at Scuola Superiore Sant'Anna.
The study is part of the Third-Arm project, previously funded by the Swiss National Science Foundation (NCCR Robotics), that aims to provide a wearable robotic arm to assist in daily tasks or to help in search and rescue. Micera believes that exploring the cognitive limitations of third-arm control may actually provide gateways towards better understanding of the human brain.
Micera continues, ""The main motivation of this third arm control is to understand the nervous system. If you challenge the brain to do something that is completely new, you can learn if the brain has the capacity to do it and if it's possible to facilitate this learning. We can then transfer this knowledge to develop, for example, assistive devices for people with disabilities, or rehabilitation protocols after stroke.""
""We want to understand if our brains are hardwired to control what nature has given us, and we've shown that the human brain can adapt to coordinate new limbs in tandem with our biological ones,"" explains Solaiman Shokur, co-PI of the study and EPFL Senior Scientist at the Neuro-X Institute. ""It's about acquiring new motor functions, enhancement beyond the existing functions of a given user, be it a healthy individual or a disabled one. From a nervous system perspective, it's a continuum between rehabilitation and augmentation.""
To explore the cognitive constraints of augmentation, the researchers first built a virtual environment to test a healthy user's capacity to control a virtual arm using movement of his or her diaphragm. They found that diaphragm control does not interfere with actions like controlling one's physiological arms, one's speech or gaze.

In this virtual reality setup, the user is equipped with a belt that measures diaphragm movement. Wearing a virtual reality headset, the user sees three arms: the right arm and hand, the left arm and hand, and a third arm between the two with a symmetric, six-fingered hand.
""We made this hand symmetric to avoid any bias towards either the left or the right hand,"" explains Giulia Dominijanni, PhD student at EPFL's Neuro-X Institute.
In the virtual environment, the user is then prompted to reach out with either the left hand, the right hand, or in the middle with the symmetric hand. In the real environment, the user holds onto an exoskeleton with both arms, which allows for control of the virtual left and right arms. Movement detected by the belt around the diaphragm is used for controlling the virtual middle, symmetric arm. The setup was tested on 61 healthy subjects in over 150 sessions.
""Diaphragm control of the third arm is actually very intuitive, with participants learning to control the extra limb very quickly,"" explains Dominijanni. ""Moreover, our control strategy is inherently independent from the biological limbs and we show that diaphragm control does not impact a user's ability to speak coherently.""
The researchers also successfully tested diaphragm control with an actual robotic arm, a simplified one that consists of a rod that can be extended out, and back in. When the user contracts the diaphragm, the rod is extended out. In an experiment similar to the VR environment, the user is asked to reach and hover over target circles with her left or right hand, or with the robotic rod.
Besides the diaphragm, but not reported in the study, vestigial ear muscles have also been tested for feasibility in performing new tasks. In this approach, a user is equipped with ear sensors and trained to use fine ear muscle movement to control the displacement of a computer mouse.
""Users could potentially use these ear muscles to control an extra limb,"" says Shokur, emphasizing that these alternative control strategies may help one day for the development of rehabilitation protocols for people with motor deficiencies.
Part of the third arm project, previous studies regarding the control of robotic arms have been focused on helping amputees. The latest Science Robotics study is a step beyond repairing the human body towards augmentation.
""Our next step is to explore the use of more complex robotic devices using our various control strategies, to perform real-life tasks, both inside and outside of the laboratory. Only then will we be able to grasp the real potential of this approach,"" concludes Micera.

","score: 14.205565129972712, grade_level: '14'","score: 15.052973574608643, grade_levels: ['college_graduate'], ages: [24, 100]",10.1126/scirobotics.adh1438,"Extra robotic arms (XRAs) are gaining interest in neuroscience and robotics, offering potential tools for daily activities. However, this compelling opportunity poses new challenges for sensorimotor control strategies and human-machine interfaces (HMIs). A key unsolved challenge is allowing users to proficiently control XRAs without hindering their existing functions. To address this, we propose a pipeline to identify suitable HMIs given a defined task to accomplish with the XRA. Following such a scheme, we assessed a multimodal motor HMI based on gaze detection and diaphragmatic respiration in a purposely designed modular neurorobotic platform integrating virtual reality and a bilateral upper limb exoskeleton. Our results show that the proposed HMI does not interfere with speaking or visual exploration and that it can be used to control an extra virtual arm independently from the biological ones or in coordination with them. Participants showed significant improvements in performance with daily training and retention of learning, with no further improvements when artificial haptic feedback was provided. As a final proof of concept, naïve and experienced participants used a simplified version of the HMI to control a wearable XRA. Our analysis indicates how the presented HMI can be effectively used to control XRAs. The observation that experienced users achieved a success rate 22.2% higher than that of naïve users, combined with the result that naïve users showed average success rates of 74% when they first engaged with the system, endorses the viability of both the virtual reality–based testing and training and the proposed pipeline."
"
Computational models that mimic the structure and function of the human auditory system could help researchers design better hearing aids, cochlear implants, and brain-machine interfaces. A new study from MIT has found that modern computational models derived from machine learning are moving closer to this goal.

In the largest study yet of deep neural networks that have been trained to perform auditory tasks, the MIT team showed that most of these models generate internal representations that share properties of representations seen in the human brain when people are listening to the same sounds.
The study also offers insight into how to best train this type of model: The researchers found that models trained on auditory input including background noise more closely mimic the activation patterns of the human auditory cortex.
""What sets this study apart is it is the most comprehensive comparison of these kinds of models to the auditory system so far. The study suggests that models that are derived from machine learning are a step in the right direction, and it gives us some clues as to what tends to make them better models of the brain,"" says Josh McDermott, an associate professor of brain and cognitive sciences at MIT, a member of MIT's McGovern Institute for Brain Research and Center for Brains, Minds, and Machines, and the senior author of the study.
MIT graduate student Greta Tuckute and Jenelle Feather PhD '22 are the lead authors of the open-access paper, which appears today in PLOS Biology.
Models of hearing
Deep neural networks are computational models that consists of many layers of information-processing units that can be trained on huge volumes of data to perform specific tasks. This type of model has become widely used in many applications, and neuroscientists have begun to explore the possibility that these systems can also be used to describe how the human brain performs certain tasks.

""These models that are built with machine learning are able to mediate behaviors on a scale that really wasn't possible with previous types of models, and that has led to interest in whether or not the representations in the models might capture things that are happening in the brain,"" Tuckute says.
When a neural network is performing a task, its processing units generate activation patterns in response to each audio input it receives, such as a word or other type of sound. Those model representations of the input can be compared to the activation patterns seen in fMRI brain scans of people listening to the same input.
In 2018, McDermott and then-graduate student Alexander Kell reported that when they trained a neural network to perform auditory tasks (such as recognizing words from an audio signal), the internal representations generated by the model showed similarity to those seen in fMRI scans of people listening to the same sounds.
Since then, these types of models have become widely used, so McDermott's research group set out to evaluate a larger set of models, to see if the ability to approximate the neural representations seen in the human brain is a general trait of these models.
For this study, the researchers analyzed nine publicly available deep neural network models that had been trained to perform auditory tasks, and they also created 14 models of their own, based on two different architectures. Most of these models were trained to perform a single task -- recognizing words, identifying the speaker, recognizing environmental sounds, and identifying musical genre -- while two of them were trained to perform multiple tasks.
When the researchers presented these models with natural sounds that had been used as stimuli in human fMRI experiments, they found that the internal model representations tended to exhibit similarity with those generated by the human brain. The models whose representations were most similar to those seen in the brain were models that had been trained on more than one task and had been trained on auditory input that included background noise.

""If you train models in noise, they give better brain predictions than if you don't, which is intuitively reasonable because a lot of real-world hearing involves hearing in noise, and that's plausibly something the auditory system is adapted to,"" Feather says.
Hierarchical processing
The new study also supports the idea that the human auditory cortex has some degree of hierarchical organization, in which processing is divided into stages that support distinct computational functions. As in the 2018 study, the researchers found that representations generated in earlier stages of the model most closely resemble those seen in the primary auditory cortex, while representations generated in later model stages more closely resemble those generated in brain regions beyond the primary cortex.
Additionally, the researchers found that models that had been trained on different tasks were better at replicating different aspects of audition. For example, models trained on a speech-related task more closely resembled speech-selective areas.
""Even though the model has seen the exact same training data and the architecture is the same, when you optimize for one particular task, you can see that it selectively explains specific tuning properties in the brain,"" Tuckute says.
McDermott's lab now plans to make use of their findings to try to develop models that are even more successful at reproducing human brain responses. In addition to helping scientists learn more about how the brain may be organized, such models could also be used to help develop better hearing aids, cochlear implants, and brain-machine interfaces.
""A goal of our field is to end up with a computer model that can predict brain responses and behavior. We think that if we are successful in reaching that goal, it will open a lot of doors,"" McDermott says.
The research was funded by the National Institutes of Health, an Amazon Fellowship from the Science Hub, an International Doctoral Fellowship from the American Association of University Women, an MIT Friends of McGovern Institute Fellowship, and a Department of Energy Computational Science Graduate Fellowship.

","score: 17.374616128809347, grade_level: '17'","score: 19.902668094810316, grade_levels: ['college_graduate'], ages: [24, 100]",10.1371/journal.pbio.3002366,"Models that predict brain responses to stimuli provide one measure of understanding of a sensory system and have many potential applications in science and engineering. Deep artificial neural networks have emerged as the leading such predictive models of the visual system but are less explored in audition. Prior work provided examples of audio-trained neural networks that produced good predictions of auditory cortical fMRI responses and exhibited correspondence between model stages and brain regions, but left it unclear whether these results generalize to other neural network models and, thus, how to further improve models in this domain. We evaluated model-brain correspondence for publicly available audio neural network models along with in-house models trained on 4 different tasks. Most tested models outpredicted standard spectromporal filter-bank models of auditory cortex and exhibited systematic model-brain correspondence: Middle stages best predicted primary auditory cortex, while deep stages best predicted non-primary cortex. However, some state-of-the-art models produced substantially worse brain predictions. Models trained to recognize speech in background noise produced better brain predictions than models trained to recognize speech in quiet, potentially because hearing in noise imposes constraints on biological auditory representations. The training task influenced the prediction quality for specific cortical tuning properties, with best overall predictions resulting from models trained on multiple tasks. The results generally support the promise of deep neural networks as models of audition, though they also indicate that current models do not explain auditory cortical responses in their entirety."
"
For the first time ever, an international team of researchers has created a complete cell atlas of a whole mammalian brain. This atlas serves as a map for the mouse brain, describing the type, location, and molecular information of more than 32 million cells and providing information on connectivity between these cells. The mouse is the most commonly used vertebrate experimental model in neuroscience research, and this cellular map paves the way for a greater understanding of the human brain -- arguably the most powerful computer in the world. The cell atlas also lays the foundation for the development of a new generation of precision therapeutics for people with mental and neurological disorders of the brain.

The findings were funded by the National Institutes of Health's Brain Research Through Advancing Innovative Neurotechnologies® Initiative, or The BRAIN Initiative®, and appear in a collection of 10 papers published in Nature.
""The mouse atlas has brought the intricate network of mammalian brain cells into unprecedented focus, giving researchers the details needed to understand human brain function and diseases,"" said Joshua A. Gordon, M.D., Ph.D., Director of the National Institute of Mental Health, part of the National Institutes of Health.
The cell atlas describes the types of cells in each region of the mouse brain and their organization within those regions. In addition to this structural information, the cell atlas provides an incredibly detailed catalog of the cell's transcriptome -- the complete set of gene readouts in a cell, which contains instructions for making proteins and other cellular products. The transcriptomic information included in the atlas is hierarchically organized, detailing cell classes, subclasses, and thousands of individual cell clusters within the brain.
The atlas also characterizes the cell epigenome -- chemical modifications to a cell's DNA and chromosomes that alter the way the cell's genetic information is expressed -- detailing thousands of epigenomic cell types and millions of candidate genetic regulation elements for different brain cell types.
Together, the structural, transcriptomic, and epigenetic information included in this atlas provide an unprecedented map of cellular organization and diversity across the mouse brain. The atlas also provides an accounting of the neurotransmitters and neuropeptides used by different cells and the relationship among cell types within the brain. This information can be used as a detailed blueprint for how chemical signals are initiated and transmitted in different parts of the brain. Those electrical signals are the basis for how brain circuits operate and how the brain functions overall.
""This product is a testament to the power of this unprecedented, cross-cutting collaboration and paves our path for more precision brain treatments,"" said John Ngai, Ph.D., Director of the NIH BRAIN Initiative.""
Of the 10 studies included in this collection, seven are funded through the NIH BRAIN Initiative Cell Census Network (BICCN), and two are funded through the larger NIH BRAIN Initiative. The core aim of the BICCN, a groundbreaking, cross-collaborative effort to understand the brain's cellular makeup, is to develop a comprehensive inventory of the cells in the brain -- where they are, how they develop, how they work together, and how they regulate their activity -- to better understand how brain disorders develop, progress, and are best treated.
""By leveraging the unique nature of its multi-disciplinary and international collaboration, the BICCN was able to accomplish what no other team of scientists has been able to before,"" said Dr. Ngai. ""Now we are ready to take the next big step -- completing the cell maps of the human brain and the nonhuman primate brain.""
The BRAIN Initiative Cell Atlas Network (BICAN) is the next stage in the NIH BRAIN Initiative's effort to understand the cell and cellular functions of the mammalian brain. BICAN is a transformative project that, together with two other large-scale projects -- the BRAIN Initiative Connectivity Across Scales and the Armamentarium for Precision Brain Cell Access -- aim to revolutionize neuroscience research by illuminating foundational principles governing the circuit basis of behavior and informing new approaches to treating human brain disorders.

","score: 17.531636505570933, grade_level: '18'","score: 19.815937122986305, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41586-023-06812-z,"The mammalian brain consists of millions to billions of cells that are organized into many cell types with specific spatial distribution patterns and structural and functional properties1–3. Here we report a comprehensive and high-resolution transcriptomic and spatial cell-type atlas for the whole adult mouse brain. The cell-type atlas was created by combining a single-cell RNA-sequencing (scRNA-seq) dataset of around 7 million cells profiled (approximately 4.0 million cells passing quality control), and a spatial transcriptomic dataset of approximately 4.3 million cells using multiplexed error-robust fluorescence in situ hybridization (MERFISH). The atlas is hierarchically organized into 4 nested levels of classification: 34 classes, 338 subclasses, 1,201 supertypes and 5,322 clusters. We present an online platform, Allen Brain Cell Atlas, to visualize the mouse whole-brain cell-type atlas along with the single-cell RNA-sequencing and MERFISH datasets. We systematically analysed the neuronal and non-neuronal cell types across the brain and identified a high degree of correspondence between transcriptomic identity and spatial specificity for each cell type. The results reveal unique features of cell-type organization in different brain regions—in particular, a dichotomy between the dorsal and ventral parts of the brain. The dorsal part contains relatively fewer yet highly divergent neuronal types, whereas the ventral part contains more numerous neuronal types that are more closely related to each other. Our study also uncovered extraordinary diversity and heterogeneity in neurotransmitter and neuropeptide expression and co-expression patterns in different cell types. Finally, we found that transcription factors are major determinants of cell-type classification and identified a combinatorial transcription factor code that defines cell types across all parts of the brain. The whole mouse brain transcriptomic and spatial cell-type atlas establishes a benchmark reference atlas and a foundational resource for integrative investigations of cellular and circuit function, development and evolution of the mammalian brain."
"
Machine learning has been found to predict well the outcomes of many health conditions. Now, researchers from Japan have found a way to predict whether people with severe shortsightedness will have good or bad vision in the future.

In a study recently published in JAMA Ophthalmology, researchers from the Tokyo Medical and Dental University (TMDU) developed a machine-learning model that works well for predicting -- and visualizing -- the risk of visual impairment over the long term.
People with extreme shortsightedness (called high myopia) can clearly see objects that are near to them but cannot focus on objects at a distance. Contacts, glasses, or surgery can be used to correct their vision, but having high myopia is not just inconvenient; half of the time it leads to a condition called pathologic myopia, and complications from pathologic myopia are the leading causes of blindness.
""We know that machine-learning algorithms work well on tasks such as identifying changes and complications in myopia,"" says Yining Wang, lead author of the study, ""but in this study, we wanted to investigate something different, namely how good these algorithms are at long-term predictions.""
To do this, the team performed a cohort study and looked at the visual acuity of 967 Japanese patients at TDMU's Advanced Clinical Center for Myopia after 3 and 5 years had passed. They formed a dataset from 34 variables that are commonly collected during ophthalmic examinations, such as age, current visual acuity, and the diameter of the cornea. They then tested several popular machine-learning models such as random forests and support vector machines. Of these models, the logistic regression-based model performed the best at predicting visual impairment at 5 years.
However, predicting outcomes is only part of the story. ""It's also important to present the model's output in a way that is easy for patients to understand and convenient for making clinical decisions,"" says Kyoko Ohno-Matsui, senior author. To do this, the researchers used a nomogram to visualize the classification model. Each variable is assigned a line with a length that indicates how important it is for predicting visual acuity. These lengths can be converted into points that can be added up to obtain a final score explaining the risk of visual impairment in future.
People who permanently lose their vision often suffer both financially and physically as a result of their loss of independence. The decrease in global productivity caused by severe visual impairment was estimated to be USD94.5 billion in 2019. Although the model still has to be evaluated on a wider population, this study has shown that machine-learning models have good potential to help address this increasingly important public health concern, which will benefit both individuals and society as a whole.

","score: 14.378565523948456, grade_level: '14'","score: 15.463744225626066, grade_levels: ['college_graduate'], ages: [24, 100]",10.1001/jamaophthalmol.2023.4786,"High myopia is a global concern due to its escalating prevalence and the potential risk of severe visual impairment caused by pathologic myopia. Using artificial intelligence to estimate future visual acuity (VA) could help clinicians to identify and monitor patients with a high risk of vision reduction in advance. To develop machine learning models to predict VA at 3 and 5 years in patients with high myopia. This retrospective, single-center, cohort study was performed on patients whose best-corrected VA (BCVA) at 3 and 5 years was known. The ophthalmic examinations of these patients were performed between October 2011 and May 2021. Thirty-four variables, including general information, basic ophthalmic information, and categories of myopic maculopathy based on fundus and optical coherence tomography images, were collected from the medical records for analysis. Regression models were developed to predict BCVA at 3 and 5 years, and a binary classification model was developed to predict the risk of developing visual impairment at 5 years. The performance of models was evaluated by discrimination metrics, calibration belts, and decision curve analysis. The importance of relative variables was assessed by explainable artificial intelligence techniques. A total of 1616 eyes from 967 patients (mean [SD] age, 58.5 [14.0] years; 678 female [70.1%]) were included in this analysis. Findings showed that support vector machines presented the best prediction of BCVA at 3 years (R2 = 0.682; 95% CI, 0.625-0.733) and random forest at 5 years (R2 = 0.660; 95% CI, 0.604-0.710). To predict the risk of visual impairment at 5 years, logistic regression presented the best performance (area under the receiver operating characteristic curve = 0.870; 95% CI, 0.816-0.912). The baseline BCVA (logMAR odds ratio [OR], 0.298; 95% CI, 0.235-0.378; P &amp;lt; .001), prior myopic macular neovascularization (OR, 3.290; 95% CI, 2.209-4.899; P &amp;lt; .001), age (OR, 1.578; 95% CI, 1.227-2.028; P &amp;lt; .001), and category 4 myopic maculopathy (OR, 4.899; 95% CI, 1.431-16.769; P = .01) were the 4 most important predicting variables and associated with increased risk of visual impairment at 5 years. Study results suggest that developing models for accurate prediction of the long-term VA for highly myopic eyes based on clinical and imaging information is feasible. Such models could be used for the clinical assessments of future visual acuity."
"
The images of Israeli child hostages being freed from Hamas captivity are heartwarming, but for most of these children, the release is just the start of a long rehabilitation process. Countless studies have shown that exposure to warfare, abuse and other traumatic events at a young age significantly raises the risk of ill health, social problems and mental health issues later in life. Now, a new study by researchers at the Weizmann Institute of Science provides a reason for optimism. In research conducted on mice, published Friday in Science Advances, a team headed by Prof. Alon Chen discovered brain mechanisms that go awry as a result of exposure to trauma in infancy and showed that these changes may be reversible if treated early.

Our brains have a wonderful quality known as plasticity, the ability to change throughout our lives. As may be expected, in our early years, when the brain is still developing, it is at peak plasticity. This manifests in, for example, the aptitude for learning languages, but this also entails a heightened sensitivity to traumatic events, which are liable to leave a scar that only intensifies with age. Many studies provide evidence for the latter effect, but very little is known about the way that exposure to trauma at a young age affects the different kinds of brain cells and the communication between them in adulthood.
Chen's laboratory in Weizmann's Brain Sciences Department focuses on the molecular and behavioral aspects of the response to stress. In previous studies, Chen's team examined how stress during pregnancy affects mouse offspring when they reach maturity. In the current research, the scientists, led by Dr. Aron Kos, studied how trauma experienced shortly after birth affects mouse pups later in life. To advance the understanding of this topic, the researchers pulled together the strengths of Chen's lab: its expertise in exploring the brain's molecular processes at the highest possible resolution, using genetic sequencing on the level of individual cells; the ability to use cameras to track dozens of behavioral variables in a rich social environment intended to recreate natural living conditions; and the ability to process the massive quantities of data generated in this environment, using machine learning and artificial intelligence tools.
This comprehensive behavioral mapping revealed that mice exposed after birth to a traumatic event -- in the case of this study, being neglected by their mothers -- displayed a variety of behaviors indicating that they found themselves at the bottom of the dominance hierarchy. ""Equivalent behaviors in humans might include high levels of introversion, social anxiety and having an avoidant personality, all known to be characteristic of posttrauma,"" says Dr. Juan Pablo Lopez, a former postdoctoral fellow in Chen's joint laboratory at Weizmann and the Max Planck Institute of Psychiatry in Munich, and today head of a research group in the Department of Neuroscience at the Karolinska Institute in Stockholm.
In the next stage of the study, the researchers exposed some of the adult mice that had experienced trauma in infancy to a stressful social situation: bullying by other mice. Ultimately, they created four groups of adult mice: those that had not been exposed to any trauma; those that had not been exposed to trauma in infancy but were subjected to bullying as adults; mice that were exposed to trauma only in infancy; and mice that were exposed to both trauma in infancy and bullying as adults. To find out how exposure to early trauma disrupts the brain and what happens as a result of this in adulthood, the researchers carried out a meticulous comparison of the four groups, using RNA sequencing at the single-cell level in the hippocampus, a brain area known to play an important role in social functioning. The comparison revealed that early trauma left a mark on different types of cells, primarily affecting gene expression in two subpopulations of neurons, those belonging to the glutamatergic excitatory system and those belonging to the GABA inhibitory system. This effect was especially strong in mice that had been exposed to both trauma in infancy and bullying as adults.
Cells in the brain communicate with one another by means of electrical signals, which can be excitatory, that is, stimulating, or inhibitory. An excitatory signal encourages communication between brain cells, whereas an inhibitory signal represses it, like the gas and brake pedals in a car. Normal brain functioning requires a balance between the excitatory and inhibitory signals, which is lacking in many psychiatric disorders. One of the ways of assessing the brain's electrical activity and the balance between excitatory and inhibitory signals is through electrophysiological measurements. Such measurements, performed in the hippocampus of the mice by Dr. Julien Dine, a former staff scientist at the Weizmann Institute and currently a pharmaceutical electrophysiologist, supported the molecular findings: Exposure to trauma in early childhood disrupted the balance between excitatory and inhibitory signals in adulthood.
Having discovered a brain mechanism that is disrupted in adulthood as a result of early trauma -- and having identified this disruption as an imbalance between the excitatory and inhibitory signals -- the researchers tried to find a way to fix it. During a brief treatment window shortly after the early trauma, they gave the mice a well-known antianxiety drug -- diazepam, known commercially as Valium -- which affects the GABA inhibitory system. This short course of treatment led to results that were nothing less than stunning: The treated mice were able to fully or almost fully avoid the behavioral future that awaited them and were no longer at the foot of the social ladder. ""Understanding the molecular and functional mechanisms allowed us to neutralize the negative behavioral impact of trauma with a drug given shortly after exposure to traumatic incidents,"" Kos explains. ""This certainly should not be seen as a recommendation to treat young trauma patients with drugs, but our findings do highlight the importance of early treatment for successful rehabilitation.""
Intense, ongoing stress can, at any age, contribute to disease, from psychiatric disorders to obesity and diabetes. But in the first years of life -- and also in the womb -- such stress can have dramatic ramifications. ""The wars in Israel, Ukraine, Sudan and elsewhere, and the unprecedented global refugee crisis that is caused, in part, by climate change, alongside an increased understanding of the long-term harm caused by exposure to war and violence at a young age -- all these highlight the need for better rehabilitation capabilities,"" says Chen. ""Our new study identifies a key brain mechanism that is especially sensitive to childhood trauma. But the most exciting part is the prospect of using the plasticity of the young brain to help it recover, avoiding the toll this trauma can exact in adulthood.""
Also participating in the study were Dr. Joeri Bordes, Carlo de Donno, Dr. Elena Brivio, Stoyo Karamihalev, Dr. Alec Dick, Lucas Miranda, Rainer Stoffel, Cornelia Flachskamm and Dr. Mathias V. Schmidt from the Max Planck Institute of Psychiatry; Dr. Malte D. Luecken, Dr. Maren Büttner and Prof. Fabian J. Theis from the Helmholtz Zentrum München; Dr. Suellen Almeida-Correa from Weizmann's Brain Sciences Department; and Serena Gasperoni from the Karolinska Institute.
Prof. Alon Chen holds the Vera and John Schwartz Professorial Chair in Neurobiology. His research is supported by the Ruhman Family Laboratory for Research on the Neurobiology of Stress; and by the Licht Family.

","score: 17.670065520065524, grade_level: '18'","score: 19.545798525798524, grade_levels: ['college_graduate'], ages: [24, 100]",10.1126/sciadv.adj3793,"Adverse events in early life can modulate the response to additional stressors later in life and increase the risk of developing psychiatric disorders. The underlying molecular mechanisms responsible for these effects remain unclear. Here, we uncover that early life adversity (ELA) in mice leads to social subordination. Using single-cell RNA sequencing (scRNA-seq), we identified cell type–specific changes in the transcriptional state of glutamatergic and GABAergic neurons in the ventral hippocampus of ELA mice after exposure to acute social stress in adulthood. These findings were reflected by an alteration in excitatory and inhibitory synaptic transmission induced by ELA in response to acute social stress. Finally, enhancing the inhibitory network function through transient diazepam treatment during an early developmental sensitive period reversed the ELA-induced social subordination. Collectively, this study significantly advances our understanding of the molecular, physiological, and behavioral alterations induced by ELA, uncovering a previously unknown cell type–specific vulnerability to ELA."
"
New research from Binghamton University, State University of New York shows how some workplace gossip could reduce the likelihood of employee turnover and, as a result, potentially boost an organization's effectiveness.

""Organizations should be aware of the impact of positive gossip because turnover can be a very important factor in dictating an organization's success,"" said Jinhee Moon, a doctoral student at the Binghamton University School of Management who conducted the study with a team of other researchers. ""To make employees participate in positive gossip, the organization should do the right things by treating their employees well, and being aware their behaviors can show they care about their employees.""
While studies linked to workplace gossip aren't new, Moon's work builds upon previous research by exploring how employees who gossip might experience social gains. Moon previously worked on a study that dealt with why people participate in gossipy behavior at their workplace, and the recent publication is connected to her own leadership research focus at SOM, which centers on interpersonal relationships and social networks.
For the recent study, Moon and fellow researchers surveyed 338 health workers in South Korea on positive and negative forms of workplace gossip related to their organizations and management. Some of the topics included: ""At work, I sometimes complain about my organization when management is absent."" ""If I feel treated badly by management, I talk about this to my colleagues."" ""I sometimes praise my organization's capability when the management is absent.""Moon said the research showed gossip is viewed as more valuable when people positively talk about their management or organization. Health workers who participated in the survey expressed more interest in information they could use to enhance or maintain their organizational status.
The study also indicated no relationship between negative gossip and coercive power in the workplace, which Moon said proved contrary to what researchers had expected.
""We expected that if you participate in negative gossip, maybe you're trying to appear powerful or controlling or want to 'beat someone up,' but we couldn't find any supportive results,"" Moon said. ""If anything, we found that people didn't value that type of gossip as information and just saw it as someone who wants to complain. So, if you're thinking about negative workplace gossip, you might want to save your time because there's no positive impact for you.""
But one of the most helpful aspects of the research, as Moon saw it, was how it highlighted that participating in positive gossip among one's coworkers could reduce the chances of voluntary employee turnover.
""It can be very hard just to quit your job, and if you're experiencing difficulty where you work, maybe you want to participate in positive gossip with your colleagues and talk about some of the more bearable aspects of the organization,"" Moon said. ""Eventually, that can help you gain some personal power. It's a very convenient way to reduce negative feelings toward your own workplace, which can help you more in the long run.""

","score: 15.007120622568095, grade_level: '15'","score: 16.113650419823877, grade_levels: ['college_graduate'], ages: [24, 100]",10.1177/10596011231203758,"Although workplace gossip is ubiquitous, more scholarship is needed to determine how employees may use gossip to attain valuable social resources at work—namely, their experience of power. Drawing from the gossip literature and research on power in the workplace, we identify proximal (i.e., increased power accrual) and distal (i.e., diminished voluntary turnover) positive outcomes for employees enacting negative and positive gossip about the organization at work. Using a sample of 338 nurses, we found that positive workplace gossip about the organization increases expert power. Our analysis further revealed that positive workplace gossip about the organization had a negative indirect effect on the voluntary turnover of gossip actors via their expert power. Our findings contribute to the organizational literature on the benefits of gossip to actors and serve to further enrich the emerging literature which has considered the relationship between power and turnover. An important implication of our research is that organizations need to recognize the dynamics of organization-directed gossip and its potential to serve as a source of social power for employees and a retention driver for those who accrue power in expertise."
"
A new global study led by Kai Ruggeri, PhD, at Columbia Mailman School of Public Health involving over 80 collaborators from more than 30 countries underscores the crucial role of behavioral sciences in formulating policy decisions, while also asserting the need for clear standards for what evidence gets used in policy decisions. The findings are published in the journal Nature.

In April 2020, a group of researchers published a highly influential paper with 19 policy recommendations around COVID-19 based on insights from the behavioral sciences. The paper was a large collaboration of over 40 experts, led by Jay Van Bavel of New York University and Robb Willer of Stanford, and was cited thousands of times by governments, researchers, and public figures. Its recommendations covered topics such as official messaging on social distancing, how to get a vaccine once they were available, and the need to work within communities to create real impact. Now, Ruggeri et al.'s new paper in Nature evaluates evidence since the first paper's publication supports its claims and their applicability for policymaking.
""Governments around the world formulated pandemic policy strategies explicitly on the basis of the behavioral concepts highlighted in the 2020 paper by Jay J. Van Bavel et al.,"" says Ruggeri, a professor health policy and management at Columbia University's Mailman School of Public Health. ""Given concerns over a lack of public trust in science, particularly in the context of COVID-19, we believed it was important to evaluate the evidence for public policy recommendations, in a way that promotes transparency and builds trust.""
Two independent teams of 72 experts -- including both the 2020 paper's authors, as well as an independent team of evaluators -- reviewed 747 pandemic-related research articles to assess the extent to which claims in the original paper provided valid policy guidance. They treated studies conducted (and replicated) in real-world settings across large populations in multiple settings as the highest level, and flagged arguments that were not backed by empirical evidence.
Alex Haslam, PhD, professor of psychology from the University of Queensland in Australia and study co-author, says, ""In recent years, there has been a lot of discussion about the limitations of psychological and behavioral science, especially in the face of the so-called 'replication crisis.' As a counterpoint to this, what this research showed is that there is a core of good theory in these fields that provides a strong basis for both scientific prediction and public policy. This theory may not always be flashy, but it is the bedrock of good social science, and this study confirms that it is something we can rely on for guidance when we need it.""
The study finds evidence for 18 of 19 claims in the 2020 paper, including those related to sense of identity and community connectedness, leadership and trust, public health messaging, social cohesion, and misinformation. Of the 18, the 2020 paper correctly identified 16 relevant behavioral concepts during the pandemic as well as likely barriers to mitigating spread of the disease and social challenges that would be faced by policymakers. The researchers found no effect for two proposed policies related to effective public messaging (that messages should emphasize benefits to the recipient, and that they should focus on protecting others). Notably, the team found no evidence to review for one high-profile recommendation in the 2020 paper, which suggested the phrasing ""physical distancing"" is preferable to ""social distancing.""
The most strongly supported claims were the importance of interventions to combat misinformation and polarization, which proved to be vital for ensuring adherence to public health guidelines. Research also underlined the point that, to be effective, messaging needs to emanate from trusted leaders and to emphasize positive social norms.

Public health interventions that received the most attention were not necessarily the ones best supported by the most evidence. For example, handwashing was widely promoted as a strategy for stopping the spread of COVID, yet study effects were small to null, particularly compared to masking, isolation, distancing, and vaccines.
Regarding masking, early guidelines in some countries suggested the practice would not minimize COVID-19, but subsequent evidence pointed to the effectiveness of masking. Likewise, research also undermined guidance on the impacts of school closures and disinfecting surfaces. ""While there are understandable pressures to issue guidelines quickly during a crisis, making policy decisions without adequate evidence can be costly in many ways,"" says study co-author Katherine Baicker, PhD, Provost of the University of Chicago. ""As new scientific evidence comes in over time, some people may view evolving policy guidance as a sign of incompetence -- or even conspiracy -- undermining trust in expertise. Policymakers must balance the need for expedience with the need for robust evidence and credibility.""
The new study also identifies several domains missing from the 2020 paper. These included threat and risk perception, the role of inequality and racism, skepticism toward science, incentivizing behaviors beyond simply describing benefits (e.g., by providing financial rewards for vaccination) and the absence of clear leadership.
Finally, the research team provides recommendations to help researchers and policymakers respond to future pandemics and disasters. These include the need to study global populations, to do more field testing, and to be more specific in formulating testable questions. ""The value of field testing what really works to change health behaviors can't be overstated, and the strongest conclusions we've been able to draw in this article were often thanks to partnerships researchers forged with local governments and healthcare providers to carefully evaluate what actually adds value in the middle of a crisis,"" says study co-author Katy Milkman, PhD, professor at the Wharton School of the University of Pennsylvania. The researchers also encourage scientists to forge more alliances with policymakers and decisionmakers -- in local government, hospitals, schools, the media, and beyond.
""This work has the potential to increase transparency and build trust in science and public health, and to directly inform the development of tools and knowledge for the next pandemic or other crisis. Researchers can be a viable source of policy advice in the context of a crisis, and our recommendations point to ways to further improve this role of social and behavioral science,"" says study co-senior author Robb Willer, PhD, professor of sociology at Stanford University.
""This new paper rigorously evaluated policy recommendations from our original team to see if they were accurate, using large amounts of evidence and a new team of independent reviewers from around the globe. In addition to confirming the vast majority of our original claims, it sets a new gold standard for evaluating evidence when policy decisions, particularly urgent ones, must be made,"" says Jay Van Bavel, PhD, professor of psychology, New York University, lead author of the landmark 2020 article, and co-senior author of the new paper.

","score: 17.039649122807017, grade_level: '17'","score: 18.590842105263157, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41586-023-06840-9,"Scientific evidence regularly guides policy decisions1, with behavioural science increasingly part of this process2. In April 2020, an influential paper3 proposed 19 policy recommendations (‘claims’) detailing how evidence from behavioural science could contribute to efforts to reduce impacts and end the COVID-19 pandemic. Here we assess 747 pandemic-related research articles that empirically investigated those claims. We report the scale of evidence and whether evidence supports them to indicate applicability for policymaking. Two independent teams, involving 72 reviewers, found evidence for 18 of 19 claims, with both teams finding evidence supporting 16 (89%) of those 18 claims. The strongest evidence supported claims that anticipated culture, polarization and misinformation would be associated with policy effectiveness. Claims suggesting trusted leaders and positive social norms increased adherence to behavioural interventions also had strong empirical support, as did appealing to social consensus or bipartisan agreement. Targeted language in messaging yielded mixed effects and there were no effects for highlighting individual benefits or protecting others. No available evidence existed to assess any distinct differences in effects between using the terms ‘physical distancing’ and ‘social distancing’. Analysis of 463 papers containing data showed generally large samples; 418 involved human participants with a mean of 16,848 (median of 1,699). That statistical power underscored improved suitability of behavioural science research for informing policy decisions. Furthermore, by implementing a standardized approach to evidence selection and synthesis, we amplify broader implications for advancing scientific evidence in policy formulation and prioritization."
"
New research reveals that neurons in the preoptic hypothalamus -- the region of the brain that regulates sleep and body temperature -- are rhythmically activated during non-rapid eye movement sleep (NREM). Stress activates these brain cells out of turn, causing ""microarousals,"" that interrupt sleep cycles and decrease the duration of sleep episodes, according to research from Perelman School of Medicine at the University of Pennsylvania, published today in Current Biology.

While our bodies are at rest when we are asleep, our brains are still very active during four different stages of sleep. In each 90-minute sleep cycle, there are three stages of NREM sleep, and one stage of rapid eye movement (REM) sleep. During the first two stages of NREM sleep, brain waves, heartbeat, and breathing slow, and body temperature decreases. Stage two also includes unique brain activity, called spindles and K-complexes, which are short bursts of activity responsible for processing outside stimuli, as well as for consolidating memory. Stage three of the NREM sleep cycle is when the body releases growth hormone, which is important for repairing the body, keeping the immune system healthy, and further improving memory. During phase three, brain waves are larger, called delta waves. REM sleep, which happens in this phase when dreaming normally occurs, is also critical for memory formation, emotional processing, and brain development.
""When you have a bad night of sleep, you notice that your memory isn't as good as it normally is, or your emotions are all over the place -- but a bad night of sleep interrupts so many other processes throughout your body. This is even more heightened in individuals with stress-related sleep disorders,"" said senior author, Shinjae Chung, PhD, an assistant professor of Neuroscience. ""It's crucial to understand the biology driving the brain activity in these crucial stages of sleep, and how stimuli like stress can disrupt it, so that we might someday develop therapies to help individuals have more restful sleep that allows their brain to complete these important processes.""
The researchers monitored the activity in the preoptic area (POA) of the hypothalamus of mice during their natural sleep and found that glutamatergic neurons (VGLUT2) are rhythmically activated during NREM sleep. They also found that VGLUT2 neurons were most active during wakefulness, and less active during NREM and REM sleep.
During microarousals in NREM sleep, VGLUT2 neurons were the only active neurons within the POA, and their signals started to increase in the time before a microarousal. To confirm that active VGLUT2 neurons were indeed the cause of microarousal, the researchers stimulated the VGLUT2 neurons in sleeping subjects, which immediately increased the amount of microarousals and wakefulness.
Next, to illustrate the connection between stress and increased VGLUT2 neuron activation, researchers exposed subjects to a stressor, which increased awake time and microarousals, and decreased overall time spent in REM and NREM sleep. Researchers also noted increased VGLUT2 neuron activity during NREM sleep in the stressed subjects. What's more, when researchers inhibited VGLUT 2 neurons, microarousals during NREM sleep decreased, and NREM sleep episodes were longer.
""The glutamatergic neurons in the hypothalamus give us a promising target for developing treatments for stress-related sleep disorders,"" said first author, Jennifer Smith, a graduate researcher in Chung's lab. ""Being able to reduce interruptions during the important stages of non-REM sleep by suppressing VGLUT2 activity would be groundbreaking for individuals struggling with disrupted sleep from disorders like insomnia or PTSD.""
This research was supported by the National Institute of Neurological Disorders and Stroke (R01NS110865).

","score: 14.704732754682503, grade_level: '15'","score: 17.218734582000913, grade_levels: ['college_graduate'], ages: [24, 100]",10.1101/2022.11.30.518589,"Sleep disturbances are detrimental for our behavioral and emotional well-being. Stressful events disrupt sleep, in particular by inducing brief awakenings (microarousals, MAs) resulting in sleep fragmentation. The preoptic area of the hypothalamus (POA) is crucial for sleep control. However, how POA neurons contribute to the regulation of MAs and thereby impact sleep quality is unknown. Using fiber photometry recordings in mice, we examined the activity changes of genetically defined POA subpopulations during sleep. We found that POA glutamatergic neurons are rhythmically activated in synchrony with an infraslow rhythm in the spindle band of the electroencephalogram during non-rapid eye movement sleep (NREMs) and are transiently activated during MAs. Optogenetic stimulation of these neurons strongly promotes MAs. Exposure to acute social defeat stress significantly increased the number of transients in the calcium activity of POA glutamatergic neurons during NREMs. Optogenetic inhibition during spontaneous sleep and after stress reduced MAs during NREMs and consequently consolidated sleep. Monosynaptically-restricted rabies tracing revealed that POA glutamatergic neurons are innervated by brain regions regulating stress and sleep. Our findings uncover a novel circuit mechanism by which POA excitatory neurons regulate sleep quality after stress."
"
Old, obese flies get healthier and live longer if put on a diet, University of Connecticut researchers report on Dec. 8 in PNAS. If the effect holds true for humans, it would mean it's never too late for obese people to improve their health with diet.

For way too many of us,eating too much goes along with getting old and a tendency to be obese.Different health organizations define obesity differently, but all agree it means having too much body fat, and is associated with a host of diseases related to metabolism including heart disease and diabetes. Many animal studies have shown that eating less -- meaning sharply restricting calories without malnutrition -- lengthens lifespan. While human trials have shown evidence of beneficial effects of eating less on health, especially in healthy obese individuals, studies examining effects on lifespan have been unrealistic for humans.
Now, UConn School of Medicine researchers have shown that fruit flies fed a high sugar, high protein, high calorie diet that mimics the processed modern diet have metabolic changes similar to obese humans. Switching these obese flies to a low calorie diet, even very late in life, can dramatically change their metabolisms and extend their lives.
Fruit flies live short and fast -- the lifespan of flies raised on a high calorie diet is less than 80 days, while the longest lived on a low calorie diet can reach 120 days. To test whether changes in diet late in life can change a fly's lifespan, researchers led by geneticist Blanka Rogina from UConn's Department of Genetics and Genome Sciences and the Institute for Systems Genomics raised several batches of fruit flies. Some of the flies were raised on a low calorie diet that provided just half the energy of a regular diet, while the others were raised on a high calorie diet that provided three times the usual number of calories.
In this study, they looked specifically at male flies. Young flies switched from a high calorie to a low-calorie diet at 20 days old lived very long lives, similar to the flies fed a low-calorie diet from day one.
What surprised the researchers was that switching the flies' diet to a low calorie one remained a reliable way to extend lifespan even for old flies in ill health. The older insects raised on the high calorie diet had more lipids in their bodies, and they expended more energy defending their bodies from reactive oxygen species. They also had a higher death rate than flies raised on the low-calorie diet. But when the surviving high calorie flies were switched to a low-calorie diet at 50 or even 60 days (when most of the high calorie flies had already died) their metabolisms changed, their death rate plummeted, and their lifespans lengthened.
""Our studies were performed in flies aged on a high calorie diet, akin to obese individuals, suggesting that late-life diet shift in obese humans might have remarkable beneficial impact on health,"" Rogina says.
UConn School of Medicine Genetics and Genome Sciences Chair Brent Graveley and other researchers on the team looked at the genes expressed by the high calorie flies and contrasted them with the low calorie flies. Genes that control physiological and metabolic adaptation are different between the groups.
""The remarkable finding of this study is that even after living a significant portion of their lives on a high calorie diet, flies can gain the benefits of life span extension by simply switching to a low calorie diet,"" Graveley says.
The team's results show that flies' metabolisms can adapt to a change in diet even in old age. Since many basic metabolic pathways in fruit flies are shared with humans, this study suggests that human metabolism may respond the same way, and individuals eating a high calorie diet could benefit from reducing their calorie intake in old age. The researchers are currently analyzing data from female fruit flies to see if there are any sex-related differences in response to diet shifting.

","score: 14.46568215892054, grade_level: '14'","score: 16.033898050974514, grade_levels: ['college_graduate'], ages: [24, 100]",10.1073/pnas.2311019120,"The prevalence of obesity is increasing in older adults and contributes to age-related decline. Caloric restriction (CR) alleviates obesity phenotypes and delays the onset of age-related changes. However, how late in life organisms benefit from switching from a high-(H) to a low-calorie (L) diet is unclear. We transferred male flies from a H to a L (HL) diet or vice versa (LH) at different times during life. Both shifts immediately change fly rate of aging even when applied late in life. HL shift rapidly reduces fly mortality rate to briefly lower rate than in flies on a constant L diet, and extends lifespan. Transcriptomic analysis uncovers that flies aged on H diet have acquired increased stress response, which may have temporal advantage over flies aged on L diet and leads to rapid decrease in mortality rate after HL switch. Conversely, a LH shift increases mortality rate, which is temporarily higher than in flies aged on a H diet, and shortens lifespan. Unexpectedly, more abundant transcriptomic changes accompanied LH shift, including increase in ribosome biogenesis, stress response and growth. These changes reflect protection from sudden release of ROS, energy storage, and use of energy to growth, which all likely contribute to higher mortality rate. As the beneficial effects of CR on physiology and lifespan are conserved across many organisms, our study provides framework to study underlying mechanisms of CR interventions that counteract the detrimental effects of H diets and reduce rate of aging even when initiated later in life."
"
In the past ten years, the percentage of Americans who use medical marijuana has more than doubled as state-level legalization becomes increasingly common. But despite its prevalence as a medication, the full health effects of cannabis remain unknown, especially for specific populations -- such as pregnant people -- that might be especially at risk of health complications.

Now, in a large study of more than 9,000 pregnant people from across the U.S., researchers at University of Utah Health have found that cannabis exposure during pregnancy is associated with a composite measure of unhealthy pregnancy outcomes, especially low birth weight, and that higher exposure is associated with higher risks.
Compared to most prior studies, this study was larger and measured cannabis exposure more accurately, which allowed researchers to distinguish the effects of cannabis itself from those caused by other correlated health conditions. The research publishes online on December 12 in JAMA.
""Cannabis use is not safe,"" says Robert Silver, M.D., professor of obstetrics and gynecology at U of U Health and last author on the study. ""It increases the risk of pregnancy complications. If possible, you shouldn't use cannabis during pregnancy.""
The researchers were driven to answer this question in part by the contradictory answers that many people encounter when trying to learn about the health impacts of cannabis use. ""There's so much information out there -- discussion and social media channels and on the Internet -- about cannabis use and pregnancy,"" explains Torri Metz, M.D., vice chair of research of obstetrics and gynecology at U of U Health and lead author on the study. ""I think it's hard for patients to understand what they should be worried about, if anything.""
Uncovering new risks
Indeed, some previous studies on the topic found no association between cannabis use and pregnancy complications. One hurdle facing such research, Metz says, is that there are ""so many differences between baseline characteristics of people who use and don't use cannabis during pregnancy. There's different rates of anxiety and depression."" These differences can also impact pregnancy risks, which makes it challenging to figure out the consequences related specifically to cannabis use.

The large study population, including participants from eight medical centers across the U.S., allowed the researchers to address this issue. Being able to compare pregnancy outcomes for so many participants, 610 of which had detectable levels of cannabis exposure, meant that the researchers could statistically untangle the impacts of cannabis use from many other factors, including pre-existing health conditions, nicotine exposure, and socioeconomic status.
The scientists found that cannabis exposure was associated with a 1.5-fold increase in risk: 26% of cannabis-exposed pregnant people experienced an unhealthy pregnancy outcome, versus 17% of non-exposed pregnant people. Higher levels of cannabis exposure over the course of pregnancy were associated with higher risks.
A distinguishing feature of the study was how the researchers measured cannabis exposure. While other studies had asked participants to report their own cannabis use (which has been shown to underestimate the actual rate of use by two or three times), the scientists measured the levels of a metabolic byproduct of cannabis in participants' urine samples, which gave more accurate measurements of cannabis exposure.
Open questions
To gauge impacts on pregnancy, the researchers looked at an aggregate measure of negative health outcomes, including low birth weight, pregnancy-related high blood pressure, stillbirth, and medically indicated preterm birth. Of these, the association between cannabis use and low birth weight was the strongest. All of these conditions have been linked to reduced function of the placenta, which supplies the growing baby with oxygen and nutrients.
While this type of study can't determine why cannabis is associated with negative pregnancy outcomes, previous studies in non-human primates have found that long-term cannabis exposure can interfere with blood supply to the placenta. The correlation Metz and her colleagues observed suggests that cannabis may disrupt the human placenta in a similar way.

Silver adds that the greater risk seen at higher levels of exposure is especially concerning given the high amount of THC found in newer cannabis products -- products that were barely starting to become available from 2010 to 2014, when the study data was collected. The health impacts of these more concentrated products remain largely unknown.
The researchers urge people who are considering using cannabis while pregnant to have an open conversation with their doctor. While pregnant people may turn to cannabis to alleviate nausea or anxiety, other remedies have been proven to be safe. ""There are many, many reasons people use cannabis,"" Silver says. ""But there may be alternative therapies that can help mitigate the symptoms.""
Silver emphasizes that continued research on the health impacts of cannabis is urgently needed so that patients can make informed decisions about their health. ""As long as humans are interested in using this product,"" he says, ""we ought to assess health effects both good and bad, as accurately as we can, and provide that information for folks.""
The research published as ""Cannabis Exposure and Adverse Pregnancy Outcomes Related to Placental Function"" and was carried out in collaboration with researchers from ARUP Laboratories, University of California, Irvine, The Ohio State University, Indiana University, Case Western Reserve University, University of Pennsylvania, Columbia University, Eastern Virginia Medical School, and University of Pittsburgh.
Support was provided by the National Institutes of Health and the Center for Clinical and Translational Sciences.

","score: 15.307560068301012, grade_level: '15'","score: 16.566817294792045, grade_levels: ['college_graduate'], ages: [24, 100]",10.1001/jama.2023.21146,"Cannabis use is increasing among reproductive-age individuals and the risks associated with cannabis exposure during pregnancy remain uncertain. To evaluate the association between maternal cannabis use and adverse pregnancy outcomes known to be related to placental function. Ancillary analysis of nulliparous individuals treated at 8 US medical centers with stored urine samples and abstracted pregnancy outcome data available. Participants in the Nulliparous Pregnancy Outcomes Study: Monitoring Mothers-to-Be cohort were recruited from 2010 through 2013; the drug assays and analyses for this ancillary project were completed from June 2020 through April 2023. Cannabis exposure was ascertained by urine immunoassay for 11-nor-9-carboxy-Δ9-tetrahydrocannabinol using frozen stored urine samples from study visits during the pregnancy gestational age windows of 6 weeks and 0 days to 13 weeks and 6 days (visit 1); 16 weeks and 0 days to 21 weeks and 6 days (visit 2); and 22 weeks and 0 days to 29 weeks and 6 days (visit 3). Positive results were confirmed with liquid chromatography tandem mass spectrometry. The timing of cannabis exposure was defined as only during the first trimester or ongoing exposure beyond the first trimester. The dichotomous primary composite outcome included small-for-gestational-age birth, medically indicated preterm birth, stillbirth, or hypertensive disorders of pregnancy ascertained by medical record abstraction by trained perinatal research staff with adjudication of outcomes by site investigators. Of 10 038 participants, 9257 were eligible for this analysis. Of the 610 participants (6.6%) with cannabis use, 32.4% (n = 197) had cannabis exposure only during the first trimester and 67.6% (n = 413) had ongoing exposure beyond the first trimester. Cannabis exposure was associated with the primary composite outcome (25.9% in the cannabis exposure group vs 17.4% in the no exposure group; adjusted relative risk, 1.27 [95% CI, 1.07-1.49]) in the propensity score–weighted analyses after adjustment for sociodemographic characteristics, body mass index, medical comorbidities, and active nicotine use ascertained via urine cotinine assays. In a 3-category cannabis exposure model (no exposure, exposure only during the first trimester, or ongoing exposure), cannabis use during the first trimester only was not associated with the primary composite outcome; however, ongoing cannabis use was associated with the primary composite outcome (adjusted relative risk, 1.32 [95% CI, 1.09-1.60]). In this multicenter cohort, maternal cannabis use ascertained by biological sampling was associated with adverse pregnancy outcomes related to placental dysfunction."
"
How do women picture the partner of their dreams? And how does this vary between women based on their age? 

A team of researchers led by the University of Göttingen investigated the complex relationships between age and preferences for a partner in a large, international sample of single women. The study found that most preferences for a partner showed no variation between women of different ages. However, higher age was linked to a preference for confident and assertive partners, as well as acceptance of a larger age range, in particular a higher acceptance of a partner being younger than oneself. Age was also linked to the parenting intentions of the ideal partner: consistently high in importance until approximately age 28 and then decreasing thereafter. The results were published in the Journal Human Nature.
To answer the question whether love knows no age, researchers from the University of Göttingen, Indiana University, and Queen's University Belfast, collaborated with the female health app CLUE to reach over 20,000 single women aged 18 to 67 years from nearly 150 countries via an online questionnaire. In addition to heterosexual women, this study also included two groups often neglected in psychological research: bisexual and lesbian women. Respondents were asked to rate how important attributes such as attractiveness, kindness and supportiveness, financial security and successfulness, as well as education and intelligence were to them in their partner. They were also asked to specify the youngest and oldest ages they would be happy to accept in a romantic partner. Using rigorous methods, the role of age in partner preferences was thoroughly investigated in these three groups.
Most partner preferences -- including the preference for a kind and supportive partner -- were consistently important, regardless of age. The study, however, revealed links between age and some specific preferences. ""What was particularly interesting for us is that for heterosexual women up to the age of 28, the importance of the ideal partner wanting to be or become a father remained equally high, but decreased thereafter,"" explains Laura Botzet from Göttingen University's Department of Biological Personality Psychology. Both evolutionary theories and psychological research on the ""biological clock"" would have suggested a later decline, namely between the ages of 40 and 50, when women approach the end of their reproductive phase. This unexpected earlier decrease could be linked to changing life plans, with younger women re-evaluating family goals, whilst older women, who already have children, prioritize different aspects of their relationship. The pattern varied by sexual orientation, potentially indicating different attitudes towards own children among the groups.
Botzet concludes: ""Love, it turns out, is not entirely ageless; it's nuanced. A woman's age is related to certain aspects of her desired partner, such as the preference for partners with stronger parenting intentions or the ideal age of a partner. These insights are exciting because they challenge conventional notions of how age is linked to the way women picture the partner of their dreams.""

","score: 14.65378648233487, grade_level: '15'","score: 15.496358486943166, grade_levels: ['college_graduate'], ages: [24, 100]",10.1007/s12110-023-09460-4,"Women’s capacity to reproduce varies over the life span, and developmental goals such as family formation are age-graded and shaped by social norms about the appropriate age for completing specific developmental tasks. Thus, a woman’s age may be linked to her ideas about what an ideal partner should be like. With the goals of replicating and extending prior research, in this study we examined the role of age in women’s partner preferences across the globe. We investigated associations of age with ideal long-term partner preferences in a cross-cultural sample of 17,254 single (i.e., unpartnered) heterosexual women, ages 18 to 67, from 147 countries. Data were collected via an online questionnaire, the Ideal Partner Survey. Confirming our preregistered hypotheses, we found no or only negligible age effects on preferences for kindness-supportiveness, attractiveness, financial security-successfulness, or education-intelligence. Age was, however, positively associated with preferences for confidence-assertiveness. Consistent with family formation goals, age was associated with an ideal partner’s parenting intentions (high until approximately age 30, then decreasing afterward). Age range deemed acceptable (and in particular, the discrepancy between one’s own age and the minimum ideal age of a partner) increased with age. This latter pattern also replicated in exploratory analyses based on subsamples of lesbian and bisexual women. In summary, age has a limited impact on partner preferences. Of the attributes investigated, only preference for confidence-assertiveness was linked with age. However, age range deemed acceptable and an ideal partner’s parenting intention, a dimension mostly neglected in earlier research, substantially vary with age."
"
Glioblastoma is one of the most treatment-resistant cancers, with those diagnosed surviving for less than two years.

In a new study in NPJ Genomic Medicine, researchers at the University of Notre Dame have found that a largely understudied cell could offer new insight into how the aggressive, primary brain cancer is able to resist immunotherapy.
""A decade ago, we didn't even know perivascular fibroblasts existed within the brain, and not just in the lining of the skull,"" said Meenal Datta, assistant professor of aerospace and mechanical engineering at Notre Dame and senior author on the study. ""My lab's expertise is examining tumors from an engineering and systems-based approach and looking at the novel mechanical features in rare cancers that may have been understudied or overlooked.""
Using standard bioinformatics and newer AI-based approaches, Datta's TIME Lab began analyzing different genes expressed in the tumor microenvironment related to the extracellular matrix -- or the scaffolding cells create to support future cell adhesion, migration, proliferation and differentiation -- and other various cell types. What they found was a surprising, fairly new cell type: perivascular fibroblasts. These fibroblasts are typically found in the blood vessels of a healthy brain and deposit collagen to maintain the structural integrity and functionality of brain vessels.
""It was a serendipitous discovery,"" said Maksym Zarodniuk, graduate student in the TIME Lab and the bioengineering doctorate program, and first author on the study. ""We started in a completely different direction and stumbled upon this population of cells by using a combination of both bulk and single-cell RNA sequencing analyses of patient tumors.""
In their data, researchers were able to identify two groups of patients: those with a higher proportion of perivascular fibroblasts and those with significantly less. They found that brain cancer patients with more perivascular fibroblasts in their tumors were more likely to respond poorly to immunotherapies and have poor survival outcomes.
When exploring how this is possible, the researchers found that perivascular fibroblasts support the creation of an immunosuppressive tumor microenvironment, allowing the cancer to better evade the immune system. The fibroblasts may also help the cancer resist therapies -- such as chemotherapy that targets dividing cells -- by promoting stem-like cancer cells that rarely divide, which are believed to be a major source of tumor relapse and metastasis.

""Moving forward, we want to do new experiments to confirm what we found in this paper and provide some good ground to start thinking about how to improve response to immunotherapy,"" Zarodniuk said.
Because perivascular fibroblasts are a part of a healthy brain's vasculature, Datta believes that these cells are breaking off and getting close to or infiltrating the glioblastoma tumor. However, instead of supporting healthy brain function, these fibroblasts are getting reprogrammed and helping the tumor instead.
""Most people think about the brain as being very soft, with soft cells and a soft matrix. But by putting down these fibroblasts and making these very fibrous proteins, it gives us an entirely different perspective on the structure of the brain and how it can be taken advantage of by cancer cells originating in the same organ,"" Datta said.
In addition to Datta and Zarodniuk, other Notre Dame collaborators include Jun Li, professor of applied and computational mathematics and statistics, who developed deep learning algorithms in support of this work; Xin Lu, the John M. and Mary Jo Boler Collegiate Associate Professor of Biological Sciences at Notre Dame; and Xander Steele, undergraduate student in the TIME Lab and a Grand Challenges Scholar.
Datta is an affiliated member of Notre Dame's Berthiaume Institute for Precision Health, Eck Institute for Global Health, Harper Cancer Research Institute, Lucy Family Institute for Data and Society, NDnano and Warren Center for Drug Discovery. Datta is an assistant professor in the following doctorate programs: aerospace and mechancial engineering, bioengineering and materials science and engineering.
This work was funded by the National Cancer Institute.

","score: 17.035697432338655, grade_level: '17'","score: 18.919126995142257, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41525-023-00381-w,"Excessive deposition of extracellular matrix (ECM) is a hallmark of solid tumors; however, it remains poorly understood which cellular and molecular components contribute to the formation of ECM stroma in central nervous system (CNS) tumors. Here, we undertake a pan-CNS analysis of retrospective gene expression datasets to characterize inter- and intra-tumoral heterogeneity of ECM remodeling signatures in both adult and pediatric CNS disease. We find that CNS lesions – glioblastoma in particular – can be divided into two ECM-based subtypes (ECMhi and ECMlo) that are influenced by the presence of perivascular stromal cells resembling cancer-associated fibroblasts (CAFs). Ligand-receptor network analysis predicts that perivascular fibroblasts activate signaling pathways responsible for recruitment of tumor-associated macrophages and promotion of cancer stemness. Our analysis reveals that perivascular fibroblasts are correlated with unfavorable response to immune checkpoint blockade in glioblastoma and poor patient survival across a subset of CNS tumors. We provide insights into new stroma-driven mechanisms underlying immune evasion and immunotherapy resistance in CNS tumors like glioblastoma, and discuss how targeting these perivascular fibroblasts may prove an effective approach to improving treatment response and patient survival in a variety of CNS tumors."
"
People who have been subject to abuse are more likely to experience physical and mental health effects than previously thought, according to a new study.

In a global review and meta-analysis of evidence published in Nature Medicine today, researchers have found that there are elevated risks between intimate partner violence or childhood sexual abuse, and some health conditions including major depressive disorder, maternal miscarriage for partners, and alcohol misuse and self-harm among children.
Globally, one in three ever-partnered women have experienced intimate partner violence in their lifetime, and around 20% of young women and 10% of young men have experienced some form of childhood sexual abuse. However, research investigating health outcomes associated with intimate partner violence and childhood sexual abuse has been limited.
The authors found that exposures to intimate partner violence had a moderate association with an increased risk of major depressive disorders (63%) and an increased risk of maternal abortion and miscarriage (35%). Childhood sexual abuse was shown to be moderately associated with an increased risk of alcohol use and an increased risk of self-harm (45% and 35%, respectively). The authors indicate these findings are larger in magnitude and more extensive than previously suggested.
Dr Joht Singh Chandan, Clinical Associate Professor in Public Health at the University of Birmingham and senior co-lead author of the paper said:
""This comprehensive study marks a significant step in understanding the profound health impacts of intimate partner violence against women and childhood sexual abuse. Our findings reveal not only the alarming associations these forms of violence have with conditions like major depressive disorder, miscarriage, alcohol use disorders, and self-harm, but also underscore the urgent need for robust preventive measures and support systems.""
Professor Emmanuela Gakidou from the Institute of Health Metrics and Evaluation at the University of Washington and senior co-lead author of the paper
""While we've shed light on these critical health issues, our research also highlights the gaps in current knowledge and the necessity for continued investigation to fully grasp the extensive consequences of such violence. It's imperative that we use these insights to inform policy, healthcare, and community interventions, ensuring a safer and healthier future for individuals affected by these pervasive forms of violence.""

Dr Nicholas Metheny, Assistant Professor in the School of Nursing and Health Studies at the University of Miami said:
""Our research marks a pivotal shift in how we perceive the societal and health burdens of intimate partner violence (IPV). Previously, our evidence primarily highlighted IPV's contribution to HIV and depression, substantially underestimating its broader impact. This study expands our understanding, revealing IPV's extensive influence on a wider range of poor health outcomes.
""This new perspective is crucial in elevating IPV as a public health imperative in the global sphere, hopefully igniting both political and scientific momentum towards effective prevention and intervention strategies.""
4000 studies reviewed -- 229 suitable for inclusion
Searching through papers published from seven databases, Emmanuella Gakidou and colleagues identified the available literature on intimate partner violence and childhood sexual abuse and their associated health effects. They reviewed over 4,000 studies, of which 229 studies met the criteria for inclusion. Using the burden of proof methodology (a meta-analytic approach for estimating a conservative measure of the elevated or reduced risk of a particular health outcome after exposure to a harmful or protective risk factor), they evaluated the strength of evidence connecting intimate partner violence and/or childhood sexual abuse to health outcomes, which were supported by at least three studies.
Additional potential health outcomes were also initially identified in the study, including an association of maternal hypertensive disorders with intimate partner violence and an association of smoking with childhood sexual abuse. However, the research concludes that due to a scarcity of evidence, these could not be included in the meta-analysis.
The authors note that the studies are observational and cannot demonstrate causality and highlight that their findings are limited owing to the limited number of studies that explore these relationships. They suggest that their research demonstrates the wide-ranging health effects of intimate partner violence and childhood sexual abuse but emphasize the need for further research to strengthen the evidence base.
The study was funded by the Bill and Melinda Gates Foundation.

","score: 18.216588438735176, grade_level: '18'","score: 20.762827939723323, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41591-023-02629-5,"The health impacts of intimate partner violence against women and childhood sexual abuse are not fully understood. Here we conducted a systematic review by comprehensively searching seven electronic databases for literature on intimate partner violence-associated and childhood sexual abuse-associated health effects. Following the burden of proof methodology, we evaluated the evidence strength linking intimate partner violence and/or childhood sexual abuse to health outcomes supported by at least three studies. Results indicated a moderate association of intimate partner violence with major depressive disorder and with maternal abortion and miscarriage (63% and 35% increased risk, respectively). HIV/AIDS, anxiety disorders and self-harm exhibited weak associations with intimate partner violence. Fifteen outcomes were evaluated for their relationship to childhood sexual abuse, which was shown to be moderately associated with alcohol use disorders and with self-harm (45% and 35% increased risk, respectively). Associations between childhood sexual abuse and 11 additional health outcomes, such as asthma and type 2 diabetes mellitus, were found to be weak. Although our understanding remains limited by data scarcity, these health impacts are larger in magnitude and more extensive than previously reported. Renewed efforts on violence prevention and evidence-based approaches that promote healing and ensure access to care are necessary."
"
Australian researchers have flagged potential concerns over the use of social chatbots, calling for more studies into the impact of the AI software on neurodiverse people and those who find human interaction difficult.

While the AI chatbot is appealing to many people who struggle with face-to-face conversations, the technology may foster bad habits that could lead to further social isolation.
That's the view of University of South Australia and Flinders University researchers in a recent essay published in the Journal of Behavioural Addictions.
The researchers say that chatbots, now integrated into social networking platforms like Snapchat, could perpetuate communication difficulties for people with autism, anxiety and limited social skills.
Lead researcher, UniSA Psychology Honours student Andrew Franze, says the rapid development of social chatbots has pros and cons which need investigating.
""Young people with social deficiencies tend to gravitate towards companionship with online social chatbots in particular,"" Franze says.
""They offer a safe means of rehearsing social interaction with limited or no risk of negative judgement based on appearance or communication style. However, there is a risk they can become dependent on chatbots and withdraw even further from human interactions.""
Franze says the inability of chatbots to have a real ""conversation,"" or display empathy and soft emotional skills, can reinforce dysfunctional habits in many neurodiverse people.

""Some chatbots have a generally servile quality and so there is no resistance or opposing view that characterises human conversations. This means that users can control the conversation completely; they can pause it, delay it, or even terminate the conversation. All of this is counterproductive to developing appropriate social skills in the real world.""
And while social chatbots may relieve social anxiety, this relief may develop into a form of dependency that negatively impacts on actual relationships.
The researchers say that industry-linked research has promoted the benefits of commercial chatbot applications, but feedback from parents, family members, teachers and therapists is needed to gain a broader understanding of its impacts.
""We need to gather evidence about the myriad of ways that these technologies can influence vulnerable users who may be particularly drawn to them,"" Franze says. ""Only then can we develop policies and industry practices that guide the responsible and safe use of chatbots.""
""Social chatbot user (e.g., ChatGPT) among individuals with social deficits: risks and opportunities"" is published in the Journal of Behavioural Addictions. It is authored by Andrew Franze (University of South Australia); Christina R. Galanis and Daniel L. King (Flinders University).

","score: 15.313661971830985, grade_level: '15'","score: 16.17565727699531, grade_levels: ['college_graduate'], ages: [24, 100]",10.1556/2006.2023.00057,"Social chatbots powered by artificial intelligence (AI) may be particularly appealing to individuals with social deficits or conditions that affect their social functioning. In this letter, we discuss some of the noteworthy characteristics of social chatbots and how they may influence adaptive and maladaptive behaviors, including the potential for ‘dependency’ on chatbots. We call for more independent studies to evaluate the potential developmental and therapeutic effects of this increasingly popular technology."
"
Researchers at the Francis Crick Institute, UCL and MSD have identified a potential treatment target for a genetic type of epilepsy.

Developmental and epileptic encephalopathies are rare types of epilepsy which start in early childhood. One of the most common types of genetic epilepsy, CDKL5 deficiency disorder (CDD), causes seizures and impaired development. Children are currently treated with generic antiepileptic drugs, as there aren't yet any disease-targeting medications for this disorder.
CDD involves losing the function of a gene producing the CDKL5 enzyme, which phosphorylates proteins, meaning it adds an extra phosphate molecule to alter their function. Until now, researchers have not been sure how genetic mutations in CDKL5 cause CDD.
Through their research, published today in Nature Communications, the researchers examined mice which lacked the Cdkl5 gene, and used a technique called phosphoproteomics to scan for proteins which are a target for the CDKL5 enzyme.
They identified a calcium channel, Cav2.3, as a target. Cav2.3 allows calcium to enter nerve cells, exciting the cell and allowing it to pass on electrical signals. This is needed for the nervous system to function properly, but too much calcium coming into cells can result in overexcitability and seizures.
The researchers then recorded from the calcium channels to see what was happening when they were not being phosphorylated by CDKL5. The channels were able to open, but were taking a lot longer to close, leading to larger and more prolonged currents flowing through them. This implies that CDKL5 is needed to limit calcium entry into cells.
The researchers also used nerve cells derived from stem cells taken from people with CDD, again observing that phosphorylation of Cav2.3 was reduced. This suggests that Cav2.3 function is potentially altered in humans as well as mice.

Mutations in Cav2.3 that enhance channel activity are already known to cause severe early onset epilepsy in a related condition called DEE69, which shares a lot of the same symptoms of CDD. These results suggest that Cav2.3 overactivity is a common feature of both disorders, and that inhibiting Cav2.3 could help with symptoms like seizures.
Sila Ultanir, Senior Group Leader of the Kinases and Brain Development Laboratory at the Crick, said: ""At the moment, there's a clear need for drugs which specifically target the biological nature of CDD. We've made a molecular link between CDKL5 and Cav2.3, mutations in which produce similar disorders. Inhibiting Cav2.3 could be a route for trials of future targeted treatments.""
Marisol Sampedro-Castañeda, postdoctoral researcher at the Crick, and first author, said: ""Our research highlights for the first time a CDKL5 target with a link to neuronal excitability. There's scattered evidence that this calcium channel could be involved in other types of epilepsy too, so we believe that Cav2.3 inhibitors could eventually be tested more widely.
""Our findings have implications for a large group of people, from the families affected by these conditions to researchers working in the rare epilepsy field.""
This research was funded by MSD and the Loulou Foundation, a private foundation dedicated to the development of therapeutics and eventual cures for CDD.
Jill Richardson, Executive Director and Head of Neuroscience Biology at MSD, said: ""MSD is proud of this innovative research resulting from a collaboration with researchers at the Crick and UCL. We have collectively furthered our scientific understanding of the biological targets associated with the aetiologies of Developmental Epileptic Encephalopathies -- an understanding we hope will contribute toward scientific progress in this important area of high, unmet medical need.""
The researchers are now working with Lario Therapeutics, a recently launched biotech company which is seeking to develop first-in-class CaV2.3 inhibitors as precision medicines to treat CDD and related neurodevelopmental syndromes.

","score: 13.880895304753572, grade_level: '14'","score: 14.925101778944303, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41467-023-43475-w,"Developmental and epileptic encephalopathies (DEEs) are a group of rare childhood disorders characterized by severe epilepsy and cognitive deficits. Numerous DEE genes have been discovered thanks to advances in genomic diagnosis, yet putative molecular links between these disorders are unknown. CDKL5 deficiency disorder (CDD, DEE2), one of the most common genetic epilepsies, is caused by loss-of-function mutations in the brain-enriched kinase CDKL5. To elucidate CDKL5 function, we looked for CDKL5 substrates using a SILAC-based phosphoproteomic screen. We identified the voltage-gated Ca2+ channel Cav2.3 (encoded by CACNA1E) as a physiological target of CDKL5 in mice and humans. Recombinant channel electrophysiology and interdisciplinary characterization of Cav2.3 phosphomutant mice revealed that loss of Cav2.3 phosphorylation leads to channel gain-of-function via slower inactivation and enhanced cholinergic stimulation, resulting in increased neuronal excitability. Our results thus show that CDD is partly a channelopathy. The properties of unphosphorylated Cav2.3 closely resemble those described for CACNA1E gain-of-function mutations causing DEE69, a disorder sharing clinical features with CDD. We show that these two single-gene diseases are mechanistically related and could be ameliorated with Cav2.3 inhibitors."
"
As holidays near, people are sneaking shakes of their presents to try to figure out what they're getting. But present shakers might be a little less sly than they think. New research shows it's incredibly easy for people watching others shake boxes to tell what they're up to.

""There are few things more delightful than seeing a child's eyes light up as they pick up a present and wonder what might be inside,"" said author Chaz Firestone, a Johns Hopkins University assistant professor of psychological and brain sciences who investigates how vision and thought interact. ""What our work shows is that your mind is able to track the information they are seeking. Just as they might be able to tell what's inside the box by shaking it around, you can tell what they are trying to figure out when they shake it.""
In a series of experiments, the researchers asked hundreds of people to watch others shake boxes. It took just seconds for most of them to know whether the box shaker was trying to learn either how many things were in the box or the shape of things in the box. Although the boxes weren't presents, and the contents weren't smart watches, Legos or Red Ryder BB guns, if they were, the results would have been exactly the same, the researchers say.
""The way you would shake a present to find out if it's one thing or many things, or if it's a small thing versus a big thing, can be subtly different,"" said lead author Sholei Croom, a Johns Hopkins graduate student. ""But people are amazing at picking up on such subtleties.""
The deceptively simple work by Johns Hopkins University perception researchers is the first to demonstrate that people can tell what others are trying to learn just by watching their actions. The work, newly published just in time for the holidays in the journal Proceedings of the National Academy of Sciences, reveals a key, yet neglected, aspect of human cognition.
""When we present this work we always talk about Christmas presents,"" Firestone said. ""It's the perfect real-life example of our experiment.""

","score: 10.479267759562841, grade_level: '10'","score: 12.067950819672127, grade_levels: ['college'], ages: [18, 24]",10.1073/pnas.2303162120,"Many actions have instrumental aims, in which we move our bodies to achieve a physical outcome in the environment. However, we also perform actions with epistemic aims, in which we move our bodies to acquire information and learn about the world. A large literature on action recognition investigates how observers represent and understand the former class of actions; but what about the latter class? Can one person tell, just by observing another person’s movements, what they are trying to learn? Here, five experiments exploreepistemic action understanding. We filmed volunteers playing a “physics game” consisting of two rounds: Players shook an opaque box and attempted to determine i) the number of objects hidden inside, or ii) the shape of the objects inside. Then, independent subjects watched these videos and were asked to determine which videos came from which round: Who was shaking for number and who was shaking for shape? Across several variations, observers successfully determined what an actor was trying to learn, based only on their actions (i.e., how they shook the box)—even when the box’s contents were identical across rounds. These results demonstrate that humans can infer epistemic intent from physical behaviors, adding a new dimension to research on action understanding."
"
In mice and human cell cultures, MIT researchers showed that novel nanoparticles can deliver a potential therapy for inflammation in the brain, a prominent symptom in Alzheimer's disease.

Some Covid-19 vaccines safely and effectively used lipid nanoparticles (LNPs) to deliver messenger RNA to cells. A new MIT study shows that different nanoparticles could be used for a potential Alzheimer's disease (AD) therapy. In tests in multiple mouse models and with cultured human cells, a newly tailored LNP formulation effectively delivered small interfering RNA (siRNA) to the brain's microglia immune cells to suppress expression of a protein linked to excessive inflammation in Alzheimer's disease.
In a prior study the researchers showed that blocking the consequences of PU.1 protein activity helps to reduce Alzheimer's disease-related neuroinflammation and pathology. The new results, reported in the journal Advanced Materials (impact factor 29.4 ) achieves a reduction in inflammation by directly tamping down expression of the Spi1 gene that encodes PU.1. More generally, the new study also demonstrates a new way to deliver RNA to microglia, which have been difficult to target so far.
Study co-senior author Li-Huei Tsai, Picower Professor of Neuroscience and Director of The Picower Institute for Learning and Memory and Aging Brain Initiative, said she hypothesized that LNPs might work as a way to bring siRNA into microglia because the cells, which clear waste in the brain, have a strong proclivity to uptake lipid molecules. She discussed this with Robert Langer, David Koch Institute Professor, who widely known for his seminal work on nanoparticle drug delivery, They decided to test the idea of reducing PU.1 expression with an LNP-delivered siRNA.
""I still remember the day when I asked to meet with Bob to discuss the idea of testing LNPs as a payload to target inflammatory microglia,"" said Tsai, a faculty member in the Department of Brain and Cognitive Sciences. ""I am very grateful to The JPB Foundation who supported this idea without any preliminary evidence.""
Langer Lab graduate student Jason Andresen and former Tsai Lab postdoc William Ralvenius led the work and are the study's co-lead authors. Owen Fenton, a former Langer Lab postdoc who is now an assistant professor at the University of North Carolina's Eshelman School of Pharmacy, is a co-corresponding author along with Tsai and Langer. Langer is a Professor in Chemical Engineering, Biological Engineering and the Koch Institute for Integrative Cancer Research.
Perfecting a particle
The simplest way to test whether siRNA could therapeutically suppress PU.1 expression would have been to make use of an already available delivery device, but one of the first discoveries in the study is that none of eight commercially available reagents could safely and effectively transfect cultured human microglia-like cells in the lab.

Instead the team had to optimize an LNP to do the job. LNPs have four main components and by changing the structures of two of them, and by varying the ratio of lipids to RNA, the researchers were able to come up with seven formulations to try. Importantly, their testing included trying their formulations on cultured microglia that they had induced into an inflammatory state. That state, after all, is the one in which the proposed treatment is needed.
Among the seven candidates, one the team named ""MG-LNP"" stood out for its especially high delivery efficiency and safety of a test RNA cargo.
What works in a dish sometimes doesn't work in a living organism, so the team next tested their LNP formulations' effectiveness and safety in mice. Testing two different methods of injection, into the body or into the cerebrospinal fluid (CSF), they found that injection into the CSF ensured much greater efficacy in targeting microglia without affecting cells in other organs. Among the seven formulations, MG-LNP again proved the most effective at transfecting microglia. Langer said he believes this could potentially open new ways of treating certain brain diseases with nanoparticles someday.
A targeted therapy
Once they knew MG-LNP could deliver a test cargo to microglia both in human cell cultures and mice, the scientists then tested whether using it to deliver a PU.1-suppressing siRNA could reduce inflammation in microglia. In the cell cultures, a relatively low dose achieved a 42 percent reduction of PU.1 expression (which is good because microglia need at least some PU.1 to live). Indeed MG-LNP transfection did not cause the cells any harm. It also significantly reduced the transcription of the genes that PU.1 expression increases in microglia, indicating that it can reduce multiple inflammatory markers.
In all these measures, and others, MG-LNP outperformed a commercially available reagent called RNAiMAX that the scientists tested in parallel.

""These findings support the use of MG-LNP-mediated anti-PU.1 siRNA delivery as a potential therapy for neuroinflammatory diseases,"" the researchers wrote.
The final set of tests evaluated MG-LNP's performance delivering the siRNA in two mouse models of inflammation in the brain. In one, mice were exposed to LPS, a molecule that simulates infection and stimulates a systemic inflammation response. In the other model, mice exhibit severe neurodegeneration and inflammation when an enzyme called CDK5 becomes hyperactivated by a protein called p25.
In both models, injection of MG-LNPs carrying the anti-PU.1 siRNA reduced expression of PU.1 and inflammatory markers, much like in the cultured human cells.
""MG-LNP delivery of anti-PU.1 siRNA can potentially be used as an anti-inflammatory therapeutic in mice with systemic inflammation an in the CK-p25 mouse model of AD-like neuroinflammation,"" the scientists concluded, calling the results a ""proof-of-principle."" More testing will be required before the idea could be tried in human patients.
In addition to Andresen, Ralvenius, Langer, Tsai and Owen, the paper's other authors are Margaret Huston, Jay Penney and Julia Maeve Bonner.
In addition to the The JPB Foundation and The Picower Institute for Learning and Memory, the Robert and Renee Belfer Family, Eduardo Eurnekian, Lester A. Gimpelson, Jay L. and Carroll Miller, the Koch Institute, the Swiss National Science Foundation and the Alzheimer's Association provided funding for the study.

","score: 15.462857142857143, grade_level: '15'","score: 16.41262276380607, grade_levels: ['college_graduate'], ages: [24, 100]",10.1002/adma.202309225,"Neuroinflammation is a hallmark of neurodegenerative disorders including Alzheimer's disease (AD). Microglia, the brain's immune cells, express many of the AD‐risk loci identified in genome wide association studies and present a promising target for anti‐inflammatory RNA therapeutics but are difficult to transfect with current methods. Here, several lipid nanoparticle (LNP) formulations are examined, and a lead candidate that supports efficient RNA delivery in cultures of human stem cell‐derived microglia‐like cells (iMGLs) and animal models of neuroinflammation is identified. The lead microglia LNP (MG‐LNP) formulation shows minimal toxicity and improves delivery efficiency to inflammatory iMGLs, suggesting a preference for delivery into activated microglia. Intraperitoneal injection of the MG‐LNP formulation generates widespread expression of the delivered reporter construct in all organs, whereas local intracisternal injection directly into the cerebrospinal fluid leads to preferential expression in the brain. It is shown that LNP‐mediated delivery of siRNA targeting the PU.1 transcription factor, a known AD‐risk locus, successfully reduces PU.1 levels in iMGLs and reduces neuroinflammation in mice injected with LPS and in CK‐p25 mice that mimic the chronic neuroinflammation seen in AD patients. The LNP formulation represents an effective RNA delivery vehicle when applied intrathecally and can be broadly utilized to test potential neuroinflammation‐directed gene therapies."
"
A new study published in Proceedings of the National Academy of Sciences Nexus provides a better understanding of how the brain responds to injuries. Researchers at the George Washington University discovered that a protein called Snail plays a key role in coordinating the response of brain cells after an injury.

The study shows that after an injury to the central nervous system (CNS) a group of localized cells start to produce Snail, a transcription factor or protein that has been implicated in the repair process.The GW researchers show that changing how much Snail is produced can significantly affect whether the injury starts to heal efficiently or whether there is additional damage.
""Our findings reveal the intricate ways the brain responds to injuries,"" said senior author Robert Miller, the Vivian Gill Distinguished Research Professor and Vice Dean of the GW School of Medicine and Health Sciences. ""Snail appears to be a key player in coordinating these responses, opening up promising possibilities for treatments that can minimize damage and enhance recovery from neurological injuries.""
Key findings: This study identifies for the first time a special group of microglial-like cells that produce Snail. Microglial cells are found in the central nervous system. Lowering the amount of Snail produced after an injury results in inflammation and increased cell death. During this process the injury gets worse not better and there are fewer connections or synapses between brain cells. In contrast, when Snail levels are increased the outcome of brain injury improves-suggesting this protein can help limit the spread of injury-induced damage.The research raises questions about whether an experimental drug that affects Snail production could be used to limit the damage incurred after someone suffers a stroke or has been injured in an accident, Miller said. Additional studies must be done to show that increasing Snail production could curtail injury or even promote healing of the brain.
Miller and his team also plan to study the regulation of Snail in diseases like multiple sclerosis. Multiple sclerosis is a disease resulting in damage to myelin, the protective layer insulating nerve fibers in the brain. If drugs targeting Snail could be used to stop that damage, many of the future symptoms of this disease could be eased, he says.
But researchers have years of work to do before new drugs targeting Snail can be tested in clinical trials. The payoff ultimately might be drugs that can lead to accelerated healing for stroke damage, head wounds and even neurodegenerative diseases like dementia.
In addition to Miller and a team of researchers at the GW School of Medicine and Health Sciences, Cheryl Clarkson-Paredes, a senior research scientist at the GW Nanofabrication and Imaging Center, served as the lead author of the paper, ""A unique cell population expressing the Epithelial-Mesenchymal Transition-transcription factor Snail moderates microglial and astrocyte injury responses,""
The research was supported by Biogen and the National Institutes of Health.

","score: 15.673295083947338, grade_level: '16'","score: 17.826547892257516, grade_levels: ['college_graduate'], ages: [24, 100]",10.1093/pnasnexus/pgad334,"Insults to the central nervous system (CNS) elicit common glial responses including microglial activation evidenced by functional, morphological, and phenotypic changes, as well as astrocyte reactions including hypertrophy, altered process orientation, and changes in gene expression and function. However, the cellular and molecular mechanisms that initiate and modulate such glial response are less well-defined. Here we show that an adult cortical lesion generates a population of ultrastructurally unique microglial-like cells that express Epithelial-Mesenchymal Transcription factors including Snail. Knockdown of Snail with antisense oligonucleotides results in a postinjury increase in activated microglial cells, elevation in astrocyte reactivity with increased expression of C3 and phagocytosis, disruption of astrocyte junctions and neurovascular structure, increases in neuronal cell death, and reduction in cortical synapses. These changes were associated with alterations in pro-inflammatory cytokine expression. By contrast, overexpression of Snail through microglia-targeted an adeno-associated virus (AAV) improved many of the injury characteristics. Together, our results suggest that the coordination of glial responses to CNS injury is partly mediated by epithelial-mesenchymal transition-factors (EMT-Fsl)."
"
A University of Iowa-led team of international neuroscientists have obtained the first direct recordings of the human brain in the minutes before and after a brain hub crucial for language meaning was surgically disconnected. The results reveal the importance of brain hubs in neural networks and the remarkable way in which the human brain attempts to compensate when a hub is lost, with immediacy not previously observed.

Hubs are critical for connectivity 
Hubs are everywhere. The hub of a bicycle wheel, with spokes shooting out from the center, keeps the wheel from collapsing when the bicycle is ridden. Airport hubs connect cities across the world. And social hubs like coffee shops or online social networks are places people gather for interaction.
The human brain has hubs, too -- the intersection of many neuronal pathways that help coordinate brain activity required for complex functions like understanding and responding to speech. However, whether highly interconnected brain hubs are irreplaceable for certain brain functions has been controversial. By some accounts the brain, as an already highly interconnected neural network, can in principle immediately compensate for the loss of a hub, in the same way that traffic can be redirected around a blocked off city center.
With a rare experimental opportunity, the UI neurosurgical and research teams led by Matthew Howard III, MD, professor and DEO of neurosurgery, and Christopher Petkov, PhD, professor and vice chair for research in neurosurgery, have achieved a breakthrough in understanding the necessity of a single hub. By obtaining evidence for what happens when a hub required for language meaning is lost, the researchers showed both the intrinsic importance of the hub as well as the remarkable and rapid ability of the brain to adapt and at least partially attempt to immediately compensate for its loss. The findings were reported recently in the journal Nature Communications.
Evaluating the impact of losing a brain hub 
The study was conducted during surgical treatment of two patients with epilepsy. Both patients were undergoing procedures that required surgical removal of the anterior temporal lobe -- a brain hub for language meaning -- to allow the neurosurgeons access to a deeper brain area causing the patients' debilitating epileptic seizures. Before this type of surgery, neurosurgery teams often ask the patients to conduct speech and language tasks in the operating room as the team uses implanted electrodes to record activity from parts of the brain close to and distant from the planned surgery area. These recordings help the clinical team effectively treat the seizures while limiting the impact of the surgery on the patient's speech and language abilities.

Typically, the recording electrodes are not needed after the surgical resection procedure and are removed. The innovation in this study was that the neurosurgery team was able to safely complete the procedure with the recording electrodes left in place or replaced to the same location after the procedure. This made it possible to obtain rare pre- and post-operative recordings allowing the researchers to evaluate signals from brain areas far away from the hub, including speech and language areas distant from the surgery site. Analysis of the change in responses to speech sounds before and after the loss of the hub revealed a rapid disruption of signaling and subsequent partial compensation of the broader brain network.
""The rapid impact on the speech and language processing regions well removed from the surgical treatment site was surprising, but what was even more surprising was how the brain was working to compensate, albeit incompletely within this short timeframe,"" says Petkov, who also holds an appointment at Newcastle University Medical School in the UK.
The findings disprove theories challenging the necessity of specific brain hubs by showing that the hub was important to maintain normal brain processing in language.
""Neurosurgical treatment and new technologies continue to improve the treatment options provided to patients,"" says Howard, who also is a member of the Iowa Neuroscience Institute. ""Research such as this underscores the importance of safely obtaining and comparing electrical recordings pre and post operatively, particularly when a brain hub might be affected.""
According to the researchers, the observation on the nature of the immediate impact on a neural network and its rapid attempt to compensate provides evidence in support of a brain theory proposed by Professor Karl Friston at University College London, which posits that any self-organizing system at equilibrium works towards orderliness by minimizing its free energy, a resistance of the universal tendency towards disorder. These neurobiological results following human brain hub disconnection were consistent with several predictions of this and related neurobiological theories, showing how the brain works to try to regain order after the loss of one of its hubs.
In addition to Petkov and Howard, the research team included researchers in the UI Departments of Neurosurgery, Radiology, and Psychological and Brain Sciences, as well as colleagues from Newcastle University, UCL, and University of Cambridge in the UK, and from Carnegie Mellon University, University of Wisconsin-Madison, and Gonzaga University in the United States.
The research was funded in part by grants from National Institutes of Health, the Wellcome Trust. and the European Research Council.

","score: 16.91270067999839, grade_level: '17'","score: 18.660051100470767, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41467-023-42088-7,"The human brain extracts meaning using an extensive neural system for semantic knowledge. Whether broadly distributed systems depend on or can compensate after losing a highly interconnected hub is controversial. We report intracranial recordings from two patients during a speech prediction task, obtained minutes before and after neurosurgical treatment requiring disconnection of the left anterior temporal lobe (ATL), a candidate semantic knowledge hub. Informed by modern diaschisis and predictive coding frameworks, we tested hypotheses ranging from solely neural network disruption to complete compensation by the indirectly affected language-related and speech-processing sites. Immediately after ATL disconnection, we observed neurophysiological alterations in the recorded frontal and auditory sites, providing direct evidence for the importance of the ATL as a semantic hub. We also obtained evidence for rapid, albeit incomplete, attempts at neural network compensation, with neural impact largely in the forms stipulated by the predictive coding framework, in specificity, and the modern diaschisis framework, more generally. The overall results validate these frameworks and reveal an immediate impact and capability of the human brain to adjust after losing a brain hub."
"
CAMH-led pre-clinical studies using a small molecule drug have shown promise as a potential new treatment for multiple sclerosis (MS). The results have been published today in the journal Science Advances.

Expanding on Dr. Fang Liu's earlier work that identified a novel drug target for the treatment of MS, she and her team have now created a small molecule compound that is effective in two different animal models of MS. This represents a key advancement that brings this MS research closer to the clinic to impact patient care.
MS is a progressive neurological disease that currently has no cure. It is associated with a wide-range of debilitating symptoms, including problems with coordination, cognition, muscle weakness and depression. For unknown reasons, it is more common in northern latitudes and more than twice as common in women.
It is known that MS damages myelin, a protective sheath that forms around nerves in the brain and spinal cord. As the myelin damage is triggered by inflammation in the immune system, up until now all current drug treatments for MS target the immune system.
In this study, CAMH Senior Scientist Dr. Fang Liu and her team treated MS in a completely different way -- targeting the glutamate system. Study results showed that the newly synthesized lead compound not only reduced MS-like symptoms, it also may repair the damaged myelin in two different pre-clinical models of MS.
""Our compound had a stunning effect on rescuing myelin and motor function in the lab models, and I hope these effects will translate to the clinic to add to current treatments and bring new hope to patients with MS,"" said Dr. Liu. ""As with cancer chemotherapy drug cocktails, simultaneous targeting of the MS disease pathway at multiple points can have synergistic effects and result in better outcomes.""
Dr. Iain Greig, Reader in Medicinal Chemistry at the University of Aberdeen, alongside his team, are working to turn the molecules identified by Dr. Liu into advanced ""drug-like"" molecules suitable for continued development towards clinical use in patients. He added: ""In all my years as a medicinal chemist, I have never seen a more promising starting point for a drug development project. It has been a huge pleasure to be involved in this program and I am looking forward to continuing to drive it towards to the clinic.""
Much of the funding for this novel treatment for MS, which Dr. Fang and her team have been investigating for over a decade, has come from the Multiple Sclerosis Society of Canada and the National Multiple Sclerosis Society USA's Fast Forward commercial research program.
""We are pleased to have helped enable the early development of a novel neuroprotective strategy for MS, and look forward to seeing it progress through the critical next stages needed to determine its potential benefits for people living with MS,"" said Walt Kostich, PhD, head of the National MS Society (USA)'s Fast Forward commercial research program.
Dr. Liu believes that the evidence of efficacy and tolerability generated in this study for the small molecule drug makes it a good candidate to be developed for human trials. The next steps in drug development will involve some further pre-clinical research, including investigating safety and stability of the compound. CAMH and the University of Aberdeen have already filed patent applications to protect this research and are actively seeking industry partners to further advance this work towards clinical trials over the next few years.

","score: 15.251065404475046, grade_level: '15'","score: 16.490972461273664, grade_levels: ['college_graduate'], ages: [24, 100]",10.1126/sciadv.adj6187,"While most research and treatments for multiple sclerosis (MS) focus on autoimmune reactions causing demyelination, it is possible that neurodegeneration precedes the autoimmune response. Hence, glutamate receptor antagonists preventing excitotoxicity showed promise in MS animal models, though blocking glutamate signaling prevents critical neuronal functions. This study reports the discovery of a small molecule that prevents AMPA-mediated excitotoxicity by targeting an allosteric binding site. A machine learning approach was used to screen for small molecules targeting the AMPA receptor GluA2 subunit. The lead candidate has potent effects in restoring neurological function and myelination while reducing the immune response in experimental autoimmune encephalitis and cuprizone MS mouse models without affecting basal neurotransmission or learning and memory. These findings facilitate development of a treatment for MS with a different mechanism of action than current immune modulatory drugs and avoids important off-target effects of glutamate receptor antagonists. This class of MS therapeutics could be useful as an alternative or complementary treatment to existing therapies."
"
Exclusive breastfeeding for the first six months of life is proven to protect both mother and child health. According to the World Health Organization (WHO), between 2015 and 2021, 48% of mothers exclusively breastfed, meaning that their babies were not given any other food or liquids. However, this figure is based on data collected from surveys which report what a child was given in the previous 24 hours. A research team, including members from the University of Tokyo, has found that this ""24-hour recall"" method overestimates exclusive breastfeeding by about six times compared to a ""since-birth recall"" method. The 24-hour recall data also do not reflect the positive impact of in-hospital breastfeeding support and guidance. More indicators to assess child-feeding practices and mothers' experiences are needed to increase exclusive breastfeeding and to improve breastfeeding outcomes for both.

Breastfeeding is a natural behavior but also a learned behavior that requires appropriate support. The World Health Organization and the United Nations Children's Fund (UNICEF) recommend that babies be exclusively breastfed until the age of six months. Breast milk contains antibodies and hormones, along with nutrients, which can help build babies' resistance to common childhood illnesses and can even reduce the risk of some diseases in adulthood. According to the Centers for Disease Control and Prevention in the U.S., breastfeeding also benefits the mother by reducing the risk of breast and ovarian cancer, type 2 diabetes and high blood pressure. However, many women breastfeed without sufficient help or guidance, which can result in mothers struggling with this demanding task and stopping earlier.
The WHO has set a global target to increase exclusive breastfeeding in the first six months from an estimated 38% between 2006 and 2010, to over 50% by 2025. To assess progress toward this target, the WHO and UNICEF collect data on child feeding from population-based household surveys every three to five years. These surveys ask what babies under the age of 5 months were fed, and how often, within the past 24 hours. However, this 24-hour recall method has been criticized for not giving a true picture of breastfeeding practices.
""We have found that merely asking mothers whether they are currently breastfeeding overestimates the prevalence of breastfeeding and also overlooks the importance of providing proper support in maternity facilities,"" said Assistant Professor Keiko Nanishi from the Graduate School of Medicine at the University of Tokyo. ""I have long thought that 24-hour recall used by the WHO as an indicator does not reflect the responsibilities of health staff and facilities. As a mother, a pediatrician, a lactation consultant and a researcher in maternal and child health, I think breastfeeding promotion should focus on creating a mother- and baby-friendly environment including health staff and facilities implementing evidence-based infant-feeding practices.""
In a study of over 4,000 mothers in Japan, Nanishi and team compared responses to questions about breastfeeding using the 24-hour recall method and the since-birth recall method. For the latter, additional questions were asked about when breastfeeding started and finished during the months since birth, when formula milk was introduced and stopped, and when complementary feeding began. Participants were also asked about in-hospital breastfeeding support, measured against the WHO's recommended Ten Steps to Successful Breastfeeding, along with their intentions to breastfeed, social background and factors related to their experience of childbirth.
Results of the surveys showed that when using the 24-hour recall method, exclusive breastfeeding for children under 5 months was estimated to be much higher at 29.8% compared to since-birth recall, which was 4.4%. Also, exclusive breastfeeding was clearly more common when more in-hospital breastfeeding support was provided (following the WHO's Ten Steps). However, the connection between in-hospital support and exclusive breastfeeding falsely appeared to be weaker and inconsistent when relying on data from 24-hour recall, compared to since-birth recall.
""The development, implementation and improvement of health policies require appropriate indicators to evaluate factors such as the prevalence of breastfeeding in a country or region, who needs explicit support, whether the support is effective and whether breastfeeding rates are improving,"" explained Nanishi. ""While the 24-hour recall method has been widely used (for example, in The State of the World's Children report by UNICEF), we have found that using it risks misleading policymakers.""
Based on these results, Nanishi suggests that to improve breastfeeding rates, more supportive environments and policies are needed. ""Medical professionals tend to unconsciously use the 24-hour recall method in their practice. They tend to ask their clients, 'Are you currently breastfeeding your infant?' and then try to find the cause of the failure of breastfeeding in the mother. Instead, they must ask themselves, 'When this mother gave birth, did we provide her with appropriate care for breastfeeding?'"" said Nanishi.
""I would like the general public, especially mothers, to know that successful breastfeeding is not their sole responsibility, and that proper hospital care and appropriate health policies are very important,"" Nanishi explained. ""Mothers tend to blame themselves when breastfeeding does not work. But instead of blaming themselves, they have the right to ask for more appropriate policies and support. I hope for a healthy and sustainable society, and I believe breastfeeding support is essential for that.""

","score: 14.059287684976173, grade_level: '14'","score: 16.20642463004765, grade_levels: ['college_graduate'], ages: [24, 100]",10.1136/bmjgh-2023-013737,"WHO recommends exclusive breast feeding from birth to 6 months. However, to monitor populations, it recommends using the proportion of infants under 6 months who were exclusively breastfed during the previous 24 hours. To assess the usefulness of 24-hour recall, we (1) compared the prevalence of exclusive breast feeding measured by since-birth recall to the prevalence measured by 24-hour recall and (2) quantified each indicator’s association with WHO-recommended, well-established methods for in-hospital breastfeeding support. We conducted two online surveys of mothers in Japan (total n=4247) who had a healthy singleton delivery in the previous 25 months. They reported on their breast feeding (a) from birth to 5 months; or (b) during the previous 24 hours, for those with infants under 5 months; or (c) both, for those who participated in the initial survey and also in the follow-up survey. All mothers also reported on their in-hospital support. The strength of each indicator’s association with provision of in-hospital support was quantified as the area under the curve (AUC). The prevalences of exclusive breast feeding by since-birth recall were 4.4% (first survey) and 2.5% (second survey). By 24-hour recall, the prevalence appeared to be 29.8%. More in-hospital support was moderately well associated with more exclusive breast feeding measured by since-birth recall: AUC 0.72 (95%CI 0.66 to 0.78). That association is consistent with the known benefits of in-hospital support. In contrast, when exclusive breast feeding was measured by 24-hour recall, its association with in-hospital support appeared to be extremely weak: AUC 0.59 (95% CI 0.54 to 0.65). Using 24-hour recall substantially overestimates the prevalence of exclusive breast feeding since birth, and it conceals the benefits of in-hospital breastfeeding support. To monitor population achievement of exclusive breast feeding for the first 6 months, or to evaluate breastfeeding interventions, 24-hour recall of exclusive breast feeding should not be used alone."
"
Racial discrimination and bias are painful realities and increasingly recognized as detrimental to the health of adults and children.

These stressful experiences also appear to be transmitted from mother to child during pregnancy, altering the strength of infants' brain circuits, according to a new study from researchers at Columbia, Yale, and Children's Hospital of Los Angeles.
The study found similar brain changes in infants whose mothers experienced stress from adapting to a new culture during pregnancy.
""A leading hypothesis would be that the connectivity changes that we see could reduce one's ability to regulate their emotions and increase risk for mental health disorders,"" says the study's lead author Marisa Spann, PhD, the Herbert Irving Associate Professor of Medical Psychology in the Department of Psychiatry at Columbia University Vagelos College of Physicians and Surgeons.
""It remains to be seen if the connectivity differences we found lead to long-term mental health outcomes in children. Our team and others in the field still have the opportunity to test this.""
Previous research by Spann and colleagues has documented the impact of various forms of prenatal distress -- depression, stress, and anxiety -- on the infant brain. ""We work with vulnerable and underrepresented populations, and the experience of stigma and discrimination are distressingly common,"" Spann says. ""This naturally led to discussions about the impact of other stressors, like discrimination and acculturation, on the infant brain.""
In the new study, the researchers analyzed data collected from 165 young, mostly Hispanic women who had participated in an earlier study of teen pregnancy, stress, and nutrition by co-authors Catherine Monk, PhD, and Bradley Peterson, MD. The data included self-reported measures of discrimination and acculturation, along with measures of general stress, childhood trauma, depression, and socioeconomic status.

An analysis of the data showed that stress from discrimination and acculturation were separate and distinct from other types of stress and might have unique effects on the brain.
To look for these unique effects, the researchers compared the mothers' discrimination and acculturation stress to the strength of their infants' brain circuits, as measured with MRI scans. This analysis of 38 mother-infant pairs showed that infants of mothers who experienced discrimination generally had weaker connections between their amygdala and prefrontal cortex and infants of mothers who experienced acculturation stress had stronger connectivity between the amygdala and another brain region called the fusiform.
The amygdala is an area of the brain associated with emotional processing that is altered in many mood disorders. It also may be involved in ethnic and racial processing, such as differentiating faces.
""The amygdala is very sensitive to other types of prenatal stress,"" Spann says, ""and our new findings suggest that the experience of discrimination and acculturation also influences amygdala circuitry, potentially across generations.""
The take-home message, Spann says, is that ""how we treat and interact with people matters, especially during pregnancy -- a critical time point where we can see the far-reaching effects on children.""
Spann adds that more research is needed to investigate the biological mechanisms that carry the experiences of adversity from parent to offspring as well as the long-term impact of these findings. She currently is leading a study -- funded by the Community-Based Participatory Research program of Columbia's Irving Institute for Clinical and Translational Research and in collaboration with the Northern Manhattan Perinatal Partnership -- to examine the relationship between maternal experiences of discrimination and acculturative stress on the development of their infant's racial processing.

The new research was supported by the National Institute of Mental Health (grants K24MH127381, R01MH126133, and R01MH117983); the National Center for Advancing Translational Sciences (TL1TR001875); the National Health and Lung and Blood Disease Institute (R25HL096260); the BEST-DP: Biostatistics & Epidemiology Summer Training Diversity Program; Eunice Kennedy Shriver National Institute for Child Health and Human Development (K23HD092589); and an Irving Scholar Award from the Irving Institute for Clinical and Translational Research at Columbia University.
Catherine Monk and Bradley Peterson provided data from a previous study, which was supported by a grant from the National Institute of Mental Health (R01MH093677).
Catherine Monk, PhD, is the Diana Vagelos Professor of Women's Mental Health in the Department of Obstetrics & Gynecology at Columbia University Vagelos College of Physicians and Surgeons and leads the department's Center for the Transition to Parenthood. She also is professor of medical psychology in the Department of Psychiatry.
The authors declare no competing interests.

","score: 17.626004347826086, grade_level: '18'","score: 19.975733695652174, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41386-023-01765-3,"The experience of ethnic, racial, and structural inequalities is increasingly recognized as detrimental to health, and early studies suggest that its experience in pregnant mothers may affect the developing fetus. We characterized discrimination and acculturation experiences in a predominantly Hispanic sample of pregnant adolescent women and assessed their association with functional connectivity in their neonate’s brain. We collected self-report measures of acculturation, discrimination, maternal distress (i.e., perceived stress, childhood trauma, and depressive symptoms), and socioeconomic status in 165 women. Then, we performed a data-driven clustering of acculturation, discrimination, perceived stress, depressive symptoms, trauma, and socioeconomic status variables during pregnancy to determine whether discrimination or acculturation clustered into distinct factors. Discrimination and acculturation styles loaded onto different factors from perceived stress, depressive symptoms, trauma, and socioeconomic status, suggesting that they were distinct from other factors in our sample. We associated these data-driven maternal phenotypes (discrimination and acculturation styles) with measures of resting-state functional MRI connectivity of the infant amygdala (n = 38). Higher maternal report of assimilation was associated with weaker connectivity between their neonate’s amygdala and bilateral fusiform gyrus. Maternal experience of discrimination was associated with weaker connectivity between the amygdala and prefrontal cortex and stronger connectivity between the amygdala and fusiform of their neonate. Cautiously, the results may suggest a similarity to self-contained studies with adults, noting that the experience of discrimination and acculturation may influence amygdala circuitry across generations. Further prospective studies are essential that consider a more diverse population of minoritized individuals and with a comprehensive assessment of ethnic, racial, and structural factors."
"
While many of the increased risks are small, because around seven percent of babies in the UK are born moderately preterm each year, they could have significant consequences at population level, the authors of the National Institute of Health and Care Research (NIHR) funded study say.

For the study, researchers at the Universities of York, Leeds and Leicester examined data from more than 75 studies from around the world, which involved a total of over eight million children.
Compared with children born at full term, the study identified an increased risk of most developmental disorders. Whilst risks decreased with each week of gestation, there was still evidence of a small increase in risk of several developmental disorders such as cerebral palsy, developmental delay, and cognitive impairment, even when children were born ""early term,"" between 37-38 weeks.
One of the most common disorders was language delay which affected 222 per 1000 children born between 32-36 weeks, compared with 47 per 1000 for full term children. Many children face low educational attainment during the primary school years, affecting 300 per 1000 children born moderately preterm, compared to 160 per 1000 children born at full term.
While the risk of cerebral palsy is relatively low for all children, the results of the study suggest it is 14 times higher for infants born at 32 to 33 weeks compared with children born at full term.
The review also found that difficulties faced by children born at 32 to 38 weeks persist through childhood, with evidence of increased risk and prevalence of cognitive impairment and low educational achievement persisting into the high school years.
Lead author of the study, Dr Katherine Pettinger from the Department of Health Sciences at the University of York, said: ""It is important to remember that whilst our study shows an increase in risk for children born moderately early relative to their peers born at full term, many children will not experience any developmental problems.

""The reasons behind our findings are not yet clear, but babies born just a few weeks early have different brain maturation to full term children, and it is possible that birth between 32 and 38 weeks' gestation may disrupt evolution of neural connections, potentially contributing to developmental disorder.
""Many babies that are born moderately preterm are delivered early for very good reasons, for example when the mother has a health condition such as preeclampsia. However, understanding the long-term implications of birth before full term may influence obstetric decision making in some cases. It is also vital that all healthcare professionals, and particularly paediatricians, are well informed of the potential consequences of preterm birth so that they can give evidence based information to families and so opportunities for early intervention are not missed. ""
According to current guidelines from the National Institute for Health and Care Excellence (NICE) children should be monitored up until the age of two if they were born before the age of 30 weeks.
The researchers do not recommend that all children born between 32 and 38 weeks' gestation should also receive multiple routine health appointments as many will not show any signs of developmental disorders and this would place significant strain on NHS services.
However, the researchers are calling for more communication between schools, parents and health professionals and better support for teachers.
Dr Pettinger added: ""The data tells us the effects of being just a few weeks early are still there at primary school age. It therefore makes sense for teachers to be informed if they have students who are born preterm and early term and receive training on how to support them.
""Further research is now needed to look at large scale population studies to explore how incidents of developmental disorders relate to gestational age and see if the patterns we observed in the present study are replicated. We also want to look at whether children are commonly affected by more than one disorder, as understanding which conditions are likely to co-occur can help to produce more tailored interventions for children.""

","score: 16.89656701030928, grade_level: '17'","score: 19.6429970544919, grade_levels: ['college_graduate'], ages: [24, 100]",10.1542/peds.2023-061878,"Very preterm birth (&lt;32 weeks) is associated with increased risk of developmental disorders. Emerging evidence suggests children born 32 to 38 weeks might also be at risk. To determine the relative risk and prevalence of being diagnosed with, or screening positive for, developmental disorders in children born moderately preterm, late preterm, and early term compared with term (≥37 weeks) or full term (39–40/41 weeks). Medline, Embase, Psychinfo, Cumulative Index of Nursing, and Allied Health Literature. Reported ≥1 developmental disorder, provided estimates for children born 32 to 38 weeks. A single reviewer extracted data; a 20% sample was second checked. Data were pooled using random-effects meta-analyses. Seventy six studies were included. Compared with term born children, there was increased risk of most developmental disorders, particularly in the moderately preterm group, but also in late preterm and early term groups: the relative risk of cerebral palsy was, for 32 to 33 weeks: 14.1 (95% confidence intervals [CI]: 12.3–16.0), 34 to 36 weeks: 3.52 (95% CI: 3.16–3.92) and 37 to 38 weeks: 1.44 (95% CI: 1.32–1.58). Studies assessed children at different ages using varied criteria. The majority were from economically developed countries. All were published in English. Data were variably sparse; subgroup comparisons were sometimes based on single studies. Children born moderately preterm are at increased risk of being diagnosed with or screening positive for developmental disorders compared with term born children. This association is also demonstrated in late preterm and early term groups but effect sizes are smaller."
"
Once an entrepreneur always an entrepreneur? Not necessarily, says a new study by researchers at the University of Central Florida and Purdue University. Former entrepreneurs can transition from being their own boss into successful employees within an organization, especially in roles that harness their entrepreneurial spirit, according to a recent study published in Personnel Psychology. 

""With today's career paths typically spanning multiple roles across a variety of organizations, understanding the transition between someone's old work self and new work self may be critical to not only the employee's success but also the company's,"" says Jeff Gish, assistant professor of management and entrepreneurship in UCF's College of Business and the study's co-author.
Gish and co-author Jordan Nielsen, an assistant professor of management organizational behavior/human resources at Purdue, examined the identity conflict levels of former entrepreneurs who went on to work for an organization.
Research has shown that former entrepreneurs frequently experience a ""founder penalty"" when applying for jobs, losing out to applicants who have never been self-employed. Employers assume former entrepreneurs may be more difficult to manage or will jump ship to start another company and be their own boss again. This new research suggests that this need not be the case for all jobs or for all ex-entrepreneurs.
They surveyed ex-entrepreneurs about their current work identity and whether they felt they could act like an entrepreneur in their current work role or if they had to suppress their entrepreneurial spirit. They also surveyed the ex-entrepreneurs' romantic partners about whether the employee spoke highly of their current organization, engaging in boosterism or experienced burnout in the role.
Gish and Nielsen found that identity conflict between the old entrepreneurial self and the new employee self was associated with higher levels of burnout and lower levels of boosterism.
""Ex-entrepreneurs who felt a strong nostalgia for being their own boss tended to be the ones who were the most negatively affected, with the highest levels of burnout and lowest levels of boosterism,"" Nielsen says. ""To mitigate this, organizations could use interview questions to help identify those who may be more likely to suffer negative consequences or develop positions and onboarding practices that minimize this source of conflict and lay a stronger foundation for success.""

","score: 17.223481781376524, grade_level: '17'","score: 18.941647773279357, grade_levels: ['college_graduate'], ages: [24, 100]",10.1111/peps.12626,"People undergoing career transitions often bring aspects of old roles into their new work contexts, and this interface can create conflict between lingering aspects of one's work self and the newer aspects of one's work self. Yet, we know little about how this conflict between old and new selves shapes employee outcomes. We examine this issue among ex‐entrepreneurs—individuals who have transitioned from a business owner to a wage employee. Drawing from role identity theory, we develop a model of the consequences of conflict between a lingering entrepreneur identity and a current work role identity. We propose that ex‐entrepreneurs who experience higher levels of identity conflict will be more likely to experience burnout and less likely to engage in boosterism of their employer, and that these relationships are explained by lower levels of perceived professional identity growth (i.e., progressive identity). We further suggest that the negative effect of conflict on progressive identity is exacerbated by nostalgia for one's entrepreneurial past. In a three‐stage field survey of ex‐entrepreneurs and their romantic partners, we found support for these hypotheses using both partner‐rated outcomes and self‐rated outcomes. We discuss implications for the literature on entrepreneurship careers and work identity in organizations."
"
Post-surgery pain relief has shifted away from opioid-containing medications over the past seven years, but the downward trend has slowed since 2020, a new study shows.

Overall, the rate of surgery-related opioid prescriptions dropped by 36% from 2016 to the end of 2022, and the average amount of opioids in those prescriptions dropped by 46%, the study of pharmacy data finds.
That combination of declines means that the total amount of opioids dispensed to surgical patients in late 2022 was 66% lower compared with early 2016, according to the findings published in JAMA Network Open by a team from the University of Michigan.
But the rate of decline was much faster before the pandemic, the researchers report after comparing surgical opioid patterns before and after 2020. That's even after they took into account the unusual circumstances of spring 2020, when most elective surgery temporarily stopped to free up hospital capacity for COVID-19 patients and reduce unnecessary exposure to the SARS-CoV-2 virus.
Even with the overall declines, American surgery patients in late 2022 still received the equivalent of 44 5-milligram pills of hydrocodone from pharmacies after their operations on average. That's far higher than what patients need for most procedures.
""These data suggest surgical teams have substantially reduced opioid prescribing, but also suggest that efforts to right-size opioid prescriptions after surgery must continue,"" said Kao-Ping Chua, M.D., Ph.D., the senior author of the new study and an assistant professor of pediatrics at U-M. He worked with first author and former U-M research assistant Jason Zhang, who is now in medical school at Northwestern University.
The researchers also find that some types of surgeons have reduced the amount of opioids dispensed to patients more than others. For instance, reductions were particularly large in cardiothoracic surgery and ophthalmology.

Orthopedic surgeons still account for more than half of all surgical opioids dispensed to American patients, even as the rate and size of prescriptions filled by their patients dropped.
Right-sizing prescribing
The authors note that surgeons should not strive to eliminate opioid prescribing altogether.
""The goal should be to ensure that opioids are only prescribed when necessary, and that the amount of opioids prescribed matches the amount that patients need,"" said Zhang. ""Achieving these goals could help reduce the risk of opioid misuse, persistent opioid use, and diversion of pills to other people besides the patient.""
The potential for accidental exposure to opioids by others in the household, and interactions between opioids and other substances including alcohol and prescription drugs, are other reasons to focus on non-opioid surgical pain care.
Chua and colleagues have studied procedure-related opioid prescribing multiple times, including a recent study showing that the reduction in the rate of dental opioid prescribing has similarly slowed in recent years.

They have worked with the Michigan Opioid Prescribing Engagement Network (OPEN) to develop prescribing guidelines for adult and pediatric surgical care available at https://michigan-open.org/prescribing-recommendations
Surgical organizations and the Centers for Disease Control and Prevention have advised surgeons to rely less on opioid-based acute pain relief for their patients since the mid-2010s. But no studies have examined surgical opioid prescribing trends using pandemic-era data.
The new study is based on data from a company called IQVIA that tracks prescriptions dispensed at 92% of U.S. pharmacies.
In addition to Chua and Zhang, the study's authors include OPEN co-directors Jennifer Waljee, M.D., M.P.H., M.S. and Chad Brummett, M.D. All except Zhang are members of the U-M Institute for Healthcare Policy and Innovation, and Brummett co-directs the U-M Opioid Research Institute.
Chua is a member of the Susan B. Meister Child Health Evaluation and Research Center in the Department of Pediatrics, which also provided some of the funding for the study.
The study reported in this press release was funded by the National Institute on Drug Abuse of the National Institutes of Health (DA057284, DA056438, DA048110), the Benter Foundation and the Michigan Department of Health and Human Services. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.

","score: 15.63310920627853, grade_level: '16'","score: 17.07843259490148, grade_levels: ['college_graduate'], ages: [24, 100]",10.1001/jamanetworkopen.2023.46426,This cross-sectional study investigates the rate and dosing of opioid prescriptions among US surgeons from 2016 to 2022.
"
Comparing PET scans of more than 90 adults with and without mild cognitive impairment (MCI), Johns Hopkins Medicine researchers say relatively lower levels of the so-called ""happiness"" chemical, serotonin, in parts of the brain of those with MCI may play a role in memory problems including Alzheimer's disease.

The findings, first published online Sept. 13 in the Journal of Alzheimer's Disease, lend support to growing evidence that measurable changes in the brain happen in people with mild memory problems long before an Alzheimer's diagnosis, and may offer novel targets for treatments to slow or stop disease progression.
""The study shows that people with mild cognitive impairment already display loss of the serotonin transporter. This measure that reflects serotonin degeneration is associated with problems with memory, even when we take into account in our statistical model MRI measures of neurodegeneration and PET measures of the amyloid protein that are associated with Alzheimer's Disease,"" says Gwenn Smith, Ph.D., professor of psychiatry and behavioral sciences at the Johns Hopkins University School of Medicine.
MCI describes the diagnostic stage between normal brain function in aging and Alzheimer's Disease (AD). Symptoms of MCI include frequent forgetfulness of recent events, word finding difficulty, and loss of the sense of smell. Those with MCI may stay in this stage indefinitely, or progress to more severe forms of cognitive deficits, giving urgency to the search for predictive markers, and possible early prevention interventions, investigators say.
The investigators cautioned that their study showed a correlation between lower serotonin transporter levels and memory problems in MCI, and was not designed to show causation or the role of serotonin in the progression from MCI to AD. To answer these questions, further research is needed to study over time healthy controls and individuals with MCI to demonstrate the role of serotonin in disease progression.
For the study, the Hopkins scientists recruited 49 volunteers with MCI, and 45 healthy adults ages 55 and older who underwent an MRI to measure changes in brain structure and two positron emission tomography (PET) scans of their brains at Johns Hopkins between 2009 and 2022. The research team used PET scans to look specifically at the serotonin transporter -- a neurotransmitter, or brain chemical long associated with positive mood, appetite and sleep -- and to look at the amyloid-beta protein (Aβ) distribution in the brain. Aβ is thought to play a central role in the pathology of AD. Studies in mice done at Johns Hopkins have shown that serotonin degeneration occurs before the development of widespread beta-amyloid deposits in the brain. Loss of serotonin is often associated with depression, anxiety, and psychological disorders.
Researchers found that MCI patients had lower levels of the serotonin transporter, and higher levels of Aβ than healthy controls. The MCI patients had up to 25% lower serotonin transporter levels in cortical and limbic regions than healthy controls. In particular, they report, lower serotonin transporter levels were found in cortical, limbic, and subcortical regions of the brains in those with MCI, areas specifically responsible for executive function, emotion, and memory.

""The correlation we observed between lower serotonin transporters and memory problems in MCI is important because we may have identified a brain chemical that we can safely target that may improve cognitive deficits and, potentially, depressive symptoms,"" says Smith. ""If we can show that serotonin loss over time is directly involved in the transition from MCI to AD, recently developed antidepressant medications may be an effective way to improve memory deficits and depressive symptoms and thus, may be a powerful way forward to slow disease progression.""
Researchers say future studies include longitudinal follow up of individuals with MCI to compare serotonin degeneration to the increase in and Aβ levels, as well as the increase in levels of the Tau protein that is also associated with AD compared to healthy adults. They are also studying multi-modal antidepressant drugs to treat depression and memory deficits in hopes of mitigating and halting symptoms.
Other scientists at the Johns Hopkins University School of Medicine and Johns Hopkins Bloomberg School of Public Health who contributed to this research are Jennifer Coughlin, Robert Dannals, Neda Gould, Daniel Holt, Vidya Kamath, Michael Kraut, Hiroto Kuwabara, Jeannie Leoutsakos, Martin Lodge, Ayon Nandi, Najlla Nassery, Martin Pomper, Alena Savonenko, Haijuan Yan and Mark Yoon.
All authors have no conflicts to disclose.
This research was partly supported by the National Institutes of Health.

","score: 17.29354929895975, grade_level: '17'","score: 19.238020126639526, grade_levels: ['college_graduate'], ages: [24, 100]",10.3233/JAD-230570,"Background: Neuropathological and neuroimaging studies have demonstrated degeneration of the serotonin system in Alzheimer’s disease (AD). Neuroimaging studies have extended these observations to the preclinical stages of AD, mild cognitive impairment (MCI). Serotonin degeneration has been observed also in transgenic amyloid mouse models, prior to widespread cortical distribution of amyloid-β (Aβ). Objective: The present study evaluated the regional distribution of the serotonin transporter (5-HTT) and of Aβ in individuals with MCI and healthy older controls, as well as the contribution of 5-HTT and Aβ to cognitive deficits. Methods: Forty-nine MCI participants and 45 healthy older controls underwent positron emission tomography (PET) imaging of 5-HTT and Aβ, structural magnetic resonance imaging and neuropsychological assessments. Results: Lower cortical, striatal, and limbic 5-HTT and higher cortical Aβ was observed in MCIs relative to healthy controls. Lower 5-HTT, mainly in limbic regions, was correlated with greater deficits in auditory-verbal and visual-spatial memory and semantic, not phonemic fluency. Higher cortical A β was associated with greater deficits in auditory-verbal and visual-spatial memory and in semantic, not phonemic fluency. When modeling the association between cognition, gray matter volumes and Aβ, inclusion of 5-HTT in limbic and in select cortical regions significantly improved model fit for auditory-verbal and visual-spatial memory and semantic, but not phonemic fluency. Conclusions: These results support the role of serotonin degeneration in the memory and semantic fluency deficits observed in MCI."
"
An international team of researchers including experts at the Indiana University School of Medicine has identified a protein found in the brains of people with frontotemporal dementia (FTD), discovering a new target for potential treatments for the disease.

According to the National Institutes of Health, FTD results from damage to neurons in the frontal and temporal lobes of the brain. People with this type of dementia typically present symptoms, including unusual behaviors, emotional problems, trouble communicating, difficulty with work or in some cases difficulty with walking, between the ages of 25 and 65.
Neurodegenerative disorders, including dementias and Amyotrophic Lateral Sclerosis (ALS), occur when specific proteins form amyloid filaments in the nerve cells of the brain and spinal cord. The multidisciplinary team of researchers -- including members from the Medical Research Council (MRC) Laboratory of Molecular Biology, the IU School of Medicine and the University College London Queen Square Institute of Neurology -- found that in cases of FTD, a protein called TAF15 forms these amyloid filaments in the cells of the brain and the spinal cord. On December 6, they published their findings in Nature.
Bernardino Ghetti, MD is a Distinguished Professor at the IU School of Medicine and has been studying neurodegenerative dementias for 50 years. As a lead neuropathologist on the project, Ghetti and his team studied the protein aggregates from brains donated by four people who had frontotemporal dementia and motor weakness. Together with their colleagues in the UK, IU researchers used neuropathologic and molecular techniques and cutting-edge cryo-electron microscopy (cryo-EM) at atomic resolution to discover the presence of the amyloid filaments made of TAF15 protein in multiple brain areas. However it is important to note that TAF15 amyloid affects also nerve cells of the motor system.
""This discovery represents an important breakthrough that recognizes TAF15 as a potential target for the development of diagnostic and therapeutic strategies toward a lesser-known form of frontotemporal lobar degeneration associated with frontotemporal dementia,"" Ghetti said.
Additional authors on the study are the MRC Laboratory of Molecular Biology's Stephan Tetter, Diana Arseni, Alexey G. Murzin, Sew Y. Peak-Chew and Benjamin Ryskeldi-Falcon; the University College London's Yazead Buhidma and Tammaryn Lashley; and the IU School of Medicine's Holly J. Garringer, Kathy L. Newell, Ruben Vidal and Liana G. Apostolova.
The study was in part funded by the NIH's National Institute on Aging and National Instiute of Neurological Disorders and Stroke.

","score: 18.794466501240695, grade_level: '19'","score: 20.074342431761785, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41586-023-06801-2,"Frontotemporal lobar degeneration (FTLD) causes frontotemporal dementia (FTD), the most common form of dementia after Alzheimer’s disease, and is often also associated with motor disorders1. The pathological hallmarks of FTLD are neuronal inclusions of specific, abnormally assembled proteins2. In the majority of cases the inclusions contain amyloid filament assemblies of TAR DNA-binding protein 43 (TDP-43) or tau, with distinct filament structures characterizing different FTLD subtypes3,4. The presence of amyloid filaments and their identities and structures in the remaining approximately 10% of FTLD cases are unknown but are widely believed to be composed of the protein fused in sarcoma (FUS, also known as translocated in liposarcoma). As such, these cases are commonly referred to as FTLD–FUS. Here we used cryogenic electron microscopy (cryo-EM) to determine the structures of amyloid filaments extracted from the prefrontal and temporal cortices of four individuals with FTLD–FUS. Surprisingly, we found abundant amyloid filaments of the FUS homologue TATA-binding protein-associated factor 15 (TAF15, also known as TATA-binding protein-associated factor 2N) rather than of FUS itself. The filament fold is formed from residues 7–99 in the low-complexity domain (LCD) of TAF15 and was identical between individuals. Furthermore, we found TAF15 filaments with the same fold in the motor cortex and brainstem of two of the individuals, both showing upper and lower motor neuron pathology. The formation of TAF15 amyloid filaments with a characteristic fold in FTLD establishes TAF15 proteinopathy in neurodegenerative disease. The structure of TAF15 amyloid filaments provides a basis for the development of model systems of neurodegenerative disease, as well as for the design of diagnostic and therapeutic tools targeting TAF15 proteinopathy."
"
The lab of Yongchao C. Ma, PhD, at Stanley Manne Children's Research Institute at Ann & Robert H. Lurie Children's Hospital of Chicago uncovered a novel mechanism that leads to motor neuron degeneration in spinal muscular atrophy (SMA). This discovery offers a new target for treatment that overcomes important limitations of gene therapy and other current therapies for SMA.

SMA is a genetic disease that disrupts the nerve cells that control voluntary muscle movement. Symptoms of motor neuron degeneration could start at as early as 3 months of age and lead to muscle atrophy, paralysis and death, often before the child's second birthday. Gene therapy has revolutionized SMA treatment, but it only works for a subgroup of patients and it can be too toxic.
Dr. Ma and team found that in SMA, increased activity of a type of enzyme called cyclin-dependent kinase 5 (Cdk5) causes defective function of mitochondria, which is a powerhouse of the cell and serves as a signaling center for many cell processes. In SMA, the mitochondrial dysfunction contributes to cell death or degeneration of motor neurons, and this occurs before symptoms develop.
The researchers also demonstrated in mouse models and human induced pluripotent stem cell (iPSC) models of SMA that the mitochondrial dysfunction and motor neuron degeneration can be stopped by a Cdk5 inhibitor. After reducing Cdk5 activity, the mice showed significant improvement in SMA symptoms. Findings were published in the Proceedings of the National Academy of Sciences (PNAS).
""We are excited to offer promise of a brand new treatment for children with SMA,"" said senior author Dr. Ma, who holds the Children's Research Fund Endowed Professorship in Neurobiology at Lurie Children's and is Associate Professor of Pediatrics, Neurology, and Neuroscience at Northwestern University Feinberg School of Medicine. ""In our previous research, we established that all patients with SMA have the mitochondrial defect. This means that inhibiting Cdk5 could treat all patients, including children whose SMA subtype makes them ineligible for gene therapy. This new approach also could potentially be used in combination with gene therapy. The currently available Cdk5 inhibitor is too toxic, so we want to develop a better inhibitor that is safer and more effective.""
Once a better inhibitor is developed, treatment could start as soon as SMA is diagnosed through newborn screening, before symptoms appear, explains Dr. Ma.
The study was supported by National Institutes of Health grants R01NS094564, R21NS106307, and RF1AG077451, as well as grants from the Hartwell Foundation, Cure SMA, and the Agape Foundation.
Research at Ann & Robert H. Lurie Children's Hospital of Chicago is conducted through Stanley Manne Children's Research Institute, which is focused on improving child health, transforming pediatric medicine and ensuring healthier futures through the relentless pursuit of knowledge. Lurie Children's is a nonprofit organization committed to providing access to exceptional care for every child. It is ranked as one of the nation's top children's hospitals by U.S. News & World Report. Lurie Children's is the pediatric training ground for Northwestern University Feinberg School of Medicine.

","score: 14.205193012116094, grade_level: '14'","score: 15.584919695688924, grade_levels: ['college_graduate'], ages: [24, 100]",10.1073/pnas.2300308120,"Spinal muscular atrophy (SMA), the top genetic cause of infant mortality, is characterized by motor neuron degeneration. Mechanisms underlying SMA pathogenesis remain largely unknown. Here, we report that the activity of cyclin-dependent kinase 5 (Cdk5) and the conversion of its activating subunit p35 to the more potent activator p25 are significantly up-regulated in mouse models and human induced pluripotent stem cell (iPSC) models of SMA. The increase of Cdk5 activity occurs before the onset of SMA phenotypes, suggesting that it may be an initiator of the disease. Importantly, aberrant Cdk5 activation causes mitochondrial defects and motor neuron degeneration, as the genetic knockout of p35 in an SMA mouse model rescues mitochondrial transport and fragmentation defects, and alleviates SMA phenotypes including motor neuron hyperexcitability, loss of excitatory synapses, neuromuscular junction denervation, and motor neuron degeneration. Inhibition of the Cdk5 signaling pathway reduces the degeneration of motor neurons derived from SMA mice and human SMA iPSCs. Altogether, our studies reveal a critical role for the aberrant activation of Cdk5 in SMA pathogenesis and suggest a potential target for therapeutic intervention."
"
Hydrogen sulfide, recognized by its characteristic rotten egg smell, is synthesized in the respiratory center -- an integral brain region governing respiration. Researchers at the University of Tsukuba have identified that hydrogen sulfide within the respiratory center plays a crucial role in maintaining the rhythm and depth of respiration by modulating neurotransmissions.

While commonly associated with the unpleasant odor of hot springs, hydrogen sulfide is naturally produced in the body. Despite its toxicity at higher concentrations, the lower concentrations generated internally are indispensable for life. Researchers from the University of Tsukuba have demonstrated the importance of hydrogen sulfide in the brain for normal respiration although the precise mechanism remained unclear.
The medullary respiratory center, responsible for the rhythm and depth of respiration, comprises various neurons dedicated for inspiration and expiration. In this study, researchers focused on the hydrogen sulfide production within the respiratory center. Results revealed that inhibiting hydrogen sulfide production alters neurotransmissions, leading to disruptions in the rhythm and depth of respiration.
Moreover, the study identified variations in this mechanism across distinct regions within the respiratory center. These results imply that hydrogen sulfide, produced in the respiratory center, exerts a modulating influence on neural circuits, contributing to the stability of respiration.
Understanding the role of hydrogen sulfide in respiration offers valuable insights into disorders characterized by respiratory irregularities and potential avenues for treatment. Furthermore, these findings deepen our understanding of how hydrogen sulfide sustains life.
This research was supported by Japan Society for the Promotion of Science Kakenhi grants (22H05557 and 23KJ0245), the Japan Science and Technology Agency SPRING (JPMJSP2124), and the Japan Foundation for Applied Enzymology Research Grant.

","score: 17.519344729344727, grade_level: '18'","score: 18.610170940170946, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41598-023-47280-9,"Hydrogen sulfide (H2S), which is synthesized in the brain, modulates the neural network. Recently, the importance of H2S in respiratory central pattern generation has been recognized, yet the function of H2S in the medullary respiratory network remains poorly understood. Here, to evaluate the functional roles of H2S in the medullary respiratory network, the Bötzinger complex (BötC), the pre-Bötzinger complex (preBötC), and the rostral ventral respiratory group (rVRG), we observed the effects of inhibition of H2S synthesis at each region on the respiratory pattern by using an in situ arterially perfused preparation of decerebrated male rats. After microinjection of an H2S synthase inhibitor, cystathionine β-synthase, into the BötC or preBötC, the amplitude of the inspiratory burst decreased and the respiratory frequency increased according to shorter expiration and inspiration, respectively. These alterations were abolished or attenuated in the presence of a blocker of excitatory synaptic transmission. On the other hand, after microinjection of the H2S synthase inhibitor into the rVRG, the amplitude of the inspiratory burst was attenuated, and the respiratory frequency decreased, which was the opposite effect to those obtained by blockade of inhibitory synaptic transmission at the rVRG. These results suggest that H2S synthesized in the BötC and preBötC functions to limit respiratory frequency by sustaining the respiratory phase and to maintain the power of inspiration. In contrast, H2S synthesized in the rVRG functions to promote respiratory frequency by modulating the interval of inspiration and to maintain the power of inspiration. The underlying mechanism might facilitate excitatory synaptic transmission and/or attenuate inhibitory synaptic transmission."
"
Aging becomes apparent in various ways, one of them being changes in memory function. But some older adults experience a faster decline in memory compared to others.

A new study by University of Arizona psychologists investigated the possible scenarios that could lead to waning memory in some older people. The researchers also studied both age-dependent and age-independent factors that could contribute to memory decline in younger and older people alike.
The study suggests that the hippocampus, a brain region associated with memory and navigation, could contribute to the difficulty in learning new environments and locations in some older adults. Neural representations in the hippocampus could explain why some people have a hard time remembering locations, said Li Zheng, a research scientist in the Department of Psychology and the lead author of the study.
""The study's findings will be helpful in predicting the level of memory decline in early stages of dementia,"" Zheng said.
Published today in the journal Proceedings of the National Academy of Sciences, the new study is designed based on a similar study conducted in rats by Carol Barnes, a Regents Professor of psychology, neurology and neuroscience.
Barnes' study investigated specialized cells in the hippocampus called ""place cells,"" or neurons that get triggered and fire when a person or animal enters a particular place. When an individual goes to another location, another place cell fires, helping the brain's hippocampus build a mental representation of each space.
When an animal or human enters a new environment, the place cells undergo a process called ""remapping."" The study observed that the older rats showed difficulties in remapping for different environments more so than younger rats, indicating an inferior spatial memory performance.

Building on Barnes' study, Zheng and her team recruited 25 younger adults and 22 older adults, all of them healthy. The participants were instructed to take part in a virtual reality experiment. On a computer screen, the younger and older adults memorized the layouts and locations of six shops in two virtual cities.
The participants were asked to complete a series of questions to test their spatial memory, while the researchers simultaneously scanned the participants' brain using a functional magnetic resonance imaging scanner. The scanner captured the neural signals in the hippocampus.
Researchers found, in line with Barnes' study, that older adults on average showed neural representations that did not differentiate well between environments when compared to younger adults.
However, the study found that there is an age-independent factor that affects memory retention. Suggesting that distinct neurons in the hippocampus serve different functions, Zheng explained that, for instance, one neuron might respond to the shape of an environment, while another responds to the ground color or other features. These neurons collaborate to create a comprehensive representation of the entire environment.
If a group of neurons takes up the same function, there's a risk that some of the features of the environment may not be accurately represented, meaning the fidelity of the neural signals gets compromised and is low, said Arne Ekstrom, a professor of cognition and neural systems at UArizona and a senior author on the paper. At this time, the reason for low-fidelity signals in younger and older adults is not clear, he said.
""Anyone with poor memory performance will show a lower-fidelity neural signal,"" Zheng said. ""Age doesn't have anything to do with that.""
The study also mentioned there is an age-dependent factor, which the researchers describe as the quality of neural signals coming from other parts of the brain into the hippocampus -- for example, visual information that comes through the back part of the brain. Even some high-performing older adults in the experiment exhibited a decrease in the quality of incoming neural signals into the hippocampus.

It has long been suspected that one age-related factor influencing memory could be the quality of the signal getting into a brain region, which may relate to changes in plasticity in the aging brain, Ekstrom said. The study's findings linked reductions in the quality of input into the hippocampus with age and worse spatial memory, he said.
Insights gained from the remapping index and fidelity of neural signals can be useful in predicting how much memory decline can occur in people diagnosed with dementia, Zheng said.
In the near future, the research team is planning to replicate the study with immersive virtual reality experiments, which Zheng said would use body-based cues and navigation to the target in a more naturalistic way.

","score: 14.654748387096777, grade_level: '15'","score: 15.315774193548386, grade_levels: ['college_graduate'], ages: [24, 100]",10.1073/pnas.2307884120,"Older adults show declines in spatial memory, although the extent of these alterations is not uniform across the healthy older population. Here, we investigate the stability of neural representations for the same and different spatial environments in a sample of younger and older adults using high-resolution functional MRI of the medial temporal lobes. Older adults showed, on average, lower neural pattern similarity for retrieving the same environment and more variable neural patterns compared to young adults. We also found a positive association between spatial distance discrimination and the distinctiveness of neural patterns between environments. Our analyses suggested that one source for this association was the extent of informational connectivity to CA1 from other subfields, which was dependent on age, while another source was the fidelity of signals within CA1 itself, which was independent of age. Together, our findings suggest both age-dependent and independent neural contributions to spatial memory performance."
"
Light therapy leads to significant improvements in sleep and psycho-behavioral symptoms for patients with Alzheimer's disease, according to a new study published this week in the open-access journal PLOS ONE by Qinghui Meng of Weifang Medical University, China, and colleagues.

The cognitive decline associated with Alzheimer's disease is often accompanied by sleep disturbances and psycho-behavioral symptoms including apathetic and depressive behavior, agitation and aggression. Photobiomodulation is a non-pharmacological therapy that uses light energy to stimulate the suprachiasmic nucleus (SCN), a sleep modulator in the brain. Despite light therapy receiving increased attention as a potential intervention for Alzheimer's, a systematic evaluation of its efficacy and safety has been unavailable.
In the new study, researchers searched multiple research databases to identify all randomized controlled trials related to light therapy intervention for Alzheimer's disease or dementia. Fifteen high-quality trials with available methods and relevant outcomes were selected for further analysis. The included trials were written in English, published between 2005 and 2022, and performed in seven countries. They included a combined 598 patients.
The meta-analysis of all fifteen trials found that light therapy significantly improved sleep efficiency, increased interdaily stability (a measure of the strength of circadian rhythms), and reduced intradaily variability (a measure of how frequently someone transitions between rest and activity during the day). In patients with Alzheimer's disease, light therapy also alleviated depression and reduced patient agitation and caregiver burden.
Given the limited sample sizes in studies included in this meta-analysis, the authors advocate for larger future studies, which could also explore if bright light exposure could cause any adverse behavior in patients. They conclude that light therapy is a promising treatment option for some symptoms of Alzheimer's disease.
The authors add: ""Light therapy improves sleep and psycho-behavioral symptoms in patients with Alzheimer's disease and has relatively few side effects, suggesting that it may be a promising treatment option for patients with Alzheimer's disease.""

","score: 17.58299902629017, grade_level: '18'","score: 19.32666260954236, grade_levels: ['college_graduate'], ages: [24, 100]",10.1371/journal.pone.0293977,"Although Alzheimer’s disease (AD) mainly affects cognitive function, it is often accompanied by sleep disorders and psychobehavioral symptoms. These symptoms, including depression, agitation, and psychotic symptoms, are prominent hospitalization causes among patients with AD. Currently, relatively more research exists on light therapy for sleep disorders, while those on psychobehavioral symptoms are gradually increasing. However, no consensus exists on these results because of the vulnerability of light therapy to multiple factors, including light intensity and duration. Thus, further research investigating this aspect is warranted. To evaluate the efficacy of light therapy in improving sleep disorders and psychobehavioural symptoms in patients with AD. In this meta-analysis, relevant literature was searched in Embase, the Clinical Trials Registry, Web of Science, PubMed, and the Cochrane Library up to December 2022. Furthermore, a fixed-effects model was used for data analysis. Fifteen randomized controlled trials involving 598 patients with AD were included. In the case of sleep disorders, our meta-analysis revealed that light therapy significantly improved sleep efficiency (MD = −2.42, 95% CI = −3.37 to −1.48, p < 0.00001), increased interdaily stability (MD = −0.04, 95% CI = −0.05 to −0.03, p < 0.00001), and reduced intradaily variability (MD = −0.07, 95% CI = −0.10 to −0.05, p < 0.00001). With respect to psychotic behavior, light therapy was found to alleviate depression (MD = −2.55, 95% CI = −2.98 to −2.12, p < 0.00001) as well as reduce agitation (MD = −3.97, 95% CI = −5.09 to −2.84, p < 0.00001) and caregiver burden (MD = −3.57, 95% CI = −5.28 to −1.87, p < 0.00001). Light therapy leads to significant improvement in sleep and psychobehavioral symptoms and is associated with relatively fewer side effects in patients with AD, indicating its potential as a promising treatment option for AD."
"
In a new study of more than 50,000 Korean adolescents, those who used a smartphone for more than 4 hours per day had higher rates of adverse mental health and substance use. Jin-Hwa Moon and Jong Ho Cha of Hanyang University Medical Center, Korea, and colleagues present these findings in the open-access journal PLOS ONE on December 6, 2023.

Prior research has shown that smartphone use among adolescents has increased in recent years, and that this usage may be associated with higher risk of adverse health -- such as psychiatric disorders, sleep issues, eye-related problems, and musculoskeletal disorders. However, growing evidence suggests that at least some daily internet usage may be associated with better physical and mental health for adolescents.
To deepen understanding of the relationship between adolescents' use of smartphones and health, Moon, Cha and colleagues analyzed data on more than 50,000 adolescent participants in the ongoing Korea Youth Risk Behavior Web-based Survey collected in 2017 and in 2020. The data included the approximate number of daily hours each participant spent on a smartphone as well as various health measures. The statistical analysis employed propensity score matching to help account for other factors that could be linked to health outcomes, such as age, sex, and socioeconomic status.
The researchers found that in 2020, the percentage of adolescents in the study who used a smartphone more than 2 hours per day was 85.7 percent -- up from 64.3 percent in 2017. Adolescents who used a smartphone for more than 4 hours per day had higher rates of stress, thoughts of suicide, and substance use than those with usage below 4 hours per day. However, adolescents that used a smartphone 1-2 hours per day encountered fewer problems than adolescents who did not use a smartphone at all.
The authors note that this study does not confirm a causal relationship between smartphone use and adverse health outcomes. Nonetheless, the findings could help inform usage guidelines for adolescents -- especially if daily usage continues to rise.
The authors add: ""This research shows the impact of using smart devices for more than 4 hours a day on adolescent health.""

","score: 13.891025641025646, grade_level: '14'","score: 16.28089308996089, grade_levels: ['college_graduate'], ages: [24, 100]",10.1371/journal.pone.0294553,"We aimed to investigate the association between smartphone use and adverse behavioral health outcomes using nationwide Korea Youth Risk Behavior Web-based Survey data for 2017 and 2020. The 2020 data (N = 54,809) were used to analyze the relationships between daily smartphone usage time (non-user, 0–2 h [hour], 2–4 h, 4–6 h, 6–8 h, and > 8 h), and adverse health outcomes (stress, sleep, depression, suicide, substance use, and smartphone overdependence). A 1:1 propensity score matching (PSM) was used to control for confounding variables. A total of 40,998 adolescents with < 4 h/day and > 4 h/day of usage were included. Adolescents’ mean smartphone usage time in 2020 increased compared to that in 2017 (weighted % of > 2 h/day; 64.3% vs. 85.7%). The curvilinear relationships between smartphone usage time and adverse health outcomes were prominent after > 4 h/day. Adolescents using smartphones 2–4 h/day showed no increased adverse health outcomes compared to non-users, except for smartphone overdependence. Using a smartphone > 4 h/day was significantly associated with stress perception (1.16; 1.11–1.22), suicidal ideation (1.22; 1.13–1.31), and substance use (alcohol, 1.66; 1.57–1.75) after PSM. Our study demonstrated the curvilinear relationship between smartphone usage time and adverse health outcomes in adolescents. Our findings can help establish smartphone usage guidelines for adolescents."
"
Most neurodegenerative diseases, including dementias, involve proteins aggregating into filaments called amyloids. In most of these diseases, researchers have identified the proteins that aggregate, allowing them to target these proteins for diagnostic tests and treatments.

But, in around 10% of cases of frontotemporal dementia, scientists had yet to identify the rogue protein. Now, scientists have pinpointed aggregated structures of the protein TAF15 in these cases.
Frontotemporal dementia results from the degeneration of the frontal and temporal lobes of the brain, which control emotions, personality and behaviour, as well speech and understanding of words. It tends to start at a younger age than Alzheimer's disease, often being diagnosed in people aged 45 to 65, although it can also affect younger or older people.
In a paper published today in the journal Nature, research led by scientists from the Medical Research Council (MRC) Laboratory of Molecular Biology, in Cambridge, UK, has identified aggregated structures of a protein that could provide a target for the future development of diagnostic tests and treatments.
Dr Benjamin Ryskeldi-Falcon, who led the study at the MRC Laboratory of Molecular Biology, said: ""This discovery transforms our understanding of the molecular basis of frontotemporal dementia. It is a rare finding of a new member of the small group of proteins known to aggregate in neurodegenerative disease.
""Now that we have identified the key protein and its structure, we can start to target it for the diagnosis and therapy of this type of frontotemporal dementia, similar to strategies already in the pipeline for targeting the aggregates of amyloid-beta and tau proteins that characterise Alzheimer's disease.""
The scientists used cutting-edge cryo-electron microscopy (cryo-EM) to study protein aggregates from the brains of four people who had this type of frontotemporal dementia at atomic resolution. The donated brains were identified by Tammaryn Lashley at the University College London Queen Square Institute of Neurology and Bernardino Ghetti at the Indiana University School of Medicine.

In this type of dementia, scientists had long thought that a protein called FUS aggregated, based on similarities with other neurodegenerative diseases.
Using cryo-EM, the researchers at the MRC Laboratory of Molecular Biology were able to identify that the protein aggregates from each brain had the same atomic structure. Surprisingly, the protein was not FUS -- it was another protein called TAF15.
Dr Stephan Tetter, also from the MRC Laboratory of Molecular Biology, who is first author on the paper, said: ""This is an unexpected result because, before this study, TAF15 was not known to form amyloid filaments in neurodegenerative diseases and no structures of the protein existed. Cryo-EM is transforming our understanding of the molecular pathology of dementia and neurodegenerative diseases more broadly by giving us insights that were beyond the capabilities of previous technologies.""
Dr Ryskeldi-Falcon added: ""The technical challenge of performing cryo-EM meant that we were only able to look at the brains of four individuals. However, now that we know the key protein and its structure, we have the potential to develop tools to screen for these abnormal protein aggregates in hundreds of patient samples to test how widespread they are.""
Frontotemporal dementia and motor neuron disease
Some people who have frontotemporal dementia also have motor neuron disease, a condition in which individuals progressively lose control over their muscles. In this study, two of the individuals who donated their brains had signs of both diseases. For these individuals, the researchers identified the same aggregated structure of TAF15 in brain regions associated with motor neuron disease.

Dr Ryskeldi-Falcon said: ""The presence of the same TAF15 aggregates in two individuals who had frontotemporal dementia and signs of motor neuron disease raises the possibility that TAF15 may contribute to both diseases. We are now studying whether aberrant aggregated TAF15 is present in people who have motor neurone disease in the absence of frontotemporal dementia.""
This study was funded by the Medical Research Council, Alzheimer's Research UK, the US National Institutes of Health, the Alzheimer's Society, the Association for Frontotemporal Degeneration, the Swiss National Science Foundation, and the Leverhulme Trust.
Dr Charlotte Durkin, Head of the Medical Research Council's Molecular and Cellular Medicine Board, said:
""Decades of world-leading research at the MRC Laboratory of Molecular Biology brought the breakthrough of cryoelectron microscopy -- gaining Dr Richard Henderson a Nobel Prize in 2017. This latest study identifying the protein linked to a type of frontotemporal dementia continues the MRC LMB's success in elucidating dementia-related protein structures by cryoEM, which includes the first structure for the key dementia protein tau. Knowing the identity and basic structure of these filaments in this rare form of early-onset dementia is vital to developing early diagnostic tests and drugs to combat their formation.""

","score: 16.912117636461897, grade_level: '17'","score: 18.436742028463343, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41586-023-06801-2,"Frontotemporal lobar degeneration (FTLD) causes frontotemporal dementia (FTD), the most common form of dementia after Alzheimer’s disease, and is often also associated with motor disorders1. The pathological hallmarks of FTLD are neuronal inclusions of specific, abnormally assembled proteins2. In the majority of cases the inclusions contain amyloid filament assemblies of TAR DNA-binding protein 43 (TDP-43) or tau, with distinct filament structures characterizing different FTLD subtypes3,4. The presence of amyloid filaments and their identities and structures in the remaining approximately 10% of FTLD cases are unknown but are widely believed to be composed of the protein fused in sarcoma (FUS, also known as translocated in liposarcoma). As such, these cases are commonly referred to as FTLD–FUS. Here we used cryogenic electron microscopy (cryo-EM) to determine the structures of amyloid filaments extracted from the prefrontal and temporal cortices of four individuals with FTLD–FUS. Surprisingly, we found abundant amyloid filaments of the FUS homologue TATA-binding protein-associated factor 15 (TAF15, also known as TATA-binding protein-associated factor 2N) rather than of FUS itself. The filament fold is formed from residues 7–99 in the low-complexity domain (LCD) of TAF15 and was identical between individuals. Furthermore, we found TAF15 filaments with the same fold in the motor cortex and brainstem of two of the individuals, both showing upper and lower motor neuron pathology. The formation of TAF15 amyloid filaments with a characteristic fold in FTLD establishes TAF15 proteinopathy in neurodegenerative disease. The structure of TAF15 amyloid filaments provides a basis for the development of model systems of neurodegenerative disease, as well as for the design of diagnostic and therapeutic tools targeting TAF15 proteinopathy."
"
In some diseases, the underlying processes can start years before a diagnosis is made. A new study finds that people who later develop multiple sclerosis (MS) are more likely to have conditions like depression, constipation and urinary tract infections five years before their MS diagnosis than people who do not develop MS. The study, which is published in the December 5, 2023, online issue of Neurology®, the medical journal of the American Academy of Neurology, also found that sexual problems and bladder infections, or cystitis, are more likely in people who later develop MS.

The conditions were also more likely to occur in people who had other autoimmune diseases, lupus and Crohn's disease.
""Knowing that these conditions may be prodromal symptoms or even early-stage symptoms of MS would not necessarily lead to earlier diagnosis of the disease in the general population, since these conditions are common and could also be signs of other diseases, but this information could be helpful for people who are at a higher risk of developing MS, such as people with a family history of the disease or those who show signs of MS on brain scans but do not have any symptoms of the disease,"" said study author Celine Louapre, MD, PhD, of Sorbonne University in Paris, France.
The study involved 20,174 people newly diagnosed with MS. They were each matched with three people who did not have MS of the same age and sex, for a total of 54,790 people. Then the people with MS were also compared to 30,477 people with Crohn's disease and 7,337 people with lupus. MS, Crohn's disease and lupus are all autoimmune diseases. They all affect women more often than men and affect young adults.
Then researchers used the medical records database to see whether the participants had any of 113 diseases and symptoms in the five years before and after their diagnosis, or before that matching date for the people who did not have an autoimmune disease.
The people with MS were 22% more likely to have depression five years before their diagnosis than the people without MS. They were 50% more likely to have constipation, 38% more likely to have urinary tract infections, 47% more likely to have sexual problems, and 21% more likely to have cystitis, or bladder infections.
For depression, 14% of the people with MS had prescriptions for antidepressants five years before diagnosis, compared to 10% of the people who did not have MS. By five years after diagnosis, 37% of people with MS had antidepressant prescriptions, compared to 19% of those without MS.
""Of course, not everyone who has these symptoms will go on to develop MS,"" Louapre said. ""We're hoping that eventually these early signs will help us understand the biological mechanisms that occur in the body before the actual symptoms of the disease develop.""
A limitation of the study was that data was not available for other factors that could influence people's risk of developing MS, such as education level, ethnicity and socioeconomic status.
The study was supported by the French National Research Agency.

","score: 14.91352941176471, grade_level: '15'","score: 16.41777149321267, grade_levels: ['college_graduate'], ages: [24, 100]",10.1212/WNL.0000000000207981,"Previous studies have reported a possible prodrome in multiple sclerosis (MS) defined by nonspecific symptoms including mood disorder or genitourinary symptoms and increased health care use detected several years before diagnosis. This study aimed to evaluate agnostically the associations between diseases and symptoms diagnosed in primary care and the risk of MS relative to controls and 2 other autoimmune inflammatory diseases with similar population characteristics, namely lupus and Crohn disease (CD). A case-control study was conducted using electronic health records from the Health Improvement Network database in the United Kingdom and France. We agnostically assessed the associations between 113 diseases and symptoms in the 5 years before and after diagnosis in patients with subsequent diagnosis of MS. Individuals with a diagnosis of MS were compared with individuals without MS and individuals with 2 other autoimmune diseases, CD and lupus. The study population consisted of patients with MS (n = 20,174), patients without MS (n = 54,790), patients with CD (n = 30,477), and patients with lupus (n = 7,337). Twelve ICD-10 codes were significantly positively associated with the risk of MS compared with controls without MS. After considering ICD-10 codes suggestive of neurologic symptoms as the first diagnosis of MS, 5 ICD-10 codes remained significantly associated with MS: depression (UK: odds ratio 1.22, 95% CI 1.11–1.34), sexual dysfunction (1.47, 1.11–1.95), constipation (1.5, 1.27–1.78), cystitis (1.21, 1.05–1.39), and urinary tract infections of unspecified site (1.38, 1.18–1.61). However, none of these conditions was selectively associated with MS in comparisons with both lupus and CD. All 5 ICD-10 codes identified were still associated with MS during the 5 years after diagnosis. We identified 5 health conditions associated with subsequent MS diagnosis, which may be considered not only prodromal but also early-stage symptoms. However, these health conditions overlap with prodrome of 2 other autoimmune diseases; hence, they lack specificity to MS."
"
A new organoid model of the dopaminergic system sheds lights on its intricate functionality and potential implications for Parkinson's disease. The model, developed by the group of Jürgen Knoblich at the Institute of Molecular Biotechnology (IMBA) of the Austrian Academy of Sciences, replicates the dopaminergic system's structure, connectivity, and functionality. The study, published on December 5 in Nature Methods, also uncovers the enduring effects of chronic cocaine exposure on the dopaminergic circuit, even after withdrawal.

A completed run, the early morning hit of caffeine, the smell of cookies in the oven -- these rewarding moments are all due to a hit of the neurotransmitter dopamine, released by neurons in a neural network in our brain, called the ""dopaminergic reward pathway."" Apart from mediating the feeling of ""reward,"" dopaminergic neurons also play a crucial role in fine motor control, which is lost in diseases such as Parkinson's disease. Despite dopamine's importance, key features of the system are not yet understood, and no cure for Parkinson's disease exists. In their new study, the group of Jürgen Knoblich at IMBA developed an organoid model of the dopaminergic system, which not only recapitulates the system's morphology and nerve projections, but also its functionality.
A model of Parkinson's disease 
Tremor and a loss of motor control are characteristic symptoms of Parkinson's disease and are due to a loss of neurons that release the neurotransmitter dopamine, called dopaminergic neurons. When dopaminergic neurons die, fine motor control is lost and patients develop tremors and uncontrollable movements. Although the loss of dopaminergic neurons is crucial in the development of Parkinson's disease, the mechanisms how this happens, and how we can prevent -- or even repair -- the dopaminergic system is not yet understood.
Animal models for Parkinson's disease have provided some insight into Parkinsons disease, however as rodents do not naturally develop Parkinson's disease, animal studies proved unsatisfactory in recapitulating hallmark features of the disease. In addition, the human brain contains many more dopaminergic neurons, which also wire up differently within the human brain, sending projections to the striatum and the cortex. ""We sought to develop an in vitro model that recapitulates these human features in so called brain organoids,"" explains Daniel Reumann, previously a PhD student in the lab of Jürgen Knoblich at IMBA, and first author of the paper. ""Brain organoids are human stem cell derived three-dimensional structures, which can be used to understand both human brain development, as well as function,"" he explains further.
The team first developed organoid models of the so-called ventral midbrain, striatum and cortex -- the regions linked by neurons in the dopaminergic system -- and then developed a method for fusing these organoids together. As happens in the human brain, the dopaminergic neurons of the midbrain organoid send out projections to the striatum and the cortex organoids. ""Somewhat surprisingly, we observed a high level of dopaminergic innervation, as well as synapses forming between dopaminergic neurons and neurons in striatum and cortex,"" Reumann recalls.
To assess whether these neurons and synapses are functional, the team collaborated with Cedric Bardy's group at SAHMRI and Flinders University, Australia, to investigate if neurons in this system would start to form functional neural networks. And indeed, when the researchers stimulated the midbrain which contains dopaminergic neurons, neurons in the striatum and cortex responded to the stimulation. ""We successfully modelled the dopaminergic circuit in vitro, as the cells not only wire correctly, but also function together,"" Reumann sums up.
The organoid model of the dopaminergic system could be used to improve cell therapies for Parkinson's disease. In first clinical studies, researchers have injected precursors of dopaminergic neurons into the striatum, to try and make up for the lost natural innervation. However, these studies have had mixed success. In collaboration with the lab of Malin Parmar at Lund University, Sweden, the team demonstrated that dopaminergic progenitor cells injected into the dopaminergic organoid model mature into neurons and extend neuronal projections within the organoid. ""Our organoid system could serve as a platform to test conditions for cell therapies, allowing us to observe how precursor cells behave in a three-dimensional human environment,"" Jürgen Knoblich, the study's corresponding author, explains. ""This allows researchers to study how progenitors can be differentiated more efficiently and provides a platform which allows to study how to recruit dopaminergic axons to target regions, all in a high-throughput manner.""
Insights into the reward system 
Dopaminergic neurons also fire whenever we feel rewarded, thus forming the basis of the ""reward pathway"" in our brains. But what happens when dopaminergic signaling is perturbed, such as in addiction? To investigate this question, the researchers made use of a well-known dopamine reuptake inhibitor, cocaine. When the organoids were exposed to cocaine chronically, over 80 days, the dopaminergic circuit changed functionally, morphologically and transcriptionally. These changes persisted, even when cocaine exposure was stopped 25 days before the end of the experiment, which simulated the withdrawal condition. ""Even after almost a month after stopping cocaine exposure, the effects of cocaine on the dopaminergic circuit were still visible, which means that we can now investigate what the long-term effects of dopaminergic overstimulation are in a human-specific in vitro system,"" Reumann summarizes.

","score: 16.890122997711675, grade_level: '17'","score: 18.13125, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41592-023-02080-x,"Ventral midbrain dopaminergic neurons project to the striatum as well as the cortex and are involved in movement control and reward-related cognition. In Parkinson’s disease, nigrostriatal midbrain dopaminergic neurons degenerate and cause typical Parkinson’s disease motor-related impairments, while the dysfunction of mesocorticolimbic midbrain dopaminergic neurons is implicated in addiction and neuropsychiatric disorders. Study of the development and selective neurodegeneration of the human dopaminergic system, however, has been limited due to the lack of an appropriate model and access to human material. Here, we have developed a human in vitro model that recapitulates key aspects of dopaminergic innervation of the striatum and cortex. These spatially arranged ventral midbrain–striatum–cortical organoids (MISCOs) can be used to study dopaminergic neuron maturation, innervation and function with implications for cell therapy and addiction research. We detail protocols for growing ventral midbrain, striatal and cortical organoids and describe how they fuse in a linear manner when placed in custom embedding molds. We report the formation of functional long-range dopaminergic connections to striatal and cortical tissues in MISCOs, and show that injected, ventral midbrain-patterned progenitors can mature and innervate the tissue. Using these assembloids, we examine dopaminergic circuit perturbations and show that chronic cocaine treatment causes long-lasting morphological, functional and transcriptional changes that persist upon drug withdrawal. Thus, our method opens new avenues to investigate human dopaminergic cell transplantation and circuitry reconstruction as well as the effect of drugs on the human dopaminergic system."
"
The future treatment of Parkinson’s Disease has undergone tremendous  development in recent years. Now, a breakthrough in research has  emerged, delivering the strongest results for both side-effect-free and  long-lasting treatment effects.

The results are just published in Nature Communications under the title ""Enhanced production of mesencephalic dopaminergic neurons from lineage-restricted human undifferentiated stem cells.""
In the new research findings, DANDRITE group leader and Associate Professor Mark Denham has developed a method that ensures much higher purity of the so-called dopamine cells, which are crucial in connection with Parkinson's disease.
""Stem cells offer promising potential for treating Parkinson's disease by transforming into specific nerve cells. However, the precision of this transformation poses a significant challenge with current methods, resulting in low purity,"" Mark explains.
Achieving high purity is critical for effectively restoring movement in patients.
In the Denham Lab, stem cells were genetically engineered to prevent them from generating the incorrect types of nerve cells. The newly engineered stem cells have an enhanced ability to produce the specific nerve cells required for Parkinson's treatment known as the dopaminergic cells.
Furthermore, the researchers show that the genetically engineered stem cells led to the restoration of movement in animal models. This breakthrough is a potential new therapeutic approach for treating Parkinson's disease patients.
Experiments on rats have shown that both the quantity and purity of cultured stem cells are critical for the number and duration of treatments.
""Using our genetically engineered cells we generate a higher purity of dopamine cells, for patients this will reduce the recovery time and diminish the risk of relapse and medication use. My goal is to help patients stay off their medication, which requires high purity. So, my next step is to transfer my method to clinical trials,"" Marks states.

","score: 13.879333333333332, grade_level: '14'","score: 15.071599999999997, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41467-023-43471-0,"Current differentiation protocols for generating mesencephalic dopaminergic (mesDA) neurons from human pluripotent stem cells result in grafts containing only a small proportion of mesDA neurons when transplanted in vivo. In this study, we develop lineage-restricted undifferentiated stem cells (LR-USCs) from pluripotent stem cells, which enhances their potential for differentiating into caudal midbrain floor plate progenitors and mesDA neurons. Using a ventral midbrain protocol, 69% of LR-USCs become bona fide caudal midbrain floor plate progenitors, compared to only 25% of human embryonic stem cells (hESCs). Importantly, LR-USCs generate significantly more mesDA neurons under midbrain and hindbrain conditions in vitro and in vivo. We demonstrate that midbrain-patterned LR-USC progenitors transplanted into 6-hydroxydopamine-lesioned rats restore function in a clinically relevant non-pharmacological behavioral test, whereas midbrain-patterned hESC-derived progenitors do not. This strategy demonstrates how lineage restriction can prevent the development of undesirable lineages and enhance the conditions necessary for mesDA neuron generation."
"
Lipids are the main constituents of our cell membranes, which are formed as lipid bilayers. The distribution of lipids is far from uniform; it is asymmetric, with different lipid compositions in the outside and inside layers. This asymmetry is essential for a variety of cellular functions, from maintaining membrane homeostasis to enabling cell signaling and numerous other physiological processes at or across membranes.

P4-ATPases, also known as flippases, are key players in creating and maintaining this lipid asymmetry. These enzymes actively transport lipids from the outside (exoplasmic) leaflet to the inside (cytosolic) leaflet coupled to ATP hydrolysis and ensure the proper distributions of lipids. The ATP8B1-CDC50A flippase complex in particular, has been the subject of the current study, leading to several new, groundbreaking discoveries.
The function of ATP8B1 lipid flippase is critical for the regulation of bile production, a vital substance in our digestive system, but the direct link within bile producing liver cells remains unknown. Additionally, recent studies have spotlighted the relevance of genetic variants in the regulatory segment of the ATP8B1 gene as a strong genetic marker for Alzheimer's resilience. Interestingly, mutations that impair function of the closely related ATP8B4 lipid flippase are oppositely important risk factors of Alzheimer's. It is therefore of strong interest to understand how ATP8B1 is linked to these processes and pathologies.
In the new study, the research team employed state-of-the-art cryo-electron microscopy techniques to capture nine different states associated with the lipid transport and determine structures at 2.4 to 3.1 Å resolution for these states. These structural insights, combined with functional and computational studies, reveal the inner workings of the human flippase ATP8B1-CDC50A complex and its fine regulation by specific regulatory lipids known as phosphoinositides, or PIPs.
These findings open doors to a deeper understanding of how lipid flippases operate and the intricate roles they play in cellular processes and are regulated. Importantly, the study also resolves earlier discrepancies about the ATP8B1 transport substrates.
Noteworthy, both up- or down-regulation of ATP8B1-CDC50 function could potentially be of interest in drug discovery, and the study also reveals critical information to such applications.

","score: 15.583429097605897, grade_level: '16'","score: 16.905976058931863, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41467-023-42828-9,"Asymmetric distribution of phospholipids in eukaryotic membranes is essential for cell integrity, signaling pathways, and vesicular trafficking. P4-ATPases, also known as flippases, participate in creating and maintaining this asymmetry through active transport of phospholipids from the exoplasmic to the cytosolic leaflet. Here, we present a total of nine cryo-electron microscopy structures of the human flippase ATP8B1-CDC50A complex at 2.4 to 3.1 Å overall resolution, along with functional and computational studies, addressing the autophosphorylation steps from ATP, substrate recognition and occlusion, as well as a phosphoinositide binding site. We find that the P4-ATPase transport site is occupied by water upon phosphorylation from ATP. Additionally, we identify two different autoinhibited states, a closed and an outward-open conformation. Furthermore, we identify and characterize the PI(3,4,5)P3 binding site of ATP8B1 in an electropositive pocket between transmembrane segments 5, 7, 8, and 10. Our study also highlights the structural basis of a broad lipid specificity of ATP8B1 and adds phosphatidylinositol as a transport substrate for ATP8B1. We report a critical role of the sn-2 ester bond of glycerophospholipids in substrate recognition by ATP8B1 through conserved S403. These findings provide fundamental insights into ATP8B1 catalytic cycle and regulation, and substrate recognition in P4-ATPases."
"
Neurodegenerative diseases are characterized by the deposition of clumped proteins in the brain and progressive neuronal cell death. Although the causal link between protein aggregates and neurodegeneration is clear, it is still unclear in what way misfolded proteins trigger cell death. A team headed by Professor Jörg Tatzelt, head of the Department of Biochemistry of Neurodegenerative Diseases at Ruhr University Bochum, Germany, showed that misfolded prion proteins can inactivate the TDP-43 protein. TDP-43 is essential for maintaining protein balance in all cell types, especially in nerve cells. A dysfunction of TDP43 is associated with amyotrophic lateral sclerosis and frontotemporal dementia. 

Protein aggregates and neurodegeneration
The causes of neurodegenerative diseases such as Alzheimer's disease, Parkinson's disease, frontotemporal dementia and prion diseases can be many and varied. But there is a common denominator, namely protein misfolding and the occurrence of protein deposits in the brain. ""Various approaches and models have shown that misfolded proteins play a crucial role in the disease process,"" says Jörg Tatzelt. ""Still, there's an ongoing debate about the nature of the harmful protein species and how misfolded proteins selectively damage specific neurons.""
Studies on genes associated with pathologies have revealed two basic mechanisms by which misfolded proteins can lead to neurodegeneration: Firstly, misfolding can cause the protein to acquire toxic activity. Secondly, the misfolding can lead to a loss of the physiological function of the protein, which impairs important physiological processes in the cell.
""The assumption used to be that every neurodegenerative disease was characterized by the misfolding of a specific protein,"" explains Jörg Tatzelt. ""However, it has since been shown that misfolded proteins that are produced more frequently in one disease can also induce the aggregation of other proteins, a mechanism referred to as cross-seeding.""
The prion protein and TDP-43
TDP-43 (TAR DNA-binding protein 43) is a protein that helps to translate genetic information into specific proteins. It thus helps to maintain the protein balance in nerve cells. The clumping of TDP-43 in the cell is a characteristic feature in the brains of patients suffering from amyotrophic lateral sclerosis or frontotemporal dementia.

Misfolding of the prion protein triggers prion diseases such as Creutzfeldt-Jakob disease. All research findings to date indicate that the misfolded prion protein acquires toxic activity. However, the exact mechanisms by which disease-associated prion proteins trigger the death of nerve cells are only partially understood.
TDP-43 loses its physiological function through PrP-mediated cross-seeding
Using in vitro and cell culture approaches, animal models and brain samples from patients with Creutzfeldt-Jakob disease, the researchers showed that misfolded prion proteins can trigger the clumping and inactivation of TDP-43. The prion proteins interact with TDP-43 in vitro and in cells, thus inducing the formation of TDP aggregates in the cell. As a result, TDP-43-dependent splicing activity in the cell nucleus is significantly reduced, leading to altered protein expression. ""Prion protein and TDP-43 are partners in crime in neurodegenerative diseases, so to speak,"" says Jörg Tatzelt.
An analysis of brain samples showed that in some Creutzfeld-Jacob patients, TDP-43 aggregates were found alongside the prion protein deposits. This study has revealed a new mechanism of how disease-associated prion proteins can affect physiological signaling pathways through cross-seeding.

","score: 14.293979562043798, grade_level: '14'","score: 15.89912408759124, grade_levels: ['college_graduate'], ages: [24, 100]",10.1093/brain/awad289,"A common pathological denominator of various neurodegenerative diseases is the accumulation of protein aggregates. Neurotoxic effects are caused by a loss of the physiological activity of the aggregating protein and/or a gain of toxic function of the misfolded protein conformers. In transmissible spongiform encephalopathies or prion diseases, neurodegeneration is caused by aberrantly folded isoforms of the prion protein (PrP). However, it is poorly understood how pathogenic PrP conformers interfere with neuronal viability. Employing in vitro approaches, cell culture, animal models and patients’ brain samples, we show that misfolded PrP can induce aggregation and inactivation of TAR DNA-binding protein-43 (TDP-43). Purified PrP aggregates interact with TDP-43 in vitro and in cells and induce the conversion of soluble TDP-43 into non-dynamic protein assemblies. Similarly, mislocalized PrP conformers in the cytosol bind to and sequester TDP-43 in cytosolic aggregates. As a consequence, TDP-43-dependent splicing activity in the nucleus is significantly decreased, leading to altered protein expression in cells with cytosolic PrP aggregates. Finally, we present evidence for cytosolic TDP-43 aggregates in neurons of transgenic flies expressing mammalian PrP and Creutzfeldt–Jakob disease patients. Our study identified a novel mechanism of how aberrant PrP conformers impair physiological pathways by cross-seeding."
"
Research has shown that children who experience adversity during their early years may undergo faster biological aging. Nonetheless, Nonetheless, recently published research reveals that positive parenting interventions can potentially shield children from this consequence, helping slow the epigenetic aging process.

A new study with research from the lab of Justin Parent, an assistant professor of psychology at the University of Rhode Island, has discovered that enhancing positive parenting through a family-centered program resulted in lower levels of accelerated biological aging for children who had experienced high levels of adversity. The findings suggest that positive parenting programs can help children exposed to hardships turn back the clock and build biologically based resilience.
""Our biological age or clock can sometimes tick faster than our chronological age,"" Parent said. ""We know experiences like trauma, maltreatment, chronic stress, living in neighborhoods with high violence -- all can cause wear and tear that physically ages you faster than you should. We wanted to see if supporting families who are facing adversities increase positive parenting behaviors has an impact on either reversing or buffering those negative effects.""
The study was led by Dr. Alexandra Sullivan as part of a National Institute of Child Health and Human Development training grant the National Institute of Minority Health and Health Disparities and involved researchers at URI as well as Florida International University and Stanford University. Families with children with delays in development and disruptive behavior were randomized to receive parent-child interaction therapy (PCIT) sessions via telehealth to learn positive parenting skills or to a control group. For the intervention, therapists interact with the families, directly coaching parents in real time on how to increase warmth and support while avoiding negative parenting behaviors like yelling or hitting.
""We know positive parenting programs like this work. They reduce disruptive behavior, increase positive parenting skills, and help families feel less stressed,"" Parent said. ""Now, from this study, we are beginning to learn that increases in positive parenting for children with higher adversity have the potential to slow this biological aging process or potentially reverse it. Children exposed to high levels of adversity displayed lower epigenetic age acceleration when parents increased positive and decreased negative parenting practices.""
Parent is continuing to expand the study at URI, and his team will be exploring the epigenetic mechanisms of risk and resilience. His aspirational goal for this research is to develop a saliva-based biomarker for identifying children at risk for mental health struggles and develop biologically informed personalized prevention services to families.
""I hope this provides support for the importance of helping families and increasing access to services for families in need,"" Parent said. ""Hopefully, policy makers and others will prioritize this.""

","score: 16.547493313882814, grade_level: '17'","score: 17.978492584488208, grade_levels: ['college_graduate'], ages: [24, 100]",10.1177/09567976231194221,"This study examined whether children exposed to adversity would exhibit lower epigenetic age acceleration in the context of improved parenting. Children with developmental delays and externalizing behavior problems ( N = 62; Mage = 36.26 months; 70.97% boys, 29.03% girls; 71% Latinx, 22.6% Black) were drawn from a larger randomized controlled trial (RCT), which randomized them to receive Internet-delivered parent–child interaction therapy (iPCIT; n = 30) or community referrals as usual (RAU; n = 32). Epigenetic age acceleration was estimated with the pediatric buccal epigenetic clock, using saliva. Adversity was assessed using parent, family, and neighborhood-level cumulative-risk indicators. Adversity interacted with Time 2 (T2) observations of positive and negative-parenting practices to predict epigenetic age acceleration 1.5 years later, regardless of treatment assignment. Children exposed to more adversity displayed lower epigenetic age acceleration when parents evidenced increased positive ( b = −0.15, p = .001) and decreased negative ( b = −0.12, p = .01) parenting practices."
"
Contrary to current understanding, the brains of human newborns aren't significantly less developed compared to other primate species, but appear so because so much brain development happens after birth, finds a new study led by UCL researchers.

The study, published in Nature Ecology & Evolution, found that humans are born with brains at a development level that's typical for similar primate species, but the human brains grow so much larger and more complex than other species after birth, it gives the false impression that human newborns are underdeveloped, or ""altricial.""
Lead author Dr Aida Gomez-Robles (UCL Anthropology) said: ""This new work changes the overall understanding around the evolution of human brain development. Humans seem so much more helpless when they're young compared to other primates not because their brains are comparatively underdeveloped but because they still have much further to go.""
One way that scientists compare the brain development of different species is by measuring the size of their brains as newborns to their brain size as adults. Humans are born with a relatively smaller percentage of their adult brain size, compared to other primates, making it seem they're born less developed. However, this new research shows that this measure is misleading as other measurements of human brain development show humans are largely in line with other species of primates such as chimpanzees, bonobos, gorillas and orangutans.
The research challenges a prevailing understanding of evolutionary human brain development. Up to now, because of their helplessness and poor muscle control, it's long been believed that humans are born with comparatively less developed brains than other primates. This was thought to be the result of an evolutionary compromise so babies' heads could fit through their mother's birth canal, which would require them to further develop outside of the womb.
Based on this understanding, scientists suggested that because humans emerged comparatively underdeveloped, their brains are more malleable in the earliest period of life and more easily affected by environmental stimuli as they grow. It was thought that this underdevelopment at birth encouraged greater brain plasticity, ultimately facilitating human intelligence.
Instead, the researchers found that while human brains do take longer than other species to grow to full capacity, it's not because they come out significantly less developed at birth, but because their brains grow so much more later in life. The researchers added that their findings don't negate the importance of brain plasticity in human evolution but make it unlikely that this enhanced plasticity resulted from being born less developed than other primates.
To understand the evolutionary development of human brains, the researchers analysed the brain development of 140 different mammal species including modern primates, rodents, carnivores, as well as the fossils of early humans and related ancestral hominins. They compared the length of fetal gestation in modern mammals, the relative size of newborn brains and bodies to their adult size, and the overall brain size of newborns and adults to understand the evolution of human brains.
They found that while there are major variations in brain development at birth between disparate mammal species, primates are relatively consistent with each other. Humans are not born at significantly lower levels of development than modern primates, nor their hominin ancestors. Similarly, human gestation period is not shorter than it would be expected when compared to other primates.
The research was supported by the National Science Foundation and the National Institutes of Health in the US.

","score: 16.34342233856894, grade_level: '16'","score: 18.475314136125654, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41559-023-02253-z,"Human newborns are considered altricial compared with other primates because they are relatively underdeveloped at birth. However, in a broader comparative context, other mammals are more altricial than humans. It has been proposed that altricial development evolved secondarily in humans due to obstetrical or metabolic constraints, and in association with increased brain plasticity. To explore this association, we used comparative data from 140 placental mammals to measure how altriciality evolved in humans and other species. We also estimated how changes in brain size and gestation length influenced the timing of neurodevelopment during hominin evolution. Based on our data, humans show the highest evolutionary rate to become more altricial (measured as the proportion of adult brain size at birth) across all placental mammals, but this results primarily from the pronounced postnatal enlargement of brain size rather than neonatal changes. In addition, we show that only a small number of neurodevelopmental events were shifted to the postnatal period during hominin evolution, and that they were primarily related to the myelination of certain brain pathways. These results indicate that the perception of human altriciality is mostly driven by postnatal changes, and they point to a possible association between the timing of myelination and human neuroplasticity."
"
Alcohol is the most common addictive substance in the world. Every year in the U.S. excessive alcohol use costs $249 billion and causes approximately 88,000 deaths, as well as various chronic diseases and social issues. Alcohol use disorder, a highly prevalent, chronic, relapsing disorder, affects more than 14 million people in the U.S. alone, in addition to being severely under-treated, with only three modestly effective pharmacological therapies available.

Chronic exposure to alcohol has been shown to produce profound neuroadaptations in specific brain regions, including the recruitment of key stress neurotransmitters, ultimately causing changes in the body that sustain excessive drinking. The area of the brain known as the ""bed nucleus of the stria terminalis"" (BNST) is critically involved in the behavioral response to stress as well as in chronic, pathological alcohol use.
Researchers from Boston University Chobanian & Avedisian School of Medicine have identified that a peptide called pituitary adenylate cyclase activating polypeptide (PACAP), is involved in heavy alcohol drinking. In addition, they have discovered that this peptide acts in the BNST area.
Using an established experimental model for heavy, intermittent alcohol drinking, the researchers observed that during withdrawal this model showed increased levels of the stress neuropeptide PACAP selectively in the BNST, compared to the control model.
Interestingly, a similar increase was also observed in the levels of another stress neuropeptide closely related to PACAP, the calcitonin gene-related peptide, or CGRP. Both peptides have been implicated in stress as well as pain sensitivity, but their role in alcohol addiction is less established.
The researchers then used a virus in a transgenic model to block the neural pathways containing PACAP that specifically arrive to the BNST. ""We found that inhibiting PACAP to the BNST dramatically reduced heavy ethanol drinking,"" explained co-corresponding author Valentina Sabino, PhD, co-director of the School's Laboratory of Addictive Disorders as well as professor of pharmacology, physiology & biophysics.
According to the researchers, these results provide evidence that this protein mediates the addictive properties of alcohol. ""We found a key player, PACAP, driving heavy alcohol drinking, which can be targeted for the development of novel pharmacological therapies,"" added co-corresponding author Pietro Cottone, PhD, associate professor of pharmacology, physiology & biophysics and co-director of the Laboratory of Addictive Disorders.
These findings appear online in the journal eNeuro.
Funding for this study was to grants number AA026051 (PC), AA025038 (VS), and AA024439 (VS) from the National Institute on Alcohol and Alcoholism (NIAAA), the Boston University Undergraduate Research Opportunities Program (UROP), the Boston University Micro and Nano Imaging Facility and the Office of the Director of the National Institutes of Health (S10OD024993).

","score: 18.091724828375288, grade_level: '18'","score: 19.09588386727689, grade_levels: ['college_graduate'], ages: [24, 100]",10.1523/ENEURO.0424-23.2023,"Alcohol use disorder (AUD) is a complex psychiatric disease characterized by periods of heavy drinking and periods of withdrawal. Chronic exposure to ethanol causes profound neuroadaptations in the extended amygdala, which cause allostatic changes promoting excessive drinking. The bed nucleus of the stria terminalis (BNST), a brain region involved in both excessive drinking and anxiety-like behavior, shows particularly high levels of pituitary adenylate cyclase-activating polypeptide (PACAP), a key mediator of the stress response. Recently, a role for PACAP in withdrawal-induced alcohol drinking and anxiety-like behavior in alcohol-dependent rats has been proposed; whether the PACAP system of the BNST is also recruited in other models of alcohol addiction and whether it is of local or nonlocal origin is currently unknown. Here, we show that PACAP immunoreactivity is increased selectively in the BNST of C57BL/6J mice exposed to a chronic, intermittent access to ethanol. While pituitary adenylate cyclase-activating polypeptide (PACAP) type 1 receptor-expressing cells were unchanged by chronic alcohol, the levels of a peptide closely related to PACAP, the calcitonin gene-related neuropeptide, were found to also be increased in the BNST. Finally, using a retrograde chemogenetic approach in PACAP-ires-Cre mice, we found that the inhibition of PACAP neuronal afferents to the BNST reduced heavy ethanol drinking. Our data suggest that the PACAP system of the BNST is recruited by chronic, voluntary alcohol drinking in mice and that nonlocally originating PACAP projections to the BNST regulate heavy alcohol intake, indicating that this system may represent a promising target for novel AUD therapies."
"
Following an 18-month meditation programme can improve the wellbeing of older adults, finds a new randomised controlled trial by an international team co-led by UCL.

The findings, published in PLOS ONE, show that meditation can improve people's awareness, connection to others, and insight.
While the meditation training did not confer significant benefits on two commonly used measures of psychological wellbeing and quality of life, the researchers say their findings may reveal limitations in existing methods of tracking wellbeing.
Lead author Marco Schlosser (UCL Psychiatry and University of Geneva) said: ""As the global population ages, it is increasingly crucial to understand how we can support older adults in maintaining and deepening their psychological wellbeing. In our study, we tested whether long-term meditation training can enhance important dimensions of wellbeing. Our findings suggest that meditation is a promising non-pharmacological approach to support human flourishing in late life.""
The study is the longest randomised meditation training trial conducted to date, and explored the impact of an 18-month meditation programme on the psychological wellbeing of more than 130 healthy French-speaking people aged 65 to 84. The study, led by Principal Investigator Professor Gaël Chételat, took place in Caen, France. It was conducted by the European Union's Horizon 2020-funded Medit-Ageing (Silver Santé Study) research group which involves UCL, Inserm, University of Geneva, Université de Caen Normandy, Lyon Neuroscience Research Center, University of Liège, Technische Universität Dresden, and Friedrich Schiller University Jena.
The researchers compared a meditation programme, which included a nine-month mindfulness module followed by a nine-month loving kindness and compassion module, delivered by weekly group sessions (two hours long), daily home practice (at least 20 minutes), and one retreat day, with a group that did English language training (as a comparison group) and a no-intervention control group.
The team found that meditation training significantly impacted a global score that measures the wellbeing dimensions of awareness, connection, and insight. Awareness describes an undistracted and intimate attentiveness to one's thoughts, feelings, and surroundings, which can support a sense of calm and deep satisfaction. Connection captures feelings such as respect, gratitude, and kinship that can support more positive relationships with others. Insight refers to a self-knowledge and understanding of how thoughts and feelings participate in shaping our perception -- and how to transform unhelpful patterns of thought relating to ourselves and the world.

The benefits of meditation training to an established measure of psychological quality of life were not superior to English language training, while neither intervention significantly impacted another widely used measure of psychological wellbeing. The researchers suggest this may be because these two established measures do not cover the qualities and depth of human flourishing that can potentially be cultivated by longer-term meditation training, so benefits to awareness, connection and insight are missed.
The programme did not benefit everyone equally, as participants who reported lower levels of psychological wellbeing at the start of the trial showed greater improvements compared to those who already had higher levels of wellbeing.
Co-author Dr Natalie Marchant (UCL Psychiatry) said: ""We hope that further research will clarify which people are most likely to benefit from meditation training, as it may confer stronger benefits on some specific groups. Now that we have evidence that meditation training can help older adults, we hope that further refinements in partnership with colleagues from other research disciplines could make meditation programmes even more beneficial.""
Senior author Dr Antoine Lutz (Lyon Neuroscience Research Center, Inserm, France) said: ""By showing the potential of meditation programmes, our findings pave the way for more targeted and effective programmes that can help older adults flourish, as we seek to go beyond simply preventing disease or ill-health, and instead take a holistic approach to helping people across the full spectrum of human wellbeing.""

","score: 18.387936507936512, grade_level: '18'","score: 21.27914285714286, grade_levels: ['college_graduate'], ages: [24, 100]",10.1371/journal.pone.0294753,"As the world population is ageing, it is vital to understand how older adults can maintain and deepen their psychological well-being as they are confronted with the unique challenges of ageing in a complex world. Theoretical work has highlighted the promising role of intentional mental training such as meditation practice for enhancing human flourishing. However, meditation-based randomised controlled trials in older adults are lacking. We aimed to investigate the effects of meditation training on psychological well-being in older adults. This study presents a secondary analysis of the Age-Well trial (ClinicalTrials.gov: NCT02977819), which randomised 137 healthy older adults (age range: 65 to 84 years) to an 18-month meditation training, an active comparator (English language training), or a passive control. Well-being was measured at baseline, mid-intervention, and 18-month post-randomisation using the Psychological Well-being Scale (PWBS), the World Health Organisation’s Quality of Life (QoL) Assessment psychological subscale, and composite scores reflecting the meditation-based well-being dimensions of awareness, connection, insight, and a global score comprising the average of these meditation-based dimensions. The 18-month meditation training was superior to English training on changes in the global score (0.54 [95% CI: 0.26, 0.82], p = 0.0002) and the subscales of awareness, connection, insight, and superior to no-intervention only on changes in the global score (0.54 [95% CI: 0.26, 0.82], p = 0.0002) and awareness. Between-group differences in psychological QoL in favour of meditation did not remain significant after adjusting for multiple comparisons. There were no between-group differences in PWBS total score. Within the meditation group, psychological QoL, awareness, insight, and the global score increased significantly from baseline to 18-month post-randomisation. The longest randomised meditation training conducted to date enhanced a global composite score reflecting the meditation-based well-being dimensions of awareness, connection, and insight in older adults. Future research is needed to delineate the cognitive, affective, and behavioural factors that predict responsiveness to meditation and thus help refine the development of tailored meditation training."
"
What happens in the human brain when we learn from positive and negative experiences? To help answer that question and better understand decision-making and human behavior, scientists are studying dopamine.

Dopamine is a neurotransmitter produced in the brain that serves as a chemical messenger, facilitating communication between nerve cells in the brain and the body. It is involved in functions such as movement, cognition and learning. While dopamine is most known for its association with positive emotions, scientists are also exploring its role in negative experiences.
Now, a new study from researchers at Wake Forest University School of Medicine shows that dopamine release in the human brain plays a crucial role in encoding both reward and punishment prediction errors. This means that dopamine is involved in the process of learning from both positive and negative experiences, allowing the brain to adjust and adapt its behavior based on the outcomes of these experiences.
The study was published today in Science Advances.
""Previously, research has shown that dopamine plays an important role in how animals learn from 'rewarding' (and possibly 'punishing') experiences. But, little work has been done to directly assess what dopamine does on fast timescales in the human brain,"" said Kenneth T. Kishida, Ph.D., associate professor of physiology and pharmacology and neurosurgery at Wake Forest University School of Medicine. ""This is the first study in humans to examine how dopamine encodes rewards and punishments and whether dopamine reflects an 'optimal' teaching signal that is used in today's most advanced artificial intelligence research.""
For the study, researchers on Kishida's team utilized fast-scan cyclic voltammetry, an electrochemical technique, paired with machine learning, to detect and measure dopamine levels in real-time (i.e., 10 measurements per second). However, this method is challenging and can only be performed during invasive procedures such as deep-brain stimulation (DBS) brain surgery. DBS is commonly employed to treat conditions such as Parkinson's disease, essential tremor, obsessive-compulsive disorder and epilepsy.
Kishida's team collaborated with Atrium Health Wake Forest Baptist neurosurgeons Stephen B. Tatter, M.D., and Adrian W. Laxton, M.D., who are also both faculty members in the Department of Neurosurgery at Wake Forest University School of Medicine, to insert a carbon fiber microelectrode deep into the brain of three participants at Atrium Health Wake Forest Baptist Medical Center who were scheduled to receive DBS to treat essential tremor.

While the participants were awake in the operating room, they played a simple computer game. As they played the game, dopamine measurements were taken in the striatum, a part of the brain that is important for cognition, decision-making, and coordinated movements.
During the game, participants' choices were either rewarded or punished with real monetary gains or losses. The game was divided into three stages in which participants learned from positive or negative feedback to make choices that maximized rewards and minimized penalties. Dopamine levels were measured continuously, once every 100 milliseconds, throughout each of the three stages of the game.
""We found that dopamine not only plays a role in signaling both positive and negative experiences in the brain, but it seems to do so in a way that is optimal when trying to learn from those outcomes. What was also interesting, is that it seems like there may be independent pathways in the brain that separately engage the dopamine system for rewarding versus punishing experiences. Our results reveal a surprising result that these two pathways may encode rewarding and punishing experiences on slightly shifted timescales separated by only 200 to 400 milliseconds in time,"" Kishida said.
Kishida believes that this level of understanding may lead to a better understanding of how the dopamine system is affected in humans with psychiatric and neurological disorders. Kishida said additional research is needed to understand how dopamine signaling is altered in psychiatric and neurological disorders.
""Traditionally, dopamine is often referred to as 'the pleasure neurotransmitter,""' Kishida said. ""However, our work provides evidence that this is not the way to think about dopamine. Instead, dopamine is a crucial part of a sophisticated system that teaches our brain and guides our behavior. That dopamine is also involved in teaching our brain about punishing experiences is an important discovery and may provide new directions in research to help us better understand the mechanisms underlying depression, addiction, and related psychiatric and neurological disorders.""
This study was supported by grants from the National Institutes of Health: R01MH121099, R01DA048096, R01MH124115, P50DA006634, 5KL2TR001420, F31DA053174, T32DA041349 and F30DA053176.

","score: 15.32306666666667, grade_level: '15'","score: 16.836840000000002, grade_levels: ['college_graduate'], ages: [24, 100]",10.1126/sciadv.adi4927,"In the mammalian brain, midbrain dopamine neuron activity is hypothesized to encode reward prediction errors that promote learning and guide behavior by causing rapid changes in dopamine levels in target brain regions. This hypothesis (and alternatives regarding dopamine’s role in punishment-learning) has limited direct evidence in humans. We report intracranial, subsecond measurements of dopamine release in human striatum measured, while volunteers (i.e., patients undergoing deep brain stimulation surgery) performed a probabilistic reward and punishment learning choice task designed to test whether dopamine release encodes only reward prediction errors or whether dopamine release may also encode adaptive punishment learning signals. Results demonstrate that extracellular dopamine levels can encode both reward and punishment prediction errors within distinct time intervals via independent valence-specific pathways in the human brain."
"
Parents should speak to their babies using sing-song speech, like nursery rhymes, as soon as possible, say researchers. That's because babies learn languages from rhythmic information, not phonetic information, in their first months.

Phonetic information -- the smallest sound elements of speech, typically represented by the alphabet -- is considered by many linguists to be the foundation of language. Infants are thought to learn these small sound elements and add them together to make words. But a new study suggests that phonetic information is learnt too late and slowly for this to be the case.
Instead, rhythmic speech helps babies learn language by emphasising the boundaries of individual words and is effective even in the first months of life.
Researchers from the University of Cambridge and Trinity College Dublin investigated babies' ability to process phonetic information during their first year.
Their study, published today in the journal Nature Communications, found that phonetic information wasn't successfully encoded until seven months old, and was still sparse at 11 months old when babies began to say their first words.
""Our research shows that the individual sounds of speech are not processed reliably until around seven months, even though most infants can recognise familiar words like 'bottle' by this point,"" said Cambridge neuroscientist, Professor Usha Goswami. ""From then individual speech sounds are still added in very slowly -- too slowly to form the basis of language.""
The researchers recorded patterns of electrical brain activity in 50 infants at four, seven and eleven months old as they watched a video of a primary school teacher singing 18 nursery rhymes to an infant. Low frequency bands of brainwaves were fed through a special algorithm, which produced a 'read out' of the phonological information that was being encoded.

The researchers found that phonetic encoding in babies emerged gradually over the first year of life, beginning with labial sounds (e.g. d for ""daddy"") and nasal sounds (e.g. m for ""mummy""), with the 'read out' progressively looking more like that of adults
First author, Professor Giovanni Di Liberto, a cognitive and computer scientist at Trinity College Dublin and a researcher at the ADAPT Centre, said: ""This is the first evidence we have of how brain activity relates to phonetic information changes over time in response to continuous speech.""
Previously, studies have relied on comparing the responses to nonsense syllables, like ""bif"" and ""bof"" instead.
The current study forms part of the BabyRhythm project led by Goswami, which is investigating how language is learnt and how this is related to dyslexia and developmental language disorder.
Goswami believes that it is rhythmic information -- the stress or emphasis on different syllables of words and the rise and fall of tone -- that is the key to language learning. A sister study, also part of the BabyRhythm project, has shown that rhythmic speech information was processed by babies at two months old -- and individual differences predicted later language outcomes. The experiment was also conducted with adults who showed an identical 'read out' of rhythm and syllables to babies.
""We believe that speech rhythm information is the hidden glue underpinning the development of a well-functioning language system,"" said Goswami. ""Infants can use rhythmic information like a scaffold or skeleton to add phonetic information on to. For example, they might learn that the rhythm pattern of English words is typically strong-weak, as in 'daddy' or 'mummy', with the stress on the first syllable. They can use this rhythm pattern to guess where one word ends and another begins when listening to natural speech.""
""Parents should talk and sing to their babies as much as possible or use infant directed speech like nursery rhymes because it will make a difference to language outcome,"" she added.

Goswami explained that rhythm is a universal aspect of every language all over the world. ""In all language that babies are exposed to there is a strong beat structure with a strong syllable twice a second. We're biologically programmed to emphasise this when speaking to babies.""
Goswami says that there is a long history in trying to explain dyslexia and developmental language disorder in terms of phonetic problems but that the evidence doesn't add up. She believes that individual differences in children's language originate with rhythm.
The research was funded by the European Research Council under the European Union's Horizon 2020 research and innovation programme and by Science Foundation Ireland.

","score: 13.158179508161421, grade_level: '13'","score: 14.640832077178175, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41467-023-43490-x,"Even prior to producing their first words, infants are developing a sophisticated speech processing system, with robust word recognition present by 4–6 months of age. These emergent linguistic skills, observed with behavioural investigations, are likely to rely on increasingly sophisticated neural underpinnings. The infant brain is known to robustly track the speech envelope, however previous cortical tracking studies were unable to demonstrate the presence of phonetic feature encoding. Here we utilise temporal response functions computed from electrophysiological responses to nursery rhymes to investigate the cortical encoding of phonetic features in a longitudinal cohort of infants when aged 4, 7 and 11 months, as well as adults. The analyses reveal an increasingly detailed and acoustically invariant phonetic encoding emerging over the first year of life, providing neurophysiological evidence that the pre-verbal human cortex learns phonetic categories. By contrast, we found no credible evidence for age-related increases in cortical tracking of the acoustic spectrogram."
"
Researchers have developed a new experiment to better understand what people view as moral and immoral decisions related to driving vehicles, with the goal of collecting data to train autonomous vehicles how to make ""good"" decisions. The work is designed to capture a more realistic array of moral challenges in traffic than the widely discussed life-and-death scenario inspired by the so-called ""trolley problem.""

""The trolley problem presents a situation in which someone has to decide whether to intentionally kill one person (which violates a moral norm) in order to avoid the death of multiple people,"" says Dario Cecchini, first author of a paper on the work and a postdoctoral researcher at North Carolina State University.
""In recent years, the trolley problem has been utilized as a paradigm for studying moral judgment in traffic,"" Cecchini says. ""The typical situation comprises a binary choice for a self-driving car between swerving left, hitting a lethal obstacle, or proceeding forward, hitting a pedestrian crossing the street. However, these trolley-like cases are unrealistic. Drivers have to make many more realistic moral decisions every day. Should I drive over the speed limit? Should I run a red light? Should I pull over for an ambulance?""
""Those mundane decisions are important because they can ultimately lead to life-or-death situations,"" says Veljko Dubljevic, corresponding author of the paper and an associate professor in the Science, Technology & Society program at NC State.
""For example, if someone is driving 20 miles over the speed limit and runs a red light, then they may find themselves in a situation where they have to either swerve into traffic or get into a collision. There's currently very little data in the literature on how we make moral judgments about the decisions drivers make in everyday situations.""
To address that lack of data, the researchers developed a series of experiments designed to collect data on how humans make moral judgments about decisions that people make in low-stakes traffic situations. The researchers created seven different driving scenarios, such as a parent who has to decide whether to violate a traffic signal while trying to get their child to school on time. Each scenario is programmed into a virtual reality environment, so that study participants engaged in the experiment have audiovisual information about what drivers are doing when they make decisions, rather than simply reading about the scenario.
For this work, the researchers built on something called the Agent Deed Consequence (ADC) model, which posits that people take three things into account when making a moral judgment: the agent, which is the character or intent of the person who is doing something; the deed, or what is being done; and the consequence, or the outcome that resulted from the deed.

Researchers created eight different versions of each traffic scenario, varying the combinations of agent, deed and consequence. For example, in one version of the scenario where a parent is trying to get the child to school, the parent is caring, brakes at a yellow light, and gets the child to school on time. In a second version, the parent is abusive, runs a red light, and causes an accident. The other six versions alter the nature of the parent (the agent), their decision at the traffic signal (the deed), and/or the outcome of their decision (the consequence).
""The goal here is to have study participants view one version of each scenario and determine how moral the behavior of the driver was in each scenario, on a scale from one to 10,"" Cecchini says. ""This will give us robust data on what we consider moral behavior in the context of driving a vehicle, which can then be used to develop AI algorithms for moral decision making in autonomous vehicles.""
The researchers have done pilot testing to fine-tune the scenarios and ensure that they reflect believable and easily understood situations.
""The next step is to engage in large-scale data collection, getting thousands of people to participate in the experiments,"" says Dubljevic. ""We can then use that data to develop more interactive experiments with the goal of further fine-tuning our understanding of moral decision making. All of this can then be used to create algorithms for use in autonomous vehicles. We'll then need to engage in additional testing to see how those algorithms perform.""

","score: 14.037548679390788, grade_level: '14'","score: 14.81821573163679, grade_levels: ['college_graduate'], ages: [24, 100]",10.1007/s00146-023-01813-y,"The imminent deployment of autonomous vehicles requires algorithms capable of making moral decisions in relevant traffic situations. Some scholars in the ethics of autonomous vehicles hope to align such intelligent systems with human moral judgment. For this purpose, studies like the Moral Machine Experiment have collected data about human decision-making in trolley-like traffic dilemmas. This paper first argues that the trolley dilemma is an inadequate experimental paradigm for investigating traffic moral judgments because it does not include agents’ character-based considerations and is incapable of facilitating the investigation of low-stakes mundane traffic scenarios. In light of the limitations of the trolley paradigm, this paper presents an alternative experimental framework that addresses these issues. The proposed solution combines the creation of mundane traffic moral scenarios using virtual reality and the Agent-Deed-Consequences (ADC) model of moral judgment as a moral-psychological framework. This paradigm shift potentially increases the ecological validity of future studies by providing more realism and incorporating character considerations into traffic actions."
"
Combining a pair of experimental drugs may help treat malignant peripheral nerve sheath tumors with fewer harmful side effects, according to preliminary animal studies led by investigators at the Johns Hopkins Kimmel Cancer Center and Johns Hopkins Drug Discovery.

The study, published Dec. 1 in the journal Molecular Cancer Therapeutics, shows that combining an experimental drug that blocks the metabolism of the amino acid glutamine with a second experimental drug that blocks recycling of purines (building blocks of the genetic material DNA and RNA that tumors need to proliferate) reduced the size of peripheral nerve sheath tumors in mice and increased their rates of survival. The combination was less toxic than previously tested drug combinations.
""Using these broadly active metabolic inhibitors can help us better understand how these tumors are using nutrients to build what they need to make more tumor cells,"" says lead study author Kathryn Lemberg, M.D., Ph.D., an assistant professor of oncology at Johns Hopkins.
Malignant peripheral nerve sheath tumors (MPNST) are rare cancers that originate in the lining of the nerves. Individuals with the cancer predisposition syndrome neurofibromatosis type 1 (NF1) have an elevated risk of these cancers. MPNST are the leading cause of death in people with NF1 under the age of 40, Lemberg explains.
""When patients with NF1 develop these tumors, there is, unfortunately, a risk of poor outcome based on whether the tumor can be completely surgically removed, whether it responds to conventional chemotherapy, and how widespread the cancer is,"" Lemberg says. ""We need better treatments for these tumors.""
Barbara Slusher, Ph.D., M.A.S., director of Johns Hopkins Drug Discovery and senior author on the study, and colleagues developed the glutamine antagonist JHU395 that blocks the utilization of glutamine. JHU395 is particularly well-suited for blocking glutamine metabolism in nerve-associated tissues.
""Glutamine is a building block for proteins and many other large molecules that cancer cells need to grow, proliferate, survive and potentially spread,"" Lemberg says.

Previous studies by Lemberg, Slusher and colleagues showed that treatment of mouse MPNST with JHU395 yields a partial tumor reduction. The drug works by impairing one arm of purine production, the ""de novo"" pathway. However, with JHU395 treatment, the tumors can still access purine by a recycling or salvage pathway. Combining JHU395 with the classical purine antimetabolite 6-mercaptopurine (6-MP) produces a better anti-tumor response because 6-MP itself can cause liver and gastrointestinal toxicity based on the way the body metabolizes it to the active form.
Lemberg, Slusher and colleagues from the Institute of Organic Chemistry and Biochemistry (IOCB) of the Czech Academy of Sciences in Prague developed an alternate purine salvage-blocking prodrug called Pro-905. In experiments conducted in collaboration with scientists at Northwestern University, they showed that JHU395 and Pro-905 inhibit purine synthesis in human MPNST cells by distinct mechanisms, leading to decreased synthesis of new DNA and RNA. In mouse models of the condition, they found that Pro-905 in combination with JHU395 reduced tumor growth more effectively than 6-MP and with less toxicity.
""Collaboration is key to moving science forward, particularly for studies of rare tumor types that require the expertise of multiple researchers,"" Lemberg says.
Now, Lemberg and colleagues want to see if this experimental drug combination is also effective in other types of tumors that use similar pathways to grow. They also want to test whether this combination might be effective in patients with malignant peripheral nerve sheath tumors or if Pro-905 might be used as maintenance therapy to prevent tumor regrowth, as 6-MP is. Lemberg also plans to collaborate with other Johns Hopkins researchers to determine if starving the tumors of glutamine and purines might also enable the immune system to kick in and fight the tumor.
Earlier research of JHU395 showed that cancer cells steal nutrients, including glutamine, to maintain their survival, starving surrounding cells such as immune cells. Using JHU395 to cut cancer cells off from their supply of glutamine strengthens immune cells, invigorating their response to cancer, Slusher says.
""Building on observations from one study and keeping the science moving forward can open up a lot of potential routes for future therapies for diseases that need better treatment options,"" Lemberg says.
Study co-authors were Joanna Marie H. Aguilar, Jesse Alt, Diane E. Peters, Liang Zhao, Ying Wu, Naziba Nuha, Verena Staedtke, Christine A. Pratilas and Rana Rais of Johns Hopkins. Other investigators were from Northwestern University Feinberg School of Medicine in Chicago, Institute of Organic Chemistry and Biochemistry (IOCB) of the Czech Academy of Sciences in Prague, and Beth Israel Deaconess Medical Center in Boston.
The study was supported by a CureSearch for Children's Cancer Young Investigator Award, a Maryland Innovation Initiative Award, an Allegheny Health Network Award, and the National Institutes of Health (grant R01CA229451).
Lemberg and Slusher hold a patent on prodrugs of 6-mercaptopurine. Slusher and Rais are co-inventors on Johns Hopkins University patents covering novel glutamine antagonist prodrugs and their utility. These patents have been licensed to Dracen Pharmaceuticals, Inc. Slusher and Rais are also co-founders of and hold equity in Dracen Pharmaceuticals. These arrangements have been reviewed and approved by The Johns Hopkins University in accordance with its conflict-of-interest policies.

","score: 15.55376574736125, grade_level: '16'","score: 17.007264215185565, grade_levels: ['college_graduate'], ages: [24, 100]",10.1158/1535-7163.MCT-23-0258,"Malignant peripheral nerve sheath tumors (MPNST) are highly aggressive soft-tissue sarcomas that arise from neural tissues and carry a poor prognosis. Previously, we found that the glutamine amidotransferase inhibitor JHU395 partially impeded tumor growth in preclinical models of MPNST. JHU395 inhibits de novo purine synthesis in human MPNST cells and murine tumors with partial decreases in purine monophosphates. On the basis of prior studies showing enhanced efficacy when glutamine amidotransferase inhibition was combined with the antimetabolite 6-mercaptopurine (6-MP), we hypothesized that such a combination would be efficacious in MPNST. Given the known toxicity associated with 6-MP, we set out to develop a more efficient and well-tolerated drug that targets the purine salvage pathway. Here, we report the discovery of Pro-905, a phosphoramidate protide that delivered the active nucleotide antimetabolite thioguanosine monophosphate (TGMP) to tumors over 2.5 times better than equimolar 6-MP. Pro-905 effectively prevented the incorporation of purine salvage substrates into nucleic acids and inhibited colony formation of human MPNST cells in a dose-dependent manner. In addition, Pro-905 inhibited MPNST growth and was well-tolerated in both human patient-derived xenograft (PDX) and murine flank MPNST models. When combined with JHU395, Pro-905 enhanced the colony formation inhibitory potency of JHU395 in human MPNST cells and augmented the antitumor efficacy of JHU395 in mice. In summary, the dual inhibition of the de novo and purine salvage pathways in preclinical models may safely be used to enhance therapeutic efficacy against MPNST."
