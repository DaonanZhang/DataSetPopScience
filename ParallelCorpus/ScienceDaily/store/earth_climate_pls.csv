pls,fk_score,ari_score,reference,abstract
"
Astronomers have made the rare discovery of a small, cold exoplanet and its massive outer companion -- shedding light on the formation of planets like Earth.

The findings include a planet with radius and mass between that of the Earth and Neptune, with a potential orbit around its host star of 146 days. The star system also contains an outer, large companion, 100 times the mass of Jupiter.
This is a rare discovery, with exoplanets smaller and lighter than Neptune and Uranus being notoriously hard to detect, with only a few being identified to this day. Such rare systems are particularly interesting to better understand planetary formation and evolution; they are thought to be a key step for the detection of Earth-like planets around stars.
The new planetary system is discovered around the star HD88986. This star has a similar temperature to the Sun with a slightly larger radius and is bright enough to be seen by keen observers at dark sky sites across the UK, such as Bannau Brycheiniog National Park (Brecon Beacons).
This study, published in the journal Astronomy & Astrophysics, is led by Neda Heidari, an Iranian postdoctoral fellow at the Institut d'astrophysique de Paris (IAP). In the UK, Thomas Wilson, a senior research fellow at the University of Warwick, co-led the analysis of satellite data including searching for new planets. The team also includes researchers at 29 other institutes from nine countries including Switzerland, Chile, and the USA.
A cold, Neptune-like exoplanet
The planetary system includes a cold planet smaller than Neptune, a so-called sub-Neptune, HD88986b. This planet has the longest orbital period (146 days) among known exoplanets smaller than Neptune or Uranus with precise mass measurements.

Neda Heidari, IAP, explained: ""Most of the planets we've discovered and measured for their mass and radius have short orbits, typically less than 40 days. To provide a comparison with our solar system, even Mercury, the closest planet to the Sun, takes 88 days to complete its orbit. This lack of detection for planets with longer orbits raises challenges in understanding how planets form and evolve in other systems and even in our solar system. HD88986b, with its orbital period of 146 days, potentially has the longest known orbit among the population of small planets with precise measurements.""
HD88986b was detected using the SOPHIE -- a high-precision spectrograph (a machine that analyses wavelengths of light from exoplanets) at the Haute-Provence Observatory, France. SOPHIE detects and characterises exoplanets using the 'radial-velocity method'; measuring tiny motion variations of the star induced by planets orbiting it.
These observations revealed the planet and allowed the team to estimate its mass to approximately 17 times that of the Earth.
Complementary observations obtained with NASA's space telescope Transiting Exoplanet Survey Satellite (TESS) and the European Space Agency's (ESA) space telescope CHaracterising ExOPlanet Satellite (CHEOPS) indicate that the planet probably ""transits"" in front of it host star. This occurs when its orbit passes on the line of sight between the Earth and the star, partially occulting the star -- causing a decrease in its brightness that can be observed and quantified.
These observations by both satellites allowed the team to directly estimate the diameter of the planet as about twice that of the Earth. The findings of the study rely on more than 25 years of observations, also including data from ESA's Gaia satellite and the Keck Telescope in Hawaii.
Moreover, with an atmosphere temperature of only 190 Celsius degrees, HD88986b provides a rare opportunity for studying the composition of the so-called ""cold"" atmospheres, as most of the detected atmospheres for exoplanets are above 1,000 Celsius degrees.

Due to the wide orbit of the sub-Neptune HD88986b (as large as 60% of the Earth-Sun distance), HD88986b probably underwent rare interactions with other planets that may exist in the planetary system, and weak loss of mass from the strong ultraviolet radiation of the central star. It may therefore have retained its original chemical composition, allowing scientists to explore the possible scenarios for the formation and evolution of this planetary system.
Thomas Wilson, Department of Physics, University of Warwick, said: ""HD88986b is essentially a scaled-down Neptune, between the orbits of Mercury and Venus. It becomes one of the best studied small, cold exoplanets paving the way for studying its atmosphere to understand the similarity to our own planet Earth. It also orbits a star with a similar temperature to the Sun making it a precursor to the Earth-like planets to be found by the PLATO space telescope, in which Warwick plays a leading role.""
A second, outer companion
The astronomers also revealed a second, outer companion around the central star. This exoplanet is particularly massive (more than 100 times the mass of Jupiter), and its orbit has a period of several tens of years. Further observations are needed to understand its nature and better determine its properties.
Thomas Wilson added: ""We collected data from telescopes pointing at HD88986 for over 25 years making this one of the longest-studies exoplanet systems. This wealth of data revealed a second outer companion more massive than Jupiter that may have been important for the formation of the Neptune-like planet in a similar way to Jupiter in our own Solar System.""

","score: 14.523072569233882, grade_level: '15'","score: 15.702771376591876, grade_levels: ['college_graduate'], ages: [24, 100]",10.1051/0004-6361/202347897,"Transiting planets with orbital periods longer than 40 d are extremely rare among the 5000+ planets discovered so far. The lack of discoveries of this population poses a challenge to research into planetary demographics, formation, and evolution. Here, we present the detection and characterization of HD 88986 b, a potentially transiting sub-Neptune, possessing the longest orbital period among known transiting small planets (<4 R⊕) with a precise mass measurement (σM/M > 25%). Additionally, we identified the presence of a massive companion in a wider orbit around HD 88986. To validate this discovery, we used a combination of more than 25 yr of extensive radial velocity (RV) measurements (441 SOPHIE data points, 31 ELODIE data points, and 34 HIRES data points), Gaia DR3 data, 21 yr of photometric observations with the automatic photoelectric telescope (APT), two sectors of TESS data, and a 7-day observation of CHEOPS. Our analysis reveals that HD 88986 b, based on two potential single transits on sector 21 and sector 48 which are both consistent with the predicted transit time from the RV model, is potentially transiting. The joint analysis of RV and photometric data show that HD 88986 b has a radius of 2.49 ± 0.18 R⊕, a mass of 17.2−3.8+4.0 M⊕, and it orbits every 146.05−0.40+0.43 d around a subgiant HD 88986 which is one of the closest and brightest exoplanet host stars (G2Vtype, R = 1.543 ± 0.065 R⊙, V = 6.47 ± 0.01 mag, distance = 33.37 ± 0.04 pc). The nature of the outer, massive companion is still to be confirmed; a joint analysis of RVs, HIPPARCOS, and Gaia astrometric data shows that with a 3σ confidence interval, its semi-major axis is between 16.7 and 38.8 au and its mass is between 68 and 284 MJup. HD 88986 b’s wide orbit suggests the planet did not undergo significant mass loss due to extreme-ultraviolet radiation from its host star. Therefore, it probably maintained its primordial composition, allowing us to probe its formation scenario. Furthermore, the cold nature of HD 88986 b (460 ± 8 K), thanks to its long orbital period, will open up exciting opportunities for future studies of cold atmosphere composition characterization. Moreover, the existence of a massive companion alongside HD 88986 b makes this system an interesting case study for understanding planetary formation and evolution."
"
Through the Strong Heart Family Study, National Institutes of Health-supported researchers found that small declines in blood lead levelswere associated with long-term cardiovascular health improvements in American Indian adults. Participants who had the greatest reductions in blood lead levels saw their systolic blood pressure fall by about 7 mm Hg, an amount comparable to the effects of blood pressure-lowering medication.

The findings as reported from researchers at Columbia University Mailman School of Public Health and NIEHS and NHLBI are published in the Journal of the American Heart Association.
""This is a huge win for public health,"" said Anne E. Nigra, PhD, assistant professor of environmental health sciences at Columbia Mailman School of Public Health, and senior author. ""We saw that even small decreases in a person's blood lead levels can have meaningful health outcomes.""
Nigra and her co- authors, including Wil Lieberman-Cribbin, MPH, also at Columbia Mailman School, credit these improvements in large part to public health and policy changes that have occurred over the last few decades.
In addition to seeing improvements in systolic blood pressure, the investigators found that reductions in blood lead levels were associated with reductions in a marker associated with hypertrophic cardiomyopathy and heart failure.
To conduct this research, investigators partnered with 285 American Indian adults through an extension of the Strong Heart Study, the largest study following cardiovascular health outcomes and risk factors among American Indian adults. Participants lived in one of four tribal communities in Arizona, Oklahoma, North Dakota, or South Dakota.
The researchers looked at blood lead levels and blood pressure readings over time. Lead was first measured in blood collected during the 1997-1999 study visit and again in blood collected during a follow-up visit between 2006-2009. During this time, participants had their blood pressure taken and participated in medical exams, including having echocardiographs to assess their heart's structure and function. To support similar comparisons among participants, researchers controlled for multiple factors, including social variables, cardiovascular disease risks, and medical history.

At the start of the study, the average blood lead level was 2.04 µg/dL. Throughout the study, the average blood lead level fell by 0.67 µg/dL, or 33 percent. The most significant changes, categorized by participants with average starting blood lead levels of 3.21 µg/dL and who experienced reductions of about 1.78 µg/dL, or 55 percent, were linked to a 7 mm Hg reduction in systolic blood pressure.
""This is a sign that whatever is happening in these communities to reduce blood lead levels is working,"" said Mona Puggal, MPH, an epidemiologist in the Division of Cardiovascular Sciences at the National Heart, Lung, and Blood Institute (NHLBI). ""The reductions in blood pressure are also comparable to improvements you would see with lifestyle changes, such as getting 30 minutes of daily exercise, reducing salt intake, or losing weight.""
The reductions in blood lead levels observed in the study are similar to those seen in the general U.S. population following policies and efforts implemented within the past 50 years to reduce lead exposure through paint, gasoline, water, plumbing, and canned items.
""Recognizing that American Indian communities are disproportionately exposed to elevated levels of lead and other metals compared to the general U.S. population, more research needs to be done to determine how environmental agents exacerbate cardiovascular and other diseases, and more needs to be done to improve the environmental health of American Indians,"" said Lindsey A. Martin, PhD, a health science administrator at the National Institute of Environmental Health Sciences (NIHES).
The researchers point out that it is also important to investigate these findings in other communities and to look for additional ways to reduce lead exposure, especially in other populations with elevated risks for exposure and cardiovascular disease.
Co-authors are: Zheng Li, Michael Lewin, Patricia Ruiz, Agency for Toxic Substances and Disease Registry; Jeffery M. Jarrett, Centers for Disease Control and Prevention;Shelley A. Cole, Marcia O'Leary, Texas Biomedical Research Institute; Gernot Pichler, Clinic Floridsdorf, Vienna; Daichi Shimbo, Columbia University Irving Medical Center; Richard B. Devereux, Weill Cornell Medical College; Jason G. Umans, MedStar Health Research Institute and Georgetown-Howard Universities Center for Clinical and Translational Science; and Allison Kupsco and Ana Navas-Acien, Columbia Mailman School of Public Health.
The research was funded by NIEHS and NHLBI.

","score: 17.77509661835749, grade_level: '18'","score: 20.15988224637681, grade_levels: ['college_graduate'], ages: [24, 100]",10.1161/JAHA.123.031256,"Chronic lead exposure is associated with both subclinical and clinical cardiovascular disease. We evaluated whether declines in blood lead were associated with changes in systolic and diastolic blood pressure in adult American Indian participants from the SHFS (Strong Heart Family Study). Lead in whole blood was measured in 285 SHFS participants in 1997 to 1999 and 2006 to 2009. Blood pressure and measures of cardiac geometry and function were obtained in 2001 to 2003 and 2006 to 2009. We used generalized estimating equations to evaluate the association of declines in blood lead with changes in blood pressure; cardiac function and geometry measures were considered secondary. Mean blood lead was 2.04 μg/dL at baseline. After ≈10 years, mean decline in blood lead was 0.67 μg/dL. In fully adjusted models, the mean difference in systolic blood pressure comparing the highest to lowest tertile of decline (>0.91 versus <0.27 μg/dL) in blood lead was −7.08 mm Hg (95% CI, −13.16 to −1.00). A significant nonlinear association between declines in blood lead and declines in systolic blood pressure was detected, with significant linear associations where blood lead decline was 0.1 μg/dL or higher. Declines in blood lead were nonsignificantly associated with declines in diastolic blood pressure and significantly associated with declines in interventricular septum thickness. Declines in blood lead levels in American Indian adults, even when small (0.1–1.0 μg/dL), were associated with reductions in systolic blood pressure. These findings suggest the need to further study the cardiovascular impacts of reducing lead exposures and the importance of lead exposure prevention."
"
Endocrine-disrupting chemicals (EDCs) in plastics pose a serious threat to public health and cost the U.S. an estimated $250 billion in increased health care costs in 2018, according to new research published in the Journal of the Endocrine Society.

Plastics contain many hazardous, endocrine-disrupting chemicals that leach and contaminate humans and the environment. These chemicals disturb the body's hormone systems and can cause cancer, diabetes, reproductive disorders, neurological impairments of developing fetuses and children, and death.
Potential options under discussion as part of a Global Plastics Treaty include interventions to reduce EDC exposure to protect public health and the environment, and data on the health costs of EDCs could help move this initiative forward.
""Our study found plastics contribute substantially to disease and associated social costs in the U.S., about $250 billion in 2018 alone. These costs are equivalent to 1.22% of the Gross Domestic Product. The diseases due to plastics run the entire life course from preterm birth to obesity, heart disease and cancers,"" said study author Leonardo Trasande, M.D., M.P.P., of NYU Grossman School of Medicine and NYU Wagner Graduate School of Public Service in New York, N.Y. Trasande has represented the Society at intergovernmental meetings to address plastic pollution and its health effects.
""Our study drives home the need to address chemicals used in plastic materials as part of the Global Plastics Treaty,"" Trasande said. ""Actions through the Global Plastics Treaty and other policy initiatives will reduce these costs in proportion to the actual reductions in chemical exposures achieved.""
The researchers analyzed existing studies on EDCs to identify how many diseases and disabilities were attributed to chemicals in plastics. The chemicals they studied commonly found in plastics included polybrominated diphenyl ethers (PBDE), phthalates, bisphenols, and poly- and perfluoroalkyl substances (PFAS).
The researchers updated previously published data on disease burden and cost estimates for these chemicals in the United States to 2018. They combined the data and estimated $250 billion in disease burden from plastic exposure in 2018.

""This study shows that preventing plastic pollution can reduce the incidence of disease, disability and early death, and its attendant human suffering and health care costs,"" said co-author Michael Belliveau, Executive Director of Defend Our Health based in Portland, Maine. ""Policymakers and market leaders must detoxify and slash the use of petrochemical plastics and endocrine-disrupting chemicals. We urge negotiators to finalize a Global Plastics Treaty that caps and reduces plastic production and eliminate EDCs as plastics additives.""
Most of the cost burden was from polybrominated diphenyl ethers (PBDE) exposure which is associated with diseases such as cancer. Sixty-seven billion in health costs was due to phthalate exposure which is linked to preterm birth, reduced sperm count and childhood obesity, and $22 billion was due to PFAS exposure which is associated with kidney failure and gestational diabetes.
The other authors of this study are Roopa Krithivasan of Defend Our Health; Kevin Park of NYU Grossman School of Medicine; and Vladislav Obsekov of Children's Hospital of Philadelphia in Philadelphia, Pa.
The study was funded by the National Institutes of Health's National Institute of Environmental Health Sciences and The Passport Foundation.

","score: 15.63689887640449, grade_level: '16'","score: 17.58685393258427, grade_levels: ['college_graduate'], ages: [24, 100]",10.1210/jendso/bvad163,"Chemicals used in plastics have been described to contribute to disease and disability, but attributable fractions have not been quantified to assess specific contributions. Without this information, interventions proposed as part of the Global Plastics Treaty cannot be evaluated for potential benefits. To accurately inform the tradeoffs involved in the ongoing reliance on plastic production as a source of economic productivity in the United States, we calculated the attributable disease burden and cost due to chemicals used in plastic materials in 2018. We first analyzed the existing literature to identify plastic-related fractions (PRF) of disease and disability for specific polybrominated diphenylethers (PBDE), phthalates, bisphenols, and polyfluoroalkyl substances and perfluoroalkyl substances (PFAS). We then updated previously published disease burden and cost estimates for these chemicals in the United States to 2018. By uniting these data, we computed estimates of attributable disease burden and costs due to plastics in the United States. We identified PRFs of 97.5% for bisphenol A (96.25-98.75% for sensitivity analysis), 98% (96%-99%) for di-2-ethylhexylphthalate, 100% (71%-100%) for butyl phthalates and benzyl phthalates, 98% (97%-99%) for PBDE-47, and 93% (16%-96%) for PFAS. In total, we estimate $249 billion (sensitivity analysis: $226 billion-$289 billion) in plastic-attributable disease burden in 2018. The majority of these costs arose as a result of PBDE exposure, though $66.7 billion ($64.7 billion-67.3 billion) was due to phthalate exposure and $22.4 billion was due to PFAS exposure (sensitivity analysis: $3.85-$60.1 billion). Plastics contribute substantially to disease and associated social costs in the United States, accounting for 1.22% of the gross domestic product. The costs of plastic pollution will continue to accumulate as long as exposures continue at current levels. Actions through the Global Plastics Treaty and other policy initiatives will reduce these costs in proportion to the actual reductions in chemical exposures achieved."
"
Florida's 156-mile-long Indian River Lagoon (IRL) borders five different counties and has five inlets that connect the lagoon with the Atlantic Ocean. In recent years, this estuary has experienced numerous phytoplankton bloom events due to increased seasonal temperatures coupled with environmental impacts.

Algal blooms produce a myriad of small organic molecules, many of which can be toxic to humans and animals. Among these phycotoxin producers is Microcystis aeruginosa, a freshwater cyanobacterium, which can be found in the Southern IRL. Measurable amounts of microcystins have been found in nasal swabs of people who live and work near the area, although finding microcystins in mucosal membranes may be evidence that the body is doing its job to eliminate them.
To help uncover potential human health hazards associated with harmful algae blooms in the IRL, researchers from Florida Atlantic University's Harbor Branch Oceanographic Institute collected water samples from 20 sites within the lagoon during wet and dry seasons over a three-year period. The samples were extracted to concentrate organic molecules and these extracts were used in testing. To identify the presence of known or emerging toxins, researchers used a panel of immortalized human cell lines corresponding to the liver, kidney and brain to measure cytotoxicity. Human cell lines engineered to express ion transporters, red blood cells, and the activity against a protein phosphatase enzyme, also were used in the study. These cells and biological activities were selected as they are known to be affected by algal toxins and show unique patterns of activity for known toxins.
Samples were tested at high concentrations to detect as many metabolites as possible, and those that presented more than 50 percent cytotoxicity were considered active. Samples that exhibited high toxicity were further subjected to liquid chromatography-high resolution mass spectrometry analysis to assess the metabolites present in the sample.
Results of the study, published in the journal Toxins, show that each control toxin induced a consistent pattern of cytotoxicity in the panel of human cell lines assayed. During blooms, cytotoxicity due to a single type of toxin was obvious from this pattern. In the absence of blooms, the observed cytotoxicity reflected either a mixture of toxins or it was caused by an unidentified toxin.
""The most interesting observation from our study is that with the cell lines used, we could follow the patterns of known toxins,"" said Esther Guzmán, Ph.D., corresponding author and a research professor at FAU Harbor Branch. ""Known toxins were seen only during blooms. Because cell toxicity was seen in the absence of blooms, it suggests that there might be either emergent toxins or a combination of toxins present at those times. Our findings suggest that other toxins with the potential to be harmful to human health may be present in the lagoon.""
Among the study findings, the most northern sites of the lagoon exhibited less toxicity than sites to the south. Cytotoxic blooms were seen both in the south (Microcystis) and the north (Pyrodinium) of the lagoon. In the absence of blooms, South Fork, South Fork 2, North Fork and Middle Estuary (sites one to four) in the Southern IRL and Banana River, and North Banana River (NASA) (sites 14 and 15) in the Northern IRL appeared to have the most cytotoxicity during the time of the assessment.

In contrast, Jensen, Fort Pierce Inlet, Harbor Branch Link Port Canal, Vero Beach Land/Ocean Biogeochemical Observatory, and Vero Beach Barber Bridge (sites six to 10) appeared healthier as there were few samples with cytotoxicity above 50 percent in these sites, although there was statistically significant variation in these sites.
""A major question we sought to answer in this study was whether there are unrecognized toxins or other signaling molecules associated with harmful algal blooms in the lagoon,"" said Amy Wright, Ph.D., co-author and a research professor, FAU Harbor Branch. ""The data collected to date suggest that this is indeed the case. Importantly, using an assay panel to assess the presence of toxic materials could allow for better monitoring of human health impacts, especially from emerging toxins within the system.""
The researchers note that microcystins are primarily a threat to human health in the lagoon during blooms, and because of the necessity of active transport, the toxin would need to be ingested or inhaled to present a threat to humans.
""Ingestion can be avoided by filtering water through activated charcoal,"" said Guzmán. ""Similarly, effects due to inhalation are effectively blocked by the mucus membrane, which traps toxins that are subsequently eliminated through coughing. However, pet and wildlife exposures can still occur.""
Study co-authors Tara A. Peterson, coordinator, cancer cell biology, FAU Harbor Branch; Priscilla Winder, Ph.D., a chemistry research associate, FAU Harbor Branch; Kirstie T. Francis, Ph.D., an FAU graduate and current postdoctoral fellow in molecular microbiology, Mote Marine Laboratory; Malcolm McFarland, Ph.D., an assistant research professor in phytoplankton ecology, FAU Harbor Branch; Jill C. Roberts, a chemical scientist, FAU Harbor Branch; and Jennifer Sandle, a chemical scientist, FAU Harbor Branch.
This research was funded by a discretionary grant to the Florida Center for Coastal and Human Health from the Harbor Branch Oceanographic Institute Foundation.

","score: 15.542764561707035, grade_level: '16'","score: 16.65016219723183, grade_levels: ['college_graduate'], ages: [24, 100]",10.3390/toxins15110664,"The Indian River Lagoon (IRL), a 156-mile-long estuary located on the eastern coast of Florida, experiences phytoplankton bloom events due to increased seasonal temperatures coupled with anthropogenic impacts. This study aimed to gather data on the toxicity to human cells and to identify secondary metabolites found in water samples collected in the IRL. Water samples from 20 sites of the IRL were collected during the wet and dry seasons over a three-year period. A panel of cell lines was used to test cytotoxicity. Hemagglutination, hemolysis, and inhibition of protein phosphatase 2A (PP2A) were also measured. Cytotoxic blooms were seen both in the south (Microcystis) and the north (Pyrodinium) of the IRL. Each toxin induced a consistent pattern of cytotoxicity in the panel of human cell lines assayed. During blooms, cytotoxicity due to a single type of toxin is obvious from this pattern. In the absence of blooms, the cytotoxicity seen reflected either a mixture of toxins or it was caused by an unidentified toxin. These observations suggest that other toxins with the potential to be harmful to human health may be present in the IRL. Moreover, the presence of toxins in the IRL is not always associated with blooms of known toxin-producing organisms."
"
Hummingbirds use two distinct sensory strategies to control their flight, depending on whether they're hovering or in forward motion, according to new research by University of British Columbia (UBC) zoologists.

""When in forward fight, hummingbirds rely on what we call an 'internal forward model' -- almost an ingrained, intuitive autopilot -- to gauge speed,"" says Dr. Vikram B. Baliga, lead author of a new study on hummingbird locomotion published in Proceedings of the Royal Society B. ""There's just too much information coming in to rely directly on every visual cue from your surroundings.""
""But when hovering or dealing with cues that might require a change in altitude, we found they rely much more on real-time, direct visual feedback from their environment.""
The findings not only provide insights on how the tiny, agile birds perceive the world during transitions in flight, but could inform the programming of onboard navigation for next generation autonomous flying and hovering vehicles.
Hummingbird flight recorder
The researchers had hummingbirds perform repeated flights from a perch to a feeder in a four-metre tunnel. To test how the birds reacted to a variety of visual stimuli, the team projected patterns on the chamber's front and side walls. Each flight was videoed.
In some scenarios, the researchers projected vertical stripes moving at various speeds on the side walls to mimic degrees of forward motion. Sometimes, horizontal stripes on the side mimicked changes in altitude. On the front wall, the researchers projected rotating swirls, designed to create the illusion of a change in position.
""If the birds were taking their cues directly from visual stimuli, we'd expect them to adjust their forward velocity to the speed of vertical stripes on the side walls,"" says Dr. Baliga. ""But while the birds did change velocity or stop altogether depending on the patterns, there wasn't a neat correlation.""
However, during flight, the hummingbirds did adjust more directly to stimuli indicating a change in altitude. And during hovering, the birds also worked to adjust their position much more closely to shifting spirals the research team projected on the front wall.
""Our experiments were designed to investigate how hummingbirds control flight speed,"" says Dr. Doug Altshuler, senior author on the paper. ""But because the hummingbirds took spontaneous breaks to hover during their flights, we uncovered these two distinct strategies to control different aspects of their trajectories.""

","score: 13.211783681214424, grade_level: '13'","score: 14.627432491607067, grade_levels: ['college_graduate'], ages: [24, 100]",10.1098/rspb.2023.2155,"The detection of optic flow is important for generating optomotor responses to mediate retinal image stabilization, and it can also be used during ongoing locomotion for centring and velocity control. Previous work in hummingbirds has separately examined the roles of optic flow during hovering and when centring through a narrow passage during forward flight. To develop a hypothesis for the visual control of forward flight velocity, we examined the behaviour of hummingbirds in a flight tunnel where optic flow could be systematically manipulated. In all treatments, the animals exhibited periods of forward flight interspersed with bouts of spontaneous hovering. Hummingbirds flew fastest when they had a reliable signal of optic flow. All optic flow manipulations caused slower flight, suggesting that hummingbirds had an expected optic flow magnitude that was disrupted. In addition, upward and downward optic flow drove optomotor responses for maintaining altitude during forward flight. When hummingbirds made voluntary transitions to hovering, optomotor responses were observed to all directions. Collectively, these results are consistent with hummingbirds controlling flight speed via mechanisms that use an internal forward model to predict expected optic flow whereas flight altitude and hovering position are controlled more directly by sensory feedback from the environment."
"
A new study shows that human hunting and land use have a decisive influence on red deer density in Europe. Red deer density is only reduced when wolves, lynx and bears co-occur at the same site.

Alongside the occasional bison and elk, red deer are Europe's largest native wild animal. An international study led by wildlife ecologists from the University of Freiburg has now investigated the factors that affect the red deer population in a particular area.
The researchers were able to show that the population density of the animals in Europe is primarily influenced by human hunting and land use and not by large predators such as wolves, lynx and brown bears. ""While large carnivores are often considered key factors in controlling prey populations in undisturbed ecosystems, this is less visible in human-dominated landscapes. Our study illustrates that these interactions are context-dependent,"" says Dr. Suzanne T. S. van Beeck Calkoen, former PhD student at the Chair of Wildlife Ecology and Management at the University of Freiburg and first author of the study.
The researchers collected data on the population density of red deer at over 492 study sites in 28 European countries and analysed the influence of various factors such as habitat productivity, the presence of large carnivores, human activities, climatic variables and the protection status of the area. The evaluation of the data showed that human hunting reduced red deer density more than the presence of all large carnivores. Human land use, on the other hand, led to an increase in red deer density. In most cases, the presence of large carnivores had no statistically significant effect on the red deer population. Only when the three predators wolf, lynx and bear occurred together in one area did the number of red deer decrease. However, the study published in the Journal of Applied Ecology did not investigate how the presence of predators affects the behavior of red deer.
The return of the wolf 
The study also sheds new light on the ongoing debate about the return of the wolf to Central Europe, notes Prof. Dr. Marco Heurich, Professor of Wildlife Ecology and Conservation Biology at the Faculty of Environment and Natural Resources at the University of Freiburg and initiator of the study. ""Our research shows that the return of a large carnivore such as the wolf alone does not have a major impact on the occurrence of red deer. This is because in Central Europe, human influences predominate both indirectly through interventions in the red deer's habitat and directly through the killing of the animals."" In addition, the mortality rate of wolves in Central European landscapes is very high, mainly due to road traffic, which further limits their influence on prey populations. ""However, we also found a high variability in red deer densities, which indicates that there may be specific situations in which large carnivores do have an impact. Investigating this will be the task of future studies,"" states Heurich.

","score: 14.38863179074447, grade_level: '14'","score: 15.028363867415017, grade_levels: ['college_graduate'], ages: [24, 100]",10.1111/1365-2664.14526,"Terrestrial ecosystems are shaped by interacting top‐down and bottom‐up processes, with the magnitude of top‐down control by large carnivores largely depending on environmental productivity. While carnivore‐induced numerical effects on ungulate prey populations have been demonstrated in large, relatively undisturbed ecosystems, whether large carnivores can play a similar role in more human‐dominated systems is a clear knowledge gap. As humans influence both predator and prey in a variety of ways, the ecological impacts of large carnivores can be largely modified. We quantified the interactive effects of human activities and large carnivore presence on red deer (Cervus elaphus) population density and how their impacts interacted and varied with environmental productivity. Data on red deer density were collected based on a literature survey encompassing 492 study sites across 28 European countries. Variation in density across study sites was analysed using a generalized additive model in which productivity, carnivore presence (grey wolf, European lynx, Brown bear), human activities (hunting, intensity of human land‐use activity), site protection status and climatic variables served as predictors. The results showed that a reduction in deer density only occurred when wolf, lynx and bear co‐occurred within the same site. In the absence of large carnivores, red deer density varied along a productivity gradient without a clear pattern. Although a linear relationship with productivity in the presence of all three large carnivore species was found, this was not statistically significant. Moreover, hunting by humans had a stronger effect than the presence of all large carnivores in reducing red deer density and red deer density increased with increasing intensity of human land use, with stronger large carnivore effects (all three carnivore species present) at sites with low human land‐use activities. Synthesis and applications. This study provides evidence for the dominant role played by humans (i.e. hunting, land‐use activities) relative to large carnivores in reducing red deer density across European human‐dominated landscapes. These findings suggest that when we would like large carnivores to exert numeric effects, we should focus on minimizing human impacts to allow the ecological impacts of large carnivores on ecosystem functioning. Terrestrial ecosystems are shaped by interacting top‐down and bottom‐up processes, with the magnitude of top‐down control by large carnivores largely depending on environmental productivity. While carnivore‐induced numerical effects on ungulate prey populations have been demonstrated in large, relatively undisturbed ecosystems, whether large carnivores can play a similar role in more human‐dominated systems is a clear knowledge gap. As humans influence both predator and prey in a variety of ways, the ecological impacts of large carnivores can be largely modified. We quantified the interactive effects of human activities and large carnivore presence on red deer (Cervus elaphus) population density and how their impacts interacted and varied with environmental productivity. Data on red deer density were collected based on a literature survey encompassing 492 study sites across 28 European countries. Variation in density across study sites was analysed using a generalized additive model in which productivity, carnivore presence (grey wolf, European lynx, Brown bear), human activities (hunting, intensity of human land‐use activity), site protection status and climatic variables served as predictors. The results showed that a reduction in deer density only occurred when wolf, lynx and bear co‐occurred within the same site. In the absence of large carnivores, red deer density varied along a productivity gradient without a clear pattern. Although a linear relationship with productivity in the presence of all three large carnivore species was found, this was not statistically significant. Moreover, hunting by humans had a stronger effect than the presence of all large carnivores in reducing red deer density and red deer density increased with increasing intensity of human land use, with stronger large carnivore effects (all three carnivore species present) at sites with low human land‐use activities. Synthesis and applications. This study provides evidence for the dominant role played by humans (i.e. hunting, land‐use activities) relative to large carnivores in reducing red deer density across European human‐dominated landscapes. These findings suggest that when we would like large carnivores to exert numeric effects, we should focus on minimizing human impacts to allow the ecological impacts of large carnivores on ecosystem functioning."
"
The ocean contains about 60 times more carbon than the atmosphere, in part due to a key process in the marine carbon cycle called the biological carbon pump (BCP). In this process, carbon dioxide (CO2) is converted to organic matter through photosynthesis and subsequently sinks as the so-called ""export flux"" from the surface ocean waters to the deep sea. As it sinks, bacterial decomposition processes break down the organic matter back into inorganic carbon, thus storing CO2 in the interior ocean. The BCP keeps atmospheric CO2 levels significantly lower than they would be in a hypothetical world without the BCP. So far, so good.

But one crucial aspect is often overlooked, says Dr Ivy Frenger, a climate researcher at the GEOMAR Helmholtz Centre for Ocean Research Kiel: ""You have to consider the ocean circulation, because it determines how much of the biologically produced CO2 can actually accumulate in the interior ocean in the long term, isolated from exchange with the atmosphere."" Looking at the effect of the BCP only in terms of the export flux is like trying to explain the balance of a bank account by looking only at the deposits. ""But there are gains and losses.""
Changes in the BCP are an important research topic in the context of climate change. Ivy Frenger notes that when considering the impact of the BCP on atmospheric CO2, it is common to focus on the export flux and neglect the ocean circulation. She and six international colleagues have therefore published an opinion paper entitled ""Misconceptions of the marine biological carbon pump in a changing climate: Thinking outside the 'export' box.""
In their paper, the scientists aim to address the misconception that there is a direct link between the global export flux -- equivalent to deposits -- and the biogenic storage of CO2 in the ocean, and hence, atmospheric CO2 -- the equivalent to the bank account balance. ""There is no such simple correlation,"" says Dr Frenger. The ""withdrawal"" side also needs to be taken into account.
A much simpler and scientifically more accurate approach, she says, would be to directly estimate the CO2 reservoir resulting from biological processes in the interior ocean. Such an estimate can be made by measuring the oxygen content of the ocean's interior along with its physical state, such as temperature. Changes in these variables under climate change would also explain a seemingly paradoxical response of the BCP under anthropogenic climate change: Despite a decreasing export flux, carbon storage due to the biological pump in the interior ocean increases. This is because changes in ocean circulation delay the return of biologically stored carbon from the ocean interior to the surface. As in the bank account analogy: while the deposits are lower, if the withdrawals are reduced to an even greater extent there will be a net increase. Accordingly, for climate change, this feedback results in more CO2 being stored in the ocean's interior than would be the case without the biological carbon pump. Co-author Angela Landolfi remarks: ""It is important to note that this effect is small when compared to the continuing massive anthropogenic CO2 emissions from fossil fuels.""
The scientists hope that their opinion paper and proposed approach will contribute to a clearer understanding of the influence of the marine biological carbon pump on atmospheric CO2 in a changing world. A broader view of the biological carbon pump is particularly important for proposals to remove CO2 from the atmosphere through marine CDR approaches, such as artificially stimulating marine biology.

","score: 14.029636949202168, grade_level: '14'","score: 14.945821256038649, grade_levels: ['college_graduate'], ages: [24, 100]",10.1111/gcb.17124,"The marine biological carbon pump (BCP) stores carbon in the ocean interior, isolating it from exchange with the atmosphere and thereby coregulating atmospheric carbon dioxide (CO2). As the BCP commonly is equated with the flux of organic material to the ocean interior, termed “export flux,” a change in export flux is perceived to directly impact atmospheric CO2, and thus climate. Here, we recap how this perception contrasts with current understanding of the BCP, emphasizing the lack of a direct relationship between global export flux and atmospheric CO2. We argue for the use of the storage of carbon of biological origin in the ocean interior as a diagnostic that directly relates to atmospheric CO2, as a way forward to quantify the changes in the BCP in a changing climate. The diagnostic is conveniently applicable to both climate model data and increasingly available observational data. It can explain a seemingly paradoxical response under anthropogenic climate change: Despite a decrease in export flux, the BCP intensifies due to a longer reemergence time of biogenically stored carbon back to the ocean surface and thereby provides a negative feedback to increasing atmospheric CO2. This feedback is notably small compared with anthropogenic CO2 emissions and other carbon‐climate feedbacks. In this Opinion paper, we advocate for a comprehensive view of the BCP's impact on atmospheric CO2, providing a prerequisite for assessing the effectiveness of marine CO2 removal approaches that target marine biology."
"
Free charge carriers in perovskite solar cells likely have a special form of protection from recombination, researchers at Forschungszentrum Jülich have discovered by means of innovative photoluminescence measurements.

Highly efficient and relatively inexpensive to produce -- perovskite solar cells have been the subject of repeated surprises in recent years. Scientists at Forschungszentrum Jülich have now discovered another special feature of the cells using a new photoluminescence measurement technique. They found that the loss of charge carriers in this type of cell follows different physical laws than those known for most semiconductors. This may be one of the main reasons for their high level of efficiency. The results were presented in the journal Nature Materials.
Perovskite solar cells are regarded as highly promising for photovoltaics, even if their stability leaves much to be desired. Cells of this type are inexpensive to print and very efficient. In the last decade, their efficiency has doubled to over 25 % and is therefore currently on a par with conventional solar cells made of silicon. Further improvements also appear to be possible in the future.
""An important factor here is the question of how long excited charge carriers remain in the material, in other words their lifetime,"" explains Thomas Kirchartz. ""Understanding the processes is crucial to further improving the efficiency of perovskite-based solar cells."" The electrical engineer is the head of a working group on organic and hybrid solar cells at Forschungszentrum Jülich's Institute of Energy and Climate Research (IEK-5).
It's the lifetime that counts
In a solar cell, electrons are dislodged by photons and raised to a higher energy level from the valence band to the conduction band. Only then can they move more freely and flow through an external circuit. They can only contribute to electrical energy generation if their lifetime is long enough for them to pass through the absorber material to the electrical contact. An excited electron also leaves a hole in the underlying valence band -- a mobile vacancy that can be moved through the material like a positive charge carrier.
It is mainly defects in the crystal lattice which ensure that excited electrons quickly fall back down to lower energy levels again. The electrons affected are then no longer able to contribute to the current flow. ""This mechanism is also known as recombination and is the main loss process of every solar cell,"" says Kirchartz.

Recombination crucial for efficiency
No solar cell is perfect on an atomic level; each one has different types of defects due to the manufacturing process. These defects or foreign atoms in the lattice structure are the collection points where electrons and holes tend to come together. The electrons then fall back into the valence band and become worthless in terms of electricity generation.
""It had previously been assumed that recombination is predominantly triggered by defects that are energetically located in the middle between the valence and conduction bands. This is because these deep defects are similarly accessible to excited electrons and their counterparts, the holes,"" says Kirchartz. Indeed, this is likely true for most types of solar cells.
Shallow defects dominate
However, Kirchartz and his team have now disproved this assumption for perovskite solar cells and shown that the shallow defects are ultimately decisive in terms of their final efficiency. Unlike the deep defects, they are not located in the middle of the band gap, but very close to the valence or conduction band.
""The cause of this unusual behavior has not yet been fully clarified,"" Kirchartz adds. ""It is reasonable to assume that deep defects simply cannot exist in these materials. This restriction may also be one of the reasons for the particularly high efficiency of the cells.""
New HDR measurement technique with extended dynamic range
The observation was only made possible by innovative transient photoluminescence measurements. In previous measurements, it was not possible to distinguish loss processes caused by shallow defects from those caused by other factors.
The new measuring method developed by Thomas Kirchartz and his team at Forschungszentrum Jülich delivers data with a significantly increased dynamic range compared to conventional technology, i.e. data over a larger measuring range and with better fine gradation. The process is based on a similar principle to HDR image in high dynamic range quality. The dynamic range of the camera is increased by superimposing different images or measurements -- in this case signals with different levels of amplification -- to create a data set.

","score: 12.852589969648793, grade_level: '13'","score: 13.285969070674959, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41563-023-01771-2,"Quantifying recombination in halide perovskites is a crucial prerequisite to control and improve the performance of perovskite-based solar cells. While both steady-state and transient photoluminescence are frequently used to assess recombination in perovskite absorbers, quantitative analyses within a consistent model are seldom reported. We use transient photoluminescence measurements with a large dynamic range of more than ten orders of magnitude on triple-cation perovskite films showing long-lived photoluminescence transients featuring continuously changing decay times that range from tens of nanoseconds to hundreds of microseconds. We quantitatively explain both the transient and steady-state photoluminescence with the presence of a high density of shallow defects and consequent high rates of charge carrier trapping, thereby showing that deep defects do not affect the recombination dynamics. The complex carrier kinetics caused by emission and recombination processes via shallow defects imply that the reporting of only single lifetime values, as is routinely done in the literature, is meaningless for such materials. We show that the features indicative for shallow defects seen in the bare films remain dominant in finished devices and are therefore also crucial to understanding the performance of perovskite solar cells."
"
An international group of researchers finds that conserving about half of global land area could maintain nearly all of nature's contributions to people and still meet biodiversity targets for tens of thousands of species. But the same priority areas are at risk of conflict with human development with only 18% of that land area protected. This research is published Nature Communications.

""Biodiversity, climate, and sustainable development cannot be considered in isolation,"" said lead author Rachel Neugarten at the Cornell Lab of Ornithology. ""We must also factor in nature's contributions to human well-being, including clean water, carbon storage, crop pollination, flood mitigation, coastal protection, and more.""
""We face enormous challenges. With limited resources available to address climate change, biodiversity loss, poverty and water insecurity, we must be strategic and find ways to tackle more than one challenge at a time,"" said senior author Amanda Rodewald, Garvin Professor and Senior Director of the Center for Avian Population Studies at the Cornell Lab.
Study authors found that roughly half (44-49%) of global land area, excluding Antarctica, provides nearly all (90%) current levels of nature's services to people while also conserving biodiversity for 27,000 species of birds, mammals, reptiles, and amphibians. But their findings also point to potential conflict because 37% of the land areas are highly suitable for development by agriculture, renewable energy, oil and gas, mining, or urban expansion. Such high development potential, coupled with the fact that few priority areas are currently protected, means that successful conservation will require creative solutions. Such solutions carefully accommodate human activities through sustainable use and multifunctional landscape planning, particularly in the growing areas of renewable energy and commercial agriculture.
""If designed carefully, renewable energy development can be compatible with biodiversity conservation and ecosystem services to people,"" Neugarten said. ""Examples of this include livestock grazing under wind farms or cultivating native pollinator gardens under solar panels. But there's a real risk that achieving renewable energy goals could conflict with nature conservation goals without careful planning.""
This study is based on a global-scale optimization of land uses to identify joint priorities for biodiversity and nature's contributions to people.
""Focusing on regions of high conservation value that are also under high development pressure reveals some unlikely areas that don't always garner global conservation attention -- these include working landscapes in the southeastern United States, southern Brazil and Uruguay, southeastern Australia, and Eurasia,"" said Patrick Roehrdanz, Conservation International's director of climate change and biodiversity and a co-author on the study.
""These maps can help decision-makers in government, conservation NGOs, as well as donors and funding agencies to identify nature's key benefits for people and species -- benefits that are key to our survival and wellbeing,"" said World Wildlife Fund Global Biodiversity Lead Scientist Becky Chaplin-Kramer. ""Such spatial information is critical for implementation of conservation commitments made under the Kunming-Montreal Global Biodiversity Framework as well as climate mitigation and adaptation commitments made during the recent COP28 in Dubai.""

","score: 18.159703998232825, grade_level: '18'","score: 19.697291804727193, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41467-023-43832-9,"Meeting global commitments to conservation, climate, and sustainable development requires consideration of synergies and tradeoffs among targets. We evaluate the spatial congruence of ecosystems providing globally high levels of nature’s contributions to people, biodiversity, and areas with high development potential across several sectors. We find that conserving approximately half of global land area through protection or sustainable management could provide 90% of the current levels of ten of nature’s contributions to people and meet minimum representation targets for 26,709 terrestrial vertebrate species. This finding supports recent commitments by national governments under the Global Biodiversity Framework to conserve at least 30% of global lands and waters, and proposals to conserve half of the Earth. More than one-third of areas required for conserving nature’s contributions to people and species are also highly suitable for agriculture, renewable energy, oil and gas, mining, or urban expansion. This indicates potential conflicts among conservation, climate and development goals."
"
Peatlands are affected more by drought than expected. This is concerning, as these ecosystems are an important ally in the fight against climate change. Following long periods of drought, peat is able to absorb little to no extra carbon (CO2). Increasing biodiversity also does little to make peat more drought-resilient. These are the conclusions drawn by researchers from Radboud University in a publication appearing today in Proceedings of the Royal Society B.

Peat is a vast carbon sink: per square metre it is able to store more CO2 than any other ecosystem in the world. The peatlands of the Netherlands, but also those in places such as Scandinavia and the Baltic states, therefore play an important role in the fight against climate change. However, peat is coming under increasing pressure and is extremely sensitive to the dry summers we are experiencing as a result of climate change. This is what researchers from the Radboud Institute for Biological and Environmental Sciences have concluded.
'In our lab, under controlled conditions, we first ensured that large blocks of peat were well moistened over a long period of time', explains lead author Bjorn Robroek. 'We then slowly dried the peat out. One half was exposed to mild drought, with the water level roughly five centimetres lower than the peat itself. The other half was subjected to extreme drought conditions; in this case the water was twenty centimetres below the peat. This is comparable to a period of three weeks without rain -- something that has also become increasingly common in the Netherlands in recent years.'
These experiments revealed that peat exposed to mild drought still absorbs a reasonable amount of carbon. Robroek: 'Under extreme drought conditions, however, the peat can hardly take on any more carbon. In the event of an extended period of drought it even releases the carbon again.'
Biodiversity
Drought not only affects peatlands, of course. Dry summers have made other ecosystems more fragile too. However, in the case of grasslands, for example, we now have methods to combat problems caused by drought. Increasing the biodiversity in this kind of ecosystem (by incorporating a greater number of different plants), as in the case of the Future Dikes project, keeps the ecosystem healthy and resilient.
Nevertheless, according to Robroek, when it comes to peatlands, improving biodiversity in this way is of little use in terms of tackling drought. 'The different mosses that we tested in our peat experiments do little to nothing to combat drought. That does not mean that biodiversity is not important for peat: it helps with carbon storage, for example. But in the battle against drought a different approach is needed.'
Politics
There are little things that consumers can do to protect peat. 'Buy peat-free potting substrate and compost, for example', cautions Robroek. 'In the end, however, this is mainly a problem that will have to be solved at political level. In the past the buffer zones alongside rivers often consisted of peatland, but today much of this is grassland intended for agricultural use. These areas are constantly mowed and ploughed and therefore hardly retain any water. As a result, water from these floodplains drains more quickly into the rivers, causing flooding.'
'Switching over to natural management methods costs time and money, but will have huge benefits in the future. Peatlands, even lowland peat areas, will then retain considerably more water and therefore offer much better protection. You could compare this to a sponge that gradually releases water back to the landscape. In such places peat is also the most effective option when it comes to storing carbon.'

","score: 10.062772097467754, grade_level: '10'","score: 10.859763497372192, grade_levels: ['11'], ages: [16, 17]",10.1098/rspb.2023.2622,"Terrestrial wetland ecosystems challenge biodiversity–ecosystem function theory, which generally links high species diversity to stable ecosystem functions. An open question in ecosystem ecology is whether assemblages of co-occurring peat mosses contribute to the stability of peatland ecosystem processes. We conducted a two-species ( Sphagnum cuspidatum , Sphagnum medium ) replacement series mesocosm experiment to evaluate the resistance, resilience, and recovery rates of net ecosystem CO 2 exchange (NEE) under mild and deep water table drawdown. Our results show a positive effect of mild water table drawdown on NEE with no apparent role for peat moss mixture. Our study indicates that the carbon uptake capacity by peat moss mixtures is rather resilient to mild water table drawdown, but seriously affected by deeper drought conditions. Co-occurring peat moss species seem to enhance the resilience of the carbon uptake function (i.e. ability of NEE to return to pre-perturbation levels) of peat moss mixtures only slightly. These findings suggest that assemblages of co-occurring Sphagnum mosses do only marginally contribute to the stability of ecosystem functions in peatlands under drought conditions. Above all, our results highlight that predicted severe droughts can gravely affect the sink capacity of peatlands, with only a small extenuating role for peat moss mixtures."
"
Snow is one of the most contradictory cues we have for understanding climate change. As in many recent winters, the lack of snowfall in December seemed to preview our global warming future, with peaks from Oregon to New Hampshire more brown than white and the American Southwest facing a severe snow drought.

On the other hand, record blizzards like those in early 2023 that buried California mountain communities, replenished parched reservoirs, and dropped 11 feet of snow on northern Arizona defy our conceptions of life on a warming planet.
Similarly, scientific data from ground observations, satellites, and climate models do not agree on whether global warming is consistently chipping away at the snowpacks that accumulate in high-elevation mountains, complicating efforts to manage the water scarcity that would result for many population centers.
Now, a new Dartmouth study cuts through the uncertainty in these observations and provides evidence that seasonal snowpacks throughout most of the Northern Hemisphere have indeed shrunk significantly over the past 40 years due to human-driven climate change. The sharpest global warming-related reductions in snowpack -- between 10% to 20% per decade -- are in the Southwestern and Northeastern United States, as well as in Central and Eastern Europe.
The researchers report in the journal Nature that the extent and speed of this loss potentially puts the hundreds of millions of people in North America, Europe, and Asia who depend on snow for their water on the precipice of a crisis that continued warming will amplify.
""We were most concerned with how warming is affecting the amount of water stored in snow. The loss of that reservoir is the most immediate and potent risk that climate change poses to society in terms of diminishing snowfall and accumulation,"" said first author Alexander Gottlieb, a PhD student in the Ecology, Evolution, Environment and Society graduate program at Dartmouth.
""Our work identifies the watersheds that have experienced historical snow loss and those that will be most vulnerable to rapid snowpack declines with further warming,"" Gottlieb said. ""The train has left the station for regions such as the Southwestern and Northeastern United States. By the end of the 21st century, we expect these places to be close to snow-free by the end of March. We're on that path and not particularly well adapted when it comes to water scarcity.""
Water security is only one dimension of snow loss, said Justin Mankin, an associate professor of geography and the paper's senior author.

The Hudson, Susquehanna, Delaware, Connecticut, and Merrimack watersheds in the Northeastern U.S., where water scarcity is not as dire, experienced among the steepest declines in snowpack. But these heavy losses threaten economies in states such as Vermont, New York, and New Hampshire that depend on winter recreation, Mankin said -- even machine-made snow has a temperature threshold many areas are fast approaching.
""The recreational implications are emblematic of the ways in which global warming disproportionately affects the most vulnerable communities,"" Mankin said. ""Ski resorts at lower elevations and latitudes have already been contending with year-on-year snow loss. This will just accelerate, making the business model inviable.""
""We'll likely see further consolidation of skiing into large, well-resourced resorts at the expense of small and medium-sized ski areas that have such crucial local economic and cultural values. It will be a loss that will ripple through communities,"" he said.
In the study, Gottlieb and Mankin focused on how global warming's influence on temperature and precipitation drove changes in snowpack in 169 river basins across the Northern Hemisphere from 1981 through 2020. The loss of snowpacks potentially means less meltwater in spring for rivers, streams, and soils downstream when ecosystems and people demand water.
Gottlieb and Mankin programmed a machine learning model to examine thousands of observations and climate-model experiments that captured snowpack, temperature, precipitation, and runoff data for Northern Hemisphere watersheds.
This not only let them identify where snowpack losses occurred due to warming, it also gave them the ability to examine the counteracting influence of climate-driven changes in temperature and precipitation, which decrease and increase snowpack thickness, respectively.

The researchers identified the uncertainties that the models and observations shared so they could home in on what scientists had previously missed when gauging the effect of climate change on snow. A 2021 study by Gottlieb and Mankin similarly leveraged uncertainties in how scientists measure snow depth and define snow drought to improve predictions of water availability.
Snow comes with uncertainties that have masked the effects of global warming, Mankin said. ""People assume that snow is easy to measure, that it simply declines with warming, and that its loss implies the same impacts everywhere. None of these are the case,"" Mankin said.
""Snow observations are tricky at the regional scales most relevant for assessing water security,"" Mankin said. ""Snow is very sensitive to within-winter variations in temperature and precipitation, and the risks from snow loss are not the same in New England as in the Southwest, or for a village in the Alps as in high-mountain Asia.""
Gottlieb and Mankin in fact found that 80% of the Northern Hemisphere's snowpacks -- which are in its far-northern and high-elevation reaches -- experienced minimal losses. Snowpacks actually expanded in vast swaths of Alaska, Canada, and Central Asia as climate change increased the precipitation that falls as snow in these frigid regions.
But it is the remaining 20% of the snowpack that exists around -- and provides water for -- many of the hemisphere's major population centers that has diminished. Since 1981, documented declines in snowpack for these regions have been largely inconsistent due to the uncertainty in observations and naturally occurring variations in climate.
But Gottlieb and Mankin found that a steady pattern of annual declines in snow accumulation emerge quickly -- and leave population centers suddenly and chronically short on new supplies of water from snowmelt.
Many snow-dependent watersheds now find themselves dangerously near a temperature threshold Gottlieb and Mankin call a ""snow-loss cliff."" This means that as average winter temperatures in a watershed increase beyond 17 degrees Fahrenheit (minus 8 degrees Celsius), snow loss accelerates even with only modest increases in local average temperatures.
Many highly populated watersheds that rely on snow for water supply are going to see accelerating losses over the next few decades, Mankin said.
""It means that water managers who rely on snowmelt can't wait for all the observations to agree on snow loss before they prepare for permanent changes to water supplies. By then, it's too late,"" he said. ""Once a basin has fallen off that cliff, it's no longer about managing a short-term emergency until the next big snow. Instead, they will be adapting to permanent changes to water availability.""

","score: 14.509967388678643, grade_level: '15'","score: 16.263673001046783, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41586-023-06794-y,"Documenting the rate, magnitude and causes of snow loss is essential to benchmark the pace of climate change and to manage the differential water security risks of snowpack declines1–4. So far, however, observational uncertainties in snow mass5,6 have made the detection and attribution of human-forced snow losses elusive, undermining societal preparedness. Here we show that human-caused warming has caused declines in Northern Hemisphere-scale March snowpack over the 1981–2020 period. Using an ensemble of snowpack reconstructions, we identify robust snow trends in 82 out of 169 major Northern Hemisphere river basins, 31 of which we can confidently attribute to human influence. Most crucially, we show a generalizable and highly nonlinear temperature sensitivity of snowpack, in which snow becomes marginally more sensitive to one degree Celsius of warming as climatological winter temperatures exceed minus eight degrees Celsius. Such nonlinearity explains the lack of widespread snow loss so far and augurs much sharper declines and water security risks in the most populous basins. Together, our results emphasize that human-forced snow losses and their water consequences are attributable—even absent their clear detection in individual snow products—and will accelerate and homogenize with near-term warming, posing risks to water resources in the absence of substantial climate mitigation."
"
Giants once roamed the karst plains of southern China, three-metre tall apes weighing in at 250 kilograms. These very distant human ancestors - Gigantopithcus blacki - went extinct before humans arrived in the region, with few clues to why, and so far leaving around 2000 fossilised teeth and four jawbones as the only signs of their existence.

New evidence from this region published in Nature, uncovered by a team of Chinese, Australian and US researchers, demonstrates beyond doubt that the largest primate to walk the earth went extinct between 295,000 and 215,000 years ago, unable to adapt its food preferences and behaviours, and vulnerable to the changing climates which sealed its fate.
""The story of G. blacki is an enigma in palaeontology - how could such a mighty creature go extinct at a time when other primates were adapting and surviving? The unresolved cause of its disappearance has become the Holy Grail in this discipline,"" says palaeontologist and co-lead author Professor Yingqi Zhang, from the Institute of Vertebrate Palaeontology and Palaeoanthropology at the Chinese Academy of Sciences (IVPP).
""The IVPP has been excavating for G. blacki evidence in this region for over 10 years but without solid dating and a consistent environmental analysis, the cause of its extinction had eluded us.""
Definitive evidence revealing the story of the giant ape's extinction has come from a large-scale project collecting evidence from 22 cave sites spread across a wide region of Guangxi Province in southern China. The foundation of this study was the dating.
""It's a major feat to present a defined cause for the extinction of a species, but establishing the exact time when a species disappears from the fossil record gives us a target timeframe for an environmental reconstruction and behaviour assessment,"" says co-lead author, Macquarie University geochronologist Associate Professor Kira Westaway.
""Without robust dating, you are simply looking for clues in the wrong places.""
Six Australian universities contributed to the project. Macquarie University, Southern Cross University, Wollongong University and the University of Queensland used multiple techniques to date samples. Southern Cross also mapped G. blacki teeth to extract information on the apes' behaviours. ANU and Flinders University studied the pollen and fossil bearing sediments in the cave respectively, to reconstruct the environments in which G. blacki thrived and then disappeared.

Six different dating techniques were applied to the cave sediments and fossils, producing 157 radiometric ages. These were combined with eight sources of environmental and behavioural evidence, and applied to 11 caves containing evidence of G blacki, and also to 11 caves of a similar age range where no G. blacki evidence was found.
Luminescence dating, which measures a light-sensitive signal found in the burial sediments that encased the G. blacki fossils, was the primary technique, supported by uranium series (US) and electron-spin resonance (US-ESR) dating of the G. blacki teeth themselves.
""By direct-dating the fossil remains, we confirmed their age aligns with the luminescence sequence in the sediments where they were found, giving us a comprehensive and reliable chronology for the extinction of G. blacki,"" says Southern Cross University geochronologist Associate Professor Renaud Joannes-Boyau.
Using detailed pollen analysis, fauna reconstructions, stable isotope analysis of the teeth and a detailed analysis of the cave sediments at a micro level, the team established the environmental conditions leading up to when G blacki went extinct. Then, using trace element and dental microwear textural analysis (DMTA) of the apes' teeth, the team modelled G. blacki's behaviour while it was flourishing, compared to during the species' demise.
""Teeth provide a staggering insight into the behaviour of the species indicating stress, diversity of food sources, and repeated behaviours,"" says Associate Professor Joannes-Boyau
The findings show G.blacki went extinct between 295,000 and 215,000 years ago, much earlier than previously assumed. Before this time, G. blacki flourished in a rich and diverse forest.

By 700,000 to 600,000 years ago, the environment became more variable due to the increase in the strength of the seasons, causing a change in the structure of the forest communities.
Orangutans (genus Pongo) - a close relative of G. blacki - adapted their size, behaviour and habitat preferences as conditions changed. In comparison, G. blacki relied on a less nutritious back up food source when its preferences were unavailable, decreasing the diversity of its food. The ape became less mobile, had a reduced geographic range for foraging, and faced chronic stress and dwindling numbers.
""G. blacki was the ultimate specialist, compared to the more agile adapters like orangutans, and this ultimately led to its demise,"" says Professor Zhang.
Associate Professor Westaway says: ""With the threat of a sixth mass extinction event looming over us, there is an urgent need to understand why species go extinct.
""Exploring the reasons for past unresolved extinctions gives us a good starting point to understand primate resilience and the fate of other large animals, in the past and future.""

","score: 16.202890282131662, grade_level: '16'","score: 17.720028840125394, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41586-023-06900-0,"The largest ever primate and one of the largest of the southeast Asian megafauna, Gigantopithecus blacki1, persisted in China from about 2.0 million years until the late middle Pleistocene when it became extinct2–4. Its demise is enigmatic considering that it was one of the few Asian great apes to go extinct in the last 2.6 million years, whereas others, including orangutan, survived until the present5. The cause of the disappearance of G. blacki remains unresolved but could shed light on primate resilience and the fate of megafauna in this region6. Here we applied three multidisciplinary analyses—timing, past environments and behaviour—to 22 caves in southern China. We used 157 radiometric ages from six dating techniques to establish a timeline for the demise of G. blacki. We show that from 2.3 million years ago the environment was a mosaic of forests and grasses, providing ideal conditions for thriving G. blacki populations. However, just before and during the extinction window between 295,000 and 215,000 years ago there was enhanced environmental variability from increased seasonality, which caused changes in plant communities and an increase in open forest environments. Although its close relative Pongo weidenreichi managed to adapt its dietary preferences and behaviour to this variability, G. blacki showed signs of chronic stress and dwindling populations. Ultimately its struggle to adapt led to the extinction of the greatest primate to ever inhabit the Earth."
"
Researchers at Stockholm University have for the first time been able to study the surface of iron and ruthenium catalysts when ammonia is formed from nitrogen and hydrogen; the results are published in the scientific journal Nature. A better knowledge of the catalytic process and the possibility of finding even more efficient materials opens the door for a green transition in the currently very CO2-intensive chemical industry.

Ammonia, produced in the Haber-Bosch process, is currently one of the most essential base chemicals for the world to produce fertilizers, with an annual production of 110 million tones. The journal Nature proposed in 2001 that the Haber-Bosch process was the most critical scientific invention for humankind during the 20th century, since it has saved around 4 billion people's lives by preventing mass starvation. An estimation of the nitrogen content in our bodies' DNA and proteins shows that half of the atoms can be derived from Haber-Bosch.
""In spite of 3 Nobel Prizes (1918, 1931, and 2007) for the Haber-Bosch process, it has not been possible to experimentally investigate the catalyst surface with surface-sensitive methods under real ammonia production conditions; experimental techniques with surface sensitivity at high enough pressures and temperatures had not been achievable. Consequently, different hypotheses about the state of the iron catalyst as being metallic or in a nitride, as well as the nature of the intermediate species of importance to the reaction mechanism, could not be unambiguously verified,"" says Anders Nilsson, professor of Chemical Physics at Stockholm University.
""What enabled this study is that we have built a photoelectron spectroscopy instrument in Stockholm that allows studies of catalyst surfaces under high pressures. Thereby, we have been able to observe what happens when the reaction occurs directly,"" says David Degerman, Postdoc in Chemical Physics at Stockholm University. ""We have opened a new door into understanding ammonia production catalysis with our new instrument where we now can detect reaction intermediates and provide evidence for the reaction mechanism.""
""To have our Stockholm instrument at one of the brightest x-ray sources in the world at PETRA III in Hamburg has been crucial to conduct the study,"" says Patrick Lömker, Researcher at Stockholm University. ""We can now imagine the future with even brighter sources when the machine upgrades to PETRA IV.""
""We now have the tools to conduct research leading to new catalyst materials for ammonia production that can be used better to fit together with electrolysis-produced hydrogen for the green transition of the chemical industry,"" says Anders Nilsson.
""It is inspiring to conduct research on a topic that is so linked to a scientific success story that has helped humanity tremendously. I am eager to continue research to find new catalysts that can lessen our dependence on fossil sources. The chemical industry alone accounts for 8% of the world-wide CO2 emissions,"" says Bernadette Davies, PhD student in Materials Chemistry at Stockholm University.
""The long-term prospect of carrying out ammonia production through an electrocatalytic alternative that is directly driven by solar, or wind electricity is most appealing, and now we have tools to scientifically assist in this development,"" says Sergey Koroidov, Researcher at Stockholm University.
The study was conducted in collaboration with Deutsches Elektronen-Synchrotron (DESY) in Hamburg and the Montan University in Austria. The study included former employees at the University, Chris Goodwin, Peter Amann, Mikhail Shiplin, Jette Mathiesen and Gabriel Rodrigez.

","score: 17.17052173913044, grade_level: '17'","score: 18.832500686498854, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41586-023-06844-5,"The large-scale conversion of N2 and H2 into NH3 (refs. 1,2) over Fe and Ru catalysts3 for fertilizer production occurs through the Haber–Bosch process, which has been considered the most important scientific invention of the twentieth century4. The active component of the catalyst enabling the conversion was variously considered to be the oxide5, nitride2, metallic phase or surface nitride6, and the rate-limiting step has been associated with N2 dissociation7–9, reaction of the adsorbed nitrogen10 and also NH3 desorption11. This range of views reflects that the Haber–Bosch process operates at high temperatures and pressures, whereas surface-sensitive techniques that might differentiate between different mechanistic proposals require vacuum conditions. Mechanistic studies have accordingly long been limited to theoretical calculations12. Here we use X-ray photoelectron spectroscopy—capable of revealing the chemical state of catalytic surfaces and recently adapted to operando investigations13 of methanol14 and Fischer–Tropsch synthesis15—to determine the surface composition of Fe and Ru catalysts during NH3 production at pressures up to 1 bar and temperatures as high as 723 K. We find that, although flat and stepped Fe surfaces and Ru single-crystal surfaces all remain metallic, the latter are almost adsorbate free, whereas Fe catalysts retain a small amount of adsorbed N and develop at lower temperatures high amine (NHx) coverages on the stepped surfaces. These observations indicate that the rate-limiting step on Ru is always N2 dissociation. On Fe catalysts, by contrast and as predicted by theory16, hydrogenation of adsorbed N atoms is less efficient to the extent that the rate-limiting step switches following temperature lowering from N2 dissociation to the hydrogenation of surface species."
"
A major international collaboration of 356 scientists led by UCL researchers has found almost identical patterns of tree diversity across the world's tropical forests.

The study of over one million trees across 1,568 locations, published in Nature, found that just 2.2% of tree species make up 50% of the total number of trees in tropical forests across Africa, the Amazon, and Southeast Asia. Each continent consists of the same proportion of a few common species and many rare species.
While tropical forests are famous for their diversity, this is the first time that scientists have studied the commonest trees in the world's tropical forests.
The scientists estimate that just 1,053 species account for half of the planet's 800 billion tropical forest trees. The other half are comprised of 46,000 tree species. The number of rare species is extreme, with the rarest 39,500 species accounting for just 10% of trees.
Lead author Dr Declan Cooper (UCL Geography and UCL Centre for Biodiversity and Environment Research) said: ""Our findings have profound implications for understanding tropical forests. If we focus on understanding the commonest tree species, we can probably predict how the whole forest will respond to today's rapid environmental changes. This is especially important because tropical forests contain a tremendous amount of stored carbon, and are a globally important carbon sink.""
He continued: ""Identifying the prevalence of the most common species gives scientists a new way of looking at tropical forests. Tracking these common species may provide a new way to characterise these forests and in the future possibly gauge a forest's health more easily.""
The researchers found strikingly similar patterns in the proportion of tree species that are common, at close to 2.2%, despite the tropical forests of the Amazon, Africa and Southeast Asia each having a unique history and differing contemporary environments.

The Amazon consists of a large region of connected forest, while Southeast Asia is a region of mostly disconnected islands. People only arrived in the Amazon around 20,000 years ago, but people have been living in African and Southeast Asian forests for more than twice that length of time. In terms of the contemporary environment, African forests experience a drier and cooler climate than the other two tropical forest regions.
Given these striking differences, the near-identical patterns of tree diversity suggests that a fundamental mechanism may govern the assembly of tree communities across all the world's tropical forests. The researchers are not yet able to say what that mechanism might be and it will focus future work on identifying it.
The estimates of common species derive from statistical analyses, which does not provide the names of the common trees. To overcome this, the scientists used a technique known as resampling to estimate which are the most likely names of the common species. Their list of 1,119 tree species names, the first list of common species of the world's tropical forests, will allow researchers to focus their efforts on understanding the ecology of these species, which in turn can give scientists a short-cut to understand the whole forest.
See table below for a list of the most common tropical tree species.
Senior author, Professor Simon Lewis (UCL Geography and University of Leeds) said: ""We wanted to look at tropical forests in a new way. Focusing on a few hundred common tree species on each continent, rather than the many thousands of species that we know almost nothing about, can open new ways to understand these precious forests. This focus on the commonest species should not take away from the importance of rare species. Rare species need special attention to protect them, but quick and important gains in knowledge will come from a scientific focus on the commonest tree species.""
The researchers assembled forest inventory data from intact tropical forests that hadn't been affected by logging or fire. In each of 1,568 locations, teams identified and recorded every tree with a trunk greater than 10 centimetres in diameter, in a patch of forest, usually one hectare, which is a square of forest measuring 100 metres on each side.
Professor Lewis has been collecting and collating this data for 20 years. The effort is a collaboration of the largest plot networks across the Amazon (Amazon Tree Diversity Network; RAINFOR), Africa (African Tropical Rainforest Observatory Network, AfriTRON; Central African Plot Network), and Southeast Asia (Slik Diversity Network; T-FORCES), brought together for the first time for the published analysis.
This collaboration across hundreds of researchers, field assistants, and local communities resulted in a total of 1,003,805 trees sampled, which included 8,493 tree species, across 2,048 hectares, equivalent to almost eight square miles of forest. The teams inventoried 1,097 plots in the Amazon totalling 1,434 hectares, 368 plots in Africa totalling 450 hectares, and 103 plots in Southeast Asia totalling 164 hectares.
This research was supported by the Natural Environmental Research Council.

","score: 13.673866889756464, grade_level: '14'","score: 15.42782227179773, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41586-023-06820-z,"Trees structure the Earth’s most biodiverse ecosystem, tropical forests. The vast number of tree species presents a formidable challenge to understanding these forests, including their response to environmental change, as very little is known about most tropical tree species. A focus on the common species may circumvent this challenge. Here we investigate abundance patterns of common tree species using inventory data on 1,003,805 trees with trunk diameters of at least 10 cm across 1,568 locations1–6 in closed-canopy, structurally intact old-growth tropical forests in Africa, Amazonia and Southeast Asia. We estimate that 2.2%, 2.2% and 2.3% of species comprise 50% of the tropical trees in these regions, respectively. Extrapolating across all closed-canopy tropical forests, we estimate that just 1,053 species comprise half of Earth’s 800 billion tropical trees with trunk diameters of at least 10 cm. Despite differing biogeographic, climatic and anthropogenic histories7, we find notably consistent patterns of common species and species abundance distributions across the continents. This suggests that fundamental mechanisms of tree community assembly may apply to all tropical forests. Resampling analyses show that the most common species are likely to belong to a manageable list of known species, enabling targeted efforts to understand their ecology. Although they do not detract from the importance of rare species, our results open new opportunities to understand the world’s most diverse forests, including modelling their response to environmental change, by focusing on the common species that constitute the majority of their trees."
"
From intensifying wildfires to record-breaking floods year on year, the effects of climate change have manifested in devastating outcomes on ecosystems that threaten species all over the world. One such ecosystem in peril is coral reefs, which play a major role in sustaining biodiversity in the planet's oceans but are facing increasingly severe conditions as waters heat up leading to a phenomenon known as marine heat waves.

For nearly a decade, Katie Barott, assistant professor of biology at the University of Pennsylvania, has led a collaborative team of researchers studying two coral species in Hawaii to better understand their adaptability to the effects of climate change. Their recent paper published in the Proceedings of the National Academy of Sciences sheds light on this issue, revealing the complex and varied ways corals are adapting, or struggling to adapt, to the rapidly changing oceanic environment.
""We tracked more than 40 large coral colonies over 10 years and found that certain species have an improved ability to endure and recover from subsequent marine heat waves after surviving one such event,"" Barott says. ""It's a bit like working out; the more often you exercise, the easier it is to go through the same exercise stress.""
The researchers studied two dominant coral species in Kaneohe Bay in Oahu, Hawaii: rice coral, Montipora capitata, and finger coral, Porites compressa. Over the decade, these corals were subjected to significant marine heat waves in 2014, 2015, and 2019. These provided a unique opportunity to identify bleaching-resistant and bleaching-susceptible individuals of each species and then observe their responses to repeated heat stress. Their findings highlight the resilience of some corals while underscoring the vulnerability of others.
""One of our key discoveries is the role of ""acclimatization,"" says Kristen Brown, first author of the paper and a postdoctoral researcher in the Barott Lab. ""This refers to the ability of some corals to adjust to higher temperatures, thereby reducing their susceptibility to bleaching, a phenomenon wherein corals expel the algae living in their tissues, causing them to turn white and increasing the risk of death.""
The researchers found that bleaching-resistant individuals of both coral species remained pigmented throughout the study period, suggesting a persistent form of thermal tolerance; however, pigmentation alone was not a definitive indicator of overall health or resilience.
The researchers reveal contrasting recovery patterns between the bleaching-susceptible individuals of each species following the heat waves. Montipora capitata, despite some evidence of acclimatization, repeatedly experienced bleaching and showed significant mortality for as long as three years after the last heat wave; conversely, initially sensitive individuals of Porites compressa exhibited a remarkable capacity for recovery and acclimatization, with no bleaching or mortality during the third heat wave and most physiological metrics returning to normal within one year. This difference underscores a critical aspect of coral resilience: the ability to not only survive heat stress but to effectively recover from it.
The researchers suggest that coral responses to climate change are diverse and complex, influenced by a range of factors, including species-specific characteristics and past exposure to environmental stressors. And, as a next step, the team plans to continue monitoring and exploring aspects like coral growth, calcification, and the impacts of recurring marine heatwaves.

","score: 17.928236982369828, grade_level: '18'","score: 19.64352603526035, grade_levels: ['college_graduate'], ages: [24, 100]",10.1073/pnas.2312104120,"Increasingly frequent marine heatwaves are devastating coral reefs. Corals that survive these extreme events must rapidly recover if they are to withstand subsequent events, and long-term survival in the face of rising ocean temperatures may hinge on recovery capacity and acclimatory gains in heat tolerance over an individual’s lifespan. To better understand coral recovery trajectories in the face of successive marine heatwaves, we monitored the responses of bleaching-susceptible and bleaching-resistant individuals of two dominant coral species in Hawai’i, Montipora capitata and Porites compressa , over a decade that included three marine heatwaves. Bleaching-susceptible colonies of P. compressa exhibited beneficial acclimatization to heat stress (i.e., less bleaching) following repeat heatwaves, becoming indistinguishable from bleaching-resistant conspecifics during the third heatwave. In contrast, bleaching-susceptible M. capitata repeatedly bleached during all successive heatwaves and exhibited seasonal bleaching and substantial mortality for up to 3 y following the third heatwave. Encouragingly, bleaching-resistant individuals of both species remained pigmented across the entire time series; however, pigmentation did not necessarily indicate physiological resilience. Specifically, M. capitata displayed incremental yet only partial recovery of symbiont density and tissue biomass across both bleaching phenotypes up to 35 mo following the third heatwave as well as considerable partial mortality. Conversely, P. compressa appeared to recover across most physiological metrics within 2 y and experienced little to no mortality. Ultimately, these results indicate that even some visually robust, bleaching-resistant corals can carry the cost of recurring heatwaves over multiple years, leading to divergent recovery trajectories that may erode coral reef resilience in the Anthropocene."
"
The acidity of Antarctica's coastal waters could double by the end of the century, threatening whales, penguins and hundreds of other species that inhabit the Southern Ocean, according to new research from the Univeristy of Colorado Boulder.

Scientists projected that by 2100, the upper 650 feet (200 meters) of the ocean -- where much marine life resides -- could see more than a 100% increase in acidity compared with 1990s levels. The paper, appeared Jan. 4 in the journal Nature Communications.
""The findings are critical for our understanding of the future evolution of marine ecosystem health,"" said Nicole Lovenduski, the paper's co-author and the interim director of CU Boulder's Institute of Arctic and Alpine Research (INSTAAR).
The oceans play an important role as a buffer against climate change by absorbing nearly 30% of the CO2 emitted worldwide. But as more CO2 dissolves in the oceans, the seawater becomes more acidic. ""Human-caused CO2 emissions are at the heart of ocean acidification,"" said Cara Nissen, the paper's first author and a research scientist at INSTAAR.
The Southern Ocean, which surrounds Antarctica, is particularly susceptible to acidification, partly because colder water tends to absorb more CO2. Ocean currents in the area also contribute to the relatively acidic water conditions.
Using a computer model, Nissen, Lovenduski and the team simulated how the seawater of the Southern Ocean would change in the 21st century. They found it would become more acidic by 2100, and the situation would be severe if the world fails to cut emissions.
""It's not just the top layer of the ocean. The entire water column of the coastal Southern Ocean, even at the bottom, could experience severe acidification,"" Nissen said.

The team then investigated the conditions specifically in Antarctica's marine protected areas (MPAs). Human activities, such as fishing, are restricted in these regions to protect biodiversity. Currently, there are two MPAs in the Southern Ocean, covering about 12% of water in the region. Scientists have proposed designating three more MPAs to an international council in the past years, which would encompass about 60% of the Antarctic Ocean.
The team's model showed that both adopted and proposed MPAs would experience significant acidification by the end of the century.
For example, under the highest-emission scenario, where the world makes no efforts to cut emissions, the average acidity of the water in the Ross Sea region -- the world's largest MPA off the northern tip of Antarctica -- would increase by 104% over 1990s levels by 2100. Under an intermediate emissions scenario, the water would still become 43% more acidic.
""It's surprising to me how severe ocean acidification would be in these coastal waters,"" Nissen said.
Previous studies have shown that phytoplankton, a group of algae that forms the basis of the marine food web, grow at a slower rate or die out when the water becomes too acidic. Acidic water also weakens the shells of organisms like sea snails and sea urchins. These changes could disrupt the food web, eventually impacting top predators like whales and penguins.
The Weddell Sea is one of the three proposed MPAs located off the coast of the Antarctic Peninsula. Nissen said scientists think the Weddell Sea region could act as a climate change sanctuary for organisms, mainly because this area has the highest levels of sea ice coverage in the Antarctic. The ice shields the ocean from warming and prevents the seawater underneath from absorbing CO2 from the air, thereby reducing the rate of acidification. In addition, the region has little human activity to date.

But the model suggested that as the planet continues to warm, the sea ice will melt, and the Weddell Sea region will experience acidification on par with other MPAs under intermediate to high emission scenarios, but with a slightly delayed progression.
""The result shows that establishing the Weddell Sea region as a protected area should have high priority,"" Nissen said.
""As a scientist who typically studies the open ocean, I tend to think of Antarctic coastal areas as a conduit for climate signals to reach the global, deep ocean. This study reminded me that these dynamic Antarctic coastal areas are also themselves capable of rapid change,"" Lovenduski said.
The study suggests that the world could only avoid severe ocean acidification of the Southern Ocean under the lowest emission scenario, where society cuts CO2 emissions quickly and aggressively.
""We still have time to select our emission pathway, but we don't have much,"" Nissen said.

","score: 12.805393592856586, grade_level: '13'","score: 13.23310566303752, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41467-023-44438-x,"Antarctic coastal waters are home to several established or proposed Marine Protected Areas (MPAs) supporting exceptional biodiversity. Despite being threatened by anthropogenic climate change, uncertainties remain surrounding the future ocean acidification (OA) of these waters. Here we present 21st-century projections of OA in Antarctic MPAs under four emission scenarios using a high-resolution ocean–sea ice–biogeochemistry model with realistic ice-shelf geometry. By 2100, we project pH declines of up to 0.36 (total scale) for the top 200 m. Vigorous vertical mixing of anthropogenic carbon produces severe OA throughout the water column in coastal waters of proposed and existing MPAs. Consequently, end-of-century aragonite undersaturation is ubiquitous under the three highest emission scenarios. Given the cumulative threat to marine ecosystems by environmental change and activities such as fishing, our findings call for strong emission-mitigation efforts and further management strategies to reduce pressures on ecosystems, such as the continuation and expansion of Antarctic MPAs."
"
A study published today in IOP Publishing's journal Environmental Research: Infrastructure and Sustainability has found that green ammonia could be used to fulfil the fuel demands of over 60% of global shipping by targeting just the top 10 regional fuel ports. Researchers at the University of Oxford looked at the production costs of ammonia which are similar to very low sulphur fuels, and concluded that the fuel could be a viable option to help decarbonise international shipping by 2050.

Around USD 2 trillion will be needed to transition to a green ammonia fuel supply chain by 2050, primarily to finance supply infrastructure. The study shows that the greatest investment need is in Australia, to supply the Asian markets, with large production clusters also predicted in Chile (to supply South America), California (to supply Western U.S.A.), North-West Africa (to meet European demand), and the southern Arabian Peninsula (to meet local demand and parts of south Asia).
90% of world's physical goods trade is transported by ships which burn heavy fuel oil and emit toxic pollutants. This accounts for nearly 3% of the global greenhouse gas (GHG) emissions. As a result of this, the International Maritime Organization (IMO) committed to decarbonising international shipping in 2018, aiming to halve GHG emissions by 2050. These targets have been recently revised to net zero emissions by 2050.
After investigating the viability of diesel vessel exhaust scrubbers, green ammonia, made by electrolysing water with renewable electricity, was proposed as an alternative fuel source to quickly decarbonise the shipping industry. However, historically there has been great uncertainty as to how and where to invest to create the necessary infrastructure to deliver an efficient, viable fuel supply chain.
René Bañares-Alcántara, Professor of Chemical Engineering in the Department of Engineering Science at the University of Oxford, says: ""Shipping is one of the most challenging sectors to decarbonize because of the need for fuel with high energy density and the difficulty of coordinating different groups to produce, utilize and finance alternative (green) fuel supplies.""
To guide investors, the team at the University of Oxford developed a modelling framework to create viable scenarios for how to establish a global green ammonia fuel supply chain. The framework combines a fuel demand model, future trade scenarios and a spatial optimisation model for green ammonia production, storage, and transport, to find the best locations to meet future demand for shipping fuel.
Professor Bañares-Alcántara continues: ""The implications of this work are striking. Under the proposed model, current dependence upon oil-producing nations would be replaced by a more regionalised industry; green ammonia will be produced near the equator in countries with abundant land and high solar potential then transported to regional centres of shipping fuel demand.""

","score: 17.28480701754386, grade_level: '17'","score: 19.01394736842105, grade_levels: ['college_graduate'], ages: [24, 100]",10.1088/2634-4505/ad097a,"Green ammonia has been proposed as a technologically viable solution to decarbonise global shipping, yet there are conflicting ambitions for where global production, transport and fuelling infrastructure will be located. Here, we develop a spatial modelling framework to quantify the cost-optimal fuel supply to decarbonise shipping in 2050 using green ammonia. We find that the demand for green ammonia by 2050 could be three to four times the current (grey) ammonia production, requiring major new investments in infrastructure. Our model predicts a regionalisation of supply, entailing a few large supply clusters that will serve regional demand centres, with limited long-distance shipping of green ammonia fuel. In this cost-efficient model, practically all green ammonia production is predicted to lie within 40° latitudes North/South. To facilitate this transformation, investments worth USD 2 trillion would be needed, half of which will be required in low- and middle-income countries."
"
As climate change fuels sea level rise, younger people will migrate inland, leaving aging coastal populations -- and a host of consequences -- in their wake, a study by Florida State University researchers finds.

While destination cities will work to sustainably accommodate swelling populations, aging coastal communities will confront stark new challenges, including an outflow of vital human infrastructure such as health care workers, said Associate Professor of Sociology Matt Hauer, lead author of the study published in the Proceedings of the National Academy of Sciences.
""In the destination communities where populations are increasing you'll need more dentists, doctors, service workers, construction workers, etc.,"" Hauer said. ""So by people moving, you affect other people's likelihood of moving. You get a demographic amplification.""
Previous studies estimated where people are likely to move as a changing climate affects livability. Hauer's study also incorporates demographic data and secondary effects that revealed a host of challenges awaiting both the coastal ""sender"" communities and their destination counterparts.
""Imagine young families moving out of areas like Miami and moving to other locations and starting a family there,"" he said. ""And just by the fact that there's more people who have moved there, these indirect population processes draw even more people.""
The study concluded that these indirect processes could create 5.3 to 18 times the number of climate migrants as those directly displaced by rising seas. It also found that by 2100, median age in coastal communities could spike as much as 10 years.
""Think about who are more unlikely to move and who will be left behind in these communities; it tends to be the oldest,"" Hauer said. ""Because migration is most likely to occur in more youthful populations, areas experiencing accelerated out-migration could face accelerated population aging.""
Doctoral student Sunshine Jacobs and computational scientist Scott Kulp co-authored the study with Hauer.

The researchers developed a migration model that uses sea level rise data from Climate Central and information about migration patterns from the U.S. Internal Revenue Service. That tool allowed them to predict migration on a county-by-county basis across the country. Jacobs said the model can be adapted to research different hazards that go beyond encroaching seas.
""We only looked at sea level rise,"" she said. ""Imagine other hazards that we know cause people to move, like heat events, wildfires and economic hazards. The future uses and implications of the model are amazing.""
This work was supported by the State of Louisiana, the American Society of Adaptation Professionals, the New York State Energy Research Development Authority, and the Great Lakes Integrated Sciences Assessment.

","score: 13.516762974852863, grade_level: '14'","score: 14.038271803103264, grade_levels: ['college_graduate'], ages: [24, 100]",10.1073/pnas.2206192119,"The warnings of potential climate migration first appeared in the scientific literature in the late 1970s when increased recognition that disintegrating ice sheets could drive people to migrate from coastal cities. Since that time, scientists have modeled potential climate migration without integrating other population processes, potentially obscuring the demographic amplification of this migration. Climate migration could amplify demographic change—enhancing migration to destinations and suppressing migration to origins. Additionally, older populations are the least likely to migrate, and climate migration could accelerate population aging in origin areas. Here, we investigate climate migration under sea-level rise (SLR), a single climatic hazard, and examine both the potential demographic amplification effect and population aging by combining matrix population models, flood hazard models, and a migration model built on 40 y of environmental migration in the United States to project the US population distribution of US counties. We find that the demographic amplification of SLR for all feasible Representative Concentration Pathway-Shared Socioeconomic Pathway (RCP–SSP) scenarios in 2100 ranges between 8.6–28 M [5.7–53 M]—5.3 and 18 times the number of migrants (0.4–10 M). We also project significant aging of coastal areas as youthful populations migrate but older populations remain, accelerating population aging in origin areas. As the percentage of the population lost due to climate migration increases, the median age also increases—up to 10+ y older in some highly impacted coastal counties. Additionally, our population projection approach can be easily adapted to investigate additional or multiple climate hazards."
"
In recent years, there has been rising concern that tiny particles known as microplastics are showing up basically everywhere on Earth, from polar ice to soil, drinking water and food. Formed when plastics break down into progressively smaller bits, these particles are being consumed by humans and other creatures, with unknown potential health and ecosystem effects. One big focus of research: bottled water, which has been shown to contain tens of thousands of identifiable fragments in each container.

Now, using newly refined technology, researchers have entered a whole new plastic world: the poorly known realm of nanoplastics, the spawn of microplastics that have broken down even further. For the first time, they counted and identified these minute particles in bottled water. They found that on average, a liter contained some 240,000 detectable plastic fragments -- 10 to 100 times greater than previous estimates, which were based mainly on larger sizes.
The study was just published in the journal Proceedings of the National Academy of Sciences.
Nanoplastics are so tiny that, unlike microplastics, they can pass through intestines and lungs directly into the bloodstream and travel from there to organs including the heart and brain. They can invade individual cells, and cross through the placenta to the bodies of unborn babies. Medical scientists are racing to study the possible effects on a wide variety of biological systems.
""Previously this was just a dark area, uncharted. Toxicity studies were just guessing what's in there,"" said study coauthor Beizhan Yan, an environmental chemist at Columbia University's Lamont-Doherty Earth Observatory. ""This opens a window where we can look into a world that was not exposed to us before.""
Worldwide plastic production is approaching 400 million metric tons a year. More than 30 million tons are dumped yearly in water or on land, and many products made with plastics including synthetic textiles shed particles while still in use. Unlike natural organic matter, most plastics do not break down into relatively benign substances; they simply divide and redivide into smaller and smaller particles of the same chemical composition. Beyond single molecules, there is no theoretical limit to how small they can get.
Microplastics are defined as fragments ranging from 5 millimeters (less than a quarter inch) down to 1 micrometer, which is 1 millionth of a meter, or 1/25,000th of an inch. (A human hair is about 70 micrometers across.) Nanoplastics, which are particles below 1 micrometer, are measured in billionths of a meter.

Plastics in bottled water became a public issue largely after a 2018 study detected an average of 325 particles per liter; later studies multiplied that number many times over. Scientists suspected there were even more than they had yet counted, but good estimates stopped at sizes below 1 micrometer -- the boundary of the nano world.
""People developed methods to see nano particles, but they didn't know what they were looking at,"" said the new study's lead author, Naixin Qian, a Columbia graduate student in chemistry. She noted that previous studies could provide bulk estimates of nano mass, but for the most part could not count individual particles, nor identify which were plastics or something else.
The new study uses a technique called stimulated Raman scattering microscopy, which was co-invented by study coauthor Wei Min, a Columbia biophysicist. This involves probing samples with two simultaneous lasers that are tuned to make specific molecules resonate. Targeting seven common plastics, the researchers created a data-driven algorithm to interpret the results. ""It is one thing to detect, but another to know what you are detecting,"" said Min.
The researchers tested three popular brands of bottled water sold in the United States (they declined to name which ones), analyzing plastic particles down to just 100 nanometers in size. They spotted 110,000 to 370,000 particles in each liter, 90% of which were nanoplastics; the rest were microplastics. They also determined which of the seven specific plastics they were, and charted their shapes -- qualities that could be valuable in biomedical research.
One common one was polyethylene terephthalate or PET. This was not surprising, since that is what many water bottles are made of. (It is also used for bottled sodas, sports drinks and products such as ketchup and mayonnaise.) It probably gets into the water as bits slough off when the bottle is squeezed or gets exposed to heat. One recent study suggests that many particles enter the water when you repeatedly open or close the cap, and tiny bits abrade.
However, PET was outnumbered by polyamide, a type of nylon. Ironically, said Beizhan Yan, that probably comes from plastic filters used to supposedly purify the water before it is bottled. Other common plastics the researchers found: polystyrene, polyvinyl chloride and polymethyl methacrylate, all used in various industrial processes.

A somewhat disturbing thought: the seven plastic types the researchers searched for accounted for only about 10% of all the nanoparticles they found in samples; they have no idea what the rest are. If they are all nanoplastics, that means they could number in the tens of millions per liter. But they could be almost anything, ""indicating the complicated particle composition inside the seemingly simple water sample,"" the authors write. ""The common existence of natural organic matter certainly requires prudent distinguishment.""
The researchers are now reaching beyond bottled water. ""There is a huge world of nanoplastics to be studied,"" said Min. He noted that by mass, nanoplastics comprise far less than microplastics, but ""it's not size that matters. It's the numbers, because the smaller things are, the more easily they can get inside us.""
Among other things, the team plans to look at tap water, which also has been shown to contain microplastics, though far less than bottled water. Beizhan Yan is running a project to study microplastics and nanoplastics that end up in wastewater when people do laundry -- by his count so far, millions per 10-pound load, coming off synthetic materials that comprise many items. (He and colleagues are designing filters to reduce the pollution from commercial and residential washing machines.) The team will soon identify particles in snow that British collaborators trekking by foot across western Antarctica are currently collecting. They also are collaborating with environmental health experts to measure nanoplastics in various human tissues and examine their developmental and neurologic effects.
""It is not totally unexpected to find so much of this stuff,"" said Qian. ""The idea is that the smaller things get, the more of them there are.""
The study was coauthored by Xin Gao and Xiaoqi Lang of the Columbia chemistry department; Huipeng Deng and Teodora Maria Bratu of Lamont-Doherty; Qixuan Chen of Columbia's Mailman School of Public Health; and Phoebe Stapleton of Rutgers University.

","score: 12.224712322160599, grade_level: '12'","score: 12.95985531709669, grade_levels: ['college'], ages: [18, 24]",10.1073/pnas.2300582121,"Plastics are now omnipresent in our daily lives. The existence of microplastics (1 µm to 5 mm in length) and possibly even nanoplastics (<1 μm) has recently raised health concerns. In particular, nanoplastics are believed to be more toxic since their smaller size renders them much more amenable, compared to microplastics, to enter the human body. However, detecting nanoplastics imposes tremendous analytical challenges on both the nano-level sensitivity and the plastic-identifying specificity, leading to a knowledge gap in this mysterious nanoworld surrounding us. To address these challenges, we developed a hyperspectral stimulated Raman scattering (SRS) imaging platform with an automated plastic identification algorithm that allows micro-nano plastic analysis at the single-particle level with high chemical specificity and throughput. We first validated the sensitivity enhancement of the narrow band of SRS to enable high-speed single nanoplastic detection below 100 nm. We then devised a data-driven spectral matching algorithm to address spectral identification challenges imposed by sensitive narrow-band hyperspectral imaging and achieve robust determination of common plastic polymers. With the established technique, we studied the micro-nano plastics from bottled water as a model system. We successfully detected and identified nanoplastics from major plastic types. Micro-nano plastics concentrations were estimated to be about 2.4 ± 1.3 × 10 5 particles per liter of bottled water, about 90% of which are nanoplastics. This is orders of magnitude more than the microplastic abundance reported previously in bottled water. High-throughput single-particle counting revealed extraordinary particle heterogeneity and nonorthogonality between plastic composition and morphologies; the resulting multidimensional profiling sheds light on the science of nanoplastics."
"
A global study organized and led by Colorado State University scientists shows that the effects of extreme drought -- which is expected to increase in frequency with climate change -- has been greatly underestimated for grasslands and shrublands.

The findings -- published in Proceedings of the National Academy of Sciences -- quantify the impact of extreme short-term drought on grassland and shrubland ecosystems across six continents with a level of detail that was not previously possible.
It is the first time an experiment this extensive has been undertaken to generate a baseline understanding of the potential losses of plant productivity in these vital ecosystems.
Melinda Smith, a professor in the Department of Biology at CSU, led the study and is the first author on the paper. She said the observed reduction in a key carbon cycle process after a single 1-in-100-year drought event greatly exceeds previously reported losses for grasslands and shrublands.
""We were able to determine that the loss of aboveground plant growth -- a key measure of ecosystem function -- was 60% greater when short-term drought was extreme compared to the less severe droughts that have been more commonly experienced historically,"" she said. ""Past studies suffered from methodological differences when estimating the impacts of extreme drought in natural ecosystems, but our standardized, distributed approach here addressed that problem.""
Smith added that the project also showcases the variability in drought response across grassland and shrubland ecosystems -- offering both a review of the global impacts of climate change as well as a glimpse into which areas will be most stressed or most resilient in the coming years.
Gathering global extreme drought data on grassland and shrubland ecosystems Known as the International Drought Experiment, the newly published research originally dates back to 2013 as part of the National Science Foundation's Drought-Net Research Coordination Network. Altogether, there are more than 170 authors representing institutions from around the world cited in the new PNAS study, which was completed over the last four years.

To gather their data, researchers built rainfall manipulation structures to experimentally reduce the amount of naturally occurring precipitation available to ecosystems for at least a full growing season. About half of the participating sites imposed extreme drought conditions with these structures, while the rest imposed less severe drought for comparison.
As Earth's climate continues to change, short-term droughts that are statistically extreme in intensity will become more common, with what were once considered 1-in-100-year droughts now potentially happening every two to five years, said Smith. But because of the historic rarity of extreme droughts researchers had been unable to estimate the actual magnitude of their ecological consequences.
Smith said grasslands and shrublands were perfect test areas to fill that research gap because they are easier to manipulate for study than other systems, such as forests. They also store more than 30% of the global stock of carbon and support key industries such as livestock production.
""They are key ecosystems that are scalable to the globe, which makes them highly relevant for this kind of work,"" said Smith, who also serves as chair of the Faculty Council on campus. ""Grasslands and shrublands cover between 30% and 40% of the globe and frequently see deficits in precipitation. That means they are more vulnerable to climate change.""
Findings from the sites also provide insight into how specific climates, soil and vegetation types broadly influence drought response. While the work shows that drier and less diverse sites like those in Colorado are likely to be the most vulnerable to extremes, Smith said the severity of the drought was the most consistent and important factor in determining an ecosystem's response.
""Our data suggests greater losses in drier sites, but if you are getting to the extremes -- which is what is being forecasted -- we can generally expect substantial losses no matter where you are in the world,"" she said. ""We also found that even moderate losses from less severe droughts would still likely result in large impacts to the populations that rely on these systems. And then there is a combined loss of function across the globe to consider as well.""
Smith said the team is currently examining data collected from the full four years of the project to now assess multiyear drought impacts globally.

","score: 15.139519001386962, grade_level: '15'","score: 17.60481276005548, grade_levels: ['college_graduate'], ages: [24, 100]",10.1073/pnas.2309881120,"Climate change is increasing the frequency and severity of short-term (~1 y) drought events—the most common duration of drought—globally. Yet the impact of this intensification of drought on ecosystem functioning remains poorly resolved. This is due in part to the widely disparate approaches ecologists have employed to study drought, variation in the severity and duration of drought studied, and differences among ecosystems in vegetation, edaphic and climatic attributes that can mediate drought impacts. To overcome these problems and better identify the factors that modulate drought responses, we used a coordinated distributed experiment to quantify the impact of short-term drought on grassland and shrubland ecosystems. With a standardized approach, we imposed ~a single year of drought at 100 sites on six continents. Here we show that loss of a foundational ecosystem function—aboveground net primary production (ANPP)—was 60% greater at sites that experienced statistically extreme drought (1-in-100-y event) vs. those sites where drought was nominal (historically more common) in magnitude (35% vs. 21%, respectively). This reduction in a key carbon cycle process with a single year of extreme drought greatly exceeds previously reported losses for grasslands and shrublands. Our global experiment also revealed high variability in drought response but that relative reductions in ANPP were greater in drier ecosystems and those with fewer plant species. Overall, our results demonstrate with unprecedented rigor that the global impacts of projected increases in drought severity have been significantly underestimated and that drier and less diverse sites are likely to be most vulnerable to extreme drought."
"
When imagining corals, the picture that comes to mind is usually a stationary one: a garden of rock-like structures covering sections of the ocean floor.

Reef conservation efforts typically focus on preserving established coral and protecting them from known stressors such as pollution, overfishing and runoff from coastline populations.
However, new research near Miloliʻi in the southwestern part of the island of Hawaii, shows that identifying and protecting marine ecosystems both down-current and up-current of coral reefs, specifically areas where coral larvae are more likely to survive and thrive, is crucial to future coral conservation and restoration efforts -- especially as reefs face increasing pressure from the devastating effects of climate change.
The research, completed by Arizona State University scientists and their collaborators, appears in the current issue of Proceedings of the National Academy of Sciences.
Rachel Carlson, an ASU affiliate scientist and the study's first author, along with Greg Asner, director of ASU's Center for Global Discovery and Conservation Science, Larry Crowder, professor of oceans at Stanford University, and Robin Martin, associate professor with the ASU School of Ocean Futures in the Julie Ann Wrigley Global Futures Laboratory, collaborated on the project.
Additionally, the ʻĀkoʻakoʻa Reef Restoration Program, a regional effort that fuses cultural leadership, multi-modal education, advanced science and government engagement, backed the research.
Carlson says this type of collaborative work -- partnerships combining local, Indigenous knowledge and Western science -- is crucial to mapping out a future that ensures the survival of coral populations.

""There's a lot of Indigenous knowledge about coral spawning and fish populations in West Hawaii. In this study, we addressed an open question: How connected are coral populations between embayments along this coastline?"" Carlson said. ""What we essentially found is that the major factors in helping the coral keiki, known as larvae, settle down and survive are the nearshore current and the structure of the reef.""
The study shows that the larvae more often settle in and inhabit areas with large boulders and uneven surfaces, or ""chunky features,"" said Carlson, who is also a Chancellor's Postdoctoral Fellow at the UC Davis Bodega Marine Lab. Adult coral will spawn millions of larvae into the water column and those larvae prefer to settle in places with large knolls and boulders.
This discovery is good news: These kinds of seafloor features have been mapped via ASU's Global Airborne Observatory, a highly specialized aircraft that uses several types of remote sensing technologies to track both underwater and land-based habitats. This means that the researchers have the capability to help find and map priority reefs for conservation and restoration.
""This is foundational research in several important ways,"" said Asner, the study's senior author. ""First, it gives us an understanding of the connectivity of different parts of reefs along our coastline and tells us the level of connectivity in the context of the birth, settlement and growth of corals miles apart. Second, our unique remote sensing capabilities can identify reef sites where coral restoration could be most viable in the future. Finally, these findings provide a critical building block for future restoration efforts by our ʻĀkoʻakoʻa team and collaborators.""
The group's goal is to preserve and restore vitality to Hawaii's coral reefs and coastline health.
""We as lineal descendants of the Miloliʻi area have always relied on the reef for our ʻOhana (families). Our reef is our sustenance and is of enormous cultural value to us,"" said Kaʻimi Kaupiko, president of the nonprofit organization Kalanihale, which manages the Miloliʻi Community-Based Subsistence Fishing Area where the study took place.

Asner said the intertwined nature of reefs along Hawaii's coastlines is crucial to consider in reef protection strategies. Narrowing in on one area without consideration for the reproductive corridors of corals, he said, would be akin to worrying about planting trees in a certain place and not thinking about the forest as a whole. This sentiment is echoed by Martin, who said reef connectivity is an underutilized tool in reef restoration efforts globally.
""In Hawaii and worldwide, we're trying to figure out where we should place protections and restore areas to help reefs,"" Martin said. ""This study is highly technical, but it needs to be part of that conversation and part of that work, because if you aren't protecting the upcurrent reefs, you are cutting off important reproductive areas.""
Martin said reef restoration could, for example, expand a protected area of reefs beyond just the spots that have more dense coral coverage on the ocean floor; protection efforts would also be needed in the upcurrent path that the coral larvae traveled through before they settled in a new location.
Asner adds that this research could very well help conservation efforts expand to much greater distances than have been achieved previously.
""These kinds of studies of connectivity, flow and movement are needed because the west Hawaii island coastline is longer than the whole circumference of any other island,"" Asner said. ""We have a lot of degraded reefs along our coastline, so knowing where and how to help baby corals thrive is fundamental to the ʻĀkoʻakoʻa restoration effort.""
""Our students participated in the coral study, and that also helped us to connect the dots between cultural knowledge and Western science,"" Kaupiko said. ""The study supports our (community-based subsistence fishing area) by showing that our area is ecologically connected, and thus it needs to be managed and protected as one connected reef and coastlin

","score: 15.835215674661928, grade_level: '16'","score: 17.260994965946104, grade_levels: ['college_graduate'], ages: [24, 100]",10.1073/pnas.2311661121,"Coral reefs are in decline worldwide, making it increasingly important to promote coral recruitment in new or degraded habitat. Coral reef morphology—the structural form of reef substrate—affects many aspects of reef function, yet the effect of reef morphology on coral recruitment is not well understood. We used structure-from-motion photogrammetry and airborne remote sensing to measure reef morphology (rugosity, curvature, slope, and fractal dimension) across a broad continuum of spatial scales and evaluated the effect of morphology on coral recruitment in three broadcast-spawning genera. We also measured the effect of other environmental and biotic factors such as fish density, adult coral cover, hydrodynamic larval import, and depth on coral recruitment. All variables combined explained 72% of coral recruitment in the study region. Coarse reef rugosity and curvature mapped at ≥2 m spatial resolution—such as large colonies, knolls, and boulders—were positively correlated with coral recruitment, explaining 22% of variation in recruitment. Morphology mapped at finer scales (≤32 cm resolution) was not significant. Hydrodynamic larval import was also positively related to coral recruitment in Porites and Montipora spp., and grazer fish density was linked to significantly lower recruitment in all genera. In addition, grazer density, reef morphology, and hydrodynamic import had differential effects on coral genera, reflecting genus-specific life history traits, and model performance was lower in gonochoric species. Overall, coral reef morphology is a key indicator of recruitment potential that can be detected by remote sensing, allowing potential larval sinks to be identified and factored into restoration actions."
"
An international team of researchers has found that Africa's birds of prey are facing an extinction crisis.

The report, co-led by researchers from the School of Biology at the University of St Andrews and The Peregrine Fund, and published in the journal Nature Ecology & Evolution (4 January 2024), warns of declines among nearly 90% of 42 species examined, and suggests that more than two-thirds may qualify as globally threatened.
Led by Dr Phil Shaw from St Andrews and Dr Darcy Ogada of The Peregrine Fund, the study combines counts from road surveys conducted within four African regions at intervals of c. 20-40 years and yields unprecedented insights into patterns of change in the abundance of savanna raptor species.
It shows that large raptor species had experienced significantly steeper declines than smaller species, particularly on unprotected land, where they are more vulnerable to persecution and other human pressures. Overall, raptors had declined more than twice as rapidly outside of National Parks, Reserves and other protected areas than they had within. Worryingly, many species experiencing the steepest declines had suffered a double jeopardy, having also become much more dependent on protected areas over the course of the study.
The study's authors conclude that unless many of the threats currently facing African raptors are addressed effectively, large, charismatic eagle and vulture species are unlikely to persist over much of the continent's unprotected land by the latter half of this century.
The study also highlights steep declines among raptors that are currently classified as being of 'least concern' in the global Red List of threatened species. They include African endemics such as Wahlberg's Eagle, African Hawk-eagle, Long-crested Eagle, African Harrier-hawk and Brown Snake-eagle, as well as Dark Chanting-goshawk. All of these species have declined at rates suggesting that they may now be globally threatened.
Several other familiar, widespread raptor species are now scarce or absent from unprotected land. They include one of Africa's most powerful raptors -- the Martial Eagle -- as well as the highly distinctive Bateleur.

Dr Phil Shaw commented: ""Since the 1970s, extensive areas of forest and savanna have been converted into farmland, while other pressures affecting African raptors have likewise intensified. With the human population projected to double in the next 35 years, the need to extend Africa's protected area network -- and mitigate pressures in unprotected areas -- is now greater than ever.""
Dr Darcy Ogada added: ""Africa is at a crossroads in terms of saving its magnificent birds of prey. In many areas we have watched these species nearly disappear. One of Africa's most iconic raptors, the Secretarybird, is on the brink of extinction. There's no single threat imperiling these birds, it's a combination of many human-caused ones, in other words we are seeing deaths from a thousand cuts.""
Professor Ian Newton OBE FRS, FRSE, a world-leading ornithologist who was not involved in the study, commented: ""This is an important paper which draws attention to the massive declines in predatory birds which have occurred across much of Africa during recent decades. This was the continent over which, only 50 years ago, pristine populations of spectacular raptors were evident almost everywhere, bringing excitement and wonder to visitors from many parts of the world. The causes of the declines are many -- from rampant habitat destruction to growing use of poisons by farmers and poachers and expanding powerline networks -- all ultimately due to expansions in human numbers, livestock grazing and other activities. Let us hope that more research can be done and, more importantly, that these birds can be protected over ever more areas, measures largely dependent on the education and goodwill of local people.""
Raptors of all sizes lead an increasingly perilous existence on Africa's unprotected land, where suitable habitat, food supplies and breeding sites have been drastically reduced, and persecution from pastoralists, ivory poachers and farmers is now widespread. Other significant threats include unintentional poisoning, electrocution on power poles and collision with powerlines and wind turbines, as well as killing for food and belief-based uses.
The late Dr Jean Marc Thiollay laid the foundation for this study in the 1970s, by initiating a remarkable long-term monitoring effort in West Africa, where the average decline rate was more than twice that of other regions. The Peregrine Fund's Dr Ralph Buij, who has re-surveyed some of the original areas, noted that: ""the human footprint is particularly high throughout West Africa's savannas, and the near complete disappearance of many raptors outside that region's relatively small and fragmented protected area network reflects an ecological collapse that is increasingly affecting other parts of the continent. Some raptors that occur mostly in West Africa, such as the little-known Beaudouin's Snake-eagle, are vanishing into oblivion.""
The study's findings highlight the importance of strengthening the protection of Africa's natural habitats and aligns with the Convention on Biological Diversity's COP15 goal of expanding conservation areas to cover 30% of land by 2030. They also demonstrate the need to restore natural habitats within unprotected areas, reduce the impact of energy infrastructure, improve legislation for species protection, and establish long-term monitoring and evaluation of the conservation status of African raptors. Crucially, there is a pressing need to try to increase public involvement in raptor conservation efforts.
To this end, the study's authors have developed the African Raptor Leadership Grant to address the immediate need for more research and conservation programs. It supports educational and mentoring opportunities for emerging African scientists, boosting local conservation initiatives and knowledge of raptors across the continent. This initiative, which was launched in 2023, awarded its first grant to Joan Banda, a raptor research student at AP Leventis Ornithological Research Institute in Nigeria, who will be studying threats to African owls.

","score: 16.70118341307815, grade_level: '17'","score: 18.630960446570974, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41559-023-02236-0,"The conversion of natural habitats to farmland is a major cause of biodiversity loss and poses the greatest extinction risk to birds worldwide. Tropical raptors are of particular concern, being relatively slow-breeding apex predators and scavengers, whose disappearance can trigger extensive cascading effects. Many of Africa’s raptors are at considerable risk from habitat conversion, prey-base depletion and persecution, driven principally by human population expansion. Here we describe multiregional trends among 42 African raptor species, 88% of which have declined over a ca. 20–40-yr period, with 69% exceeding the International Union for Conservation of Nature criteria classifying species at risk of extinction. Large raptors had experienced significantly steeper declines than smaller species, and this disparity was more pronounced on unprotected land. Declines were greater in West Africa than elsewhere, and more than twice as severe outside of protected areas (PAs) than within. Worryingly, species suffering the steepest declines had become significantly more dependent on PAs, demonstrating the importance of expanding conservation areas to cover 30% of land by 2030—a key target agreed at the UN Convention on Biological Diversity COP15. Our findings also highlight the significance of a recent African-led proposal to strengthen PA management—initiatives considered fundamental to safeguarding global biodiversity, ecosystem functioning and climate resilience."
"
Hydrogen energy has emerged as a promising alternative to fossil fuels, offering a clean and sustainable energy source. However, the development of low-cost and efficient catalysts for hydrogen evolution reaction remains a crucial challenge. A research team led by scientists from City University of Hong Kong (CityU) has recently developed a novel strategy to engineer stable and efficient ultrathin nanosheet catalysts by forming Turing structures with multiple nanotwin crystals. This innovative discovery paves the way for enhanced catalyst performance for green hydrogen production.

Producing hydrogen through the process of water electrolysis with net-zero carbon emissions is one of the clean hydrogen production processes. While low-dimensional nanomaterials with controllable defects or strain modifications have emerged as active electrocatalysts for hydrogen-energy conversion and utilization, the insufficient stability in these materials due to spontaneous structural degradation and strain relaxation leads to their catalytic performance degradation.
To addressing this issue, a research team led by Professor Lu Jian, Dean of the College of Engineering at CityU and Director of Hong Kong Branch of National Precious Metal Material Engineering Research Center, has recently developed a pioneering Turing structuring strategy which not only activates but also stabilizes catalysts through the introduction of high-density nanotwin crystals. This approach effectively resolves the instability problem associated with low-dimensional materials in catalytic systems, enabling efficient and long-lasting hydrogen production.
Turing patterns, known as spatiotemporal stationary patterns, are widely observed in biological and chemical systems, such as the regular surface colouring on sea-shells. The mechanism of these pattern formations is related to the reaction-diffusion theory proposed by Alan Turing, a famous English mathematician regarded as one of the fathers of modern computing, in which the activator with a smaller diffusion coefficient induces local preferential growth.
""In previous research, the fabrication of low-dimensional materials has mainly focused on structural controls for functional purposes, with few considerations on spatiotemporal controls,"" Professor Lu explained the background of this research. ""However, the Turing patterns in nanomaterials may be achieved by the anisotropic growth of nanograins of the materials. Such broken lattice symmetry has crucial crystallographic implications for the growth of specific configurations, such as two-dimensional (2D) materials with twinning and intrinsic broken symmetry. So we wanted to explore the application of Turing theory on nanocatalyst growth and the relations with crystallographic defects.""
In this research, the team used two-step approach to create superthin platinum-nickel-niobium (PtNiNb) nanosheets with strips topologically resemble Turing patterns. These Turing structures on nanosheets were formed through the constrained orientation attachment of nanograins, resulting in an intrinsically stable, high-density nanotwin network which acted as structural stabilizers which prevented spontaneous structural degradation and strain relaxation.
Moreover, the Turing patterns generated lattice straining effects which reduce the energy barrier of water dissociation and optimize the hydrogen adsorption free energy for hydrogen evolution reaction, enhancing the activity of the catalysts and providing exceptional stability. The surface of the nano-scale Turing structure exhibits a large number of twins interfaces, also rendering it an exceptionally well-suited materials for interface-dominated applications, particularly electrochemical catalysis.
In the experiments, the researchers demonstrated the potential of the newly invented Turing PtNiNb nano-catalyst as a stable hydrogen evolution catalyst with superb efficiency. It achieved 23.5 and 3.1 times increase in mass activity and stability index, respectively, compared with commercial 20% Pt/C. The Turing PtNiNb-based anion-exchange-membrane water electrolyser with a low platinum (Pt) mass loading of 0.05 mg cm−2 was also extremely reliable, as it could achieve 500 hours of stability at 1,000 mAcm−2.
""Our key findings provide valuable insights into the activation and stabilization of catalytic materials with low dimensions. It presents a fresh paradigm for enhancing catalyst performance,"" said Professor Lu. ""The Turing structure optimization strategy not only addresses the issue of stability degradation in low-dimensional materials but also serves as a versatile material optimization approach applicable to other alloying and catalytic systems, ultimately enhancing catalytic performance.""

","score: 20.067651515151514, grade_level: '20'","score: 21.570062917308682, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41467-023-40972-w,"Low-dimensional nanocrystals with controllable defects or strain modifications are newly emerging active electrocatalysts for hydrogen-energy conversion and utilization; however, a crucial challenge remains in insufficient stability due to spontaneous structural degradation and strain relaxation. Here we report a Turing structuring strategy to activate and stabilize superthin metal nanosheets by incorporating high-density nanotwins. Turing configuration, realized by constrained orientation attachment of nanograins, yields intrinsically stable nanotwin network and straining effects, which synergistically reduce the energy barrier of water dissociation and optimize the hydrogen adsorption free energy for hydrogen evolution reaction. Turing PtNiNb nanocatalyst achieves 23.5 and 3.1 times increase in mass activity and stability index, respectively, compared against commercial 20% Pt/C. The Turing PtNiNb-based anion-exchange-membrane water electrolyser with a low Pt mass loading of 0.05 mg cm−2 demonstrates at least 500 h stability at 1000 mA cm−2, disclosing the stable catalysis. Besides, this new paradigm can be extended to Ir/Pd/Ag-based nanocatalysts, illustrating the universality of Turing-type catalysts."
"
As concerns over the world's declining bird population mount, animal ecologists developed an analytical approach to better understand one of the latest threats to feathered creatures: the rise of wind and solar energy facilities.

""Bird mortality has become an unintended consequence of renewable energy development,"" said Hannah Vander Zanden, an assistant professor of biology at the University of Florida. ""If we want to minimize or even offset these fatalities, especially for vulnerable populations, we need to identify the geographic origin of affected birds. In other words, are the dead birds local or are they coming from other parts of North America?""
Birds can be killed when they collide with wind turbines, fly into solar panels they mistake for bodies of water or become singed by the intense heat from concentrating solar power plants. While the death rate of birds due to these energy facilities is far less than deaths due to domestic cats and collisions with building, efforts to mitigate this problem is important, scientists say.
Vander Zanden and colleagues performed geospatial analyses of stable hydrogen isotope data obtained from feathers of 871 individual birds found dead at solar and wind energy facilities in California, representing 24 species.
Their analysis of natural-occurring markers in the feathers provided information about where the feathers were grown based on the water the birds consumed.
""With these markers, we could determine whether the bird was local or if it was migrating from somewhere else,"" said Vander Zanden, who is the principal investigator of UF's Animal Migration and Ecology Lab.
Results from the study, which were published Friday in the journal Conservation Biology, show that the birds killed at the facilities were from a broad area across the continent. Their geographical origins varied among species and included a mix of local and nonlocal birds.

Researchers found most birds killed at solar facilities were nonlocal and peaked during the migratory periods of April and September through October. The percentage of migratory birds found at wind facilities nearly matched that of local birds, at 51%, Vander Zanden said.
""This kind of data can help inform us about best strategies to use to minimize or mitigate the fatalities,"" she said. ""For example, facilities management could work with conservationists to improve the local habitat to help protect local birds or improve other parts of the species' range where the migratory birds originate.""
The results also illustrate the power of stable isotope data to assess future population growth or decline patterns for birds due to a variety of reasons.
""Studying the remains of animals is a noninvasive approach to get information that is otherwise hard to track and apply to conservation,"" Vander Zanden said. ""It's a great way to understand the mysteries about animals.""

","score: 15.193840322198536, grade_level: '15'","score: 15.639867330016585, grade_levels: ['college_graduate'], ages: [24, 100]",10.1111/cobi.14191,"Bird populations are declining globally. Wind and solar energy can reduce emissions of fossil fuels that drive anthropogenic climate change, yet renewable‐energy production represents a potential threat to bird species. Surveys to assess potential effects at renewable‐energy facilities are exclusively local, and the geographic extent encompassed by birds killed at these facilities is largely unknown, which creates challenges for minimizing and mitigating the population‐level and cumulative effects of these fatalities. We performed geospatial analyses of stable hydrogen isotope data obtained from feathers of 871 individuals of 24 bird species found dead at solar‐ and wind‐energy facilities in California (USA). Most species had individuals with a mix of origins, ranging from 23% to 98% nonlocal. Mean minimum distances to areas of likely origin for nonlocal individuals were as close as 97 to >1250 km, and these minimum distances were larger for species found at solar‐energy facilities in deserts than at wind‐energy facilities in grasslands (Cohen's d = 6.5). Fatalities were drawn from an estimated 30–100% of species’ desingated ranges, and this percentage was significantly smaller for species with large ranges found at wind facilities (Pearson's r = −0.67). Temporal patterns in the geographic origin of fatalities suggested that migratory movements and nonmigratory movements, such as dispersal and nomadism, influence exposure to fatality risk for these birds. Our results illustrate the power of using stable isotope data to assess the geographic extent of renewable‐energy fatalities on birds. As the buildout of renewable‐energy facilities continues, accurate assessment of the geographic footprint of wildlife fatalities can be used to inform compensatory mitigation for their population‐level and cumulative effects."
"
Conservation measures have successfully stopped declines in the African savanna elephant population across southern Africa, but the pattern varies locally, according to a new study.

The evidence suggests that the long-term solution to elephant survival requires not only that areas are protected but that they are also connected to allow populations to stabilize naturally, an international research team says.
Their study, published on January 5th in the peer-reviewed journal Science Advances, collected survey estimates and calculated growth rates for more than 100 elephant populations in southern Africa between 1995 and 2020, accounting for an estimated 70% of the global savanna elephant population.
""This is the most comprehensive analysis of growth rates for any large mammal population in the world,"" said co-author Rob Guldemond, director of the Conservation Ecological Research Unit (CERU) at the University of Pretoria, in South Africa.
Overall, the survey's results are positive: There are the same number of elephants now as there were 25 years ago, a rare conservation win at a time when the planet is rapidly losing biodiversity. However, the pattern is not consistent across regions. Some areas, such as south Tanzania, eastern Zambia, and northern Zimbabwe, experienced severe declines due to illegal ivory poaching. In contrast, populations in other regions like north Botswana are booming.
""Unchecked growth isn't necessarily a good thing, however,"" says study co-author Stuart Pimm, the Doris Duke Professor of Conservation at Duke University in North Carolina. ""Rapidly increasing populations can outgrow and damage their local environment and prove hard to manage -- introducing a threat to their long-term stability,"" Pimm says.
In addition to documenting local growth rates, the team also looked at the features of the local populations to identify what makes them stable, that is neither growing nor declining.

Elephant populations in well-protected but isolated parks, sometimes called ""fortress conservation,"" grow rapidly in the absence of threats but are unsustainable in the long term. These elephants will likely need future conservation interventions, such as translocation or birth control, which are both costly and intensive endeavors.
The team found that the most stable populations occur in large, core areas that are surrounded by buffer zones. The core areas are defined by their strong levels of environmental protection and minimal human impact, whereas the buffers allow some activities such as sustainable farming, forestry, or trophy hunting. Unlike the insular fortresses, core areas are connected to other parks, allowing herds to move naturally.
""What's crucial is that you need a mix of areas with more stable core populations linked to more variable buffer areas,"" said lead author Ryan Huang, a Duke Ph.D. now doing postdoctoral research at CERU.
""These buffers absorb immigrants when core populations get too high, but also provide escape routes when elephants face poor environmental conditions or other threats such as poaching,"" Huang said.
Connecting protected areas means elephants can freely move in and out. This allows a natural equilibrium to occur without human intervention, sparing conservationists from using their limited resources to maintain balance.
""Calling for connecting parks isn't something new. Many have done so,"" Huang said. ""But surprisingly, there has not been a lot of published evidence of its effectiveness so far. This study helps quantify why this works.""
""Connecting protected areas is essential for the survival of African savanna elephants and many other animal and plant species,"" said Celesté Maré, co-author and doctoral student at Aarhus University in Denmark. ""Populations with more options for moving around are healthier and more stable, which is important given an uncertain future from climate change.""

","score: 14.45240162477786, grade_level: '14'","score: 15.382707539984771, grade_levels: ['college_graduate'], ages: [24, 100]",10.1126/sciadv.adk2896,"The influence of protected areas on the growth of African savannah elephant populations is inadequately known. Across southern Africa, elephant numbers grew at 0.16% annually for the past quarter century. Locally, much depends on metapopulation dynamics—the size and connections of individual populations. Population numbers in large, connected, and strictly protected areas typically increased, were less variable from year to year, and suffered less from poaching. Conversely, populations in buffer areas that are less protected but still connected have more variation in growth from year to year. Buffer areas also differed more in their growth rates, likely due to more threats and dispersal opportunities in the face of such dangers. Isolated populations showed consistently high growth due to a lack of emigration. This suggests that “fortress” conservation generally maintains high growth, while anthropogenic-driven source-sink dynamics within connected conservation clusters drive stability in core areas and variability in buffers."
"
In a recent tragic incident, approximately 100 elephants in Africa perished due to inadequate access to water. The United Nations Environment Programme (UNEP) issues a warning that around 2.5 billion people worldwide could face water scarcity by 2025. In the face of water shortages affecting not only human society but also the entire ecological community due to the climate crisis, it becomes crucial to adopt comprehensive measures for managing water quality and quantity to avert such pressing challenges.

A research team led by Professor Jonghun Kam and PhD candidate Kwang-Hun Lee from the Division of Environmental Science and Engineering at Pohang University of Science and Technology (POSTECH), has implemented an advanced technique employing an uncrewed surface vehicle to concurrently assess the reservoir water depths and nitrate (NO3-) concentrations from the reservoir water surface. The findings from their research were featured in Water Resources Research, an international journal dedicated to the water environment.
Monitoring available water quantity and quality uses indicators such as water depth and nitrate concentration. Nitrates, originating from atmospheric and soil nutrients, enter streams through various pathways, posing a potential threat to aquatic ecosystems and biodiversity when their levels become excessive. Fluctuation of precipitation and water usage further impact water quality, and rising water temperatures contribute to decreased dissolved oxygen, resulting in diminished water quality.
Effective management of water resources requires the dual monitoring of nitrate concentration and water depth. However, these measurements can vary significantly based on the timing and location of assessment. Traditional water depth measurement, typically taken at a single point, introduces uncertainty in estimating the total reservoir water volume. In recent times, uncrewed devices or instruments have been introduced to address this challenge, yet simultaneous measurement of nitrate concentration and water depth has proven challenging.
The research team has achieved the simultaneous measurement of nitrate concentration and water depth using an uncrewed surface vehicle. Over the course of a year, starting in 2021, an uncrewed boat equipped with electrochemical sensors and acoustic doppler current profile sensors was employed to gauge nitrate concentration and water depth in a reservoir (Daljeonji) in Pohang, North Gyeongsang Province in South Korea. The 30 measurements revealed seasonal variations with nitrate levels ranging from 1 ton to 4 tons. Following intense rainfall, the observed nitrate amount was up to 17% lower than previous readings due to rapid water expansion. This underscores the importance of considering timing and weather conditions in water quality assessments, as measurements may lead to over- or underestimation.
Furthermore, the team successfully generated a high-resolution map illustrating the cumulative nitrate content in Daljeonji Reservior based on data collected by the uncrewed surface vehicle. Despite a one-year measurement period and the study's confinement to Pohang, its significance lies in the independent development of technology capable of simultaneous measurement of nitrate concentration and water depth.
Professor Jonghun Kam who led the research explained, ""Our study has outlined both the possibilities and constraints of employing uncrewed robotics in water environment research."" He added, ""It is envisioned that this research will provide a guiding framework for the development of the next generation of the Korean national water resources management system, leveraging advanced technologies like uncrewed aerial vehicles to enhance prediction accuracy and optimize water management.""
The study was conducted with the support from the Group Research in Science and Engineering Program and the Ocean, Land, and Atmosphere Carbon Cycle System Research Program of the Ministry of Science and ICT and the National Research Foundation of Korea (2021M3I6A1086808).

","score: 18.23163009404389, grade_level: '18'","score: 19.548938871473354, grade_levels: ['college_graduate'], ages: [24, 100]",10.1029/2023WR034665,"Reliable nutrient load estimation of a reservoir is challenging due to inconsistent spatial extent and temporal frequency of water quality and quantity measurements. This study aims to collect consistent spatial extent and temporal frequency of water depths and nitrate concentrations of a reservoir in South Korea using uncrewed surface vehicle (USV). In this study, reservoir nitrate loads were estimated using four methods to examine how spatial variation in water depth and nitrate concentrations affected load estimates. Based on dual measurements of water depth and nitrate concentration, reservoir nitrate loads across 30 sampling dates (0.7 million tons of fresh water on average) ranged from one to four tons. Results showed that a point measurement of water depths and nitrate concentrations can cause up to −17% of underestimation of nitrate loads, particularly after intense rainfall events. This study highlights potential opportunities and challenges of the USV‐based dual monitoring systems for water quality and quantity."
"
A recent cold spell plunged the nation of Korea into a deep freeze, resulting in the closure of 247 national parks, the cancellation of 14 domestic flights, and the scrapping of 107 cruise ship voyages. While the cold snap brought relief by significantly reducing the prevalence of particulate matter obscuring our surroundings, a recent study indicates that, besides diminishing particulate matter, it is significantly contributing to the heightened uptake of carbon dioxide by the East Sea.

According to research conducted by a team of researchers including Professor Kitack Lee from the Division of Environmental Science & Engineering at Pohang University of Science and Technology (POSTECH), and Professor Tongsup Lee and So-Yun Kim from the Department of Oceanography at Pusan National University, the cold atmosphere in the Arctic is influencing the absorption of carbon dioxide by the East Sea. The research findings were published in Geophysical Research Letters, an international journal by the American Geophysical Union (AGU).
The research team investigated the correlation between the East Sea's surface-deep circulation and its carbon dioxide absorption capacity, drawing insights from observations in 1992, 1999, 2007, and 2019. During the initial period (1992-1999), the ocean absorbed 20 million tons of carbon dioxide annually. In the subsequent period (1999-2007), this amount decreased to under 10 million tons per year. However, in the final period (2007-2019), the carbon dioxide uptake surged to 30 million tons per year.
The team observed that the internal circulation along the East Coast within the East Sea was influenced by the Arctic cold wave. Cold air from the Arctic infiltrates the East Sea, causing the surface water, laden with carbon dioxide, to become denser. This process induces vertical ventilation as the water descends into the middle and deep ocean layers. Consequently, the intensified descent of cold air from the Arctic strengthens the internal circulation, leading to a heightened uptake of carbon dioxide in the East Sea.
Professor Kitack Lee who led the research remarked, "" The oceans represent an immense reservoir of carbon dioxide and offer a secure and sustainable avenue for mitigating atmospheric carbon dioxide levels."" He further stated, ""It is crucial to anticipate the global ocean's capacity for carbon removal as we navigate future climate changes and identify suitable methods to leverage this potential.""
In a related development, the team's earlier research uncovered the mechanism through which the ocean absorbs carbon dioxide. Approximately half of the carbon dioxide generated by human activities remains in the atmosphere with the other half entering marine and terrestrial ecosystems. With a carbon content 400,000 times greater than that of the atmosphere, the oceans present vast and promising potential for storing carbon dioxide.
The research was sponsored by the Ocean, Land, and Atmosphere Carbon Cycle System Research Program of the National Research Foundation of Korea and a research contract program of the National Institute of Fisheries Sciences of Korea.

","score: 15.705157232704405, grade_level: '16'","score: 17.552201257861633, grade_levels: ['college_graduate'], ages: [24, 100]",10.1029/2023GL105819,"Ocean ventilation is a key mechanism for transporting anthropogenic CO2 (CANTH) from the ocean surface toward its interior. We investigated the link between ocean ventilation and CANTH increase in the East Sea using data from surveys conducted in 1992, 1999, 2007, and 2019. Between 1992 and 1999, the East Sea Intermediate Water (300−1,500 m) accumulated CANTH at a rate of 0.3 ± 0.1 mol C m−2 yr−1. However, in the subsequent period (1999−2007) this rate decreased to <0.1 ± 0.1 mol C m−2 yr−1. There was a resurgence in the CANTH increase rate between 2007 and 2019, reaching 0.4 ± 0.1 mol C m−2 yr−1. The East Sea Intermediate Water ventilation changes, inferred from the changes in water column O2 level and the Arctic Oscillation‐driven winter surface temperature in the deep water formation region, were responsible for the periodic decline and recovery in CANTH increase."
"
Breakthrough could dramatically cut the use of pesticides and unlock other opportunities to bolster plant health

Scientists have engineered the microbiome of plants for the first time, boosting the prevalence of 'good' bacteria that protect the plant from disease.
The findings published in Nature Communications by researchers from the University of Southampton, China and Austria, could substantially reduce the need for environmentally destructive pesticides.
There is growing public awareness about the significance of our microbiome -- the myriad of microorganisms that live in and around our bodies, most notably in our guts. Our gut microbiomes influence our metabolism, our likelihood of getting ill, our immune system, and even our mood.
Plants too host a huge variety of bacteria, fungi, viruses, and other microorganisms that live in their roots, stems, and leaves. For the past decade, scientists have been intensively researching plant microbiomes to understand how they affect a plant's health and its vulnerability to disease.
""For the first time, we've been able to change the makeup of a plant's microbiome in a targeted way, boosting the numbers of beneficial bacteria that can protect the plant from other, harmful bacteria,"" says Dr Tomislav Cernava, co-author of the paper and Associate Professor in Plant-Microbe Interactions at the University of Southampton.
""This breakthrough could reduce reliance on pesticides, which are harmful to the environment. We've achieved this in rice crops, but the framework we've created could be applied to other plants and unlock other opportunities to improve their microbiome. For example, microbes that increase nutrient provision to crops could reduce the need for synthetic fertilisers.""
The international research team discovered that one specific gene found in the lignin biosynthesis cluster of the rice plant is involved in shaping its microbiome. Lignin is a complex polymer found in the cell walls of plants -- the biomass of some plant species consists of more than 30 per cent lignin.

First, the researchers observed that when this gene was deactivated, there was a decrease in the population of certain beneficial bacteria, confirming its importance in the makeup of the microbiome community.
The researchers then did the opposite, over-expressing the gene so it produced more of one specific type of metabolite -- a small molecule produced by the host plant during its metabolic processes. This increased the proportion of beneficial bacteria in the plant microbiome.
When these engineered plants were exposed to Xanthomonas oryzae -- a pathogen that causes bacterial blight in rice crops, they were substantially more resistant to it than wild-type rice.
Bacterial blight is common in Asia and can lead to substantial loss of rice yields. It's usually controlled by deploying polluting pesticides, so producing a crop with a protective microbiome could help bolster food security and help the environment.
The research team are now exploring how they can influence the presence of other beneficial microbes to unlock various plant health benefits.

","score: 14.459539748953976, grade_level: '14'","score: 16.64009799603612, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41467-023-44335-3,"In terrestrial ecosystems, plant leaves provide the largest biological habitat for highly diverse microbial communities, known as the phyllosphere microbiota. However, the underlying mechanisms of host-driven assembly of these ubiquitous communities remain largely elusive. Here, we conduct a large-scale and in-depth assessment of the rice phyllosphere microbiome aimed at identifying specific host-microbe links. A genome-wide association study reveals a strong association between the plant genotype and members of four bacterial orders, Pseudomonadales, Burkholderiales, Enterobacterales and Xanthomonadales. Some of the associations are specific to a distinct host genomic locus, pathway or even gene. The compound 4-hydroxycinnamic acid (4-HCA) is identified as the main driver for enrichment of bacteria belonging to Pseudomonadales. 4-HCA can be synthesized by the host plant’s OsPAL02 from the phenylpropanoid biosynthesis pathway. A knockout mutant of OsPAL02 results in reduced Pseudomonadales abundance, dysbiosis of the phyllosphere microbiota and consequently higher susceptibility of rice plants to disease. Our study provides a direct link between a specific plant metabolite and rice phyllosphere homeostasis opening possibilities for new breeding strategies."
"
A breakthrough achieved by researchers from four Israeli universities -- Tel Aviv University, The Hebrew University of Jerusalem, Bar-Ilan University and Ariel University- will enable archaeologists to identify burnt materials discovered in excavations and estimate their firing temperatures. Applying their method to findings from ancient Gath (Tell es-Safi in central Israel), the researchers validated the Biblical account: ""About this time Hazael King of Aram went up and attacked Gath and captured it. Then he turned to attack Jerusalem"" (2 Kings 12, 18). They explain that unlike previous methods, the new technique can determine whether a certain item (such as a mud brick) underwent a firing event even at relatively low temperatures, from 200°C and up. This information can be crucial for correctly interpreting the findings.

The multidisciplinary study was led by Dr. Yoav Vaknin from the Sonia & Marco Nadler Institute of Archaeology, Entin Faculty of Humanities, at Tel Aviv University, and the Palaeomagnetic Laboratory at The Hebrew University. Other contributors included: Prof. Ron Shaar from the Institute of Earth Sciences at The Hebrew University, Prof. Erez Ben-Yosef and Prof. Oded Lipschits from the Sonia & Marco Nadler Institute of Archaeology at Tel Aviv University, Prof. Aren Maeir from the Martin (Szusz) Department of Land of Israel Studies and Archaeology at Bar-Ilan University and Dr. Adi Eliyahu Beharfrom the Department of Land of Israel Studies and Archaeology and the Department of Chemical Sciences at Ariel University. The paper has been published in the scientific journal PLOS ONE.
Prof. Lipschits: ""Throughout the Bronze and Iron Ages the main building material in most parts of the Land of Israel was mud bricks. This cheap and readily available material was used to build walls in most buildings, sometimes on top of stone foundations. That's why it's so important to understand the technology used in making these bricks.""
Dr. Vaknin adds: ""During the same era dwellers of other lands, such as Mesopotamia where stone was hard to come by, would fire mud bricks in kilns to increase their strength and durability. This technique is mentioned in the story of the Tower of Babel in the Book of Genesis: ""They said one to another, Come, let us make bricks and fire them thoroughly. So they used brick for stone""(Genesis 11, 3). Most researchers, however, believe that this technology did not reach the Land of Israel until much later, with the Roman conquest. Until that time the inhabitants used sun-dried mud bricks. Thus, when bricks are found in an archaeological excavation, several questions must be asked: First, have the bricks been fired, and if so, were they fired in a kiln prior to construction or in situ, in a destructive conflagration event? Our method can provide conclusive answers.""
The new method relies on measuring the magnetic field recorded and 'locked' in the brick as it burned and cooled down. Dr. Vaknin: ""The clay from which the bricks were made contains millions of ferromagnetic particles -- minerals with magnetic properties that behave like so many tiny 'compasses' or magnets. In a sun-dried mud brick the orientation of these magnets is almost random, so that they cancel out one another. Therefore, the overall magnetic signal of the brick is weak and not uniform. Heating to 200°C or more, as happens in a fire, releases the magnetic signals of these magnetic particles and, statistically, they tend to align with the earth's magnetic field at that specific time and place. When the brick cools down, these magnetic signals remain locked in their new position and the brick attains a strong and uniformly oriented magnetic field, which can be measured with a magnetometer. This is a clear indication that the brick has, in fact, been fired.
In the second stage of the procedure, the researchers gradually 'erase' the brick's magnetic field, using a process called thermal demagnetization. This involves heating the brick in a special oven in a palaeomagnetic laboratory that neutralizes the earth's magnetic field. The heat releases the magnetic signals, which once again arrange themselves randomly, canceling each other out, and the total magnetic signal becomes weak and loses its orientation.
Dr. Vaknin: ""We conduct the process gradually. At first, we heat the sample to a temperature of 100°C, which releases the signals of only a small percentage of the magnetic minerals. We then cool it down and measure the remaining magnetic signal. We then repeat the procedure at temperatures of 150°C, 200°C, and so on, proceeding in small steps, up to 700°C. In this way the brick's magnetic field is gradually erased. The temperature at which the signal of each mineral is 'unlocked' is approximately the same as the temperature at which it was initially 'locked', and ultimately, the temperature at which the magnetic field is fully erased was reached during the original fire.""
The researchers tested the technique in the laboratory: they fired mud bricks under controlled conditions of temperature and magnetic field, measured each brick's acquired magnetic field, then gradually erased it. They found that the bricks were completely demagnetized at the temperature at which they had been burned -- proving that the method works.

Dr. Vaknin: ""Our approach enables identifying burning which occurred at much lower temperatures than any other method. Most techniques used for identifying burnt bricks are based on actual changes in the minerals, which usually occur at temperatures higher than 500°C -- when some minerals are converted into others.""
Dr. Eliyahu Behar: ""One of the common methods for identifying mineralogical changes in clay (the main component of mud bricks) due to exposure to high temperatures is based on changes in the absorption of infrared radiation by the various minerals. In this study we used this method as an additional tool to verify the results of the magnetic method."" Dr. Vaknin: ""Our method is much more sensitive than others because it targets changes in the intensity and orientation of the magnetic signal, which occur at much lower temperatures. We can begin to detect changes in the magnetic signal at temperatures as low as 100°C, and from 200°C and up the findings are conclusive.""
In addition, the method can determine the orientation in which the bricks cooled down. Dr. Vaknin: ""When a brick is fired in a kiln before construction, it records the direction of the earth's magnetic field at that specific time and place. In Israel this means north and downward. But when builders take bricks from a kiln and build a wall, they lay them in random orientations, thus randomizing the recorded signals. On the other hand, when a wall is burned in-situ, as might happen when it is destroyed by an enemy, the magnetic fields of all bricks are locked in the same orientation.""
After proving the method's validity, the researchers applied it to a specific archaeological dispute: was a specific brick structure discovered at Tell es-Safi -- identified as the Philistine city of Gath, home of Goliath -- built of pre-fired bricks or burned on location? The prevalent hypothesis, based on the Old Testament, historical sources, and Carbon-14 dating attributes the destruction of the structure to the devastation of Gath by Hazael, King of Aram Damascus, around 830 BCE. However, a previous paper by researchers including Prof. Maeir, head of the Tell es-Safi excavations, proposed that the building had not burned down, but rather collapsed over decades, and that the fired bricks found in the structure had been fired in a kiln prior to construction. If this hypothesis were correct, this would be the earliest instance of brick-firing technology discovered in the Land of Israel.
To settle the dispute, the current research team applied the new method to samples from the wall at Tell es-Safi and the collapsed debris found beside it. The findings were conclusive: the magnetic fields of all bricks and collapsed debris displayed the same orientation -- north and downwards. Dr. Vaknin: ""Our findings signify that the bricks burned and cooled down in-situ, right where they were found, namely in a conflagration in the structure itself, which collapsed within a few hours. Had the bricks been fired in a kiln and then laid in the wall, their magnetic orientations would have been random. Moreover, had the structure collapsed over time, not in a single fire event, the collapsed debris would have displayed random magnetic orientations. We believe that the main reason for our colleagues' mistaken interpretation was their inability to identify burning at temperatures below 500°C. Since heat rises, materials at the bottom of the building burned at relatively low temperatures, below 400°C, and consequently the former study did not identify them as burnt -- leading to the conclusion that the building had not been destroyed by fire. At the same time, bricks in upper parts of the wall, where temperatures were much higher, underwent mineralogical changes and were therefore identified as burnt -- leading the researchers to conclude that they had been fired in a kiln prior to construction. Our method allowed us to determine that all bricks in both the wall and debris had burned during the conflagration: those at the bottom burned at relatively low temperatures, and those that were found in higher layers or had fallen from the top -at temperatures higher than 600°C.""
Prof. Maeir: ""Our findings are very important for deciphering the intensity of the fire and scope of destruction at Gath, the largest and most powerful city in the Land of Israel at the time, as well as understanding the building methods prevailing in that era. It's important to review conclusions from previous studies, and sometimes even refute former interpretations, even if they came from your own school."" Prof. Ben-Yosef adds: ""Beyond their historical and archaeological significance, ancient building methods also had substantial ecological implications. The brick firing technology requires vast quantities of combustive materials, and in ancient times this might have led to vast deforestation and even loss of tree species in the area. For example, certain species of trees and shrubs exploited by the ancient copper industry in the Timna Valley have not recovered to this day and the industry itself ultimately collapsed once it had used up its natural fuels. Our findings indicate that the brick firing technology was probably not practiced in the Land of Israel in the times of the Kings of Judah and Israel.""

","score: 14.271701951017018, grade_level: '14'","score: 15.123891656288919, grade_levels: ['college_graduate'], ages: [24, 100]",10.1371/journal.pone.0289424,"Burnt materials are very common in the archaeological record. Their identification and the reconstruction of their firing history are crucial for reliable archaeological interpretations. Commonly used methods are limited in their ability to identify and estimate heating temperatures below ~500⁰C and cannot reconstruct the orientation in which these materials were burnt. Stepwise thermal demagnetization is widely used in archaeomagnetism, but its use for identifying burnt materials and reconstructing paleotemperatures requires further experimental verification. Here we present an experimental test that has indicated that this method is useful for identifying the firing of mud bricks to 190⁰C or higher. Application of the method to oriented samples also enables reconstruction of the position in which they cooled down. Our algorithm for interpreting thermal demagnetization results was tested on 49 miniature sun-dried “mud bricks”, 46 of which were heated to a range of temperatures between 100⁰C to 700⁰C under a controlled magnetic field and three “bricks” which were not heated and used as a control group. The results enabled distinguishing between unheated material and material heated to at least 190⁰C and accurately recovering the minimum heating temperature of the latter. Fourier-Transform Infrared Spectroscopy (FTIR) on the same materials demonstrated how the two methods complement each other. We implemented the thermal demagnetization method on burnt materials from an Iron Age structure at Tell es-Safi/Gath (central Israel), which led to a revision of the previously published understanding of this archaeological context. We demonstrated that the conflagration occurred within the structure, and not only in its vicinity as previously suggested. We also showed that a previously published hypothesis that bricks were fired in a kiln prior to construction is very unlikely. Finally, we conclude that the destruction of the structure occurred in a single event and not in stages over several decades."
"
A decades-old mystery of how natural antimicrobial predatory bacteria are able to recognize and kill other bacteria may have been solved, according to new research.

In a study published today (4th January) in Nature Microbiology, researchers from the University of Birmingham and the University of Nottingham have discovered how natural antimicrobial predatory bacteria, called Bdellovibrio bacterivorous, produce fibre-like proteins on their surface to ensnare prey.
This discovery may enable scientists to use these predators to target and kill problematic bacteria that cause issues in healthcare, food spoilage and the environment.
Professor of Structural Biology at the University of Birmingham, Andrew Lovering said: ""Since the 1960s Bdellovibrio bacterivorous has been known to hunt and kill other bacteria by entering the target cells and eating them from the inside before later bursting out. The question that had stumped scientists was 'how do these cells make a firm attachment when we know how varied their bacterial targets are?'""
Professor Lovering and Professor Liz Sockett, from the School of Life Sciences at the University of Nottingham, have been collaborating in this field for almost 15 years. The breakthrough came when Sam Greenwood an undergraduate student, and Asmaa Al-Bayati, a PhD student in the Sockett lab, discovered that the Bdellovibrio predators lay down a sturdy vesicle (a ""pinched-off"" part of the predator cell envelope) when invading their prey.
Professor Liz Sockett explained: ""The vesicle creates a kind of airlock or keyhole allowing Bdellovibrio entry into the prey cell. We were then able to isolate this vesicle from the dead prey, which is a first in this field. The vesicle was analysed to reveal the tools used during the preceding event of predator/prey contact. We thought of it as a bit like a locksmith leaving the pick, or key, as evidence, in the keyhole.
""By looking at the vesicle contents, we discovered that because Bdellovibrio doesn't know which bacteria it will meet, it deploys a range of similar prey recognition molecules on its surface, creating lots of different 'keys' to 'unlock' lots of different types of prey.""
The researchers then undertook an individual analysis of the molecules, demonstrating that they form long fibres, approximately ten times longer than common globular proteins. This allows them to operate at a distance and ""feel"" for prey in the vicinity.

In total, the labs counted 21 different fibres. Researchers Dr Simon Caulton, Dr Carey Lambert and Dr Jess Tyson worked on how they operated both at the cellular and molecular level. They were supported by fibre gene-engineering by Paul Radford and Rob Till. The team then began to attempt linking a particular fibre to a particular prey-surface molecule. Finding out which fibre matches which prey, could enable an engineering approach which sees bespoke predators targeting different types of bacteria.
Professor Lovering continued: ""Because the predator strain we were looking at comes from the soil it has a wide killing range, making this identification of these fibre and prey pairs very difficult. However, on the fifth attempt to find the partners we discovered a chemical signature on the outside of prey bacteria that was a tight fit to the fibre tip. This is the first time a feature of Bdellovibrio has been matched to prey selection.""
Scientists in this field will now be able to use these discoveries to ask which fibre set is used by the different predators they study and potentially attribute these to specific prey. Improving understanding of these predator bacteria could enable their usage as antibiotics, to kill bacteria that degrade food, or ones which are harmful to the environment.
Professor Lovering concluded: ""We know that these bacteria can be helpful, and by fully understanding how they operate and find their prey, it opens up a world of new discoveries and possibilities.""
The research was funded by the Wellcome Trust Investigator in Science Award (209437/Z/17/Z).

","score: 14.126045966228894, grade_level: '14'","score: 15.0728388836773, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41564-023-01552-2,"Predatory bacteria, like the model endoperiplasmic bacterium Bdellovibrio bacteriovorus, show several adaptations relevant to their requirements for locating, entering and killing other bacteria. The mechanisms underlying prey recognition and handling remain obscure. Here we use complementary genetic, microscopic and structural methods to address this deficit. During invasion, the B. bacteriovorus protein CpoB concentrates into a vesicular compartment that is deposited into the prey periplasm. Proteomic and structural analyses of vesicle contents reveal several fibre-like proteins, which we name the mosaic adhesive trimer (MAT) superfamily, and show localization on the predator surface before prey encounter. These dynamic proteins indicate a variety of binding capabilities, and we confirm that one MAT member shows specificity for surface glycans from a particular prey. Our study shows that the B. bacteriovorus MAT protein repertoire enables a broad means for the recognition and handling of diverse prey epitopes encountered during bacterial predation and invasion."
"
As one of the top predators roaming Antarctica, the sizeable southern elephant seal has its fair selection of the menu. But it turns out they don't just want to eat anything and everything.

According to new research led by UNSW Sydney, male southern elephant seals have their own favourite foods, and they like to stick with them. In other words, they're very fussy eaters.
""They could have the pick of the buffet, and yet each male southern elephant seal eats a lot of the same food, which is just a fraction of what's on offer,"" says Andrea Cormack, lead author of the study and a PhD candidate at UNSW Science. ""So, they are extremely picky eaters, each with their own unique favourite foods they go after, whether it be fish, squid species, crustaceans or octopus.""
The research, published in the journal Marine Ecology Progress Series, is one of the first studies to analyse the diet of adult male southern elephant seals, who, despite their impressive size, are relatively understudied compared to females.
""We didn't specifically compare males to females in this study,"" Ms Cormack says. ""But females are known to maintain a fairly specialised diet between one another, just nowhere near as extreme as what we found with the males in our research.""
Unlocking chemical clues from whiskers
Getting close enough to study the diet of male elephant seals in the wild can be challenging, given their powerful stature and aggressive temperament -- they can weigh up to four tonnes, dwarfing their more placid, smaller female counterparts. Instead, researchers analyse the composition of smaller hard tissues back in the lab that keep more detailed records of the animal's activity.

For the study, the scientists inferred the eating habits of 31 male southern elephant seals from the Western Antarctic Peninsula by analysing a whisker sample from each seal that holds stable isotopes -- chemical clues about the food they've eaten in the past. Each whisker analysed in the study contained up to a year's worth of data about each seal's culinary preferences, allowing the team to assemble the most extensive picture of the male southern elephant seal diet to date.
""These guys are out in the water foraging for months, and then fasting for two to three months on land during breeding season, so it's hard to gather a lot of information about their diet through study methods like stomach analysis,"" Ms Cormack says. ""But by analysing hard tissues that store an inert chemical record of what they've eaten, we can start putting together the pieces about their eating habits.""
The researchers found nearly all male southern elephant seals are specialists, with individuals consistently eating the same food items over time with little variation in diet. Most of those could be considered extreme specialists, feeding on less than 20 per cent of their population's range of food types. Meanwhile, only one seal in the sample was a generalist and chose to eat a broad diet of various food sources.
""The Antarctic ecosystem has a lot of variety, but male southern elephant seals don't like to mix it up,"" says Professor Tracey Rogers, a marine ecologist at UNSW Science and senior author of the study. ""They each have their own favourite foods, and they stick to them despite all the options available.""
Speculation over specialisation
The study found variation between individuals strongly correlated with their body size. The heaviest seals ate further up the trophic scale, or food chain. In other words, they ate larger energy-dense prey, such as big squid, leading up to the breeding season.

But it wasn't the size of the seal that determined specialisation. Even seals on the smaller end that fed further down the trophic scale on smaller fish were extreme specialists from early adult life.
""They were all consistently picky on their food type regardless of size,"" Ms Cormack says. ""For these guys, who can lose up to 50 per cent of their body weight during the breeding season when they're fasting on land, what you choose to eat could be very important.""
The researchers don't know the exact cause of the male's picky eating habits but suspect it could be for many reasons including gape size, which may dictate ideal prey size and feeding technique, to fluctuations in seasonal and yearly food availability.
Specialisation could also have several ramifications. Concentrating on different, but very particular food types -- even a subpar option- could help improve foraging success rates between male seals and allow them to gain the size needed to compete with other males for breeding rights.
""We know from previous studies that individuals will often return each year to the same feeding grounds in search of their favourite foods,"" Ms Cormack says. ""But we need more studies to be confident about exactly what's driving specialisation and how it impacts breeding success.""
The looming climate change threat
Thanks to conservation efforts, the southern elephant seal population has bounced back after being hunted to near extinction and is no longer considered endangered. However, the risk of a new decline looms from the impact of climate change.
For example, climate-driven shifts in the Southern Ocean can change krill availability, which impacts the seal's ability to feed on its favourite foods further up the food chain. Furthermore, emerging pathogens like deadly avian flu strains also threaten to wipe out populations.
""The Western Antarctic Peninsula, where these incredible animals live, is one of the areas experiencing the greatest changes from ocean warming,"" Prof. Rogers says. ""The ice cliffs are almost completely gone, and the periods where new ice forms are getting shorter, changing the whole ecosystem. This could be problematic, with climate change affecting food resource availability across the Southern Ocean, which is why we need more research.""
Future work would also benefit from increasing the sample size to make the analysis more robust, looking at groups of male southern elephant seals from other colonies around the Southern Ocean, and more satellite tracking to improve our understanding of the seal's dietary specialisation.
""We've established a solid foundation of evidence about how the individual diets of male southern elephant seals vary,"" Ms Cormack says. ""It would be great to do more longitudinal-type studies to continue building our knowledge of male southern elephant seal specialisation and its impacts.""

","score: 13.128323597096834, grade_level: '13'","score: 14.579337935917863, grade_levels: ['college_graduate'], ages: [24, 100]",10.3354/meps14472,"Although dietary studies have provided important insights into the causes and ramifications of diet variation for the southern elephant seal (SES) (Mirounga leonina), adult males are comparatively underrepresented within that literature. Individual males can vary morphologically as well as behaviourally, leading to differences in their life history trajectories and outcomes. Therefore, to improve our understanding of the male diet, we sought to determine the degree of dietary variation between as well as within individuals from the West Antarctic Peninsula. Secondly, we investigated whether individual morphological traits, seasonality, and year influenced their dietary variation. Whiskers were sampled from 31 adult male seals and used to measure the bulk stable isotope nitrogen (δ15N). We sequentially segmented each whisker to create a time series of datapoints for each individual, allowing us to compare δ15N variation within each seal as well as assess variation between the seals. We then investigated the relationships between male dietary variation and body length, girth, season, and year. We found that adult male SESs maintained an extremely specialised diet. Variation between individuals was strongly correlated with their body size, with larger seals feeding higher up the trophic web. Interestingly, seasonality and year both influenced variation within the seals’ diets, but only year was seen to influence the variability between seals. We discuss the possible causes and ramifications of dietary specialisation for the SES and highlight the need for combined tracking and stable isotope investigations to improve our understanding of the ontogeny of the seals’ dietary specialisation."
"
The Santa Barbara Channel's kelp forests and its sandy beaches are intimately connected. Giant kelp, the foundation species of rocky reefs, serves as a major part of the beach food web as fronds of the giant seaweed break away from the forest and are transported to the beach. But the relationship goes deeper.

In a paper published this week in the Proceedings of the National Academy of Sciences, a team of scientists demonstrated that kelp forests can do more than supply food to tiny, hungry crustaceans living in the sand. They can also influence the dynamics of the sandy beach food web.
""The amount of kelp on the reef changes through time in a way where the peaks and low points in abundance across several kelp forests are matched together,"" said lead author Jonathan Walter, a senior researcher at the University of California, Davis, and its Center for Watershed Sciences. ""That's what we refer to as synchrony. It is related to the ability of systems to persist in the face of changing environmental conditions. A little asynchrony allows systems to be resistant to fluctuations and therefore more stable.""
The study uncovers the role of synchrony in the beach food web, with broader implications as the climate shifts in ways that might change how linked ecosystems perform their functions.
Revealing synchrony's role in these ecosystems fills a key knowledge gap in our understanding of the connection of reef and beach.
""The kelp forest and the beach are both highly dynamic ecosystems,"" said co-author Jenny Dugan, a coastal marine ecologist at UC Santa Barbara. ""How the dynamics of those two ecosystems interact and behave is the key question here, especially with the beach system so dependent on the kelp forest.""
In sync
Though a natural and ubiquitous phenomenon, synchrony and its implications are not yet fully understood.

The research team sought to understand whether and how kelp wrack (detritus) could affect the beach ecosystem's dynamics. For instance, how might species respond to the changing environment, and how resilient is the beach ecosystem to disturbances?
To address these questions, the study used long-term data from UCSB's Santa Barbara Coastal Long Term Ecological Research site, which is supported by the National Science Foundation. The team's model was built on a time series of wind, wave, wrack, and beach-width data at five sandy beaches over 11 years.
It revealed patterns of synchrony -- where the abundance of kelp wrack on beaches could be explained by kelp abundance in the forest, wave action, and beach width fluctuating together. At the longest timescales, kelp forest biomass and beach width were the biggest drivers of kelp wrack on the beaches.
Beach melodies
""We found time lags in this synchrony that were important,"" Dugan said. ""It wasn't as simple as everything changing at the same time -- it was like separate songs or melodies that came together in different ways. This made the patterns more complex, which is why it required the type of analyses we used.""
Importantly, the researchers found this synchrony crossed from ocean to shore. The abundance of predatory shorebirds, like sandpipers and plovers, lagged behind the deposition of wrack on beaches.

""Once on the beach, kelp wrack feeds a highly productive community of small invertebrates -- crustaceans and insects -- that are in turn a favorite food of shorebirds,"" Dugan explained. The cross-system synchrony is particularly notable because the beach ecosystem relies so heavily on kelp subsidies, she added.
Dynamic nature
""The dynamic nature of kelp forests, in terms of their high productivity and turnover, is unique for ecosystems structured around foundation species,"" said co-author and coastal ecologist Kyle Emery, a researcher in the UCSB Marine Science Institute. ""It allows us to observe change many times over compared to other foundation species and gives us the ability to observe many different system states, processes and functions. This enabled us to more rapidly analyze these questions of cross-ecosystem synchrony.""
The study was funded by the Santa Barbara Coastal Long Term Ecological Research, National Science Foundation, McDonnell Foundation and Humboldt Foundation.

","score: 12.396880471932207, grade_level: '12'","score: 13.396967086474085, grade_levels: ['college_graduate'], ages: [24, 100]",10.1073/pnas.2310052120,"Cross-ecosystem subsidies are critical to ecosystem structure and function, especially in recipient ecosystems where they are the primary source of organic matter to the food web. Subsidies are indicative of processes connecting ecosystems and can couple ecological dynamics across system boundaries. However, the degree to which such flows can induce cross-ecosystem cascades of spatial synchrony, the tendency for system fluctuations to be correlated across locations, is not well understood. Synchrony has destabilizing effects on ecosystems, adding to the importance of understanding spatiotemporal patterns of synchrony transmission. In order to understand whether and how spatial synchrony cascades across the marine-terrestrial boundary via resource subsidies, we studied the relationship between giant kelp forests on rocky nearshore reefs and sandy beach ecosystems that receive resource subsidies in the form of kelp wrack (detritus). We found that synchrony cascades from rocky reefs to sandy beaches, with spatiotemporal patterns mediated by fluctuations in live kelp biomass, wave action, and beach width. Moreover, wrack deposition synchronized local abundances of shorebirds that move among beaches seeking to forage on wrack-associated invertebrates, demonstrating that synchrony due to subsidies propagates across trophic levels in the recipient ecosystem. Synchronizing resource subsidies likely play an underappreciated role in the spatiotemporal structure, functioning, and stability of ecosystems."
"
Oregon State University researchers have discovered vitamin B1 produced by microbes in rivers, findings that may offer hope for vitamin-deficient salmon populations.

Findings were published in Applied and Environmental Microbiology.
The authors say the study in California's Central Valley represents a novel piece of an important physiological puzzle involving Chinook salmon, a keystone species that holds significant cultural, ecological and economic importance in the Pacific Northwest and Alaska.
Christopher Suffridge, senior research associate in the Department of Microbiology in the OSU College of Science, and doctoral student Kelly Shannon examined concentrations of thiamine and the microbial communities in rivers of the Sacramento River watershed. Thiamine is the compound commonly referred to as vitamin B1 and is critical to cellular function in all living organisms.
""This study is the first-ever report of thiamine compounds in salmon spawning rivers and the associated gravels where salmon spawn,"" Suffridge said. ""This source of thiamine has potential implications for reducing health impacts on naturally spawning salmon that are suffering from thiamine deficiency complex.""
TDC, an emerging threat to the stability of West Coast salmon populations, has affected salmon and trout in lake systems in northeastern North America and Atlantic salmon in the Baltic Sea.
Chinook salmon in the Central Valley have recently been diagnosed with TDC, the researchers note. Afflicted female salmon that return to rivers and streams to spawn can pass the deficiency on to their hatchlings, which have problems swimming and experience high mortality rates.

""In California, most hatchery-spawning Chinook salmon are treated with thiamine to prevent TDC,"" Suffridge said. ""However, it was previously unknown if there was a source of thiamine in the environment that could potentially rescue naturally spawning salmon afflicted with TDC. We have now identified microbially produced thiamine in natural salmon spawning habitats.""
""It's a complicated issue,"" Shannon added. ""The broader context is that Central Valley Chinook salmon, as well as some populations of salmon in other places, are becoming thiamine deficient because of shifts in their diet in their feeding grounds.""
Historically, Shannon said, Central Valley Chinook salmon ate a diverse, healthy diet consisting of many different species of prey fish. But in recent years, shifts in the ocean ecosystem have caused northern anchovy populations to explode, meaning they've become the primary dietary component for salmon. This change in diet is the likely cause of TDC, he said.
""Northern anchovies are high in an enzyme called thiaminase that degrades thiamine,"" Shannon said. ""So by the time many California Central Valley Chinook salmon are ready to spawn they have been feeding on so many anchovies that they have become deficient in thiamine from the activity of the thiaminase enzyme in anchovies.""
The results of the new study implicate river sediments as likely sources of microbial thiamine, which could supplement early life stages of Chinook salmon that experience TDC, he said. Future studies will examine to what degree environmental thiamine acquisition by adult Chinook salmon, their incubating eggs and hatched fry could alleviate the negative health outcomes caused by TDC.
""It was unknown if the vitamin could even be measured in rivers in the first place, and the thiamine concentrations we measured were much lower -- more than a million times lower -- than a hatchery thiamine bath,"" Shannon added. ""The data have implications for salmon health but are not concrete enough to say anything definitive. More research is needed to determine what role the environmental thiamine might play, but obviously learning that it's there is an important first step.""
The collaboration included Rick Colwell, a professor in the OSU College of Earth, Ocean, and Atmospheric Sciences, and Hailey Matthews, who graduated from the Oregon State Honors College in June 2023.
Also taking part in the study were scientists from the National Oceanic and Atmospheric Administration, the University of California, Davis, Bronx Community College and the California Department of Water Resources.
The California Department of Fish and Wildlife was the primary funder of this research. Additional support was provided by the National Science Foundation.

","score: 14.48966022853676, grade_level: '14'","score: 15.750189604611187, grade_levels: ['college_graduate'], ages: [24, 100]",10.1128/aem.01760-23,"Thiamine deficiency complex (TDC) is a major emerging threat to global populations of culturally and economically important populations of salmonids. Salmonid eggs and embryos can assimilate exogenous thiamine, and evidence suggests that microbial communities in benthic environments can produce substantial amounts of thiamine. We therefore hypothesize that natural dissolved pools of thiamine exist in the surface water and hyporheic zones of riverine habitats where salmonids with TDC migrate, spawn, and begin their lives. To examine the relationship between dissolved thiamine-related compounds (dTRCs) and their microbial source, we determined the concentrations of these metabolites and the compositions of microbial communities in surface and hyporheic waters of the Sacramento River, California and its tributaries. Here we determine that all dTRCs are present in femto-picomolar concentrations in a range of critically important salmon spawning habitats. We observed that thiamine concentrations in the Sacramento River system are orders of magnitude lower than those of marine waters, indicating substantial differences in thiamine cycling between these two environments. Our data suggest that the hyporheic zone is likely the source of thiamine to the overlying surface water. Temporal variations in dTRC concentrations were observed where the highest concentrations existed when Chinook salmon were actively spawning. Significant correlations were seen between the richness of microbial taxa and dTRC concentrations, particularly in the hyporheic zone, which would influence the conditions where embryonic salmon incubate. Together, these results indicate a connection between microbial communities in freshwater habitats and the availability of thiamine to spawning TDC-impacted California Central Valley Chinook salmon. Pacific salmon are keystone species with considerable economic importance and immeasurable cultural significance to Pacific Northwest indigenous peoples. Thiamine deficiency complex has recently been diagnosed as an emerging threat to the health and stability of multiple populations of salmonids ranging from California to Alaska. Microbial biosynthesis is the major source of thiamine in marine and aquatic environments. Despite this importance, the concentrations of thiamine and the identities of the microbial communities that cycle it are largely unknown. Here we investigate microbial communities and their relationship to thiamine in Chinook salmon spawning habitats in California’s Sacramento River system to gain an understanding of how thiamine availability impacts salmonids suffering from thiamine deficiency complex."
"
Seismic events that coincided with sudden drops in pressure within the Nord Stream 1 and 2 natural gas pipelines in September 2022 alerted the world to the rupture of pipelines in the western Baltic Sea. The suspected act of sabotage, which reportedly used explosive charges to rupture the pipelines, is still under investigation by multiple countries.

A new study published in The Seismic Record provides further evidence that the Nord Stream seismic signals came from a complex source. The signals lasted longer than would be expected from a single explosive source, the researchers say, and were more like the signals detected from an underwater volcano or a pipeline venting gas.
The initial signals from seismic events detected on 26 September 2022 ""may be dominated by energy generated by the rapid venting of high-pressure gas, which means it may be difficult to assess the source size and characteristics of any explosive charges used to rupture the Nord Stream pipelines,"" said Ross Heyburn of AWE Blacknest.
The Nord Stream events offer a rare opportunity to study seismic and infrasound signals from the rupture of an underwater gas pipeline, Heyburn and colleagues noted. The researchers had access to data collected by local and regional seismic networks as well as seismic and infrasound data collected by the International Monitoring System (IMS), a global network that detects nuclear and other explosions for the Comprehensive Nuclear-Test-Ban Treaty Organization (CTBTO).
""To the best of our knowledge, this was the first time that the IMS has recorded signals from an underwater event associated with a gas pipeline rupture,"" Heyburn said. ""The events therefore provided an opportunity to observe the characteristics of signals, such as the long durations, generated by this type of source.""
Natural seismic activity in the region is low, but the research team was able to analyze the Nord Stream signals with the help of seismic data from a few small earthquakes and explosions detonated during a 2019 NATO operation in the region to clear World War II British ground mines.
One of the methods seismologists use to determine whether a seismic event is caused by an explosion or an earthquake is to measure the ratio of P to S waves for the event. Explosions usually have a higher ratio of P to S waves than earthquakes, and the Nord Stream events are very different to nearby earthquakes in this regard, the researchers concluded.

The spectra of seismic signals from underwater explosions sometimes display a series of modulations caused by interference between the primary pulse generated by the explosion and later pulses generated by the changing size of the gas bubble created by the explosion. Heyburn and colleagues did not observe this series of modulations clearly, which suggests a complex source rather than a simple explosion source for the Nord Stream events.
One of the most striking features of the Nord Stream events is its long-lasting seismic and infrasonic signals, the researchers found. These signals decay slowly over thousands of seconds -- much longer than would be expected from an impulsive, single explosion event. For instance, the infrasound recorded in Southern Germany from the first Nord Stream event of 26 September lasted about 2000 seconds, while an impulsive event recorded at that distance would normally last no longer than about 600 seconds.
These long-lasting seismic and infrasound signals are likely the result of vibrations caused by high-pressure gas venting rapidly from the pipeline into the water and atmosphere. The lengthy signals are similar to seismic signals caused by roaring flames when pipelines explode on land, Heyburn and colleagues concluded, noting that the duration of the Nord Stream infrasound signals was similar to those detected during venting by underwater volcanoes.
The researchers also compared the magnitude of the first Nord Stream pipeline seismic event to the magnitude of a seismic event on 7 October 2023 caused by the underwater rupture of the Balticconnector gas pipeline connecting Finland and Estonia. The Balticconnector pipeline rupture is thought to have been caused by a ship's anchor rather than an explosive charge.
The difference in seismic magnitudes between the two events ""is consistent with the estimated potential energy ratio of the gas in each of these pipelines,"" Heyburn explained, which suggests that seismoacoustic signals from the initial Nord Stream event were dominated by the rapid venting of high-pressure gas.

","score: 17.00086520376176, grade_level: '17'","score: 19.493796865203763, grade_levels: ['college_graduate'], ages: [24, 100]",10.1785/0320230047,"On 26 September 2022, two seismic events were detected by regional seismic networks, coincident with media-reported leaks from the Nord Stream gas pipelines in the western Baltic Sea. In this study, we analyze seismic and infrasound signals from these two events and compare the seismic signals with those from other nearby seismic events such as underwater explosions and presumed earthquakes. Arrival times of seismic signals from the events on 26 September 2022 are used to show that the epicenters for both the events are in the vicinity of the Nord Stream pipelines. Signals from the two events display features that are characteristic of sources occurring near the seafloor. Observed P/S ratios from the Nord Stream events are also different from those observed for nearby presumed earthquakes. The observed seismic and infrasound signals are longer duration than would be expected from a single explosive source and show similarities with those observed from underwater volcano eruptions and gas pipeline explosions. The difference between seismic magnitudes estimated for the first Nord Stream pipeline event (MLP 2.32) and an event associated with the rupture of the Balticconnector pipeline on 7 October 2023 (MLP 1.09) is consistent with the estimated potential energy ratio of the gas in the pipelines. This suggests that the initial seismic signals from the first Nord Stream event may be dominated by energy generated by the venting of gas."
"
Chlorophyll plays a pivotal role in photosynthesis, which is why plants have evolved to have high chlorophyll levels in their leaves. However, making this pigment is expensive because plants invest a significant portion of the available nitrogen in both chlorophyll and the special proteins that bind it. As a result, nitrogen is unavailable for other processes. In a new study, researchers reduced the chlorophyll levels in leaves to see if the plant would invest the nitrogen saved into other process that might improve nutritional quality.

Over the past few decades, researchers have been trying to increase crop yield to meet the global food demand. One of their biggest challenges has been to improve the photosynthetic efficiency of agricultural crops.
When light hits a leaf, one of three things can happen: the leaf can absorb the light for photosynthesis, the leaf can reflect it back into the atmosphere, or the light can pass through the leaf. Unfortunately, even though a fully green leaf absorbs over 90% of the light that hits it, the leaf doesn't use it all for photosynthesis.
""We grow our crop plants at very high densities. As a result, although the leaves at the top of the canopy have more light, they cannot use it all and the layer below is light starved,"" said Don Ort (GEGC leader/CABBI/BSD), a professor of integrative biology. ""Our rationale was to reduce the amount of chlorophyll at the top of the canopy so more light can penetrate and be used more efficiently lower in the canopy.""
In the current study, the researchers engineered tobacco plants to have lower chlorophyll levels as the crop canopy grows more dense.
""Previous models have shown that if you have lower chlorophyll levels before you have a dense canopy, it is detrimental to plant growth,"" Ort said. ""We wanted to take plants that have full canopies and ensure that the new leaves that are added on top have lower chlorophyll levels.""
To do so, the researchers used small RNAs that interfere with key steps in chlorophyll synthesis. The production of these small RNAs were put under the control of an inducible promoter -- a piece of DNA that responds to a specific signal and directs the cell to produce RNA.

In the study, the researchers used an ethanol-inducible promoter. When they sprayed the leaves with ethanol, the resulting small RNAs interfered with the synthesis of chlorophyll, creating a canopy that had a lighter shade of green.
""We found that even when chlorophyll synthesis decreased 70%, there was no inhibition of growth,"" said Young Cho, a postdoctoral researcher in the Ort lab and the study's lead author. ""Although we had theoretically predicted this result, observing these pale green or yellow plants growing normally was astonishing, considering that such discoloration typically indicates plant illness.""
The researchers had also hypothesized that decreasing the amount of chlorophyll would influence other aspects of plant growth because it would free up the nitrogen that was being invested into making the pigment and associated proteins. They were proven right when they saw that the seed nitrogen concentration was 17% higher in the plants in which the ethanol-inducible promoter controlling the interfering small RNAs were activated.
""We had also expected an increase in yield because as you get more light into the canopy, you would expect it to be used more efficiently,"" Ort said. ""However, we didn't detect an increase, which probably means that the plants did not invest enough of the extra nitrogen to improve the photosynthetic capacity in the lower parts of the canopy. This result gives us another engineering target.""
In their future work, the researchers will test whether they can get similar results with light-inducible promoters, which farmers will find easier to use. ""Ethanol-inducible promoters are very convenient and important research tools. However, farmers will not want to spray an entire field with ethanol, so we need to look at other promoters that respond to the intensity or the color of light,"" Ort said.
This work is supported by the research project Realizing Increased Photosynthetic Efficiency (RIPE) which is funded by the Bill & Melinda Gates Foundation, Foundation for Food and Agriculture Research, and U.K. Foreign, Commonwealth & Development Office.

","score: 13.043945077670369, grade_level: '13'","score: 14.147048045015147, grade_levels: ['college_graduate'], ages: [24, 100]",10.1111/pce.14737,"Chlorophyll is the major light‐absorbing pigment for plant photosynthesis. While evolution has been selected for high chlorophyll content in leaves, previous work suggests that domesticated crops grown in modern high‐density agricultural environments overinvest in chlorophyll production, thereby lowering light use and nitrogen use efficiency. To investigate the potential benefits of reducing chlorophyll levels, we created ethanol‐inducible RNAi tobacco mutants that suppress Mg‐chelatase subunit I (CHLI) with small RNA within 3 h of induction and reduce chlorophyll within 5 days in field conditions. We initiated chlorophyll reduction later in plant development to avoid the highly sensitive seedling stage and to allow young plants to have full green leaves to maximise light interception before canopy formation. This study demonstrated that leaf chlorophyll reduction >60% during seed‐filling stages increased tobacco seed nitrogen concentration by as much as 17% while canopy photosynthesis, biomass and seed yields were maintained. These results indicate that time‐specific reduction of chlorophyll could be a novel strategy that decouples the inverse relationship between yield and seed nitrogen by utilising saved nitrogen from the reduction of chlorophyll while maintaining full carbon assimilation capacity."
"
Primates -- and this includes humans -- are thought of as highly social animals. Many species of monkeys and apes live in groups. Lemurs and other Strepsirrhines, often colloquially referred to as ""wet-nosed"" primates, in contrast, have long been believed to be solitary creatures, and it has often been suggested that other forms of social organization evolved later. Previous studies have therefore attempted to explain how and when pair-living evolved in primates.

More recent research, however, indicates that many nocturnal Strepsirrhines, which are more challenging to investigate, are not in fact solitary but live in pairs of males and females. But what does this mean for the social organization forms of the ancestors of all primates? And why do some species of monkey live in groups, while others are pair-living or solitary?
Different forms of social organization
Researchers at the Universities of Zurich and Strasbourg have now examined these questions. For their study, Charlotte Olivier from the Hubert Curien Pluridisciplinary Institute collected detailed information on the composition of social units in primate populations in the wild. Over several years, the researchers built a detailed database, which covered almost 500 populations from over 200 primate species, from primary field studies.
More than half of the primate species recorded in the database exhibited more than one form of social organization. ""The most common social organization were groups in which multiple females and multiple males lived together, for example chimpanzees or macaques, followed by groups with only one male and multiple females -- such as in gorillas or langurs,"" says last author Adrian Jaeggi from the University of Zurich. ""But one-quarter of all species lived in pairs.""
Smaller ancestors coupled up
Taking into account several socioecological and life history variables such as body size, diet or habitat, the researchers calculated the probability of different forms of social organization, including for our ancestors who lived some 70 million years ago. The calculations were based on complex statistical models developed by Jordan Martin at UZH's Institute of Evolutionary Medicine.
To reconstruct the ancestral state of primates, the researchers relied on fossils, which showed that ancestral primates were relatively small-bodied and arboreal -- factors that strongly correlate with pair-living. ""Our model shows that the ancestral social organization of primates was variable and that pair-living was by far the most likely form,"" says Martin. Only about 15 percent of our ancestors were solitary, he adds. ""Living in larger groups therefore only evolved later in the history of primates.""
Pairs with benefits
In other words, the social structure of early primates was likely more similar to that of humans today than previously assumed. ""Many, but by no means all of us, live in pairs while also being a part of extended families and larger groups and societies,"" Jaeggi says. However, pair-living among early primates did not equate to sexual monogamy or cooperative infant care, he adds. ""It is more likely that a specific female and a specific male would be seen together for most of the time and share the same home range and sleeping site, which was more advantageous to them than solitary living,"" explains last author Carsten Schradin from Strasbourg. This enabled them to fend off competitors or keep each other warm, for example.

","score: 14.186696969696971, grade_level: '14'","score: 14.528896969696973, grade_levels: ['college_graduate'], ages: [24, 100]",10.1073/pnas.2215401120,"Explaining the evolution of primate social organization has been fundamental to understand human sociality and social evolution more broadly. It has often been suggested that the ancestor of all primates was solitary and that other forms of social organization evolved later, with transitions being driven by various life history traits and ecological factors. However, recent research showed that many understudied primate species previously assumed to be solitary actually live in pairs, and intraspecific variation in social organization is common. We built a detailed database from primary field studies quantifying the number of social units expressing different social organizations in each population. We used Bayesian phylogenetic models to infer the probability of each social organization, conditional on several socioecological and life history predictors. Here, we show that when intraspecific variation is accounted for, the ancestral social organization of primates was inferred to be variable, with the most common social organization being pair-living but with approximately 10 to 20% of social units of the ancestral population deviating from this pattern by being solitary living. Body size and activity patterns had large effects on transitions between types of social organizations. As in other mammalian clades, pair-living is closely linked to small body size and likely more common in ancestral species. Our results challenge the assumption that ancestral primates were solitary and that pair-living evolved afterward emphasizing the importance of focusing on field data and accounting for intraspecific variation, providing a flexible statistical framework for doing so."
"
Perovskite nanosheets show distinctive characteristics with significant applications in science and technology. In a recent study, researchers from Korea and UK achieved enhanced signal amplification in CsPbBr3 perovskite nanosheets with a unique waveguide pattern, which enhanced both gain and thermal stability. These advancements carry wide-ranging implications for laser, sensor, and solar cell applications, and can potentially influence areas like environmental monitoring, industrial processes, and healthcare.

Perovskite materials are still attracting a lot of interest in solar cell applications. Now, the nanostructures of perovskite materials are being considered as a new laser medium. Over the years, light amplification in perovskite quantum dots has been reported, but most of the works present inadequate quantitative analysis. To assess the light amplification ability, ""gain coefficient"" is necessary, whereby the essential characteristic of a laser medium is revealed. An efficient laser medium is one that has a large gain.
Scientists have been exploring ways to boost this gain. Now, in a recent study, a team of researchers, led by Professor Kwangseuk Kyhm from the Department of Optics & Mechatronics at Pusan National University in Korea, has managed to enhance signal amplification in perovskite nanosheets of CsPbBr3 with a unique waveguide pattern. Their study was published in the journal Light: Science & Applications on 24 November 2023.
Perovskite nanosheets are two-dimensional structures arranged in sheet-like configurations on the nanoscale and possess characteristics that make them valuable for various applications. Their achievement overcomes the shortcomings of CsPbBr3 quantum dots, whose gain is inherently limited due to the Auger process, which essentially shortens the decay time for population inversion (a state in which more members of the system are in higher, excited states than in lower, unexcited energy states). Prof. Kyhm explains: ""Perovskite nanosheets can be a new laser medium, and this work has demonstrated that light amplification can be achieved based on tiny perovskite nanosheets that are synthesized chemically.""
The researchers also proposed a new gain analysis of ""gain contour"" to overcome the limit of earlier gain analysis. While the old method provides a gain spectrum, it is unable to analyze the gain saturation for long optical stripe lengths. Because the ""gain contour"" illustrates the variation of the gain with respect to spectrum energy and optical stripe length, it is very convenient to analyze the local gain variation along spectrum energy and optical stripe length.
The researchers also studied the excitation and temperature dependence of the gain contour and the patterned waveguide, based on polyurethane-acrylate, which boosted both the gain and thermal stability of perovskite nanosheets. This enhancement was attributed to improved optical confinement and heat dissipation, which was facilitated by the two-dimensional center-of-mass confined excitons and localized states arising from the inhomogeneous sheet thickness and the defect states.
The implementation of such a patterned waveguide is promising for efficient and controlled signal amplification and can contribute to the development of more reliable and versatile devices based on perovskite nanosheets, including lasers, sensors, and solar cells. In addition, it could also impact industries related to encryption and decryption of information, neuromorphic computing, and visible light communication. Furthermore, enhanced amplification and increased efficiencies can help perovskite solar cells compete better with traditional silicon-based solar cells.
The study is also poised to significantly influence optics and photonics. The insights gained can help optimize laser operation, enhance signal transmission in optical communication, and improve sensitivity in photodetectors. This, in turn, could allow devices to operate more reliably.
In the long term, when intense light is needed at the nanoscale, perovskite nanosheets can be combined with other nanostructures, allowing the amplified light to serve as an optical probe. However, the successful application of perovskite nanosheets in diverse areas, including consumer products like smartphones and lighting, would depend on overcoming challenges related to their stability, scalability, and toxicity.
""So far, perovskite quantum dots have been studied for lasers, but such zero-dimensional structures have fundamental limits. In this regard, our work suggests that the two-dimensional structure of perovskite nanosheets can be an alternative solution,"" concludes Prof. Kyhm.

","score: 16.45275351213282, grade_level: '16'","score: 17.686419923371652, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41377-023-01313-0,"Optical gain enhancement of two-dimensional CsPbBr3 nanosheets was studied when the amplified spontaneous emission is guided by a patterned structure of polyurethane-acrylate. Given the uncertainties and pitfalls in retrieving a gain coefficient from the variable stripe length method, a gain contour $$g(\hslash \omega ,x)$$ g ( ℏ ω , x ) was obtained in the plane of spectrum energy (ℏω) and stripe length (x), whereby an average gain was obtained, and gain saturation was analysed. Excitation and temperature dependence of the gain contour show that the waveguide enhances both gain and thermal stability due to the increased optical confinement and heat dissipation, and the gain origins were attributed to the two-dimensional excitons and the localized states."
"
If you have a deep-seated, nagging worry over dropping your phone in molten lava, you're in luck.

A research team led by materials scientists at Duke University has developed a method for rapidly discovering a new class of materials with heat and electronic tolerances so rugged that they that could enable devices to function at lava-like temperatures above several thousands of degrees Fahrenheit.
Harder than steel and stable in chemically corrosive environments, these materials could also form the basis of new wear- and corrosion-resistant coatings, thermoelectrics, batteries, catalysts and radiation-resistant devices.
The recipes for these materials -- ceramics made using transition metals carbonitrides or borides -- were discovered through a new computational method called Disordered Enthalpy-Entropy Descriptor (DEED). In its first demonstration, the program predicted the synthesizability of 900 new formulations of high-performance materials, 17 of which were then tested and successfully produced in laboratories.
The results appear online January 3 in the journal Nature and include contributions from collaborators at Penn State University, Missouri University of Science and Technology, North Carolina State University, and State University of New York at Buffalo.
""The capability of rapidly discovering synthesizable compositions will allow researchers to focus on optimizing their industry-disrupting properties,"" said Stefano Curtarolo, the Edmund T. Pratt Jr. School Distinguished Professor of Mechanical Engineering and Materials Science at Duke.
The Curtarolo group maintains the Duke Automatic-FLOW for Materials Database (AFLOW) -- an enormous reservoir of material properties data connected to many online tools for materials optimization. This wealth of information allows algorithms to accurately predict the properties of unexplored mixtures without having to attempt to simulate the complexities of atomic dynamics or make them in the laboratory.

For the past several years, the Curtarolo group has been working to develop predictive powers for ""high-entropy"" materials that derive enhanced stability from a chaotic mixture of atoms rather than relying solely on the orderly atomic structure of conventional materials. In 2018, they discovered high-entropy carbides, which were a simpler, special-case scenario.
""The high-entropy carbides all had a relatively uniform amount of enthalpy, so we could ignore part of the equation,"" Curtarolo said. ""But to predict new ceramic recipes with other transition metals, we had to address the enthalpy.""
To better understand the concepts of entropy and enthalpy in this application, think of a 10-year-old trying to construct a doghouse out of a giant pile of Legos. Even with limited types of building blocks, there would be many possible design outcomes.
In simple terms, enthalpy is a measure of how sturdy each design is, and entropy a measure of the number of possible designs that all have similar strength. The first promotes ordered configurations, like those that might be found in instruction booklets. The latter captures the unavoidable chaos that would occur as the child puts more time and energy into the increasingly confusing construction effort. Both are a measure of the amount of energy and heat that end up being absorbed into the final product.
""To rapidly quantify both enthalpy and entropy, we had to calculate the energy contained within the hundreds of thousands of various combinations of ingredients that we could potentially create instead of the ceramics we're looking for,"" Curtarolo said. ""It was a mammoth undertaking.""
Besides predicting new recipes for stable disordered ceramics, DEED also helps direct their further analysis to discover their inherent properties. To find the optimal ceramics for various applications, researchers will need to refine these calculations and physically test them in laboratories.

DEED is specifically tailored to a production method called hot-pressed sintering. This involves taking powdered forms of the constituent compounds and heating them in a vacuum to as high as 4000 degrees Fahrenheit while applying pressure for times that can be as long as a few hours. Between all the preparation, reaction and cooling times, the entire process takes more than eight hours.
""The final step in synthesis, called spark plasma sintering, is an emerging method in materials science that is common in research labs,"" said William Fahrenholtz, the Curators' Distinguished Professor of Ceramic Engineering at Missouri S&T.
The finished ceramics have a metallic appearance and look dark grey or black. They feel like metal alloys such as stainless steel and have a similar density, but they are much darker in appearance. And even though they appear metallic, they are hard and brittle like conventional ceramics.
Moving forward, the group expects other researchers to begin using DEED to synthesize and test the properties of new ceramic materials for various applications. Given the incredible array of potential properties and uses, they believe it's only a matter of time before some of them enter commercial production.
""Spark plasma sintering or field assisted sintering technology (FAST) is not a common technique in industry yet,"" added Doug Wolfe, professor of materials science and engineering and associate vice president for research at Penn State. ""However, current ceramic manufacturers could pivot to making these materials by making small adjustments to existing processes and facilities.""
This research was primarily supported by a five-year, $7.5 million grant through the US Department of Defense's Multidisciplinary University Research Initiative (MURI) competition led by Curtarolo (N00014-21-1-2515, N00014-23-1-2615) and the Department of Defense High Performance Computing Modernization Program (HPC-Frontier).

","score: 16.005904330963155, grade_level: '16'","score: 17.3007094376212, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41586-023-06786-y,"The need for improved functionalities in extreme environments is fuelling interest in high-entropy ceramics1–3. Except for the computational discovery of high-entropy carbides, performed with the entropy-forming-ability descriptor4, most innovation has been slowly driven by experimental means1–3. Hence, advancement in the field needs more theoretical contributions. Here we introduce disordered enthalpy–entropy descriptor (DEED), a descriptor that captures the balance between entropy gains and enthalpy costs, allowing the correct classification of functional synthesizability of multicomponent ceramics, regardless of chemistry and structure. To make our calculations possible, we have developed a convolutional algorithm that drastically reduces computational resources. Moreover, DEED guides the experimental discovery of new single-phase high-entropy carbonitrides and borides. This work, integrated into the AFLOW computational ecosystem, provides an array of potential new candidates, ripe for experimental discoveries."
"
n Hawaiʻi and across much of Oceania, Pacific Islanders celebrate the connections between their islands and the ocean that surrounds them. ""As descendants of the ocean, the dearth of Native Hawaiians and Pacific Islanders (NHPI) in ocean science seems inconsonant,"" writes a team of authors that includes University of Hawai'i (UH) at Mānoa faculty, students, and alumni in an article in a special issue of the journal Oceanography, ""Building Diversity, Equity, and Inclusion in the Ocean Sciences. The authors ask, ""Where are all our island people in the ocean sciences?""

""To understand the root causes of this disparity and potential solutions, UH faculty, staff and students approached this problem through the lens of voyagers, examining the past course of history of the peoples of the Pacific and attempts to make headwinds in programs focused on increasing participation in ocean sciences,"" said co-author Rosie Alegado, associate professor in the UH Mānoa School of Ocean and Earth Science and Technology (SOEST).
The article highlights programs in SOEST that are aimed at reducing barriers for Native Hawaiians in the geosciences -- including summer bridge programs, internships, and other professional development programs. And, in better defining the persistent, systemic, and collective barriers that NHPI face within the western society and the academy, the authors identify gaps that conventional professional development programs aimed at minoritized groups in the geosciences have been unsuccessful in filling.
""One of the biggest gaps that we found related to Native Hawaiian-serving programs within the ocean sciences is that while many may be culturally based, few are Native Hawaiian led,"" said lead author Haunani Kane, SOEST assistant professor. ""Native Hawaiians are often overlooked in the development and leadership of Native Hawaiian and Pacific Islander-serving programs. Programs led by Native Hawaiian scientists and community members ensure that they are culturally centered safe spaces for students to collectively grow their identities as both Native Hawaiians and scientists.""
Importantly, the authors shared lessons learned from building two waʻa (canoes)-programs specifically designed to carry students forward toward futures that center oceanic ways of knowing.
SOEST Maile Mentoring Bridge
The SOEST Maile Mentoring Bridge program (Maile) was founded in 2013 with the goal of attracting and retaining more NHPIs into geoscience degree programs and careers. The foundation of Maile was to build and foster robust partnerships with neighboring community colleges within the UH system. Maile mentees are carefully paired with experienced mentors -- SOEST graduate students, postdocs, or recent graduates.

""Looking back on the last 10 years of my life, the Maile Mentoring program has made such a huge impact,"" said Diamond Tachera, study co-author and alumni and co-director of Maile. ""As an undergraduate student, it was so important for me to see people, especially wāhine (women), who looked like me working and thriving in their scientific fields. Being part of the Maile ʻohana as a graduate student mentor also helped me to build confidence in myself as I continued to struggle to find my place and identity in academia. I will be forever grateful for the support and aloha that comes with being part of the Maile ʻohana.""
""I believe the Maile Mentoring program has been successful because it places an emphasis on meeting the needs of the whole student, not just their research endeavors,"" said Alegado. ""In focusing on creating a nurturing environment in SOEST, we place a stronger emphasis on retention of students, not just recruitment, which increases completion and graduation rates for NHPI.""
The MEGA Lab
To overcome traditional barriers related to retention of NHPIs in the ocean sciences, the multiscale environmental graphical analysis (MEGA) Lab, a predominantly Native Hawaiian-led lab and nonprofit physically located in Hilo, Hawai'i, developed a research program that prioritizes inclusive research experiences. Foundational to their success has been incorporating community members and cultural values into research projects, and creating global partnerships that value Native Hawaiian research.
As a way to creatively explore what Native science and kuleana (responsibility) could look like if research and cultural priorities were equally weighted in all aspects of the research design, the MEGA Lab assembled a Native Hawaiian research team to embark on a 15-day voyage to Papahānaumokuākea Marine National Monument.
""That trip inspired me to re-imagine what research looks like when it's grounded in our ʻōiwi perspectives and how I can contribute to create more room for that to happen,"" said Kainalu Steward, graduate student in the SOEST Department of Earth Sciences. ""That experience helped me find kuleana in this collective work at the monument and reinforced my interest in pursuing higher education.""
Looking to the horizon

""Moving forward, we believe that in order to make progress in the representation, retainment, and success of Native Hawaiians and Pacific islanders in STEM, we must first address the historical and ongoing traumas of Native Hawaiians and Pacific Islanders through active engagement in reclamation of cultural identities and knowledge,"" said Kane. ""We also believe student success requires building community support systems both within and beyond UH where students can safely explore their whole identity as Indigenous scientists.""
The MEGA Lab founders are also calling for a culture change in academia and their ""experiment to disrupt the hierarchical and stereotypical structures that exist in science and act as barriers to inclusion,"" as they write in a second article in the special issue of Oceanography, provides a template. ""Our goal was to create an interdisciplinary and inter-institutional lab that promotes an inclusive, equitable, and uplifting team environment where everyone can thrive in a fun and productive workspace.""
""All of the work we do to support Native Hawaiians, women, and other underrepresented groups (the fish) can only have limited success given our current toxic workplace culture (the fishbowl),"" said Barbara Bruno, faculty specialist at SOEST and co-author of the first article. ""The fishbowl -- ​not the fish -- ​ needs to change.""
""Academia can often be reluctant to change, which is unfortunate as much of the workplace culture can serve as barriers to inclusion in STEM,"" said John Burns, lead author of the second article and associate professor at UH Hilo. ""We must embrace open-mindedness and be ready to transform the very culture of science in order to enhance diversity. Diverse perspectives and ideas not only foster a healthy work environment but can also serve as our most powerful asset, fueling the drive for new discoveries.""

","score: 17.664403966131903, grade_level: '18'","score: 19.726070911319077, grade_levels: ['college_graduate'], ages: [24, 100]",10.5670/oceanog.2024.137,"In Hawai‘i and across much of Oceania, Pacific Islanders celebrate the connections between our islands and the ocean that surrounds us. Since the beginning of time, we have relied upon precise observations of marine and celestial realms to intentionally navigate thousands of miles across vast expanses of open ocean. Through our migrations, we have created—and continue to create—purposeful relationships by observing the movements of swells, weather patterns, celestial bodies, and marine life. In direct opposition to colonial Western thought, we view Oceania as a metaphorical road that connects rather than separates island people (Hau’ofa, 1994). As descendants of the ocean, the dearth of Native Hawaiians and Pacific Islanders (NHPIs) in ocean science seems inconsonant. We wonder, where are all our island people in the ocean sciences? In better defining the persistent, systemic, and collective barriers that NHPIs face within Western society and the academy, we identify gaps that conventional professional development programs aimed at minoritized groups in the geosciences have been unsuccessful in filling. We share lessons learned from building two wa‘a (canoes) in programs that center oceanic ways of knowing."
"
Plastic litter is a growing problem around the world, and new research shows that the bottom of Lake Tahoe is no exception. In one of the first studies to utilize scuba divers to collect litter from a lakebed, 673 plastic items were counted from just a small fraction of the lake.

In the study, published in the November issue of the journal Applied Spectroscopy, researchers from DRI and the UC Davis Tahoe Environmental Research Center teamed up with the nonprofit Clean Up the Lake to take a close look at the litter. First, scientists broke it down into categories based on use (such as food containers and water bottles), followed by the chemical composition of the plastic. The knowledge gained can help scientists better understand the source of large pieces of litter in the lake, as well as whether they're a significant source of microplastics as larger pieces break down and degrade. Previous research found that the waters of Lake Tahoe contain high levels of microplastics, defined as plastics smaller than a pencil eraser.
""There's very little work on submerged plastic litter in lakes,"" said Monica Arienzo, Ph.D., associate research professor of hydrology at DRI and one of the study's lead authors. ""And I think that's a real issue, because when we think about how plastics may be moving in freshwater systems, there's a good chance that they'll end up in a lake.""
To collect the litter, research divers swam transects along the lakebed near Lake Tahoe Nevada State Park and Zephyr Cove, covering 9.3 kilometers. They found an average of 83 pieces of plastic litter per kilometer, with the lakebed near Hidden Beach and South Sand Harbor showing significantly more (140 items/km and 124 items/km, respectively). No stretches of the lakebed surveyed were free of plastic litter.
The most common plastic litter categories were food containers, bottles, plastic bags, and toys, along with many items that couldn't be categorized.
""There's a lot of education we can do, as well as continuing to work on reducing the use of those plastics,"" Arienzo says. ""Because we have to start thinking about turning that plastic pipe off.""
Arienzo and co-author Julia Davidson, then an undergraduate student working in Arienzo's lab, also identified the types of plastic that made up 516 of the litter samples. Using an instrument that uses infrared light to fingerprint and identify the material, they found that the six most common plastics were polyvinyl chloride (PVC), polystyrene, polyester/polyethylene terephthalate, polyethylene, polypropylene, and polyamide. Collecting this information can contribute to Arienzo's ongoing microplastics research in the region, helping to identify the sources of the small plastic fragments.
""When we study microplastics, we only have the chemical information, or the plastic type,"" Davidson says. ""We don't know where it came from -- a plastic bag, toy, or otherwise -- because it's just a tiny piece of plastic. But now we can use this litter data to point to the dominant types of plastics and compare them to microplastic data.""
The study can help inform efforts by Tahoe-area communities to address plastic litter, such as South Lake Tahoe's 2022 ban on single-use plastic bottles and Truckee's ban on single-use food containers. The research also highlights ways that scientists can work with nonprofits to collect data that can address local environmental concerns.
""I think one of the things that's really cool about this project is the collaboration between DRI, Clean Up the Lake, and UC Davis at Tahoe,"" Arienzo says. ""It demonstrates the power of bringing together a nonprofit that really wants to clean up Tahoe, while collecting data in the process that can help answer scientific questions.""

","score: 13.32842027822365, grade_level: '13'","score: 14.71359684323167, grade_levels: ['college_graduate'], ages: [24, 100]",10.1177/00037028231201174,"Monitoring plastic litter in the environment is critical to understanding the amount, sources, transport, fate, and environmental impact of this pollutant. However, few studies have monitored plastic litter on lakebeds which are potentially important environments for determining the fate and transport of plastic litter in freshwater basins. In this study, a self-contained underwater breathing apparatus was used for litter collection at the lakebed along five transects in Lake Tahoe, United States. Litter was brought to the surface and characterized by litter type. Plastic litter was subsampled, and polymer composition was determined using attenuated total reflection Fourier transform infrared spectroscopy. The average plastic litter from the lakebed for the five dive transects was 83 ± 49 items per kilometer. The top plastic litter categories were other plastic litter (plastic litter that did not fall in another category), followed by food containers, bottles <2 L, plastic bags, and toys. These results are in line with prior studies on submerged litter, and intervention approaches or ongoing education are needed. The six polymers most frequently detected in the subsamples were polyvinyl chloride, polystyrene/expanded polystyrene, polyethylene terephthalate/polyester, polyethylene, polypropylene, and polyamide. These observations reflect global plastic production and microplastic studies from lake surface water and sediments. We found that some litter subcategories were primarily comprised of a single polymer type, therefore, in studies where the polymer type cannot be measured but litter is categorized, these results could provide an estimate of the total polymer composition for select litter categories."
"
Central features of human evolution may stop our species from resolving global environmental problems like climate change, says a new study led by the University of Maine.

Humans have come to dominate the planet with tools and systems to exploit natural resources that were refined over thousands of years through the process of cultural adaptation to the environment. University of Maine evolutionary biologist Tim Waring wanted to know how this process of cultural adaptation to the environment might influence the goal of solving global environmental problems. What he found was counterintuitive.
The project sought to understand three core questions: how human evolution has operated in the context of environmental resources, how human evolution has contributed to the multiple global environmental crises and how global environmental limits might change the outcomes of human evolution in the future.
Waring's team outlined their findings in a new paper published in Philosophical Transactions of the Royal Society B. Other authors of the study include Zach Wood, UMaine alumni, and Eörs Szathmáry, a professor at Eötvös LorándUniversity in Budapest, Hungary.
Human expansion
The study explored how human societies' use of the environment changed over our evolutionary history. The research team investigated changes in the ecological niche of human populations, including factors such as the natural resources they used, how intensively they were used, what systems and methods emerged to use those resources and the environmental impacts that resulted from their usage.
This effort revealed a set of common patterns. Over the last 100,000 years, human groups have progressively used more types of resources, with more intensity, at greater scales and with greater environmental impacts. Those groups often then spread to new environments with new resources.

The global human expansion was facilitated by the process of cultural adaptation to the environment. This leads to the accumulation of adaptive cultural traits -- social systems and technology to help exploit and control environmental resources such as agricultural practices, fishing methods, irrigation infrastructure, energy technology and social systems for managing each of these.
""Human evolution is mostly driven by cultural change, which is faster than genetic evolution. That greater speed of adaptation has made it possible for humans to colonize all habitable land worldwide,"" says Waring, associate professor with the UMaine Senator George J. Mitchell Center for Sustainability Solutions and the School of Economics.
Moreover, this process accelerates because of a positive feedback process: as groups get larger, they accumulate adaptive cultural traits more rapidly, which provides more resources and enables faster growth.
""For the last 100,000 years, this has been good news for our species as a whole."" Waring says, ""but this expansion has depended on large amounts of available resources and space.""
Today, humans have also run out of space. We have reached the physical limits of the biosphere and laid claim to most of the resources it has to offer. Our expansion also is catching up with us. Our cultural adaptations, particularly the industrial use of fossil fuels, have created dangerous global environmental problems that jeopardize our safety and access to future resources.
Global limits
To see what these findings mean for solving global challenges like climate change, the research team looked at when and how sustainable human systems emerged in the past. Waring and his colleagues found two general patterns. First, sustainable systems tend to grow and spread only after groups have struggled or failed to maintain their resources in the first place. For example, the U.S. regulated industrial sulfur and nitrogen dioxide emissions in 1990, but only after we had determined that they caused acid rain and acidified many water bodies in the Northeast. This delayed action presents a major problem today as we threaten other global limits. For climate change, humans need to solve the problem before we cause a crash.

Second, researchers also found evidence that strong systems of environmental protection tend to address problems within existing societies, not between them. For example, managing regional water systems requires regional cooperation, regional infrastructure and technology, and these arise through regional cultural evolution. The presence of societies of the right scale is, therefore, a critical limiting factor.
Tackling the climate crisis effectively will probably require new worldwide regulatory, economic and social systems -- ones that generate greater cooperation and authority than existing systems like the Paris Agreement. To establish and operate those systems, humans need a functional social system for the planet, which we don't have.
""One problem is that we don't have a coordinated global society which could implement these systems,"" says Waring, ""We only have sub-global groups, which probably won't suffice. But you can imagine cooperative treaties to address these shared challenges. So, that's the easy problem.""
The other problem is much worse, Waring says. In a world filled with sub-global groups, cultural evolution among these groups will tend to solve the wrong problems, benefitting the interests of nations and corporations and delaying action on shared priorities. Cultural evolution among groups would tend to exacerbate resource competition and could lead to direct conflict between groups and even global human dieback.
""This means global challenges like climate change are much harder to solve than previously considered,"" says Waring. ""It's not just that they are the hardest thing our species has ever done. They absolutely are. The bigger problem is that central features in human evolution are likely working against our ability to solve them. To solve global collective challenges we have to swim upstream.""
Looking forward
Waring and his colleagues think that their analysis can help navigate the future of human evolution on a limited Earth. Their paper is the first to propose that human evolution may oppose the emergence of collective global problems and further research is needed to develop and test this theory.
Waring's team proposes several applied research efforts to better understand the drivers of cultural evolution and search for ways to reduce global environmental competition, given how human evolution works. For example, research is needed to document the patterns and strength of human cultural evolution in the past and present. Studies could focus on the past processes that lead to the human domination of the biosphere, and on the ways cultural adaptation to the environment is occurring today.
But if the general outline proves to be correct, and human evolution tends to oppose collective solutions to global environmental problems, as the authors suggest, then some very pressing questions need to be answered. This includes whether we can use this knowledge to improve the global response to climate change.
""There is hope, of course, that humans may solve climate change. We have built cooperative governance before, although never like this: in a rush at a global scale."" Waring says.
The growth of international environmental policy provides some hope. Successful examples include the Montreal Protocol to limit ozone-depleting gasses, and the global moratorium on commercial whaling.
New efforts should include fostering more intentional, peaceful and ethical systems of mutual self-limitation, particularly through market regulations and enforceable treaties, that bind human groups across the planet together ever more tightly into a functional unit.
But that model may not work for climate change.
""Our paper explains why and how building cooperative governance at the global scale is different, and helps researchers and policymakers be more clear-headed about how to work toward global solutions,"" says Waring.
This new research could lead to a novel policy mechanism to address the climate crisis: modifying the process of adaptive change among corporations and nations may be a powerful way to address global environmental risks.
As for whether humans can continue to survive on a limited planet, Waring says ""we don't have any solutions for this idea of a long-term evolutionary trap, as we barely understand the problem."" says Waring.
""If our conclusions are even close to being correct, we need to study this much more carefully,"" he says.

","score: 13.217317511346447, grade_level: '13'","score: 14.161823940998481, grade_levels: ['college_graduate'], ages: [24, 100]",10.1098/rstb.2022.0259,"We propose that the global environmental crises of the Anthropocene are the outcome of a ratcheting process in long-term human evolution which has favoured groups of increased size and greater environmental exploitation. To explore this hypothesis, we review the changes in the human ecological niche. Evidence indicates the growth of the human niche has been facilitated by group-level cultural traits for environmental control. Following this logic, sustaining the biosphere under intense human use will probably require global cultural traits, including legal and technical systems. We investigate the conditions for the evolution of global cultural traits. We estimate that our species does not exhibit adequate population structure to evolve these traits. Our analysis suggests that characteristic patterns of human group-level cultural evolution created the Anthropocene and will work against global collective solutions to the environmental challenges it poses. We illustrate the implications of this theory with alternative evolutionary paths for humanity. We conclude that our species must alter longstanding patterns of cultural evolution to avoid environmental disaster and escalating between-group competition. We propose an applied research and policy programme with the goal of avoiding these outcomes. This article is part of the theme issue ‘Evolution and sustainability: gathering the strands for an Anthropocene synthesis’."
"
As climate change continues to impact people across South Florida, the need for adaptive responses becomes increasingly important.

A recent study led by researchers at the University of Miami Rosenstiel School of Marine, Atmospheric, and Earth Science, assessed the perspectives of 76 diverse South Florida climate adaptation professionals. The study titled, ""Practitioner perspectives on climate mobilities in South Florida"" was published in the December issue of the Journal Oxford Open Climate Change, and explores the expectations and concerns of practitioners from the private sector, community-based organizations, and government agencies about the region's ability to adapt in the face of increasing sea level rise and diverse consequences for where people live and move, also known as climate mobility.
Conducted through extensive interviews, the research underscores the growing significance of climate mobility as a crucial adaptive response in the face of increased climate challenges. While previous studies have primarily focused on resident perspectives on mobility, this study delves into the views of professionals, offering insights that could potentially shape future strategies and outcomes.
""This study is a deep dive aiming to understand the perspectives of leading experts on where we are right now in our climate responses in South Florida,"" said Katharine Mach, lead author of the study and a professor and chair of the Department of Environmental Science and Policy at the Rosenstiel School. ""These types of conversations are crucial to our prospects for unleashing innovations and successes in regional climate adaptations and preparedness.""
Key findings reveal a consensus among the professionals about the inevitability of various forms of climate mobilities in South Florida. Anticipated movements of people and infrastructure assets away from hazardous areas were highlighted, indicating an urgent need for comprehensive adaptation planning.
However, while recognizing the necessity of climate mobility strategies, the interviewed practitioners expressed concerns regarding the current impact of such movements. They highlighted issues of distributional inequities, socio-cultural disruptions, and financial disparities arising from ongoing migrations and gentrification in which climate plays some role.
The findings illuminated a critical gap between individual preparedness among practitioners and the overall readiness of the region to support and manage the expected climate-driven relocations. This discrepancy raises concerns about collective-action failures and the urgency for a more ambitious, long-term transition plan.

Climate mobilities, while presenting benefits, also pose significant challenges. They serve as a path for adaptation planning and policies, prompting crucial questions about incorporation into policy planning and the need for fundamental innovations.
According to the researchers, the study serves as an intervention itself, providing insights that might otherwise remain unexplored, fostering a deeper understanding of the challenges and opportunities associated with climate mobilities. The findings aim to inform and guide policymakers, stakeholders, and practitioners toward more proactive and inclusive approaches to climate adaptation.
The study's authors include: Katharine J. Mach1,2, Jennifer Niemann1,2, Rosalind Donald3, Jessica Owley1,2,4, Nadia A. Seeteram5, A.R. Siders 6,7, Xavier I. Cortada 2,4,8,9, Alex Nyburg10, Adam Roberti11, Ian A. Wright12
1 Department of Environmental Science and Policy, Rosenstiel School of Marine, Atmospheric, and Earth Science, University of Miami, Miami, FL, USA. 2 Leonard and Jayne Abess Center for Ecosystem Science and Policy, University of Miami, Coral Gables, FL, USA. 3 School of Communication, American University, Washington, DC, USA. 4 University of Miami School of Law, Coral Gables, FL, USA. 5 Columbia Climate School, Columbia University, New York, NY, USA. 6 Disaster Research Center, University of Delaware, Newark, DE, USA. 7 Biden School of Public Policy and Administration and Geography and Spatial Sciences, University of Delaware, Newark, DE, USA. 8 Department of Art and Art History, University of Miami, Coral Gables, FL, USA. 9 Department of Pediatrics, University of Miami Leonard M. Miller School of Medicine, Miami, FL, USA. 10 Department of Biology, University of Miami, Coral Gables, FL, USA. 11 Xavier Cortada Foundation, Pinecrest Gardens, FL, USA. 12 Department of Economics, University of Miami, Coral Gables, FL, USA
The study was supported by the University of Miami Laboratory for Integrative Knowledge (U-LINK), the Leonard and Jayne Abess Center for Ecosystem Science and Policy, the Rosenstiel School of Marine, Atmospheric, and Earth Science and the U.S. National Science Foundation, award numbers 2034308 and 2034239.

","score: 16.584961284230406, grade_level: '17'","score: 17.315760151085932, grade_levels: ['college_graduate'], ages: [24, 100]",10.1093/oxfclm/kgad015,"Moving away from hazardous areas may be an important adaptive response under intensifying climate change, but to date such movement has been controversial and conducted with limited government or private-sector support. Research has emphasized resident perspectives on mobility, but understanding how professionals view it may open new avenues to shape future outcomes. Based on 76 interviews with professionals involved in climate responses in South Florida, we evaluate perceptions of adaptation goals, the potential role of climate mobilities in pathways supporting those goals, and associated constraints and enablers. The practitioners interviewed anticipate multiple types of climate mobilities will occur in the region, at increasing scales. Interviewees perceive climate mobilities at present, especially migration and gentrification where climate plays some role, as causing distributional inequities and financial and sociocultural disruptions, and they view existing adaptive strategies as best serving those who already have adequate resources, despite practitioners’ personal commitments to social justice goals. Although many practitioners feel prepared for their own, limited roles related to climate mobilities, they judge the region as a whole as being unprepared to support the retreat they see as inevitable, with a need for a more ambitious long-term transition plan. Achieving this need will be difficult, as practitioners indicate that climate mobilities remain hard to talk about politically. Nevertheless, interviewees believe some households are already considering moving in response to climate risks. Discussions of climate mobilities, through interviews and far beyond, may encourage more mindful choices about and engagement in climate-driven transformations."
"
Major cities on the U.S. Atlantic coast are sinking, in some cases as much as 5 millimeters per year -- a decline at the ocean's edge that well outpaces global sea level rise, confirms new research from Virginia Tech and the U.S. Geological Survey.

Particularly hard hit population centers such as New York City and Long Island, Baltimore, and Virginia Beach and Norfolk are seeing areas of rapid ""subsidence,"" or sinking land, alongside more slowly sinking or relatively stable ground, increasing the risk to roadways, runways, building foundations, rail lines, and pipelines, according to a study published today in the Proceedings of the National Academies of Sciences.
""Continuous unmitigated subsidence on the U.S. East Coast should cause concern,"" said lead author Leonard Ohenhen, a graduate student working with Associate Professor Manoochehr Shirzaei at Virginia Tech's Earth Observation and Innovation Lab. ""This is particularly in areas with a high population and property density and a historical complacency toward infrastructure maintenance.""
Shirzaei and his research team pulled together a vast collection of data points measured by space-based radar satellites and used this highly accurate information to build digital terrain maps that show exactly where sinking landscapes present risks to the health of vital infrastructure. Using the publicly available satellite imagery, Shirzaei and Ohenhen measured millions of occurrences of land subsidence spanning multiple years. They then created some of the world's first high resolution depictions of the land subsidence.
These groundbreaking new maps show that a large area of the East Coast is sinking at least 2 mm per year, with several areas along the mid-Atlantic coast of up to 3,700 square kilometers, or more than 1,400 square miles, sinking more than 5 mm per year, more than the current 4 mm per year global rate of sea level rise.
""We measured subsidence rates of 2 mm per year affecting more than 2 million people and 800,000 properties on the East Coast,"" Shirzaei said. ""We know to some extent that the land is sinking. Through this study, we highlight that sinking of the land is not an intangible threat. It affects you and I and everyone, it may be gradual, but the impacts are real.""
In several cities along the East Coast, multiple critical infrastructures such as roads, railways, airports, and levees are affected by differing subsidence rates.
""Here, the problem is not just that the land is sinking. The problem is that the hotspots of sinking land intersect directly with population and infrastructure hubs,"" said Ohenhen. ""For example, significant areas of critical infrastructure in New York, including JFK and LaGuardia airports and its runways, along with the railway systems, are affected by subsidence rates exceeding 2 mm per year. The effects of these right now and into the future are potential damage to infrastructure and increased flood risks.""
The new findings appear in the open access journal PNAS Nexus. In the work ""Slowly but surely: Exposure of communities and infrastructure to subsidence on the US east coast,"" Virginia Tech and U.S. Geological Survey (USGS) scientists measured how much the land along the East Coast has sunk and which areas, populations, and critical infrastructure within 100 km of the coast are at risk of land subsidence. Subsidence can undermine building foundations; damage roads, gas, and water lines; cause building collapse; and exacerbate coastal flooding -- especially when paired with sea level rise caused by climate change.
""This information is needed. No one else is providing it,"" said Patrick Barnard, a research geologist with the USGS and co-author of the study. ""Shirzaei and his Virginia Tech team stepped into that niche with his technical expertise and is providing something extremely valuable.""

","score: 14.312507776318519, grade_level: '14'","score: 15.91372226446395, grade_levels: ['college_graduate'], ages: [24, 100]",10.1093/pnasnexus/pgad426,"Coastal communities are vulnerable to multihazards, which are exacerbated by land subsidence. On the US east coast, the high density of population and assets amplifies the region's exposure to coastal hazards. We utilized measurements of vertical land motion rates obtained from analysis of radar datasets to evaluate the subsidence-hazard exposure to population, assets, and infrastructure systems/facilities along the US east coast. Here, we show that 2,000 to 74,000 km2 land area, 1.2 to 14 million people, 476,000 to 6.3 million properties, and &gt;50% of infrastructures in major cities such as New York, Baltimore, and Norfolk are exposed to subsidence rates between 1 and 2 mm per year. Additionally, our analysis indicates a notable trend: as subsidence rates increase, the extent of area exposed to these hazards correspondingly decreases. Our analysis has far-reaching implications for community and infrastructure resilience planning, emphasizing the need for a targeted approach in transitioning from reactive to proactive hazard mitigation strategies in the era of climate change."
"
Synthetic biology offers the opportunity to build biochemical pathways for the capture and conversion of carbon dioxide (CO2). Researchers at the Max-Planck-Institute for Terrestrial Microbiology have developed a synthetic biochemical cycle that directly converts CO2 into the central building block Acetyl-CoA. The researchers were able to implement each of the three cycle modules in the bacterium E.coli, which represents a major step towards realizing synthetic CO2 fixing pathways within the context of living cells.

Developing new ways for the capture and conversion of CO2 is key to tackle the climate emergency. Synthetic biology opens avenues for designing new-to-nature CO2-fixation pathways that capture CO2 more efficiently than those developed by nature. However, realizing those new-to-nature pathways in different in vitro and in vivo systems is still a fundamental challenge. Now, researchers in Tobias Erb's group have designed and constructed a new synthetic CO2-fixation pathway, the so-called THETA cycle. It contains several central metabolites as intermediates, and with the central building block, acetyl-CoA, as its output. This characteristic makes it possible to be divided into modules and integrated into the central metabolism of E. coli.
The entire THETA cycle involves 17 biocatalysts, and was designed around the two fastest CO2-fixing enzymes known to date: crotonyl-CoA carboxylase/reductase and phosphoenolpyruvate carboxylase. The researchers found these powerful biocatalysts in bacteria. Although each of the carboxylases can capture CO2 more than 10 times faster than RubisCO, the CO2-fixing enzyme in chloroplasts, evolution itself has not brought these capable enzymes together in natural photosynthesis.
The THETA cycle converts two CO2 molecules into one acetyl-CoA in one cycle. Acetyl-CoA is a central metabolite in almost all cellular metabolism and serves as the building block for a wide array of vital biomolecules, including biofuels, biomaterials, and pharmaceuticals, making it a compound of great interest in biotechnological applications. Upon constructing the cycle in test tubes, the researchers could confirm its functionality. Then the training began: through rational and machine learning-guided optimization over several rounds of experiments, the team was able to improve the acetyl-CoA yield by a factor of 100. In order to test its in vivo feasibility, incorporation into the living cell should be carried out step by step. To this end, the researchers divided the THETA cycle into three modules, each of which was successfully implemented into the bacterium E. coli. The functionality of these modules was verified through growth-coupled selection and/or isotopic labelling.
""What is special about this cycle is that it contains several intermediates that serve as central metabolites in the bacterium's metabolism. This overlap offers the opportunity to develop a modular approach for its implementation."" explains Shanshan Luo, lead author of the study. ""We were able to demonstrate the functionality of the three individual modules in E. coli. However, we have not yet succeeded in closing the entire cycle so that E. coli can grow completely with CO2,"" she adds. Closing the THETA cycle is still a major challenge, as all of the 17 reactions need to be synchronized with the natural metabolism of E. coli, which naturally involves hundreds to thousands of reactions. However, demonstrating the whole cycle in vivo is not the only goal, the researcher emphasizes. ""Our cycle has the potential to become a versatile platform for producing valuable compounds directly from CO2 through extending its output molecule, acetyl-CoA."" says Shanshan Luo.
""Bringing parts of the THETA cycle into living cells is an important proof-of-principle for synthetic biology,"" adds Tobias Erb. ""Such modular implementation of this cycle in E. coli paves the way to the realization of highly complex, orthogonal new-to-nature CO2-fixation pathways in cell factories. We are learning to completely reprogram the cellular metabolism to create a synthetic autotrophic operating system for the cell.""

","score: 14.421469534050182, grade_level: '14'","score: 14.554052227342552, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41929-023-01079-z,"Synthetic biology offers the opportunity to build solutions for improved capture and conversion of carbon dioxide (CO2) that outcompete those evolved by nature. Here we demonstrate the design and construction of a new-to-nature CO2-fixation pathway, the reductive tricarboxylic acid branch/4-hydroxybutyryl-CoA/ethylmalonyl-CoA/acetyl-CoA (THETA) cycle. The THETA cycle encompasses 17 enzymes from 9 organisms and revolves around two of the most efficient CO2-fixing enzymes described in nature, crotonyl-CoA carboxylase/reductase and phosphoenolpyruvate carboxylase. Here using rational and machine learning-guided optimization approaches, we improved the yield of the cycle by two orders of magnitude and demonstrated the formation of different biochemical building blocks directly from CO2. Furthermore, we separated the THETA cycle into three modules that we successfully implemented in vivo by exploiting the natural plasticity of Escherichia coli metabolism. Growth-based selection and/or 13C-labelling confirmed the activity of three different modules, demonstrating the first step towards realizing highly orthogonal and complex CO2-fixation pathways in the background of living cells."
"
Farmers in sub-Saharan Africa need to diversify away from growing maize and switch to crops that are resilient to climate change and supply key micronutrients for the population, say researchers.

Maize is a staple crop across the region where it is grown and consumed in vast quantities. 
Led by Dr Stewart Jennings from the University of Leeds, the study argues that diversification towards fruits, vegetables and crops such as cassava, millet and sorghum will improve nutrition security in the country, with people getting sufficient micronutrients essential for good health.
The study also says the quantity of food produced must increase -- and unless yields are boosted to an unprecedented level, more land will have to be brought into agricultural production.
Sub-Saharan Africa is home to around 1.2 billion people, and according to figures from the World Bank, the population will grow by an additional 740 million people by 2050. 
Farmers will have to boost the amount of food grown at a time when climate change will result in increasingly extreme conditions, affecting what crops can be grown.
The researchers say the population is at risk of ""food and nutrition insecurity"" unless effective ways of adapting to climate change are identified. Integral to any decisions is a requirement that crops need to be nutritious and provide sufficient energy for the population. 
Professor Jennie Macdiarmid, from the Rowett Institute at the University of Aberdeen and one of the authors of the paper, said: ""The study has highlighted the need to place nutrition at the heart of agricultural policy to avoid the long-term unintended consequence of failing to produce food that can deliver the nutritional needs of the population.

""If policy solutions focus only on increasing production of calories and adapting to be climate smart, it is likely there will be negative consequences for health through nutritionally poor diets.""
The study -- Stakeholder-driven transformative adaptation is needed for climate-smart nutrition security in sub-Saharan Africa - is published in the scientific journal Nature Food. 
More than 50 researchers contributed to the investigation, which involved talking to policymakers and other stakeholders in the food and agriculture sectors in four countries in sub-Saharan Africa: Malawi, South Africa, Tanzania and Zambia.
'Agriculture and nutrition policies can sit in siloes'
The researchers used the iFEED assessment framework to investigate policy options to create an agricultural system that is resilient to climate change and would supply enough nutritionally-adequate food to meet the food and nutritional needs of the population.
""Too often food, agriculture and nutrition policies sit in siloes across different government departments,"" said Dr Jennings, a Research Fellow in the School of Earth and Environment at the University of Leeds. 
""This study provides holistic evidence that combines information on environmental impacts of food system changes and the changes needed for population level nutrition security. The research shows that action can be taken to adapt to climate change and improve nutrition security in sub-Saharan Africa."" 

Stakeholders in each country identified key uncertainties in the future of the food system. iFEED explores these uncertain futures and identifies key policy issues that decision makers working in the agriculture and food sectors need to consider. 
The scientists say there needs to be a fundamental shift - or ""transformative approach"" - in agriculture to incorporate nutritional needs. 
Diversifying into soybean production is one option. Soybean crops are more likely to withstand the impacts of climate change compared to maize. Dr Ndashe Kapulu, from the Zambia Agriculture Research Institute and contributing author to the study has been involved in studies to assess how soybean could improve the income of commercial and small-scale farmers.
He said: ""Many countries in sub-Saharan Africa will be better able to handle climate change and other stresses if they have more diverse food systems, such as the transition to soybean production in Zambia.
""As scientists, we need to generate enough evidence in our research to help make changes that support and guide actions to make the agrifood system more resilient.""
Increasing the production and consumption of animal-based products in sub-Saharan Africa could also improve nutritional quality of diets but the scientists warn that it should not reach the unsustainable production levels seen in some higher income countries. 
More animal-based products would cause a rise in greenhouse gas emissions, although the researchers say that this could be tolerable given sub-Saharan Africa's need to reduce the risk of nutritionally-inadequate diets -- and that its greenhouse gas emissions are relatively low.
The study involved researchers from a number of organisation including the University of Leeds, University of Aberdeen, the Met Office, Chatham House and FANRPAN.
iFEED is a database - developed in part by the University of Leeds under the GCRF AFRICAP programme and the CGIAR Initiative on Climate Resilience -- to help decision makers deliver food system policies which are resilient to climate change and deliver nutritious food -- reducing the risk of food and nutrition insecurity. 

","score: 16.994425087108016, grade_level: '17'","score: 18.69280836236934, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s43016-023-00901-y,"Improving nutrition security in sub-Saharan Africa under increasing climate risks and population growth requires a strong and contextualized evidence base. Yet, to date, few studies have assessed climate-smart agriculture and nutrition security simultaneously. Here we use an integrated assessment framework (iFEED) to explore stakeholder-driven scenarios of food system transformation towards climate-smart nutrition security in Malawi, South Africa, Tanzania and Zambia. iFEED translates climate–food–emissions modelling into policy-relevant information using model output implication statements. Results show that diversifying agricultural production towards more micronutrient-rich foods is necessary to achieve an adequate population-level nutrient supply by mid-century. Agricultural areas must expand unless unprecedented rapid yield improvements are achieved. While these transformations are challenging to accomplish and often associated with increased greenhouse gas emissions, the alternative for a nutrition-secure future is to rely increasingly on imports, which would outsource emissions and be economically and politically challenging given the large import increases required."
"
Amid the threat of dramatic sea level rise, coastal communities face unprecedented dangers, but a new study reveals that as flooding intensifies, disadvantaged populations will be the ones to experience some of the most severe burdens of climate change.

While accelerating sea level rise will result in widespread intermittent flooding and long-term inundation in many coastal communities, the paper, recently published in Nature Communications, showed that when these levels increase above 4 feet, minority populations will be disproportionately at risk of isolation.
Rising sea levels could lead to isolation by disrupting transportation networks and roads, meaning that those affected lose access to essential locations such as critical emergency services and schools.
The study further exposed that renters and older adults face a greater risk of isolation, highlighting the growing connection between historical drivers of existing social inequality and the groups that incur the most risk of climate change.
According to Kelsea Best, lead author of the study and an assistant professor of civil, environmental and geodetic engineering at The Ohio State University, the first step in better characterizing these threats is changing how researchers assess community risk, as most studies measure this by exclusively determining impacts via direct flooding. But concentrating on this sole measurement neglects more complex aftereffects of sea level rise, such as isolation, and reinforces inequality in coastal areas, Best said.
""We need to re-conceptualize how we measure who is burdened by sea level rise because there are so many ways that people might be burdened before their home is flooded,"" she said.
Current reports estimate that around 20 million coastal residents in the U.S. will be affected by rising sea levels by 2030, but the paper notes that this number doesn't include the whole impact global warming will have on certain communities and demographics.

Notably, because people need access to essential places like grocery stores, public schools, hospitals and fire stations, Best and her colleagues argue that an inability to reach these places impacts individuals just as negatively as if they were living in inundated homes themselves, and should be documented as such.
Most importantly, their results expose one of the main reasons for these vast differences in risk: A group's risk of isolation is intimately entwined with specific road networks and where vital services are located in relation to where affected individuals reside.
They identified these disparities in risk by overlaying OpenStreetMap (OSM) road network data with National Oceanographic and Atmospheric Administration (NOAA) mean higher high water (MHHW) scenarios. These projections were then combined with recent census data to estimate the percentage of a population that would be left out or missed in estimates of who would be impacted by sea level rise if researchers only counted those who suffered direct inundation.
""If we take a one-size-fits-all approach, or a seemingly 'neutral' approach to understanding who gets access to safe, affordable housing and community in a world with climate change, then we're really just exacerbating these inequities and it's not good enough,"" said Best. ""We have to deliberately seek to provide access to adaptation resources to groups of people who have historically been left out and therefore have fewer resources to respond in the first place.""
The researchers showed that Hispanic populations are often overrepresented in the total citizenry for being at risk of isolation beginning at 4 feet of sea level rise, and Black populations are overrepresented after 6 feet. Alternatively, white populations are underrepresented after 5 feet of sea level rise.
But to determine when these disparities will begin to develop, Best's team compared two long-term sea level rise scenarios: an intermediate scenario in which global sea level rise increased by a meter by 2100, and a high scenario in which that number increased to 2 meters by the same year.
Alarmingly, the study found strong evidence that these isolation effects would set in by 2120 in the intermediate scenario and as early as 2090 in the high scenario. ""This timeline matters from a planning and adaptation perspective,"" said Best. ""Part of why we included the temporal piece is to say this issue would not be as much of a problem if we had urgent, aggressive mitigation.
""The effects of climate change are going to be further reaching and more cascading than might be directly obvious, and those effects are not going to be felt equitably,"" said Best. ""So we need to be thinking about those populations most at risk from the beginning and develop policies to support them.""
The work was supported by the Clark Distinguished Chair Endowment (given to study co-author Deb. A. Neimeier of the University of Maryland) and the National Science Foundation. Other co-authors were Qian He from Rowan University, Allison C. Reilly from the University of Maryland, and Mitchell Anderson and Tom Logan from the University of Canterbury.

","score: 17.585011302211303, grade_level: '18'","score: 19.42416461916462, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41467-023-43835-6,"Within coastal communities, sea level rise (SLR) will result in widespread intermittent flooding and long-term inundation. Inundation effects will be evident, but isolation that arises from the loss of accessibility to critical services due to inundation of transportation networks may be less obvious. We examine who is most at risk of isolation due to SLR, which can inform community adaptation plans and help ensure that existing social vulnerabilities are not exacerbated. Combining socio-demographic data with an isolation metric, we identify social and economic disparities in risk of isolation under different SLR scenarios (1-10 ft) for the coastal U.S. We show that Black and Hispanic populations face a disproportionate risk of isolation at intermediate levels of SLR (4 ft and greater). Further, census tracts with higher rates of renters and older adults consistently face higher risk of isolation. These insights point to significant inequity in the burdens associated with SLR."
"
Last year, Typhoon Hinnamnor -- which caused 36 fatalities -- gained notoriety as the first super typhoon that developed at a high latitude as 25°N since Korea Meteorological Administration records began. This year in Osong, Chungcheongbuk-do, an unanticipated intense downpour, caused rivers to suddenly overflow, resulting in numerous casualties. Earth's rising temperatures are triggering unprecedented typhoons, torrential rains, and other extreme weather events. Without reliable predictions of climate extremes prompted by global warming, mitigating the resultant damages remains a challenge.

Professor Seung-Ki Min and Dr. Minkyu Lee, from the Division of Environmental Science and Engineering at Pohang University of Science and Technology (POSTECH), have used a high-resolution climate model to conduct a pioneering quantitative analysis of the impact of global warming on typhoons making landfall on the Korean Peninsula. This research has been recently published in npj Climate and Atmospheric Science.
Notably, global warming is giving way to a surge in more powerful typhoons which maintain its intensity longer and thereby cause stronger damage. Accurate typhoon prediction and damage reduction necessitate better understanding of the global warming influences, for which climate model simulations with a km-scale resolution are eccential. However, studies quantifying the anthropogenic warming contribution to typhoons affecting Korea, especially research into the rainfall extremes accompanying typhoons, remain scant.
To overcome this, the research team designed a 3 km high-resolution regional climate model simulation to investigate the impact of global warming on typhoon intensity and extreme precipitation. Four extremely strong typhoons that made landfall on the Korean Peninsula between 2011 and 2020 were chosen for simulation under current climate condition and counterfactual condition without human-induced warming. To reduce the uncertainties in regional sea surface temperature changes due to global warming, they utilized diverse ocean warming patterns estimated from CMIP6multiple climate models.
The findings show that accounting for global warming from human activities augmented overall typhoon intensity and precipitation. The research team observed that the impact of warming was pronounced more strongly at maximum typhoon intensity than the average intensity. This implies more frequent occurrences of powerful super typhoons over East Asia in the future. Additionally, the area exposed to extreme rainfall generated by typhoons expanded 16 to 37 percent due to warmer climate conditions. Further, the expansion of extreme precipitation area is attributed to the strengthening of upward motion near the typhoon center and the increase in atmospheric water vapor due to the ocean surface warming.
Professor Min explained, ""Our results from high-resolution climate model simulations provide conclusive evidence that global warming has amplified the strength of recent typhoons making landfall on the Korean Peninsula. Continued escalation of global warming could lead to stronger typhoons and more extensive occurrences of rainfall extremes, demanding heightened sector-specific preparedness measures.""
This study received support from the Mid-Career Researcher Program of the National Research Foundation of Korea and the Korea Meteorological Administration Research and Development Program on Climate and Climate Change Monitoring and Prediction Information Application Technology.

","score: 17.74759958932238, grade_level: '18'","score: 19.65298767967146, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41612-023-00509-w,"Understanding how global warming affects tropical cyclone (TC) intensity and precipitation for target regions is essential to preparing for associated damages but detailed processes remain uncertain. This study provides the first quantification of anthropogenic influences on TC characteristics affecting South Korea using convection-permitting model (CPM) simulations (3 km resolution). For the observed four recent TCs that strongly affected South Korea, CPM simulations were performed under current (ALL) and counterfactual conditions without human influences (NAT). The observed sea surface temperature and lateral boundary conditions were used for ALL while changes attributable to human influences (estimated using CMIP6 multimodel simulations) were removed from observed boundary conditions for NAT runs. ALL experiments captured the observed TC intensity and precipitation reasonably. After removing human influences, TC intensity and precipitation were reduced in NAT experiments. Importantly, areas with extreme precipitation (i.e., having precipitation larger than 150 mm) were found to expand by 16–37% in ALL compared to NAT, which was induced by an enhanced upward motion near the TC core and an increase of background water vapor in line with warming. Further, the role of increased moisture was found to become important as TC moves to mid-latitudes. This study provides valuable insights into how greenhouse warming can intensify TC-induced extreme precipitation over East Asia."
"
Of the world's various weather phenomena, fog is perhaps the most mysterious, forming and dissipating near the ground with fluctuations in air temperature and humidity interacting with the terrain itself.

While fog presents a major hazard to transportation safety, meteorologists have yet to figure out how to forecast it with the precision they have achieved for precipitation, wind and other stormy events.
This is because the physical processes resulting in fog formation are extremely complex, according to Zhaoxia Pu, a professor of atmospheric sciences at the University of Utah.
""Our understanding is limited. In order to accurately forecast fog we should better understand the process that controls fog formation,"" said Pu, who led a fog study focusing on a northern Utah valley.
Now, in a recent paper published by the American Meteorological Society, Pu and her colleagues have reported their findings from the Cold Fog Amongst Complex Terrain (CFACT) project, conceived to investigate the life cycle of cold fog in mountain valleys.
Also working on the project, funded by a $1.17 million grant from the National Science Foundation, were several other members of the U Department of Atmospheric Sciences, including Gannet Hallar and Sebastian Hoch, along with Eric Pardyjak of the Department of Mechanical Engineering, a group of scientists from the National Center for Atmospheric Research (NCAR), and Dr. Ismail Gultepe from Ontario Tech University, Canada.
Because it reduces visibility, fog poses serious hazards to the traveling public. For example, fog is the second leading cause of aircraft accidents after high winds. It leads to automobile crashes and disrupts ferry operations.

Between 1995 and 2004 in the United States, 13,720 have died in fog-related accidents.
Improving fog forecasting would make traveling more safe, Pu said.
Today, most forecasting uses a computer model known as Numerical Weather Prediction (NWP), which processes massive meteorological observations with computer models to output predictions for precipitation, temperature, and all sorts of other elements of the weather. However the current computer model doesn't work well for fog, and Pu's team hopes that improvements can be made using the masses of data they gathered over seven weeks in the winter of 2022 at several sites in the Heber Valley.
""Fog involves a lot of physics processes so it requires a computer model that can better represent all these processes,"" Pu said. ""Because fog is clouds near the ground, it requires a high-resolution model to resolve it, so we need models at a very fine scale, which are computationally very expensive. The current models (relatively coarser in resolution) are not capable of resolving the fog processes, and we need to improve the models for better fog prediction.""
Located bout 50 miles southeast of Salt Lake City, Heber Valley is nestled behind the Wasatch Mountains and framed by two major reservoirs on the Provo River.
This scenic basin is a typical mountain valley, hemmed by Mt. Timpanogos and other high peaks, with the reservoirs serving as a moisture source. The seven-week study window covered the time of year when Heber Valley is the foggiest.

Valley fog is a perfect example of how topography and atmospheric processes converge to create a distinctive weather phenomenon.
The ground is cooling overnight while denser, cooler air drops from mountain tops collecting in the valleys, in a phenomenon known as ""cold air drainage."" Cooled by the ground, the dropping air temperature can approach the dew point, and if there is sufficient moisture in the air, fog begins to form, becoming the most dense around sunrise when surface temperatures are lowest.
Winter nights create favorable conditions for different forms of fog, such as cold-air pool fog, ephemeral mountain valley fog and radiative ice fog.
The Heber Valley project homed in on cold-air fog which forms in freezing temperatures below zero degrees Celsius, according to Pu. However by observing how these varying kinds of fog form and dissipate, the researchers are continuing to learn about the meteorological conditions and physical processes governing the formation of fog.
For the CFACT study, the NCAR and U team set up two major data-collecting stations, one near Deer Creek Reservoir and another a few miles up the Provo River. These are low spots in the valley, about 5,450 feet above sea level, that see the densest fog. These sites were equipped with 100-foot towers to support an array of instruments that captured various meteorological data associated with humidity, wind, visibility, temperature, even snow depths, and soil moisture. The recordings were made from both in situ and remote-sensing platforms.
Additionally, the team recorded a lesser array of data points at nine satellite sites.
During the seven-week CFACT field campaign, nine intensive observation periods (IOPs), each conducted over 24-hour periods, yielded a dataset that included high-frequency radiosonde profiles, tethered balloon profiles, remotely sensed thermodynamic and wind profiles, surface meteorological observations and microphysical and aerosol measurements.
Besides fog IOPs, the variety of non-fog IOPs provided valuable observations for understanding near-surface inversion, ice crystal formation, moisture advection and transportation, and stable boundary layers over complex terrain, all of which are essential factors related to fog formation. Comprehensive studies are ongoing for an improved understanding of cold fog over complex terrain.
The study appeared Nov. 15 in the Bulletin of the American Meteorological Society. U researchers involved with the study included Zhaoxia Pu, Sebastian Hoch, A. Gannet Hallar, Rebecca Beal, Geraldo Carrillo-Cardenas, Xin Li and Maria Garcia of the Department of Atmospheric Sciences and Eric Pardyjak and Alexei Perelet of the Department of Mechanical Engineering.

","score: 14.98321386603995, grade_level: '15'","score: 16.078062867215046, grade_levels: ['college_graduate'], ages: [24, 100]",10.1175/BAMS-D-22-0030.1,"Cold fog forms via various thermodynamic, dynamic, and microphysical processes when the air temperature is less than 0°C. It occurs frequently during the cold season in the western United States yet is challenging to detect using standard observations and is very difficult to predict. The Cold Fog Amongst Complex Terrain (CFACT) project was conceived to investigate the life cycle of cold fog in mountain valleys. The overarching goals of the CFACT project are to 1) investigate the life cycle of cold-fog events over complex terrain with the latest observation technology, 2) improve microphysical parameterizations and visibility algorithms used in numerical weather prediction (NWP) models, and 3) develop data assimilation and analysis methods for current and next-generation (e.g., subkilometer scale) NWP models. The CFACT field campaign took place in Heber Valley, Utah, during January and February 2022, with support from NSF’s Lower Atmospheric Observing Facilities (managed by NCAR’s Earth Observing Laboratory), the University of Utah, and Ontario Technical University. A network of ground-based and aerial in situ instruments and remote sensing platforms were used to obtain comprehensive measurements of thermodynamic profiles, cloud microphysics, aerosol properties, and environmental dynamics. Nine intensive observation periods (IOPs) explored various mountainous weather and cold-fog conditions. Field observations, NWP forecasts, and large-eddy simulations provided unprecedented data sources to help understand the mechanisms associated with cold-fog weather and to identify and mitigate numerical model deficiencies in simulating winter weather over mountainous terrain. This article summarizes the CFACT field campaign, its observations, and challenges during the field campaign, including real-time fog prediction issues and future analysis."
"
Forests on the west slope of Oregon's Cascade Range experienced fire much more often between 1500 and 1895 than had been previously thought, according to new research by scientists at Oregon State University.

The findings provide important insight, the authors say, into how landscapes might adapt to climate change and future fire regimes.
James Johnston of the OSU College of Forestry led the study, which was published in Ecosphere.
""Wildland fire is a fundamental forest ecosystem process,"" he said. ""With temperatures rising and more and more area burning, we need to know as much as we can about the long-term variability in fire.""
Johnson and collaborators at Oregon State, the University of Oregon and the U.S. Forest Service gathered tree ring data at 16 sites in the southern part of the Willamette National Forest, in the general vicinity of Oakridge.
Trees form scars after cambial cells are killed by wildfire heat, he said. These scars are partially or completely covered by new tissue as a tree grows, and tree rings tell the story of when the fire exposure occurred.
Using chain saws, the scientists collected samples from 311 dead trees -- logs, short snags and stumps. Seventy-three percent of the samples were coastal Douglas-fir, and 13% were ponderosa pine. The remainder were sugar pine, noble fir, red fir, incense cedar, western red cedar, mountain hemlock and western hemlock.

""We cross-dated a total of 147,588 tree rings and identified 672 cambial injuries, 479 of which were fire scars,"" Johnston said. ""The scars allowed us to reconstruct 130 different fire years that occurred at one or more of the 16 sites before a federal policy of fire suppression went into effect early in the 20th century.""
The main takeaways: Fire was historically far more frequent in western Oregon Cascades landscapes than previously believed. Indigenous peoples likely used fire to manage large areas for resources and probably altered landscapes and fire regimes in significant ways. There are important present-day restoration opportunities for fire-adapted systems in western Oregon.""Also, our study produced little evidence of the kind of large, wind-driven fires that in 2020 burned 50,000 to 75,000 hectares in the watersheds immediately to the north and south of our study area,"" Johnston said. ""Only 39% of fire years were recorded at more than one site, only 11% were recorded at more than two sites, and only 3% at more than three sites -- in a study area of 37,000 acres, that strongly suggests that most historical fires were relatively small.""
Across all 16 sites, the average fire return interval -- the length of time between fires -- was as short as six years and as long as 165. In general the differences in those averages were strongly associated with vapor pressure deficit or VPD, basically the drying power of the atmosphere. The higher the VPD, the shorter the time between fires.
However, historical fire in stands seral to Douglas-fir -- stands that, if left alone, would end up with Douglas-fir as the dominant tree species -- was much less strongly linked with dry air.
""We interpret the extraordinary tempo of fire in those stands, and the climate pattern associated with fire there, to indicate Indigenous fire stewardship,"" Johnston said. ""We saw some of the most frequent fire return intervals ever documented in the Pacific Northwest, but the enormous volume of biomass that these moist forests accumulate over time is often partly attributed to long intervals between wildfire.""
The authors note that humans have occupied the southern part of what is now the Willamette National Forest for at least 10,000 years. A variety of Indigenous cultures, including the Molalla, Kalapuya, Tenino, Wasco, Klamath, Northern Paiute and Cayuse, probably used the area for trading, hunting and the collection of plants.

""Removals happened very quickly, with most Native people taken to the Grand Ronde, Warm Springs and Klamath reservations,"" said co-author David Lewis, a member of the Grand Ronde Tribe and an assistant professor of anthropology and Indigenous studies in OSU's College of Liberal Arts. ""Removal of the tribes took their cultural stewardship practices, their use of annual cultural fires, from the land, radically altering how the forests were managed.""
By 1856, most remaining members of Willamette Valley and western Oregon Cascades tribes had been forcibly removed to reservations. Extensive clearcut logging on the Willamette National Forest started in the late 1940s and continued for four decades.
""Now, Forest Service managers want fine-grained information about forest vegetation and historical disturbance dynamics to manage lands in ways that promote resilience to climate change,"" Johnston said.
He added that the Forest Service is working closely with the Southern Willamette Forest Collaborative, a group based in Oakridge, to plan a variety of restoration treatments.
Joining Johnston and Lewis on the paper were the College of Forestry's Micah Schmidt, now working with the Umatilla Tribe in northeastern Oregon, and Andrew Merschel. Co-authors also included William Downing of the U.S. Forest Service and the University of Oregon's Michael Coughlan.
The Oregon Department of Forestry funded the study.

","score: 13.205887587822016, grade_level: '13'","score: 14.562669789227165, grade_levels: ['college_graduate'], ages: [24, 100]",10.1002/ecs2.4735,"Detailed information about the historical range of variability in wildfire activity informs adaptation to future climate and disturbance regimes. Here, we describe one of the first annually resolved reconstructions of historical (1500–1900 ce) fire occurrence in coast Douglas‐fir dominated forests of the west slope of the Cascade Range in western Oregon. Mean fire return intervals (MFRIs) across 16 sites within our study area ranged from 6 to 165 years. Variability in MFRIs was strongly associated with average maximum summer vapor pressure deficit. Fire occurred infrequently in Douglas‐fir forest stands seral to mountain hemlock or silver fir, but fire frequency was much shorter than predicted by theory in other forest types. MFRIs within Douglas‐fir stands seral to western hemlock or grand fir ranged from 19 to 45 years, and MFRIs in stands seral to Douglas‐fir ranged from 6 to 11 years. There was little synchrony in fire occurrence or tree establishment across 16 sites separated by 4 km. The lack of synchrony in fire suggests that large, wind‐driven fire events that are often considered to be characteristic of coast Douglas‐fir forests were not an important driver of succession in our study area during the last ~400–500 years. Climate was more arid than normal during fire years in most forest types, but historical fire in stands seral to Douglas‐fir was strongly associated with antecedent moisture and less strongly associated with drought. We interpret the extraordinary tempo of fire we observed in stands seral to Douglas‐fir and the unique climate pattern associated with fire in these stands to be indicative of Indigenous fire stewardship. This study provides evidence of far more frequent historical fire in coast Douglas‐fir forests than assumed by managers or scientists—including some of the most frequent fire return intervals documented in the Pacific Northwest. We recommend additional research across the western Cascades to create a comprehensive account of historical fire in highly productive forests with significant cultural, economic, and ecological importance."
"
Barley seedlings grow on average 50% more when their root system is stimulated electrically through a new cultivation substrate. In a study published in the journal PNAS, researchers from Linköping University have developed an electrically conductive ""soil"" for soilless cultivation, known as hydroponics.

""The world population is increasing, and we also have climate change. So it's clear that we won't be able to cover the food demands of the planet with only the already existing agricultural methods. But with hydroponics we can grow food also in urban environments in very controlled settings,"" says Eleni Stavrinidou, associate professor at the Laboratory of Organic Electronics at Linköping University, and leader of the Electronic Plants group.
Her research group has now developed an electrically conductive cultivation substrate tailored to hydroponic cultivation which they call eSoil. The Linköping University researchers have shown that barley seedlings grown in the conductive ""soil"" grew up to 50% more in 15 days when their roots were stimulated electrically.
Hydroponic cultivation means that plants grow without soil, needing only water, nutrients and something their roots can attach to -- a substrate. It is a closed system that enables water recirculation so that each seedling gets exactly the nutrients it needs. Therefore, very little water is required and all nutrients remain in the system, which is not possible in traditional cultivation.
Hydroponics also enables vertical cultivation in large towers to maximise space efficiency. Crops already being cultivated in this manner include lettuce, herbs and some vegetables. Grains are not typically grown in hydroponics apart for their use as fodder. In this study the researchers show that barley seedlings can be cultivated using hydroponics and that they have a better growth rate thanks to electrical stimulation.
""In this way, we can get seedlings to grow faster with less resources. We don't yet know how it actually works, which biological mechanisms that are involved. What we have found is that seedlings process nitrogen more effectively, but it's not clear yet how the electrical stimulation impacts this process,"" says Eleni Starvrinidou.
Mineral wool is often used as cultivation substrate in hydroponics. Not only is this non-biodegradable, it is also produced with a very energy intensive process. The electronic cultivation substrate eSoil is made of cellulose, the most abundant biopolymer, mixed with a conductive polymer called PEDOT. This combination as such is not new, but this is the first time it has been used for plant cultivation and for creating an interface for plants in this manner.
Previous research has used high voltage to stimulate the roots. The advantage of the Linköping researchers' ""soil"" is that it has very low energy consumption and no high voltage danger. Eleni Stavrinidou believes that the new study will open the pathway for new research areas to develop further hydroponic cultivation.
""We can't say that hydroponics will solve the problem of food security. But it can definitely help particularly in areas with little arable land and with harsh environmental conditions.""

","score: 12.96869551919254, grade_level: '13'","score: 13.094647499617679, grade_levels: ['college_graduate'], ages: [24, 100]",10.1073/pnas.2304135120,"Active hydroponic substrates that stimulate on demand the plant growth have not been demonstrated so far. Here, we developed the eSoil, a low-power bioelectronic growth scaffold that can provide electrical stimulation to the plants’ root system and growth environment in hydroponics settings. eSoil’s active material is an organic mixed ionic electronic conductor while its main structural component is cellulose, the most abundant biopolymer. We demonstrate that barley seedlings that are widely used for fodder grow within the eSoil with the root system integrated within its porous matrix. Simply by polarizing the eSoil, seedling growth is accelerated resulting in increase of dry weight on average by 50% after 15 d of growth. The effect is evident both on root and shoot development and occurs during the growth period after the stimulation. The stimulated plants reduce and assimilate NO 3 − more efficiently than controls, a finding that may have implications on minimizing fertilizer use. However, more studies are required to provide a mechanistic understanding of the physical and biological processes involved. eSoil opens the pathway for the development of active hydroponic scaffolds that may increase crop yield in a sustainable manner."
"
Researchers led by Genki Kobayashi at the RIKEN Cluster for Pioneering Research in Japan have developed a solid electrolyte for transporting hydride ions (H?) at room temperature. This breakthrough means that the advantages of hydrogen-based solid-state batteries and fuel cells are within practical reach, including improved safety, efficiency, and energy density, which are essential for advancing towards a practical hydrogen-based energy economy.The study was published in the scientific journal Advanced Energy Materials.

For hydrogen-based energy storage and fuel to become more widespread, it needs to be safe, very efficient, and as simple as possible. Current hydrogen-based fuel cells used in electric cars work by allowing hydrogen protons to pass from one end of the fuel cell to the other through a polymer membrane when generating energy. Efficient, high-speed hydrogen movement in these fuel cells requires water, meaning that the membrane must be continually hydrated so that it does not dry out. This constraint adds an additional layer of complexity and cost to battery and fuel cell design that limits the practicality of a next-generation hydrogen-based energy economy. To overcome this problem, scientists have been struggling to find a way to conduct negative hydride ions through solid materials, particularly at room temperature.
The wait is over. ""We have achieved a true milestone,"" says Kobayashi. ""Our result is the first demonstration of a hydride ion-conducting solid electrolyte at room temperature.""
The team had been experimenting with lanthanum hydrides (LaH3-?) for several reasons; the hydrogen can be released and captured relatively easily, hydride ion conduction is very high, they can work below 100°C, and have a crystal structure. But, at room temperature, the number of hydrogens attached to lanthanum fluctuates between 2 and 3, making it impossible to have efficient conduction. This problem is called hydrogen non-stoichiometry, and was the biggest obstacle overcome in the new study. When the researchers replaced some of the lanthanum with strontium (Sr) and added just a pinch of oxygen -- for a basic formula of La1-xSrxH3-x-2yOy, they got the results they were hoping for.
The team prepared crystalline samples of the material using a process called ball-milling, followed by annealing. They studied the samples at room temperature and found that they could conduct hydride ions at a high rate. Then, they tested its performance in a solid-state fuel cell made from the new material and titanium, varying the amounts of strontium and oxygen in the formula. With an optimal value of at least 0.2 strontium, they observed complete 100% conversion of titanium to titanium hydride, or TiH2. This means that almost zero hydride ions were wasted.
""In the short-term, our results provide material design guidelines for hydride ion-conducting solid electrolytes,"" says Kobayashi. ""In the long-term, we believe this is an inflection point in the development of batteries, fuel cells, and electrolytic cells that operate by using hydrogen."" The next step will be to improve performance and create electrode materials that can reversibly absorb and release hydrogen. This would allow batteries to be recharged, as well as make it possible to place hydrogen in storage and easily release it when needed, which is a requirement for hydrogen-based energy use.

","score: 13.467897397769516, grade_level: '13'","score: 13.9042936802974, grade_levels: ['college_graduate'], ages: [24, 100]",10.1002/aenm.202301993,"Hydride ion conductors have made remarkable progress in recent years; in particular, the fluorite‐type LaH3‐δ series exhibits high conductivity around room temperature. However, its intrinsic character of hydrogen non‐stoichiometry still makes its application as a solid electrolyte challenging, for which high electronic insulation is essential. Here, Sr‐substituted LaH3‐δ with slight O2− incorporation, represented as La1‐xSrxH3‐x‐2yOy (0.1 ≤ x ≤ 0.6, y ≤ 0.171), is synthesized, which exhibits H− conductivity of 10−4 – 10−5 S cm−1 at room temperature. The galvanostatic discharge reaction using an all‐solid‐state cell composed of Ti|La1‐xSrxH3‐x‐2yOy|LaH3‐δ shows that the Ti electrode is completely hydrogenated to TiH2 for x ≥ 0.2, whereas a short circuit occurs for x = 0.1. These experimental observations, together with calculation studies on the density of states and the defect formation energy, provide clear evidence that electropositive cation, such as Sr, doping critically suppresses the electron conduction in LaH3‐δ. Achieving a superior H− conducting solid electrolyte is a novel milestone in the development of electrochemical devices that utilize its strong reducing ability (E°(H−/H2) = −2.25 V vs SHE), such as batteries with high energy density and electrolysis/fuel cells with high efficiency."
"
A lot has changed in the world since the Endangered Species Act (ESA) was enacted 50 years ago in December 1973.

Two researchers at The Ohio State University were among a group of experts invited by the journal Science to discuss how the ESA has evolved and what its future might hold.
Tanya Berger-Wolf, faculty director of Ohio State's Translational Data Analytics Institute, led a group that wrote on ""Sustainable, trustworthy, human-technology partnership."" Amy Ando, professor and chair of the university's Department of Agricultural, Environmental, and Development Economics, wrote on ""Harnessing economics for effective implementation.""
Berger-Wolf and her colleagues wrote, ""We are in the middle of a mass extinction without even knowing all that we are losing and how fast."" But technology can help address that.
For example, they note the value of tools like camera traps that survey animal species and smartphone apps that allow citizen scientists to count insects, identify bird songs and report plant observations.
New tech has allowed scientists to monitor animal and plant populations at scale for the first time, said Berger-Wolf, who is also a professor of computer science and engineering, evolution, ecology and organismal biology, and electrical and computer engineering. One challenge is to find new ways to extract all the information from these new sources of data.
""But even with all this data, we are still monitoring only a tiny fraction of the biodiversity out in the world,"" she said. ""Without that information, we don't know what we have, how different species are doing and whether our policies to protect endangered species are working.""
Most important, Berger-Wolf said, is the need to make sure to keep humans in the process. Technology needs to connect data, connect different regions of the world, connect people to nature and connect people to people.

""We don't want to sever the connection between people and nature, we want to strengthen it,"" she said.
""We cannot rely on technology to save the world's biodiversity. It has to be an intentional partnership between humans and technology and AI.""
Economics should be another partner in the fight to save endangered species, Ando said.
""There's this tendency to think that protecting endangered species is all about biology and ecology,"" Ando said. ""But various tools in economics are very helpful in making sure the work we do to implement the Endangered Species Act is successful. That is not always obvious to people.""
For example, bioeconomic research is a multidisciplinary effort between economists and biologists to work together to see how human behavior interacts with ecological processes and systems.
""We have to take into account feedback effects. People take an action, and that changes the ecosystem and that changes what people do,"" she said. ""We need to capture those feedback effects.""
The result can be novel ways to protect endangered species, such as ""pop-up"" habitat modification. For example, ranchers can take down fences temporarily while elk are migrating to allow them to move freely. Rice fields can be temporarily flooded during shorebird migration to give them a place to rest and feed on their travels.

We can ""draw upon economics to optimize the timing, location and extent of temporary actions to maximize their net benefits to society,"" Ando wrote in Science.
Another way economics can help is to develop policies that protect species before they become so threatened that they need ESA protection.
A common issue is that multiple landowners will all need to work together to protect the habitat of threatened species. But often, if some landowners take actions to protect a species, other landowners will think they don't have to.
""Economists have been working to understand how we can coordinate landowners where we don't have to implement draconian land use regulations, but still protect habitat,"" Ando said.
""That is a very promising tactic that can protect species and also reduce the cost to people of doing so.""

","score: 12.14601757217142, grade_level: '12'","score: 11.937409449524829, grade_levels: ['12'], ages: [17, 18]",10.1126/science.adn3245,"In late December 1973, the United States enacted what some would come to call “the pitbull of environmental laws.” In the 50 years since, the formidable regulatory teeth of the Endangered Species Act (ESA) have been credited with considerable successes, obliging agencies to draw upon the best available science to protect species and habitats. Yet human pressures continue to push the planet toward extinctions on a massive scale. With that prospect looming, and with scientific understanding ever changing, Science invited experts to discuss how the ESA has evolved and what its future might hold. —Brad Wible"
"
A new study led by scientists at the Smithsonian's National Museum of Natural History identifies five new species of soft-furred hedgehogs from Southeast Asia.

The study, published in the Zoological Journal of the Linnean Society, used DNA analysis and physical characteristics to describe two entirely new species of soft-furred hedgehogs and elevate three subspecies to the level of species.
The two new species, named Hylomys vorax and H. macarong, are endemic to the endangered Leuser ecosystem, a tropical rainforest in North Sumatra and Southern Vietnam, respectively. The museum specimens that were vital to describing these two new species came from the natural history collections of the Smithsonian and the Academy of Natural Sciences of Drexel University in Philadelphia where they had remained in drawers for 84 and 62 years, respectively, prior to identification.
The study -- an international collaboration between researchers at the University of Seville and the Doñana Biological Station in Spain, George Mason University and the Smithsonian's National Zoo and Conservation Biology Institute in the U.S., the Lee Kong Chian Natural History Museum in Singapore, the Natural History Museum of Geneva in Switzerland and the University of Malaya in Malaysia -- highlights that even in well-studied animal groups like mammals there are still discoveries waiting to be made, showing what is possible when modern techniques such as DNA analysis are applied to museum collections.
Soft-furred hedgehogs or gymnures are small mammals that are members of the hedgehog family, but as their common name suggests they are furry rather than spiny. Like spiny hedgehogs, they are not rodents and they have a pointy snout. Without the spines of their more well-known cousins, soft-furred hedgehogs superficially look a bit like a mixture of a mouse and a shrew with a short tail, said Arlo Hinckley, the study's lead author and a Margarita Salas Postdoctoral Fellow at the National Museum of Natural History and University of Seville. The five new species belong to a group of soft-furred hedgehogs called lesser gymnures (Hylomys) that live in Southeast Asia and previously was only recognized to have been represented by two known species.
""We were only able to identify these new hedgehogs thanks to museum staff that curated these specimens across countless decades and their original field collectors,"" Hinckley said. ""By applying modern genomic techniques like we did many years after these hedgehogs were first collected, the next generation will be able to identify even more new species.""
Hinckley said these small mammals are active during the day and night and are omnivorous, likely eating a diversity of insects and other invertebrates as well as some fruits as opportunities present themselves.

""Based on the lifestyles of their close relatives and field observations, these hedgehogs likely nest in hollows and take cover while foraging among tree roots, fallen logs, rocks, grassy areas, undergrowth and leaf litter,"" Hinckley said. ""But, because they're so understudied, we are limited to speculate about the details of their natural history.""
Hinckley first became intrigued with the gymnure group Hylomys in 2016 during his doctoral studies, especially after he sampled them in Borneo with co-author Miguel Camacho Sánchez. Preliminary genetic data and studies of several known populations of Hylomys in Southeast Asia suggested to them there might be more species in the group than were currently recognized. This sent Hinckley combing through natural history collections searching for specimens assigned to the group, many of which were only preserved skins and skulls.
When he began his research at the Smithsonian in 2022, Hinckley leveraged the National Museum of Natural History's collections to fill in geographic gaps in the specimens he had already studied with the help of Melissa Hawkins, the museum's curator of mammals.
In the end, Hinckley, Hawkins and their collaborators assembled 232 physical specimens and 85 tissue samples for genetic analysis from across the entire Hylomys group from a combination of Hinckley and Hawkins' own field collecting, as well as modern and historical museum specimens from no less than 14 natural history collections across Asia, Europe and the U.S.
Then Hinckley and his co-authors set about the lengthy process of conducting genetic analysis on the 85 tissue samples in Doñana Biological Station's ancient DNA laboratory and the museum's Laboratories of Analytical Biology. They also made rigorous physical observations and collected measurements to examine differences in the size and shape of skulls, teeth and fur on the 232 specimens.
The genetic results identified seven distinct genetic lineages in Hylomys, suggesting the number of recognized species in the group was about to increase by five, later confirmed by the team's physical observations of the specimens.

""It might be surprising for people to hear that there are still undiscovered mammals out there,"" Hawkins said. ""But there is a lot we don't know -- especially the smaller nocturnal animals that can be difficult to tell apart from one another.""
H. macarong, which has dark brown fur and measures about 14 centimeters (5.5 inches) in length, was named after a Vietnamese word for vampire (Ma cà r?ng) because males of the species possess long, fang-like incisors. Hinckley said more field study would be required to figure out what purpose the fangs might serve, but that their larger size in males suggests they could have some role in sexual selection. Males also have rust-colored chest markings that Hawkins said could have been stained by scent glands.
H. vorax also has dark brown fur but is slightly smaller than H. macarong at 12 centimeters (4.7 inches) long; it has a completely black tail, a very narrow snout and is found only on the slopes of Mount Leuser in Northern Sumatra. Hinckley and Hawkins gave the species the Latin name H. vorax after a striking description of its behavior from mammologist Frederick Ulmer, who collected the specimens that led to the species description on an expedition to Sumatra in 1939. Ulmer described the creature in his field notes, incorrectly identifying it as a type of shrew: ""They were voracious beasts often devouring the whole bait before springing the trap. Ham rind, coconut, meat, and walnuts were eaten. One shrew partially devoured the chicken head bait of a steel trap before getting caught in a nearby Schuyler trap baited with ham rind.""
The other three new species were all formerly considered to be subspecies of Hylomys suillus, but all showed sufficient genetic and physical divergence to merit the upgrade to species in their own right. They are named H. dorsalis, H. maxi and H. peguensis.
H. dorsalis hails from the mountains of Northern Borneo and features a conspicuous dark stripe that begins atop its head and bisects its back before fading around mid-body. It is about the same size as H. macarong. H. maxi is also on the larger end of the new species of soft-furred hedgehogs at 14 centimeters (5.5 inches). The species is found in mountainous regions on the Malay Peninsula and in Sumatra. H. peguensis is smaller, measuring 13 centimeters (5.1 inches), and is found in numerous countries in mainland Southeast Asia, especially Thailand, Laos and Myanmar. Its fur is a bit more yellow colored than that of the other new species, Hawkins said.
Describing new species expands humanity's scientific understanding of the natural world can be a tool for boosting conservation in threatened habitats such as Northern Sumatra's Leuser ecosystem.
""This kind of study can help governments and organizations make hard choices about where to prioritize conservation funding to maximize biodiversity,"" Hinckley said.
This research was supported by the Smithsonian, Spain's Ministry of Economy and Competitiveness as well as its Ministry of Universities, the European Union and Harvard University.

","score: 16.495706479313032, grade_level: '16'","score: 18.079625292740047, grade_levels: ['college_graduate'], ages: [24, 100]",10.1093/zoolinnean/zlad177,"We here present a comprehensive integrative taxonomic review of the genus Hylomys, using molecular (mitochondrial genomes and up to five nuclear loci) and morphological data from museum specimens across its distribution, resulting in the description of two new species and the elevation of three subspecies to specific status. This revision significantly increases the known diversity of Hylomys from two to seven extant species, challenging the traditional view of species-level diversity within gymnures. We discuss the implications of the taxonomic findings for conservation, particularly in relation to the restricted distribution ranges of several species that may be threatened by habitat loss and/or climate change. Our research emphasizes the importance of scientific collections and underscores the potential of museum genomics and additional field sampling to identify new species and improve our understanding of species diversity in poorly studied regions. Speciation events within Hylomys occurred during the Late Miocene and Early Pliocene, possibly driven by shifting climate conditions such as the strengthening of the Indian monsoon and the expansion of seasonally dry conditions. This study supports northern Sumatra and the southern Annamites as centres of localized endemicity and suggests the need for additional small mammal surveys across Sumatra’s Barisan Range."
"
An archaeological find in the Huescan Pyrenees allowed researchers to identify for the first time livestock management strategies and feeding practices which demonstrate how the first high mountain societies, at the start of the Neolithic period, were already carrying out complex livestock and farming activities, instead of being limited to the transhumance of sheep and goats. The study has been the first to combine carbon and nitrogen stable isotope analysis with archaeozoological analyses. The study, coordinated by the UAB and including the involvement of the CSIC, the University of Évora and the Government of Aragon, also documented how the economic importance of pigs in the Huescan region dates back to the Neolithic.

The research on management strategies and use of animal resources in high mountain areas during the Early Neolithic, approximately 6,500 to 7,500 years ago, was conditioned by the presumption that human occupancy of these regions were mainly seasonal and that economic practices focused greatly on making use of wild resources. With regards to livestock rearing, the role of sheep and goat transhumance in high mountain areas has stood out traditionally, while only a marginal role has been given to other livestock activities, in which the temporary maintenance of these animal flocks has been highlighted.
Researchers from the Archaeozoology Laboratory and the High Mountain Archaeology Group of the Universitat Autònoma de Barcelona (UAB), the University of Évora (HERCULES Laboratory), the Milà i Fontanals Institution-CSIC and the General Directorate of Cultural Heritage of the Government of Aragon, have now for the first time managed to characterise the livestock practices and feeding strategies of domesticated animals in high mountain regions during the Early Neolithic, specifically in the archaeological site of Coro Trasito, located in the region of Sobrarbe, Aragon. Their research has yielded new elements to be used in the study of the complexity of neolithisation processes in the Central Pyrenees.
The study conducted by the research team focused on assessing animal ecology, livestock management strategies and feeding practices implemented by the first societies settling in high mountain regions (over 1,500 metres above sea level). To do so, the team became the first to apply to high mountain contexts a combination of analysis of stable carbon and nitrogen isotopes in bone collagen -- the study of these two isotopes can be used to determine the diet and the position in the food chain of the animals -- and the archaeozoological analysis of the remains of animals from that period. Thanks to this combination, researchers were able to document that management and feeding strategies differed among flocks.
The results obtained showed that flocks belonging to these first settlers were small and formed by a few number of each species: cows, goats, sheep and pigs (Bos taurus, Capra hircus, Ovis aries and Sus domesticus), and were mainly used for their meat and milk production. In addition, researchers were able to document the rise in the economic importance of pigs (Sus domesticus) during the Neolithic.
The presence in some of the cases studied of different ways of managing the feeding of animals, with access to different pastures and the possible provision of forage, mainly from surplus agricultural products, shows that livestock practices developed at the Coro Trasito site were consolidated practices at the start of the Neolithic and related to agricultural practices. The study also demonstrates how flocks were adapted to the environmental conditions of the cave.
The results of the archaeozoological, isotopic and archaeological analyses reveal that the inhabitants of the Coro Trasito cave made use mainly of domestic resources. In addition, the presence of transformation activities related to dairy products and fat, as well as the existence of storage structures within the cave, point to the complexity of neolithisation processes in the Central Pyrenees and how these areas were rapidly integrated into an even wider and more complex economic system.

","score: 21.426635220125792, grade_level: '21'","score: 23.846415094339626, grade_levels: ['college_graduate'], ages: [24, 100]",10.3389/fearc.2023.1309907,"Research on animal management strategies in high mountain areas during the early Neolithic (5,700–4,500 cal BC) has been conditioned by the presumption that human occupations in highland areas had a prominent seasonal character and the economic practices focused mainly on the exploitation of wild resources. The results obtained in the framework of research developed recently on settlement dynamics during the early Neolithic in the highland areas indicate the existence of relatively permanent occupations and the exploitation of domestic resources. Regarding livestock, the role of caprine transhumance in highland areas has been highlighted traditionally, conferring a marginal role to husbandry activities and emphasizing principally the temporary maintenance of herds of sheep and goats. In this study, we use the archaeozoological data and δ13C and δ15N stable isotopes composition of the faunal bones collagen to characterize the husbandry practices in Coro Trasito cave (Huesca, Spain). The results obtained demonstrated the presence of diverse herd foddering strategies within husbandry practices characterized by taxonomic diversity and multipurpose exploitation suggests that during the Neolithic, Coro Trasito cave played a more complex role than sheepfold. Moreover, the presence of the four main domestic species indicates the adaptation of herds of Coro Trasito to the cave environment, flocks with diverse dietary needs and reproductive behaviors. The results are discussed with an integrated analysis of the data related to animal management strategies in highland areas (more than 1,500 m. asl) during the early Neolithic, in particular in the central Pyrenees area. This study offers new elements to study the complexity of neolithization processes in the central Pyrenees and how these areas were quickly integrated into a broader economic system."
"
In Finland, there is a clear increase in the number of sick days taken due to depression, anxiety and sleep disorders in October and November, whereas the number of absences is lower than expected between June and September. In late autumn, the number of sick days taken is almost twice as high as in the summer and about a quarter higher than in early autumn. On the other hand, manic episodes related to bipolar disorder occur more frequently than expected during the spring and summer, when there are more daylight hours, and less frequently than expected during darker times of year.

The results can be found in a study funded by the Research Council of Finland. The study was conducted as a part of the Climate Change and Health research programme. The aim of the study was to investigate the connection between changing light levels and mental health. It is expected that due to climate change, winters in Finland will become darker while summers will become brighter.
During the study, Kela's sick leave register was used to analyse the seasonal timing of a total of 636,543 sick leaves that were due to mental health reasons over a period of 12 years. The analyses examined whether the expected number of absences was above or below the expected number of sick leaves.
""Previous studies have found that some people experience so-called winter depression (seasonal affective disorder) during the dark season. In addition to the typical symptoms of depression, kaamos depression involves an increased appetite and weight gain along with excess sleepiness, which means sleeping for longer and feeling tired during the day. The symptoms of winter depression can often be alleviated through bright light therapy,"" says Timo Partonen, a Research Professor at the Finnish Institute for Health and Welfare.
Seasonal variation can increase workloads in the workplace and in health services particularly in the autumn, when the most common types of sick leaves -- absences due to depression, anxiety and sleep disorders -- are starting to occur often.
""It's also worth considering if there are other explanations for the phenomenon apart from a dark season. For example, is there an exceptionally high amount of psychosocial stress in the workplace during autumn, which then leads to an increasing number of sick leaves,"" says Professor of Psychology Marianna Virtanen from the University of Eastern Finland.
If climate change causes summers in Finland to become brighter and winters to become darker, the study suggests that depression, anxiety and sleep disorders could increase during the winter because of those changes. However, with the exception of sleep disorders, they could also become less prevalent during the summer. In the case of bipolar disorder, darker winters could alleviate the symptoms of mania, while brighter summers could exacerbate them.

","score: 13.60012987012987, grade_level: '14'","score: 15.39170995670996, grade_levels: ['college_graduate'], ages: [24, 100]",10.1017/S2045796023000768,"Although seasonality has been documented for mental disorders, it is unknown whether similar patterns can be observed in employee sickness absence from work due to a wide range of mental disorders with different severity level, and to what extent the rate of change in light exposure plays a role. To address these limitations, we used daily based sickness absence records to examine seasonal patterns in employee sickness absence due to mental disorders. We used nationwide diagnosis-specific psychiatric sickness absence claims data from 2006 to 2017 for adult individuals aged 16–67 (n = 636,543 sickness absence episodes) in Finland, a high-latitude country with a profound variation in daylength. The smoothed time-series of the ratio of observed and expected (O/E) daily counts of episodes were estimated, adjusted for variation in all-cause sickness absence rates during the year. Unipolar depressive disorders peaked in October–November and dipped in July, with similar associations in all forms of depression. Also, anxiety and non-organic sleep disorders peaked in October–November. Anxiety disorders dipped in January–February and in July–August, while non-organic sleep disorders dipped in April–August. Manic episodes reached a peak from March to July and dipped in September–November and in January–February. Seasonality was not dependent on the severity of the depressive disorder. These results suggest a seasonal variation in sickness absence due to common mental disorders and bipolar disorder, with high peaks in depressive, anxiety and sleep disorders towards the end of the year and a peak in manic episodes starting in spring. Rapid changes in light exposure may contribute to sickness absence due to bipolar disorder. The findings can help clinicians and workplaces prepare for seasonal variations in healthcare needs."
"
The North Sea seafloor is dotted with thousands of crater-like depressions in the sediment known as pockmarks. There are probably millions of them around the world ocean. They are formed by fluid discharge such as the greenhouse gas methane or groundwater, according to common scientific understanding. The majority of these pockmarks still puzzle researchers today, as many cannot be explained by fluid seepage. ""Our results show for the first time that these depressions occur in direct connection with the habitat and behavior of porpoises and sand eels and are not formed by rising fluids,"" says Dr Jens Schneider von Deimling, lead author of the current study and geoscientist at Kiel University.

""Our high-resolution data provide a new interpretation for the formation of tens of thousands of pits on the North Sea seafloor, and we predict that the underlying mechanisms occur globally, but have been overseen until now,"" Schneider von Deimling adds. For the study, Schneider von Deimling and researchers from the Alfred Wegener Institute, the Helmholtz Centre for Polar and Marine Research (AWI), the University of Veterinary Medicine Hannover, Foundation (TiHo) as well as the Leibniz Institute for Baltic Sea Research Warnemünde (IOW) examined the seafloor in the North Sea off Heligoland down to centimeters. They also included the behavior of vertebrates such as porpoises in their analyses.
Vertebrates leave pits in the seabed of the North Sea
Most of the depressions in the seafloor in the German Bight, the team suspects, are created by porpoises and other animals in search of food, and then scoured out by bottom currents. The sand eel, a small eel-like fish that spends most of the year buried in shallow sediments, plays a key role in this process. Sand eels are not only popular with the fishing industry, but are also consumed in large quantities by porpoises. ""From analyses of the stomach contents of stranded porpoises, we know that sand eels are an important food source for the North Sea population,"" says Dr Anita Gilles of the TiHo-Institute for Terrestrial and Aquatic Wildlife Research (ITAW), who has long studied the biology of marine mammals. In their study, the researchers showed that the marine mammals leave pits in the seafloor when they hunt for buried sand eels. Although these pits resemble the familiar pockmarks, they are much shallower.
Advanced multibeam echosounder technology provides information on pit condition
The detection of the pits has only become possible in recent years with the help of modern multibeam echosounder technology, which is taught and practiced intensively at Kiel University. ""The formation mechanism of these pits, as we call them, probably also explains the existence of numerous crater-like depressions on the seafloor worldwide, which have been misinterpreted as the result of methane gas leaks,"" says geoscientist Schneider von Deimling. In the North Sea, the researchers identified 42458 of these enigmatically shaped, shallow pits with an average depth of just eleven centimeters, which differ in their morphology from the more conical craters of the pockmarks.
Schneider von Deimling works in the Kiel Marine Geophysics and Hydroacoustics working group at the Institute of Geosciences and the Kiel Marine Science (KMS) priority research area at Kiel University, and is vice chairman of the German Hydrographic Society (DHyG). As an expert in seafloor mapping, methane gas seepage and seafloor pockmarks, he never believed that the depressions in the German Bight were caused by rising fluids. ""We had to come up with an alternative hypothesis for the formation. This allowed us to predict where potential porpoise feeding sites are, and that is exactly where we found the pits -- always close to sandeel habitats. Our extensive and multidisciplinary data analysis now provides a conclusive explanation for our harbor porpoise pits hypothesis.""
An interdisciplinary approach leads to the harbor porpoise pits hypothesis

The key to the new findings was an interdisciplinary approach that brought together geological studies, geophysical sonar measurements, vertebrate behavior and feeding biology, satellite evaluation, and oceanographic analysis. By precisely analyzing millions of echosoundings collected by German research vessels, the researchers were able to locate the unusual pits. ""Using special echosounding methods, we can now measure the seafloor with centimeter precision and thus find the shallow pits. We can also look into the seafloor and see, for example, whether there is free methane gas,"" explains AWI researcher Dr Jasper Hoffmann.
Analyzing the data, collected by research vessels over thousands of nautical miles, was a mammoth task. ""With modern methods, such structures can be automatically detected and characterized in acoustic data sets and automatically analyzed in large data sets,"" says Dr. Jacob Geersen, co-author of the study.
From the North Sea into the world: results with far-reaching effects
The research team currently believes that the initial feeding pits serve as a nucleus for scouring and eventually develop into larger pits. This finding also has global implications. The scouring of sediments by vertebrates in the ocean could modulate the seafloor on a global scale and influence benthic ecosystems. In the study area alone, pits cover nine percent of the seafloor. Initial volume estimates indicate that 773369 tons of sediment have been deposited over an area of 1581 km². This is roughly equivalent to the weight of half a million cars. ""Our results have far-reaching implications from a geological and biological perspective. They can help to assess the ecological risks associated with the expansion of renewable energies in the offshore sector and thus improve marine environmental protection,"" concludes Schneider von Deimling.

","score: 14.451753528773072, grade_level: '14'","score: 16.179940282301843, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s43247-023-01102-y,"Seabed pockmarks are among the most prominent morphologic structures in the oceans. They are usually interpreted as surface manifestation of hydrocarbon fluids venting from sediments. Here we suggest an alternative hypothesis of pockmark formation based on latest multibeam echosounder data with a centimeter resolution. In the North Sea, >40,000 enigmatically shaped shallow depressions or ‘pits’ with a mean depth of 0.11 m were documented, that do not resemble known pockmark morphologies. Combining the new echosounder data with information from behavioral biology, physical oceanography, satellite remote sensing and habitat mapping, we conclude that harbor porpoises excavate sediments during benthic foraging. By grubbing the seabed, they cause sandeels to escape from the sediment and initiate the formation of seafloor pits. Time-lapse data reveals that the initially feeding pits serve as nuclei for scouring and eventually merge into larger scour-pits. With the immense number of vertebrates in the ocean, such megafauna-driven macro-bioturbation reshapes the seafloor, modulates sediment transport, and ultimately impacts associated ecosystems on a global scale."
"
An international team of scientists has found the first direct evidence linking seemingly random weather systems in the ocean with climate on a global scale. Led by Hussein Aluie, an associate professor in the University of Rochester's Department of Mechanical Engineering and staff scientist at the University's Laboratory for Laser Energetics, the team reported their findings in Science Advances.

The ocean has weather patterns like what we experience on land, but on different time and length scales, says lead author Benjamin Storer, a research associate in Aluie's Turbulence and Complex Flow Group. A weather pattern on land might last a few days and be about 500 kilometers wide, while oceanic weather patterns such as swirling eddies last three to four weeks but are about one-fifth the size.
""Scientists have long speculated that these ubiquitous and seemingly random motions in the ocean communicate with climate scales, but it has always been vague because it wasn't clear how to disentangle this complex system to measure their interactions,"" says Aluie. ""We developed a framework that can do exactly that. What we found was not what people were expecting because it requires the mediation of the atmosphere.""
The group's goal was to understand how energy passes through different channels in the ocean throughout the planet. They used a mathematical method developed by Aluie in 2019, which was subsequently implemented into an advanced code by Storer and Aluie, that allowed them to study energy transfer across different patterns ranging from the circumference of the globe down to 10 kilometers. These techniques were then applied to ocean datasets from an advanced climate model and from satellite observations.
The study revealed that ocean weather systems are both energized and weakened when interacting with climate scales, and in a pattern that mirrors the global atmospheric circulation. The researchers also found that an atmospheric band near the equator called the ""intertropical convergence zone,"" which produces 30 percent of global precipitation, causes an intense amount of energy transfer, and produces ocean turbulence.
Storer and Aluie say that studying such complex fluid motion happening at multiple scales is not easy, but that it has advantages over previous attempts to link weather to climate change. They believe the team's work creates a promising framework for better understanding the climate system.
""There's a lot of interest in how global warming and our changing climate is influencing extreme weather events,"" says Aluie. ""Usually, such research efforts are based on statistical analysis that require expansive data to have confidence in the uncertainties. We are taking a different approach based on mechanistic analysis, which alleviates some of these requirements and allow us to understand cause and effect more easily.""
The team that played a central role in the investigation also included Michele Buzzicotti, a research scientist at the University of Rome Tor Vergata; Hemant Khatri, a research associate at the University of Liverpool, and Stephen Griffies, a senior scientist at Princeton.
Support for the project included funding from the National Science Foundation, the National Aeronautics and Space Administration, and the Department of Energy.

","score: 15.862185686653774, grade_level: '16'","score: 17.292226407411178, grade_levels: ['college_graduate'], ages: [24, 100]",10.1126/sciadv.adi7420,"Here, we present an estimate for the ocean's global scale transfer of kinetic energy (KE), across scales from 10 to 40,000 km. Oceanic KE transfer between gyre scales and mesoscales is induced by the atmosphere’s Hadley, Ferrel, and polar cells, and the intertropical convergence zone induces an intense downscale KE transfer. Upscale transfer peaks at 300 gigawatts across mesoscales of 120 km in size, roughly one-third the energy input by winds into the oceanic general circulation. Nearly three quarters of this “cascade” occurs south of 15°S and penetrates almost the entire water column. The mesoscale cascade has a self-similar seasonal cycle with characteristic lag time of ≈27 days per octave of length scales; transfer across 50 km peaks in spring, while transfer across 500 km peaks in summer. KE of those mesoscales follows the same cycle but peaks ≈40 days after the peak cascade, suggesting that energy transferred across a scale is primarily deposited at a scale four times larger."
"
We often look to the smallest lifeforms for help solving the biggest problems: Microbes help make foods and beverages, cure diseases, treat waste and even clean up pollution. Yeast and bacteria can also convert plant sugars into biofuels and chemicals traditionally derived from fossil fuels -- a key component of most plans to slow climate change.

Now University of Wisconsin-Madison researchers have engineered bacteria that can produce two chemical products at the same time from underutilized plant fiber. And unlike humans, these multitasking microbes can do both things equally well.
""To my knowledge, it's one of the first times you can make two valuable products simultaneously in one microbe,"" says Tim Donohue, UW-Madison professor of bacteriology and director of the Great Lakes Bioenergy Research Center.
The discovery, detailed in a paper in the December issue of the journal Applied and Environmental Microbiology, could help make biofuels more sustainable and commercially viable.
""In principle, the strategy lowers the net greenhouse gas emissions and improves the economics,"" Donohue says. ""The amount of energy and greenhouse gas that you need to make two products in one pot is going to be less than running two pots to make one product in each pot.""
Every molecule counts
The quest to replace fossil fuels with sustainable alternatives hinges on extracting the most possible value from renewable biomass. Just as with petrochemicals, every molecule counts: Low-volume, high-value products help keep fuel more affordable.

One of the biggest barriers is a part of the plant cell wall called lignin. Lignin is the world's most abundant source of renewable aromatic carbons, but its irregular structure makes it notoriously difficult to break apart into useful components.
That's why scientists with GLBRC have studied a bacterium named Novosphingobium aromaticivorans (sometimes referred to as simply Novo), which can digest many components of lignin and is relatively easy to genetically modify.
In 2019, researchers engineered a strain of Novo that can produce a key ingredient of plastics like nylon and polyurethane known as PDC. More recently, a team in Donohue's lab discovered another modification that allows Novo to make a different plastic ingredient called ccMA.
But they didn't stop there.
""We're not going to solve our carbon emissions problem by only producing two products,"" says Ben Hall, a recent doctoral graduate who contributed to the research.
Donohue's team used genomic modeling to come up with a list of potential products that could be made from biomass aromatics. Near the top of the list was zeaxanthin, one of a group of organic pigments known as carotenoids.

Carotenoids, which give carrots, pumpkins, salmon and even flamingos their distinctive hues, are used as nutritional supplements, pharmaceuticals and cosmetics and have a cumulative market value worth tens of billions of dollars a year.
Researchers knew that Novo had the genes to produce another carotenoid with little market value. Based on the bacteria's genome sequence, they suspected zeaxanthin is a steppingstone to that less valuable carotenoid in the process that cells use to make complex molecules. It was just a matter of altering the right genes to stop the digestive assembly line at the more valuable product.
By deleting or adding selected genes, they engineered strains that produced zeaxanthin as well as other valuable carotenoids -- beta-carotene, lycopene and astaxanthin -- when grown on an aromatic compound commonly found in lignin.
Next, the team showed that the engineered bacteria could produce the same carotenoids from a liquor made from ground and treated sorghum stems, a solution that contains a mixture of aromatics that many industrial bacteria can't digest.
One pot, two products
Hall then wondered what would happen if he combined the genetic changes needed to make PDC and a carotenoid in the same microbe.
The resulting strains produced both PDC and the target carotenoid -- with no discernable loss to either yield. Even better, the bacteria accumulated carotenoids within their cells, which must be separated from the solution that contains the PDC, which they secreted.
""We're already separating the cells from the media,"" Hall says. ""Now we would have a product coming out of both.""
The next steps include testing whether engineered strains can simultaneously produce carotenoids and ccMA, which Donohue thinks they will, and to engineer strains to improve yields in industrial conditions.
While there are lucrative markets for each of these products, Donohue and Hall say the real value of the discovery is the ability to add multiple functions to this biological platform.
""To me, it's both the strategy and the products,"" Donohue says. ""Now that we've done this, I think it opens the door to see if we can create other microbial chassis that make two products.""
This work was supported by the Great Lakes Bioenergy Research Center, U.S. Department of Energy, Office of Science, Office of Biological and Environmental Research and an NIH Training Grant.

","score: 13.514526610644257, grade_level: '14'","score: 14.515966386554624, grade_levels: ['college_graduate'], ages: [24, 100]",10.1128/aem.01268-23,"Carotenoids are lipophilic compounds found in the membranes of various organisms. Individual carotenoids are also commodity chemicals, produced industrially for use as food additives, nutritional supplements, cosmetics, and pharmaceuticals. The alphaproteobacterium Novosphingobium aromaticivorans has previously been established as a potential platform microbe for converting aromatic compounds derived from lignocellulosic plant biomass into valuable extracellular products. Here, we show that N. aromaticivorans DSM 12444 cells naturally produce the carotenoid nostoxanthin, and we construct a set of gene deletion mutants that accumulate β-carotene, lycopene, or zeaxanthin, which are predicted intermediates in nostoxanthin biosynthesis as well as commodity chemicals. We also show that a mutant strain heterologously expressing a CrtW protein accumulates the carotenoid astaxanthin. When grown on vanillate as the carbon source, we find that the levels of carotenoids are not significantly affected by O 2 concentration in the tested range of 5% to 21% O 2 . We also show that these carotenoids are produced at comparable levels when strains are grown in liquor from alkaline pretreated sorghum biomass [sorghum alkaline pretreatment liquor (APL)], which contains a mixture of aromatics. Finally, we construct strains that produce zeaxanthin, β-carotene, or astaxanthin concurrently with 2-pyrone-4,6-dicarboxylic acid, a potential building block for biodegradable polymers, when grown in sorghum APL. Combined, our results show that N. aromaticivorans can simultaneously produce valuable intracellular and extracellular commodities when grown in the presence of either pure aromatics or pretreated lignocellulosic biomass. There is economic and environmental interest in generating commodity chemicals from renewable resources, such as lignocellulosic biomass, that can substitute for chemicals derived from fossil fuels. The bacterium Novosphingobium aromaticivorans is a promising microbial platform for producing commodity chemicals from lignocellulosic biomass because it can produce these from compounds in pretreated lignocellulosic biomass, which many industrial microbial catalysts cannot metabolize. Here, we show that N. aromaticivorans can be engineered to produce several valuable carotenoids. We also show that engineered N. aromaticivorans strains can produce these lipophilic chemicals concurrently with the extracellular commodity chemical 2-pyrone-4,6-dicarboxylic acid when grown in a complex liquor obtained from alkaline pretreated lignocellulosic biomass. Concurrent microbial production of valuable intra- and extracellular products can increase the economic value generated from the conversion of lignocellulosic biomass-derived compounds into commodity chemicals and facilitate the separation of water- and membrane-soluble products."
"
Scientists at the CNRS and the University of Montpellier1 have discovered that flowering plants growing in farmland are increasingly doing without insect pollinators. As reproduction becomes more difficult for them in an environment depleted in pollinating insects, the plants are evolving towards self-fertilisation. These findings are published in a paper in the journal New Phytologist dated December 20, 2023.

By comparing field pansies growing in the Paris region today with pansies from the same localities resurrected in the laboratory from seeds collected2 between 1992 and 2001, the research team found that today's flowers are 10% smaller, produce 20% less nectar, and are less visited by pollinators than their ancestors.
This rapid evolution is thought to be due to the decline in pollinator populations in Europe. Indeed, a study conducted in Germany showed that over 75% of the biomass of flying insects has vanished from protected areas in the last thirty years.
The study identified a vicious circle in which the decline in pollinators leads to reduced nectar production by flowers, which could in turn exacerbate the decline of these insects. It underlines the importance of implementing measures to counter this phenomenon as quickly as possible and thus safeguard the interactions between plants and pollinators, which have existed for millions of years.
Notes
1 Centre d'écologie fonctionnelle et évolutive (CNRS/Université de Montpellier/EPHE/IRD)
2 Seeds conserved by the Conservatoire botanique national de Bailleul and the Conservatoire botanique national du Bassin parisien.

","score: 15.990910746812386, grade_level: '16'","score: 17.914735883424406, grade_levels: ['college_graduate'], ages: [24, 100]",10.1111/nph.19422,"Plant–pollinator interactions evolved early in the angiosperm radiation. Ongoing environmental changes are however leading to pollinator declines that may cause pollen limitation to plants and change the evolutionary pressures shaping plant mating systems. We used resurrection ecology methodology to contrast ancestors and contemporary descendants in four natural populations of the field pansy (Viola arvensis) in the Paris region (France), a depauperate pollinator environment. We combine population genetics analysis, phenotypic measurements and behavioural tests on a common garden experiment. Population genetics analysis reveals 27% increase in realized selfing rates in the field during this period. We documented trait evolution towards smaller and less conspicuous corollas, reduced nectar production and reduced attractiveness to bumblebees, with these trait shifts convergent across the four studied populations. We demonstrate the rapid evolution of a selfing syndrome in the four studied plant populations, associated with a weakening of the interactions with pollinators over the last three decades. This study demonstrates that plant mating systems can evolve rapidly in natural populations in the face of ongoing environmental changes. The rapid evolution towards a selfing syndrome may in turn further accelerate pollinator declines, in an eco‐evolutionary feedback loop with broader implications to natural ecosystems. Plant–pollinator interactions evolved early in the angiosperm radiation. Ongoing environmental changes are however leading to pollinator declines that may cause pollen limitation to plants and change the evolutionary pressures shaping plant mating systems. We used resurrection ecology methodology to contrast ancestors and contemporary descendants in four natural populations of the field pansy (Viola arvensis) in the Paris region (France), a depauperate pollinator environment. We combine population genetics analysis, phenotypic measurements and behavioural tests on a common garden experiment. Population genetics analysis reveals 27% increase in realized selfing rates in the field during this period. We documented trait evolution towards smaller and less conspicuous corollas, reduced nectar production and reduced attractiveness to bumblebees, with these trait shifts convergent across the four studied populations. We demonstrate the rapid evolution of a selfing syndrome in the four studied plant populations, associated with a weakening of the interactions with pollinators over the last three decades. This study demonstrates that plant mating systems can evolve rapidly in natural populations in the face of ongoing environmental changes. The rapid evolution towards a selfing syndrome may in turn further accelerate pollinator declines, in an eco‐evolutionary feedback loop with broader implications to natural ecosystems."
"
Reducing air pollution to levels similar to those during the coronavirus pandemic could protect the glaciers in the Himalayas and prevent them from disappearing by the end of the century. This is the conclusion reached by an international research team analysing the situation during the COVID-19 lockdown in 2020. The cleaner air has ensured that less soot has been deposited on the glaciers, resulting in 0.5 to 1.5 mm less snow melting per day. The rapid retreat of glaciers and the loss of snow cover already pose a threat to the sustainable water supply of billions of people in Asia who live in the catchment areas of rivers such as the Indus, Ganges and Yangtze. If emissions of air pollutants such as soot could be reduced to at least the level of the lockdowns, snowmelt could be reduced by up to half. A switch to clean energy supplies and lower-emission modes of transport would therefore bring significant benefits for sustainable water supplies, agriculture and ecosystems in large parts of Asia, the researchers write in the journal Atmospheric Chemistry and Physics (ACP).

The mountains of the Hindu Kush Himalayas (HKH) and the highlands of Tibet in Central Asia form the largest snow-covered region outside the poles. The meltwater from these glaciers feeds rivers in India and China, which fuel agriculture, hydropower generation and the economies of these countries. The Himalayan snowmelt in spring provides around half of the annual fresh water for around 4 billion people in South Asia and East Asia. But resources are dwindling: Global warming has already led to a loss of around 40 per cent of the Himalayan glacier area compared to the Little Ice Age in the Middle Ages. With the exception of a few Karakoram glaciers, the snow mass there has also decreased significantly over the last 30 years. Model simulations for extreme scenarios show that the melting snow in the Himalayas could cause the glaciers there to disappear by the end of the 21st century. This is worrying news for the water supply of several billion people.
The fact that glaciers are becoming thinner and thinner is partly due to climate change with higher air temperatures and changes in precipitation -- in other words, long-term causes that will take decades to combat. However, short-term factors such as the distribution and deposition of light-absorbing particles such as dust and soot (black carbon (BC)) also play a major role in glacier melting. Earlier studies have already shown that soot melts the snow on glaciers more than greenhouse gases in the atmosphere. The increasing energy demand of densely populated South Asia has greatly increased emissions of greenhouse gases and soot particles in recent decades, leading to increased darkening and melting of snow.
The economic slowdown caused by the lockdown measures during the coronavirus pandemic led to a drastic decline in passenger and freight transport, industrial emissions and energy consumption in this region in 2020. As a result, air pollution with greenhouse gases and especially soot also decreased significantly: satellite observations showed cleaner snow with almost a third less light-absorbing pollution during the lockdown in Asia between March and May 2020. This led to a decrease in snowmelt of 25 to 70 mm in 2020 -- compared to the 20-year average for the months of March to May in the western Himalayas. The changes in snow absorption and surface albedo thus ensured that around 7 cubic kilometres of meltwater remained in the Indus catchment area.
The international team of researchers from India, Germany and the UK used global simulations to analyse in detail the impact of reduced air pollution over high mountains in Central Asia during the COVID-19 lockdowns between March and May 2020: They used the ECHAM6-HAMMOZ chemistry-climate model, updated with an improved soot-snow parameterisation, to compare corona time with typical air pollution conditions. The corona simulations were performed with a COVID-19 emission inventory where emissions were calculated based on Google and Apple mobility data. Various observational data was also included in the new study: Snow cover and atmospheric opacity were determined using MODIS spectral data from NASA. These data were supplemented by solar photometer measurements from two Aerosol Robotic Network (AERONET) stations in Lahore (Pakistan) and Dushanbe (Tajikistan). The AERONET measurements in Dushanbe were part of the joint German-Tajik CADEX project from 2014 to 2016, in which the Academy of Sciences of Tajikistan and TROPOS jointly analysed mineral dust over Central Asia.
The ECHAM6-HAMMOZ model simulations show that the COVID lockdown in spring 2020 led to a cleaner atmosphere over the mountains of the Hindu Kush Himalayas and the highlands of Tibet. ""The aerosol optical thickness (AOD), i.e. the atmospheric opacity, over this region decreased by around 10 per cent in April 2020 compared to before the pandemic. This is supported by measurements from NASA's Moderate Resolution Imaging Spectroradiometer (MODIS), which also show a reduction in AOD compared to the average of the last 20 years,"" reports Dr Suvarna Fadnavis from the Indian Institute of Tropical Meteorology (IITM). The decrease in soot was also observed in the ground-based measurements of the Aerosol Radiative Forcing Over India Network (ARFINET): over the Indian Gangetic Plain (>50%), Northeast India (>30%), the Himalayan regions (16%-60%) and Tibet (70%).
The reduction in anthropogenic air pollution led to less soot being deposited on the snow in large parts of the high mountains of Central Asia. According to this study, there were around 25 to 350 micrograms less soot per kilogramme of snow in spring 2020, which corresponds to up to a third of the soot concentration in the snow there. However, according to the model, soot concentrations in the snow have also risen sporadically in some areas in the Hindu Kush, the eastern Himalayas and the Kunlun Mountains. The seemingly paradoxical differences are due to the fact that soot interacts with solar radiation not only on the surface, but above all in the atmosphere. This leads to complex adjustments in atmospheric circulation and thus to changes in the transport and deposition of air pollutants. ""Our simulations show that the decrease in soot concentration in the snow and the general reduction in air pollution and associated radiative effects reduced the short-wave radiative forcing at the surface by up to 2 watts per square metre in March to May 2020, resulting in less atmospheric warming. This lower warming of the snowpack and the tropospheric column is the combined effect of less soot in the snow and the changes in atmospheric concentrations of sulphate and soot,"" explains Dr Bernd Heinold from TROPOS. ""In the model, we were able to show that the decrease in air pollution reduced snowmelt in spring 2020 by 0.5 to 1.5 millimetres per day and thus reduced the runoff meltwater in the year by up to half."" The reduction in man-made pollution during the COVID-19 lockdown has therefore benefited the high mountains of Central Asia in many ways: increased reflectivity of the snow surface, reduced snowmelt and increased snow cover, as well as an increase in stored water due to reduced surface water runoff.
""Our results make it clear that of the two processes causing the retreat of the Himalayan glaciers -- global climate change and local air pollution -- a reduction in air pollution in particular could be a short-term help,"" emphasises Prof. Ina Tegen from TROPOS. ""Even if we were to stop CO2 emissions immediately, temperatures would not initially fall. However, our results confirm the importance of reducing short-lived climate drivers such as soot and their complementary role in CO2 mitigation. Reducing air pollution to similar levels as during the COVID-19 lockdowns in 2020 could protect the Himalayan glaciers, which are otherwise at risk of disappearing by the end of the 21st century."" Since 2000, the glaciers in the Himalayas have lost almost half a metre of ice per year. If air pollution could be reduced to the level it was at during the coronavirus pandemic, for example, then snowmelt could be reduced by up to half. Clean air measures would therefore not only benefit the health of billions of people in Asia, but also the water supply, agriculture and ecosystems in large parts of Asia.

","score: 15.434876015193158, grade_level: '15'","score: 16.820590433252015, grade_levels: ['college_graduate'], ages: [24, 100]",10.5194/acp-23-10439-2023,"Abstract. The rapid melting of glaciers in the Hindu Kush Himalayas (HKH) during recent decades poses an alarming threat to water security for larger parts of Asia. If this melting persists, the entirety of the Himalayan glaciers are estimated to disappear by end of the 21st century. Here, we assess the influence of the spring 2020 COVID-19 lockdown on the HKH, demonstrating the potential benefits of a strict emission reduction roadmap. Chemistry–climate model simulations, supported by satellite and ground measurements, show that lower levels of gas and aerosol pollution during lockdown led to changes in meteorology and to a reduction in black carbon in snow (2 %–14 %) and thus a reduction in snowmelt (10 %–40 %). This caused increases in snow cover (6 %–12 %) and mass (2 %–20 %) and a decrease in runoff (5 %–55 %) over the HKH and Tibetan Plateau, ultimately leading to an enhanced snow-equivalent water (2 %–55 %). We emphasize the necessity for immediate anthropogenic pollution reductions to address the hydro-climatic threat to billions of people in southern Asia."
"
In general, frogs' teeth aren't anything to write home about -- they look like pointy little pinpricks lining the upper jaw. But one group of stream-dwelling frogs in Southeast Asia has a strange adaptation: two bony ""fangs"" jutting out of their lower jawbone. They use these fangs to battle with each other over territory and mates, and sometimes even to hunt tough-shelled prey like giant centipedes and crabs. In a new study, published in the journal PLOS ONE, researchers have described a new species of fanged frog: the smallest one ever discovered.

""This new species is tiny compared to other fanged frogs on the island where it was found, about the size of a quarter,"" says Jeff Frederick, a postdoctoral researcher at the Field Museum in Chicago and the study's lead author, who conducted the research as a doctoral candidate at the University of California, Berkeley. ""Many frogs in this genus are giant, weighing up to two pounds. At the large end, this new species weighs about the same as a dime.""
In collaboration with the Bogor Zoology Museum, a team from the McGuire Lab at Berkeley found the frogs on Sulawesi, a rugged, mountainous island that makes up part of Indonesia. ""It's a giant island with a vast network of mountains, volcanoes, lowland rainforest, and cloud forests up in the mountains. The presence of all these different habitats mean that the magnitude of biodiversity across many plants and animals we find there is unreal -- rivaling places like the Amazon,"" says Frederick.
While trekking through the jungle, members of the joint US-Indonesia amphibian and reptile research team noticed something unexpected on the leaves of tree saplings and moss-covered boulders: nests of frog eggs.
Frogs are amphibians, and they lay eggs that are encapsulated by jelly, rather than a hard, protective shell. To keep their eggs from drying out, most amphibians lay their eggs in water. To the research team's surprise, they kept spotting the terrestrial egg masses on leaves and mossy boulders several feet above the ground. Shortly after, they began to see the small, brown frogs themselves.
""Normally when we're looking for frogs, we're scanning the margins of stream banks or wading through streams to spot them directly in the water,"" Frederick says. ""After repeatedly monitoring the nests though, the team started to find attending frogs sitting on leaves hugging their little nests."" This close contact with their eggs allows the frog parents to coat the eggs with compounds that keep them moist and free from bacterial and fungal contamination.
Closer examination of the amphibian parents revealed not only that they were tiny members of the fanged frog family, complete with barely-visible fangs, but that the frogs caring for the clutches of eggs were all male. ""Male egg guarding behavior isn't totally unknown across all frogs, but it's rather uncommon,"" says Frederick.
Frederick and his colleagues hypothesize that the frogs' unusual reproductive behaviors might also relate to their smaller-than-usual fangs. Some of the frogs' relatives have bigger fangs, which help them ward off competition for spots along the river to lay their eggs in the water. Since these frogs evolved a way to lay their eggs away from the water, they may have lost the need for such big imposing fangs. (The scientific name for the new species is Limnonectes phyllofolia; phyllofolia means ""leaf-nester."")
""It's fascinating that on every subsequent expedition to Sulawesi, we're still discovering new and diverse reproductive modes,"" says Frederick. ""Our findings also underscore the importance of conserving these very special tropical habitats. Most of the animals that live in places like Sulawesi are quite unique, and habitat destruction is an ever-looming conservation issue for preserving the hyper-diversity of species we find there. Learning about animals like these frogs that are found nowhere else on Earth helps make the case for protecting these valuable ecosystems.""

","score: 12.133920972644379, grade_level: '12'","score: 13.755471124620065, grade_levels: ['college_graduate'], ages: [24, 100]",10.1371/journal.pone.0292598,"Herein, we describe a new species of terrestrially-nesting fanged frog from Sulawesi Island, Indonesia. Though male nest attendance and terrestrial egg deposition is known in one other Sulawesi fanged frog (Limnonectes arathooni), the new species exhibits a derived reproductive mode unique to the Sulawesi assemblage; male frogs guard one or more clutches of eggs festooned to leaves or mossy boulders one to two meters above small slow-moving streams, trickles, or seeps. This island endemic has thus far been collected at three sites on Sulawesi: one in the Central Core of the island, and two on the Southwest Peninsula—south of the Tempe Depression (a major biogeographical boundary). The new Limnonectes has the smallest adult body size among its Sulawesi congeners—with a maximum snout-vent length of about 30 millimeters. Beyond its unique reproductive behavior and body size, the species is further diagnosed on the basis of advertisement call and genetic distance from sympatric fanged frogs. The discovery and description of the new species highlights the remarkable reproductive trait diversity that characterizes the Sulawesi fanged frog assemblage despite that most species in this radiation have yet to be formally described."
"
Aquaporin (Aqp) 10 water channels in humans allow the free passage of water, glycerol, urea, and boric acid across cells. However, Aqp10.2b in pufferfishes allows only the passage of water and glycerol and not urea and boric acid. Researchers from the Tokyo Institute of Technology sought to understand the evolutionary timeline that resulted in the variable substrate selection mechanisms among Aqp10s. Their results indicate that Aqp10.2 in ray-finned fishes may have reduced or lost urea and boric acid permeabilities through evolution.

Aquaporins (Aqps) are proteins that form water channels in the membranes of living cells, including those of bacteria, fungi, animals, and plants. These channels facilitate water transportation across cells more rapidly than diffusion through the membrane phospholipid bilayer.
Aqp10 belongs to the aquaglyceroporin subfamily of water channels. These proteins facilitate many of our body's physiological processes, including gut function, liver and fat cell metabolism, and skin elasticity. Water and solutes, such as glycerol, urea, and boric acid, get transported through human Aqp10 depending on concentration gradients across the membrane.
Sarcopterygians, which include coelacanths, lungfish, and tetrapods (such as amphibians, reptiles, birds, and mammals), are known to have a single gene that codes for Aqp10. In contrast, actinopterygians, such as ray-finned fishes, have paralogs, or near-identical copies, of the aqp10 gene, such as aqp10.1 and aqp10.2. Interestingly, the ray-finned Japanese pufferfish has paralog called aqp10.2b that shows permeability to water and glycerol but not to urea and boric acid.
To understand how these functional differences between solute permeabilities in Aqp10 in humans and pufferfish may have evolved, a team of researchers from the School of Life Science and Technology, Tokyo Institute of Technology (Tokyo Tech), Department of Advanced Bioscience, Faculty of Agriculture, Kindai University, Department of Integrative Biology at Michigan State University, Aquamarine Fukushima AMF, and Department of Physiology and Biomedical Engineering at the Mayo Clinic College of Medicine and Science analyzed Aqp10s in different species. The study was published in Genome Biology and Evolution on December 01, 2023.
When asked about their study, Dr. Ayumi Nagashima, who is an Assistant Professor in the Department of Life Science and Technology at Tokyo Tech and the lead scientist of the study, explains, ""Evolutionary adaptations in solute selectivity could present a promising model for analyzing solute permeability evolution in aquaglyceroporins. In this study, to elucidate the evolutionary history of Aqp10 solute selectivity, we analyzed and compared the permeability and evolutionary relationships of Aqp10s in eight bony vertebrate species.""
The research team found that similar to the tetrapod and lobe-finned fish Aqp10s, Aqp10.1 in ray-finned fishes also transport water, glycerol, urea, and boric acid. On the other hand, while Aqp10.2 in ray-finned fishes transport water and glycerol, they restrict urea and boric acid passage much more than Aqp10 and Aqp10.1.
These intriguing results indicate that water, glycerol, urea, and boric acid permeabilities are plesiomorphic features of Aqp10 water channels in all tetrapods and lobe-finned fish. However, the Aqp10.2 found in ray-finned fish may have reduced or lost urea and boric acid permeability during evolution.
In this regard, Dr. Nagashima remarks, ""Our study showed that the Aqp10.2 of ray-finned fishes allows only limited or no urea or boric acid transport across it. These activities were likely systematically reduced and subsequently lost through evolution in the common ancestor of these fishes. This research is expected to shed light on the elucidation of the substrate selection mechanism of aquaglyceroporins, which contributes to nutrient transport and other processes in the future.""
In summary, this study highlights that water, glycerol, urea, and boric acid transport activities are plesiomorphic activities of Aqp10 characteristic of the ancestral type common to the studied species. Consequently, the results suggest that Aqp10.2 in actinopterygians evolved to diminish the transport activity of urea and boric acid.

","score: 16.54493987730061, grade_level: '17'","score: 17.37774539877301, grade_levels: ['college_graduate'], ages: [24, 100]",10.1093/gbe/evad221,"Aquaporin (Aqp) 10 is a member of the aquaglyceroporin subfamily of water channels, and human Aqp10 is permeable to solutes such as glycerol, urea, and boric acid. Tetrapods have a single aqp10 gene, whereas ray-finned fishes have paralogs of this gene through tandem duplication, whole-genome duplication, and subsequent deletion. A previous study on Aqps in the Japanese pufferfish Takifugu rubripes showed that one pufferfish paralog, Aqp10.2b, was permeable to water and glycerol, but not to urea and boric acid. To understand the functional differences of Aqp10s between humans and pufferfish from an evolutionary perspective, we analyzed Aqp10s from an amphibian (Xenopus laevis) and a lobe-finned fish (Protopterus annectens) and Aqp10.1 and Aqp10.2 from several ray-finned fishes (Polypterus senegalus, Lepisosteus oculatus, Danio rerio, and Clupea pallasii). The expression of tetrapod and lobe-finned fish Aqp10s and Aqp10.1-derived Aqps in ray-finned fishes in Xenopus oocytes increased the membrane permeabilities to water, glycerol, urea, and boric acid. In contrast, Aqp10.2-derived Aqps in ray-finned fishes increased water and glycerol permeabilities, whereas those of urea and boric acid were much weaker than those of Aqp10.1-derived Aqps. These results indicate that water, glycerol, urea, and boric acid permeabilities are plesiomorphic activities of Aqp10s and that the ray-finned fish-specific Aqp10.2 paralogs have secondarily reduced or lost urea and boric acid permeability."
"
A study appearing in Nature Communications based on field and greenhouse experiments at the University of Kansas shows how a boost in agricultural yield comes from planting diverse crops rather than just one plant species: Soil pathogens harmful to plants have a harder time thriving.

""It's commonly observed that diverse plant communities can be more productive and stable over time,"" said corresponding author James Bever, senior scientist with the Kansas Biological Survey & Center for Ecological Research and Foundation Distinguished Professor of Ecology & Evolutionary Biology at KU. ""Range lands with numerous species can show increased productivity. But the reason for this has been a bit of a mystery.""
While crop rotation and other farming and gardening practices long have reflected benefits of a mix of plants, the new research puts hard data to one important mechanism underpinning the observation: the numbers of microorganisms in the soil that eat plants.
""Diverse agricultural communities have the potential to keep pathogens at bay, resulting in greater yields,"" Bever said. ""What we show is that a major driver is the specialization of pathogens, particularly those specific to different plant species. These pathogens suppress yields in low-diversity communities. A significant advantage of rangeland diversity is that less biomass is consumed by pathogens, allowing more biomass for other uses, such as cattle. The same process is crucial for agricultural production.""
The new data was developed at the University of Kansas using field experiments at the KU Field Station, along with greenhouse assays and feedback modeling using computers. This project was supported by large collaborative grants to KU from the National Science Foundation and the U.S. Department of Agriculture.
""We conducted an experiment manipulating the number of plants in a plot and varying precipitation levels -- we had from one up to six species in a plot,"" Bever said. ""Then, we evaluated the composition of the soil-pathogen microbiome. What we found is that the variation in pathogen composition in monocultures significantly predicted the yield when combined. When there are distinct pathogen communities, mixing them leads to a greater release of pathogens from your neighbors. The worst scenario is when a neighboring crop has the same pathogens. In that case, you're experiencing double density -- your crop pathogens and those from your neighbor crop.""
At KU, Bever's collaborators included associate specialist Peggy Schultz as well as Haley Burrill and Laura Podzikowski, both of whom earned doctorates at KU and now are postdoctoral researchers at the University of Oregon and KU, respectively. Lead author Guangzhou Wang worked at KU as a postdoctoral researcher and now is affiliated with China Agricultural University in Beijing, where he worked on the investigation there with co-authors Fusuo Zhang and Junling Zhang. They were joined by co-author Maarten Eppinga of the University of Zurich, Switzerland.

According to Bever, the research argues against the industrial-agricultural practice of planting a single food crop over many acres of land, often referred to as ""monoculture.""
""Regarding monoculture practices, the philosophy of promoting plant diversity seems to counter prevailing practices,"" he said. ""Monoculture -- planting vast areas with a single crop -- is driven by technological reasons rather than biological ones. Practical aspects of planting and harvesting have motivated this approach. Traditional Native American agriculture and practices in the tropics involve polycultures with multiple species. In China, there's a movement towards mechanized polyculture production, challenging the predominant monoculture model in the United States. It's essential to view monoculture as a cost-benefit model with increased inputs and explore alternative methods like crop rotation to manage pathogens over time.""
Bever said mixing plants in various plots would be beneficial to home gardeners and others who cultivate plants.
""When you're gardening, you're not relying on mechanical planting and mechanical harvesting,"" he said. ""It's definitely to your advantage to mix your crops -- to plant them in heterogeneous mixes in the plot. For convenience, we might plant alternating rows of different crops. That's going to do a better job of controlling pathogens than if you just had many rows of the same crop next to each other. If you had four plots in your backyard that were discrete, you wouldn't want to put all tomatoes in one and all squash in another, and a third with herbs -- you'd want to mix them in. You'll reduce pathogens by doing that. It's what our data shows.""
Finally, Bever said his team's findings that show biodiversity prohibits pathogen growth isn't as clear-cut outside the plant kingdom. In fact, the idea is contentious in animal systems like Lyme disease.
""Our clear result in the plant world contrasts with the complexity of this literature in the animal world,"" he said. ""In the context of recent attention on pathogens, such as with COVID, the study of pathogens in ecology has been controversial. The impact of diversity on pathogen impacts, whether it increases or decreases, has been debated. Our findings for plants indicate the bigger concern is the reduction of pathogen spread with increased diversity, rather than an increase. In our study, pathogens, including soil-dwelling ones, were examined. Similar patterns were observed with foliar pathogens, as detailed in an upcoming paper. The controversy arises from differences between how pathogens affect the animal kingdom versus plants.""

","score: 12.854757461750694, grade_level: '13'","score: 13.29345121645347, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41467-023-44253-4,"Productivity benefits from diversity can arise when compatible pathogen hosts are buffered by unrelated neighbors, diluting pathogen impacts. However, the generality of pathogen dilution has been controversial and rarely tested within biodiversity manipulations. Here, we test whether soil pathogen dilution generates diversity- productivity relationships using a field biodiversity-manipulation experiment, greenhouse assays, and feedback modeling. We find that the accumulation of specialist pathogens in monocultures decreases host plant yields and that pathogen dilution predicts plant productivity gains derived from diversity. Pathogen specialization predicts the strength of the negative feedback between plant species in greenhouse assays. These feedbacks significantly predict the overyielding measured in the field the following year. This relationship strengthens when accounting for the expected dilution of pathogens in mixtures. Using a feedback model, we corroborate that pathogen dilution drives overyielding. Combined empirical and theoretical evidence indicate that specialist pathogen dilution generates overyielding and suggests that the risk of losing productivity benefits from diversity may be highest where environmental change decouples plant-microbe interactions."
"
Wild North American grapes are now less of a mystery after an international team of researchers led by the University of California, Davis, decoded and catalogued the genetic diversity of nine species of this valuable wine crop.

The research, published in the journal Genome Biology, uncovers critical traits that could accelerate grape breeding efforts, particularly in tackling challenges like climate change, saline environments and drought.
""This research marks a significant step in understanding the genetics of grapevines,"" said Dario Cantù, the senior author on the journal article and a professor in the Department of Viticulture and Enology. ""It lays the groundwork for future advancements in grape breeding by identifying key genes responsible for important traits.""
The research team developed and used state-of-the-art technology to construct a comprehensive pangenome, which is a complete genetic blueprint, of wild grape species.
This so-called super-pangenome of nine species allowed the team to map genetic diversity, identify similarities or differences among them, and pinpoint specific traits that breeders may want to incorporate. First author Noé Cochetel, a postdoctoral researcher in Cantù's lab, did the analyses and played a pivotal role in the project.
It is the first North American wild grape pangenome to be mapped and catalogued, Cantù said.
""This offers tremendous potential for advancing sustainable grape cultivation, especially in regions facing water scarcity challenges,"" said Cantù, a plant biologist who also holds the Louis P. Martini Endowed chair. ""This pangenome will enable further genetic exploration of other vital adaptive traits, essential for industry resilience, like drought tolerance, heat resistance and defense against Pierce's disease.""
Caused by a strain of the bacterium Xylella fastidiosa, Pierce's disease kills grapevines by clogging their water-conducting vessels.

Wild grape pros and cons
North American grapes are known for their resistance to disease and adaptability, but they are not favored for taste and wine quality. European grapevines like chardonnay and cabernet sauvignon are less resistant to diseases but are renowned for producing high-quality wines.
North American species have a wide geographic range. As a consequence, they have evolved to withstand diverse climatic, soil and pathogen conditions, encompassing a broad spectrum of genetic diversity.
That is why nearly all wine grapes produced worldwide are from European vines grafted onto North American rootstocks.
Ability to select traits
The detailed pangenome will empower breeders to selectively incorporate desired traits from wild grapes, such as salt tolerance, while avoiding less desirable characteristics.
""Salt tolerance is a crucial trait for rootstocks,"" Cantù noted. ""Identifying these traits at a genetic level is a major advancement for grape breeding.""

","score: 15.140230414746544, grade_level: '15'","score: 16.37356415231627, grade_levels: ['college_graduate'], ages: [24, 100]",10.1186/s13059-023-03133-2,"Capturing the genetic diversity of wild relatives is crucial for improving crops because wild species are valuable sources of agronomic traits that are essential to enhance the sustainability and adaptability of domesticated cultivars. Genetic diversity across a genus can be captured in super-pangenomes, which provide a framework for interpreting genomic variations. Here we report the sequencing, assembly, and annotation of nine wild North American grape genomes, which are phased and scaffolded at chromosome scale. We generate a reference-unbiased super-pangenome using pairwise whole-genome alignment methods, revealing the extent of the genomic diversity among wild grape species from sequence to gene level. The pangenome graph captures genomic variation between haplotypes within a species and across the different species, and it accurately assesses the similarity of hybrids to their parents. The species selected to build the pangenome are a great representation of the genus, as illustrated by capturing known allelic variants in the sex-determining region and for Pierce’s disease resistance loci. Using pangenome-wide association analysis, we demonstrate the utility of the super-pangenome by effectively mapping short reads from genus-wide samples and identifying loci associated with salt tolerance in natural populations of grapes. This study highlights how a reference-unbiased super-pangenome can reveal the genetic basis of adaptive traits from wild relatives and accelerate crop breeding research."
"
A team of UK scientists has got a step closer to making several different types of plastic much easier to recycle, using a method that could be applied to a whole range of difficult-to-recycle polymers, including rubbers, gels and adhesives.

Thermoplastics and thermosets are two types of plastics that both consist of long chains of molecules called polymers but behave differently when heated.
Thermoplastics can be heated to high temperatures, poured into a mould then cooled to make the desired shape. They can subsequently be melted and reformed into other shapes when they are recycled, however they can break when stretched or stressed.
In contrast, the polymer chains in thermoset plastics are crosslinked to form a network which makes them incredibly strong and flexible. They are often used in composite materials, paints, coatings, rubbers and gels. Unfortunately, however, the crosslinks mean that the materials burn rather than melt when heated, making them much harder to break down and recycle.
Now, researchers at the University of Bath and University of Surrey have developed a way of introducing degradable bonds into thermoset polymers to make them more easily recyclable.
Publishing in Polymer Chemistry, they made a series of polymer gels with breakable bonds incorporated into different parts of the structure, and tested whether the properties changed after the gel was degraded and reformed.
They found that whilst all the gels could be degraded to some extent, gels with breakable bonds in the polymer chains (B in the attached diagram) retained their properties much better when reformed, compared with the polymers that were broken down via the cross-linked bonds (A).

The researchers hope this model system can be applied to other types of polymers, including adhesives, sealants and elastomers.
Dr Maciek Kope?, from the University of Bath's Department of Chemistry, said: ""Thermosets are used widely in the commercial sector, in materials like resins and adhesives.
""Being able to make bonds reversible in these materials will increase their applications as well as making them more recyclable.""
The researchers aim to create a general road map of the best locations for these breakable bonds, to understand better why some bonds break more easily than others, and plan to optimise the system using other commercially used polymers.
The researchers are also looking at other applications of the work, including using crosslinked polymers as vehicles for controlled drug delivery systems.
The work was funded by the Engineering and Physical Sciences Research Council (EPSRC).

","score: 14.610426829268295, grade_level: '15'","score: 16.092768292682926, grade_levels: ['college_graduate'], ages: [24, 100]",10.1039/D3PY01008B,The influence of the cleavable bond location on degradation and reformation of poly(n-butyl acrylate) networks synthesised by RAFT polymerisation was investigated and revealed that cleavable backbones lead to more efficient network reversibility.
"
Researchers report in the journal Geohealth that local rivers and streams were the source of the Salmonella enterica contamination along coastal North Carolina after Hurricane Florence in 2018 -- not the previously suspected high number of pig farms in the region.

These findings have critical implications for controlling the spread of disease caused by antibiotic-resistant pathogens after flooding events, particularly in the coastal regions of developing countries that are being highly impacted by the increase in tropical storms.
The study, led by University of Illinois Urbana-Champaign civil and environmental engineering professor Helen Nguyen and graduate student Yuqing Mao, tracks the presence and origin of S. enterica from environmental samples from coastal North Carolina using genetic tracing.
""Infections caused by antibiotic-resistant pathogens are responsible for approximately 2.8 million human illnesses and 36,000 deaths per year in the U.S. alone,"" Nguyen said. ""These infections spread easily across the globe and are a major burden on burgeoning health care systems, but they are preventable through mitigation.""
The study reports that because human and animal fecal genetic markers are often found in flood waters, it is commonly assumed wastewater sources, septic systems and livestock farms are responsible for spreading antibiotic-resistant bacteria and genetic material into the environment. However, no known studies have conclusively identified contaminant source points.
""Coastal North Carolina is a great case study area because there is a high concentration of swine farms and private septic systems, and coastal flooding caused by tropical storms is fairly common,"" Nguyen said.
Three weeks after Hurricane Florence, Nguyen's team collected 25 water samples from water bodies downstream of the swine farms in agricultural production areas in North Carolina, 23 of which contained the S. enterica bacteria.

""We analyzed free-floating genetic markers -- chromosomes and plasmids -- using high-fidelity whole-genome sequencing and found that S. enterica in the samples collected after Hurricane Florence were not from animals or manure,"" Nguyen said.
The team genetically traced the bacteria's origin to the many small local rivers and streams in the area -- meaning that these pathogens have already established themselves in the natural environment.
""With climate change bringing warmer temperatures -- in which bacteria thrive -- and possibly larger and more frequent tropical storms, the importance of our findings needs to be realized by researchers and policymakers,"" Nguyen said. ""Agricultural and human wastewater should not be the only source considered when designing mitigation plans to prevent the spread of pathogenic bacteria after hurricanes.""
Nguyen's team plans to extend this research beyond coastal regions and is collaborating with other campus researchers to study the spread of pathogens from Canada goose feces in Illinois.
Researchers from the Carl R. Woese Institute for Genomic Biology, the Carle Illinois College of Medicine and the University of Florida also contributed to this study.
The IGB, The Grainger College of Engineering, the Allen Foundation and the EPA supported this study.

","score: 18.186425983436852, grade_level: '18'","score: 20.18797360248447, grade_levels: ['college_graduate'], ages: [24, 100]",10.1029/2023GH000877,"In many regions of the world, including the United States, human and animal fecal genetic markers have been found in flood waters. In this study, we use high‐resolution whole genomic sequencing to examine the origin and distribution of Salmonella enterica after the 2018 Hurricane Florence flooding. We specifically asked whether S. enterica isolated from water samples collected near swine farms in North Carolina shortly after Hurricane Florence had evidence of swine origin. To investigate this, we isolated and fully sequenced 18 independent S. enterica strains from 10 locations (five flooded and five unflooded). We found that all strains have extremely similar chromosomes with only five single nucleotide polymorphisms (SNPs) and possessed two plasmids assigned bioinformatically to the incompatibility groups IncFIB and IncFII. The chromosomal core genome and the IncFIB plasmid are most closely related to environmental Salmonella strains isolated previously from the southeastern US. In contrast, the IncFII plasmid was found in environmental S. enterica strains whose genomes were more divergent, suggesting the IncFII plasmid is more promiscuous than the IncFIB type. We identified 65 antibiotic resistance genes (ARGs) in each of our 18 S. enterica isolates. All ARGs were located on the Salmonella chromosome, similar to other previously characterized environmental isolates. All isolates with different SNPs were resistant to a panel of commonly used antibiotics. These results highlight the importance of environmental sources of antibiotic‐resistant S. enterica after extreme flood events."
"
A winter wonderland calls to mind piles of fluffy, glistening snow. But to reach the ground, snowflakes are swept into the turbulent atmosphere, swirling through the air instead of plummeting directly to the ground.

The path of precipitation is complex but important to more than just skiers assessing the potential powder on their alpine vacation or school children hoping for a snow day. Determining snowflake fall speed is crucial for predicting weather patterns and measuring climate change.
In Physics of Fluids, from AIP Publishing, researchers from the University of Utah report snowflake accelerations in atmospheric turbulence. They found that regardless of turbulence or snowflake type, acceleration follows a universal statistical pattern that can be described as an exponential distribution.
""Even in the tropics, precipitation often starts its lifetime as snow,"" said author Timothy Garrett. ""How fast precipitation falls greatly affects storm lifetimes and trajectories and the extent of cloud cover that may amplify or diminish climate change. Just small tweaks in model representations of snowflake fall speed can have important impacts on both storm forecasting and how fast climate can be expected to warm for a given level of elevated greenhouse gas concentrations.""
Set up in a ski area near Salt Lake City, the team battled an unprecedented 900 inches of snow. They simultaneously filmed snowfall and measured atmospheric turbulence. Using a device they invented that employs a laser light sheet, they gathered information about snowflake mass, size, and density.
""Generally, as expected, we find that low-density 'fluffy' snowflakes are most responsive to surrounding turbulent eddies,"" said Garrett.
Despite the system's complexity, the team found that snowflake accelerations follow an exponential frequency distribution with an exponent of three halves. In analyzing their data, they also discovered that fluctuations in the terminal velocity frequency distribution followed the same pattern.
""Snowflakes are complicated, and turbulence is irregular. The simplicity of the problem is actually quite mysterious, particularly given there is this correspondence between the variability of terminal velocities -- something ostensibly independent of turbulence -- and accelerations of the snowflakes as they are locally buffeted by turbulence,"" said Garrett.
Because size determines terminal velocity, a possible explanation is that the turbulence in clouds that influences snowflake size is related to the turbulence measured at the ground. Yet the factor of three halves remains a mystery.
The researchers will revisit their experiment this winter, using a mist of oil droplets to obtain a closer look at turbulence and its impact on snowflakes.

","score: 13.989939759036144, grade_level: '14'","score: 15.173457831325301, grade_levels: ['college_graduate'], ages: [24, 100]",10.1063/5.0173359,"We use a novel experimental setup to obtain the vertical velocity and acceleration statistics of snowflakes settling in atmospheric surface-layer turbulence, for Taylor microscale Reynolds numbers (Reλ) between 400 and 67 000, Stokes numbers (St) between 0.12 and 3.50, and a broad range of snowflake habits. Despite the complexity of snowflake structures and the non-uniform nature of the turbulence, we find that mean snowflake acceleration distributions can be uniquely determined from the value of St. Ensemble-averaged snowflake root mean square (rms) accelerations scale nearly linearly with St. Normalized by the rms value, the acceleration distribution is nearly exponential, with a scaling factor for the (exponent) of −3/2 that is independent of Reλ and St; kurtosis scales with Reλ, albeit weakly compared to fluid tracers in turbulence; gravitational drift with sweeping is observed for St &lt; 1. Surprisingly, the same exponential distribution describes a pseudo-acceleration calculated from fluctuations of snowflake terminal fall speed in still air. This equivalence suggests an underlying connection between how turbulence determines the trajectories of particles and the microphysics determining the evolution of their shapes and sizes."
"
Apes recognize photos of groupmates they haven't seen for more than 25 years and respond even more enthusiastically to pictures of their friends, a new study finds.

The work, which demonstrates the longest-lasting social memory ever documented outside of humans, and underscores how human culture evolved from the common ancestors we share with apes, our closest relatives, was published today in the journal Proceedings of the National Academy of Sciences.
""Chimpanzees and bonobos recognize individuals even though they haven't seen them for multiple decades,"" said senior author Christopher Krupenye, an assistant professor at Johns Hopkins University who studies animal cognition. ""And then there's this small but significant pattern of greater attention toward individuals with whom they had more positive relationships. It suggests that this is more than just familiarity, that they're keeping track of aspects of the quality of these social relationships.""
Adds lead author Laura Lewis, a biological anthropologist and comparative psychologist at University of California, Berkeley: ""We tend to think about great apes as quite different from ourselves but we have really seen these animals as possessing cognitive mechanisms that are very similar to our own, including memory. And I think that is what's so exciting about this study.""
The research team was inspired to pursue the question of how long apes remember their peers because of their own experiences working with apes -- the sense that the animals recognized them when they'd visit, even if they'd been away for a long while.
""You have the impression that they're responding like they recognize you and that to them you're really different from the average zoo guest,"" Krupenye said. ""They're excited to see you again. So our goal with this study was to ask, empirically, if that's the case: Do they really have a robust lasting memory for familiar social partners?""
The team worked with chimpanzees and bonobos at Edinburgh Zoo in Scotland, Planckendael Zoo in Belgium, and Kumamoto Sanctuary in Japan. The researchers collected photographs of apes that had either left the zoos or died, individuals that participants hadn't seen for at least nine months and in some cases for as long as 26 years. The researchers also collected information about the relationships each participant had with former groupmates -- if there had been positive or negative interactions between them, etc.

The team invited apes to participate in the experiment by offering them juice, and while they sipped it, the apes where shown two side-by-side photographs -- apes they'd once known and total strangers. Using a non-invasive eye-tracking device, the team measured where the apes looked and for how long, speculating they'd look longer at apes they recognized.
The apes looked significantly longer at former groupmates, no matter how long they'd been apart. And they looked longer still at their former friends, those they'd had more positive interactions with.
In the most extreme case during the experiment, bonobo Louise had not seen her sister Loretta nor nephew Erin for more than 26 years at the time of testing. She showed a strikingly robust looking bias toward both of them over eight trials.
The results suggest great ape social memory could last beyond 26 years, the majority of their 40 to 60-year average lifespan, and could be comparable to that of humans, which begins to decline after 15 years but can persist as long as 48 years after separation. Such long lasting social memory in both humans and our closest relatives suggests that this kind of memory was likely already present millions of years ago in our common evolutionary ancestors. This memory likely forged a foundation for the evolution of human culture and enabled the emergence of uniquely-human forms of interaction such as intergroup trade where relationships are maintained over many years of separation, the authors said.
The idea that apes remember information about the quality of their relationships, years beyond any potential functionality, is another novel and human-like finding of the work, Krupenye said.
""This pattern of social relationships shaping long-term memory in chimpanzees and bonobos is similar to what we see in humans, that our own social relationships also seem to shape our long-term memory of individuals,"" Lewis said.

The work also raises the questions of whether the apes are missing individuals they're no longer with, especially their friends and family.
""The idea that they do remember others and therefore they may miss these individuals is really a powerful cognitive mechanism and something that's been thought of as uniquely human,"" Lewis said. ""Our study doesn't determine they are doing this, but it raises questions about the possibility that they may have the ability to do so.""
The team hopes the findings deepen people's understanding of the great apes, all of which are endangered species, while shedding new light on how deeply they could be affected when poaching and deforestation separate them from their groupmates.
""This work clearly shows how fundamental and long lasting these relationships are. Disruption to those relationships is likely very damaging,"" Krupenye said.
The team would next like to explore whether these long-lasting social memories are special to great apes or something experienced by other primates. They would also like to test how rich the memories of apes are, if, for instance, they possess lasting memories for experiences as well as individuals.
The work was made possible by Templeton World Charity Foundation grant TWCF-20647 and CIFAR Azrieli Global Scholars program.
Authors include: Erin G. Wessling, a postdoctoral fellow at Harvard University and the University of Göttingen; Fumihiro Kano, a scientist at the Max Planck Institute of Animal Behavior; Jeroen M. G. Stevens of Odisee University in Belgium, and Josep Call of University of St Andrews.

","score: 14.767568300312828, grade_level: '15'","score: 16.458477580813344, grade_levels: ['college_graduate'], ages: [24, 100]",10.1073/pnas.2304903120,"Recognition and memory of familiar conspecifics provides the foundation for complex sociality and is vital to navigating an unpredictable social world [Tibbetts and Dale, Trends Ecol. Evol. 22 , 529–537 (2007)]. Human social memory incorporates content about interactions and relationships and can last for decades [Sherry and Schacter, Psychol. Rev. 94 , 439–454 (1987)]. Long-term social memory likely played a key role throughout human evolution, as our ancestors increasingly built relationships that operated across distant space and time [Malone et al ., Int. J. Primatol. 33 , 1251–1277 (2012)]. Although individual recognition is widespread among animals and sometimes lasts for years, little is known about social memory in nonhuman apes and the shared evolutionary foundations of human social memory. In a preferential-looking eye-tracking task, we presented chimpanzees and bonobos ( N = 26) with side-by-side images of a previous groupmate and a conspecific stranger of the same sex. Apes’ attention was biased toward former groupmates, indicating long-term memory for past social partners. The strength of biases toward former groupmates was not impacted by the duration apart, and our results suggest that recognition may persist for at least 26 y beyond separation. We also found significant but weak evidence that, like humans, apes may remember the quality or content of these past relationships: apes’ looking biases were stronger for individuals with whom they had more positive histories of social interaction. Long-lasting social memory likely provided key foundations for the evolution of human culture and sociality as they extended across time, space, and group boundaries."
"
Ancient bricks inscribed with the names of Mesopotamian kings have yielded important insights into a mysterious anomaly in Earth's magnetic field 3,000 years ago, according to a new study involving UCL researchers.

The research, published in the Proceedings of the National Academy of Sciences (PNAS), describes how changes in the Earth's magnetic field imprinted on iron oxide grains within ancient clay bricks, and how scientists were able to reconstruct these changes from the names of the kings inscribed on the bricks.
The team hopes that using this ""archaeomagnetism,"" which looks for signatures of the Earth's magnetic field in archaeological items, will improve the history of Earth's magnetic field, and can help better date artefacts that they previously couldn't.
Co-author Professor Mark Altaweel (UCL Institute of Archaeology) said: ""We often depend on dating methods such as radiocarbon dates to get a sense of chronology in ancient Mesopotamia. However, some of the most common cultural remains, such as bricks and ceramics, cannot typically be easily dated because they don't contain organic material. This work now helps create an important dating baseline that allows others to benefit from absolute dating using archaeomagnetism.""
The Earth's magnetic field weakens and strengthens over time, changes which imprint a distinct signature on hot minerals that are sensitive to the magnetic field. The team analysed the latent magnetic signature in grains of iron oxide minerals embedded in 32 clay bricks originating from archaeological sites throughout Mesopotamia, which now overlaps with modern day Iraq. The strength of the planet's magnetic field was imprinted upon the minerals when they were first fired by the brickmakers thousands of years ago.
At the time they were made, each brick was inscribed with the name of the reigning king which archaeologists have dated to a range of likely timespans. Together, the imprinted name and the measured magnetic strength of the iron oxide grains offered a historical map of the changes to the strength of the Earth's magnetic field.
The researchers were able to confirm the existence of the ""Levantine Iron Age geomagnetic Anomaly,"" a period when Earth's magnetic field was unusually strong around modern Iraq between about 1050 to 550 BCE for unclear reasons. Evidence of the anomaly has been detected as far away as China, Bulgaria and the Azores, but data from within the southern part of the Middle East itself had been sparse.

Lead author Professor Matthew Howland of Wichita State University said: ""By comparing ancient artefacts to what we know about ancient conditions of the magnetic field, we can estimate the dates of any artifacts that were heated up in ancient times.""
To measure the iron oxide grains, the team carefully chipped tiny fragments from broken faces of the bricks and used a magnetometer to precisely measure the fragments.
By mapping out the changes in Earth's magnetic field over time, this data also offers archaeologists a new tool to help date some ancient artefacts. The magnetic strength of iron oxide grains embedded within fired items can be measured and then matched up to the known strengths of Earth's historic magnetic field. The reigns of kings lasted from years to decades, which offers better resolution than radiocarbon dating which only pinpoints an artefact's date to within a few hundred years.
An additional benefit of the archaeomagnetic dating of the artefacts is it can help historians more precisely pinpoint the reigns of some of the ancient kings that have been somewhat ambiguous. Though the length and order of their reigns is well known, there has been disagreement within the archaeological community about the precise years they took the throne resulting from incomplete historical records. The researchers found that their technique lined up with an understanding of the kings' reigns known to archaeologists as the ""Low Chronology.""
The team also found that in five of their samples, taken during the reign of Nebuchadnezzar II from 604 to 562 BCE, the Earth's magnetic field seemed to change dramatically over a relatively short period of time, adding evidence to the hypothesis that rapid spikes in intensity are possible.
Co-author Professor Lisa Tauxe of the Scripps Institution of Oceanography (US) said: ""The geomagnetic field is one of the most enigmatic phenomena in earth sciences. The well-dated archaeological remains of the rich Mesopotamian cultures, especially bricks inscribed with names of specific kings, provide an unprecedented opportunity to study changes in the field strength in high time resolution, tracking changes that occurred over several decades or even less.""
The research was carried out with funding from the U.S.-Israel Binational Science Foundation.

","score: 16.145323076923077, grade_level: '16'","score: 18.411825293350716, grade_levels: ['college_graduate'], ages: [24, 100]",10.1073/pnas.2313361120,"This study presents 32 high-resolution geomagnetic intensity data points from Mesopotamia, spanning the 3rd to the 1st millennia BCE. These data contribute to rectifying geographic disparities in the resolution of the global archaeointensity curve that have hampered our understanding of geomagnetic field dynamics and the viability of applying archaeomagnetism as a method of absolute dating of archaeological objects. A lack of precise and well-dated intensity data in the region has also limited our ability to identify short-term fluctuations in the geomagnetic field, such as the Levantine Iron Age geomagnetic Anomaly (LIAA), a period of high field intensity from ca. 1050 to 550 BCE. This phenomenon has hitherto not been well-demonstrated in Mesopotamia, contrary to predictions from regional geomagnetic models. To address these issues, this study presents precise archaeomagnetic results from 32 inscribed baked bricks, tightly dated to the reigns of 12 Mesopotamian kings through interpretation of their inscriptions. Results confirm the presence of the high field values of the LIAA in Mesopotamia during the first millennium BCE and drastically increase the resolution of the archaeointensity curve for the 3rd–1st millennia BCE. This research establishes a baseline for the use of archaeomagnetic analysis as an absolute dating technique for archaeological materials from Mesopotamia."
"
In a spectacular new study, researchers from the University of Copenhagen have used light and chlorine to eradicate low-concentration methane from air. The result gets us closer to being able to remove greenhouse gases from livestock housing, biogas production plants and wastewater treatment plants to benefit the climate. The research has just been published in the journal Environmental Research Letters.

The Intergovernmental Panel on Climate Change (IPCC) has determined that reducing methane gas emissions will immediately reduce the rise in global temperatures. The gas is up to 85 times more potent of a greenhouse gas than CO2, and more than half of it is emitted by human sources, with cattle and fossil fuel production accounting for the largest share.
A unique new method developed by a research team at the University of Copenhagen's Department of Chemistry and spin-out company Ambient Carbon has succeeded in removing methane from air.
""A large part of our methane emissions comes from millions of low-concentration point sources like cattle and pig barns. In practice, methane from these sources has been impossible to concentrate into higher levels or remove. But our new result proves that it is possible using the reaction chamber that we've have built,"" says Matthew Stanley Johnson, the UCPH atmospheric chemistry professor who led the study.
Earlier, Johnson presented the research results at COP 28 in Dubai via an online connection and in Washington D.C. at the National Academy of Sciences, which advises the US government on science and technology.
Reactor cleans methane from air 
Methane can be burnt off from air if its concentration exceeds 4 percent. But most human-caused emissions are below 0.1 percent and therefore unable to be burned.

To remove methane from air, the researchers built a reaction chamber that, to the uninitiated, looks like an elongated metal box with heaps of hoses and measuring instruments. Inside the box, a chain reaction of chemical compounds takes place, which ends up breaking down the methane and removing a large portion of the gas from air.
""In the scientific study, we've proven that our reaction chamber can eliminate 58 percent of methane from air. And, since submitting the study, we have improved our results in the laboratory so that the reaction chamber is now at 88 percent,"" says Matthew Stanley Johnson.
Chlorine is key to the discovery. Using chlorine and the energy from light, researchers can remove methane from air much more efficiently than the way it happens in the atmosphere, where the process typically takes 10-12 years.
""Methane decomposes at a snail's pace because the gas isn't especially happy about reacting with other things in the atmosphere. However, we've discovered that, with the help of light and chlorine, we can trigger a reaction and break down the methane roughly 100 million times faster than in nature,"" explains Johnson.
Up next: livestock stalls, wastewater treatment plants and biogas plants
A 40ft shipping container will soon arrive at the Department of Chemistry. When it does, it will become a larger prototype of the reaction chamber that the researchers built in the laboratory. It will be a ""methane cleaner"" which, in principle, will be able to be connected to the ventilation system in a livestock barn.

""Today's livestock farms are high-tech facilities where ammonia is already removed from air. As such, removing methane through existing air purification systems is an obvious solution,"" explains Professor Johnson.
The same applies to biogas and wastewater treatment plants, which are some of the largest human-made sources of methane emissions in Denmark after cattle production.
As a preliminary investigation for this study, the researchers traveled around the country measuring how much methane leaks from cattle stalls, wastewater treatment plants and biogas plants. In several places, the researchers were able to document that a large amount of methane leaks into the atmosphere from these plants.
""For example, Denmark is a pioneer when it comes to producing biogas. But if just a few percent of the methane from this process escapes, it counteracts any climate gains,"" concludes Johnson.
The research is funded by a grant from Innovation Fund Denmark for the PERMA project, a part of AgriFoodTure. The research was conducted in collaboration between the University of Copenhagen, Aarhus University, Arla, Skov and the UCPH spin-out company Ambient Carbon, started and now headed by Professor Matthew Stanley Johnson. The company was started to develop MEPS (Methane Eradication Photochemical System) technology and make it available to society.
ABOUT THE METHOD
The researchers built a reaction chmaber and devised a method that simulates and greatly accelerates methane's natural degradation process.
They dubbed the method the Methane Eradication Photochemical System (MEPS) and it degrades methane 100 million times faster than in nature.
The method works by introducing chlorine molecules into a reaction chamber with methane gas. The researchers then shine UV light onto the chlorine molecules. The light's energy causes the molecules to split and form two chlorine atoms.
The chlorine atoms then steal a hydrogen atom from the methane, which then falls apart and decomposes. The chlorine product (hydrochloric acid) is captured and subsequently recycled in the chamber.
The methane turns into carbon dioxide (CO2) and carbon monoxide (CO) and hydrogen (H2) in the same way as the natural process does in the atmosphere.
More about methane (CH4):
Methane can be burned off to remove it from air, but its concentration must be over 4%, 40,000 parts per million (ppm) to be flammable. As most human-caused emissions are below 0.1 percent, they cannot be burned.
The Intergovernmental Panel on Climate Change (IPCC) has determined that reducing methane gas emissions will immediately reduce the rise in global temperatures.
Methane is a greenhouse gas that is emitted naturally from, among other things, wetlands and from man-made sources such as food production, natural gas and sewage treatment plants.
Today, methane gas is responsible for a third of the greenhouse gases that affect the climate and cause global warming.
It takes methane 10-12 years to decompose naturally in the atmosphere, where it is converted into carbon dioxide.
Over a 25-year period, methane is 85 times worse for the climate than CO2. Over a 100-year period, methane is 30 times worse for the climate than CO2.
The concentration of methane in the atmosphere has increased by 150% since the mid-1700s.
Methane alone has increased anthropogenic radiation exposure by 1.19 W/m^2, which is responsible for a 0.6 ?C increase in global average surface air temperature, according to the IPCC.

","score: 12.234528315450838, grade_level: '12'","score: 13.385940942829926, grade_levels: ['college_graduate'], ages: [24, 100]",10.1088/1748-9326/ad0e33,"Despite the urgent need, very few methods are able to efficiently remove methane from waste air with low cost and energy per unit volume, especially at the low concentrations found in emissions from e.g. wastewater treatment, livestock production, biogas production and mine ventilation. We present the first results of a novel method based on using chlorine atoms in the gas phase, thereby achieving high efficiency. A laboratory prototype of the methane eradication photochemical system (MEPS) technology achieves 58% removal efficiency with a flow capacity of 30 l min−1; a reactor volume of 90 l; UV power input at 368 nm of 110 W; chlorine concentration of 99 ppm; and a methane concentration of 55 ppm; under these conditions the apparent quantum yield (AQY) ranged from 0.48% to 0.56% and the volumetric energy consumption ranged from 36 to 244 kJ m−3. The maximum achieved AQY with this system was 0.83%. A series of steps that can be taken to further improve performance are described. These metrics show that MEPS has the potential to be a viable method for eliminating low-concentration methane from waste air."
"
The Earth is a wonderful blue and green dot covered with oceans and life, while Venus is a yellowish sterile sphere that is not only inhospitable but also sterile. However, the difference between the two bears to only a few degrees in temperature. A team of astronomers from the University of Geneva (UNIGE), with the support of the CNRS laboratories of Paris and Bordeaux, has achieved a world's first by managing to simulate the entirety of the runaway greenhouse process which can transform the climate of a planet from idyllic and perfect for life, to a place more than harsh and hostile. The scientists have also demonstrated that from initial stages of the process, the atmospheric structure and cloud coverage undergo significant changes, leading to an almost-unstoppable and very complicated to reverse runaway greenhouse effect. On Earth, a global average temperature rise of just a few tens of degrees, subsequent to a slight rise of the Sun's luminosity, would be sufficient to initiate this phenomenon and to make our planet inhabitable. These results are published in Astronomy & Astrophysics.

The idea of a runaway of the greenhouse effect is not new. In this scenario, a planet can evolve from a temperate state like on Earth to a true hell, with surface temperatures above 1000°C. The cause? Water vapor, a natural greenhouse gas. Water vapor prevents the solar irradiation absorbed by Earth to be reemitted towards the void of space, as thermal radiation. It traps heat a bit like a rescue blanket. A dash of greenhouse effect is useful -- without it, Earth would have an average temperature below the freezing point of water, looking like a ball covered with ice and hostile to life.
On the opposite, too much greenhouse effect increases the evaporation of oceans, and thus the amount of water vapor in the atmosphere. ""There is a critical threshold for this amount of water vapor, beyond which the planet cannot cool down anymore. From there, everything gets carried away until the oceans end up getting fully evaporated and the temperature reaches several hundred degrees,"" explains Guillaume Chaverot, former postdoctoral scholar in the Department of Astronomy at the UNIGE Faculty of Science and main author of the study.
World premiere
""Until now, other key studies in climatology have focused solely on either the temperate state before the runaway, or either the inhabitable state post-runaway,"" reveals Martin Turbet, researcher at CNRS laboratories of Paris and Bordeaux, and co-author of the study. ""It is the first time a team has studied the transition itself with a 3D global climate model, and has checked how the climate and the atmosphere evolve during that process.""
One of the key points of the study describes the appearance of a very peculiar cloud pattern, increasing the runaway effect, and making the process irreversible. ""From the start of the transition, we can observe some very dense clouds developing in the high atmosphere. Actually, the latter does not display anymore the temperature inversion characteristic of the Earth atmosphere and separating its two main layers: the troposphere and the stratosphere. The structure of the atmosphere is deeply altered,"" points out Guillaume Chaverot.
Serious consequences for the search of life elsewhere
This discovery is a key feature for the study of climate on other planets, and in particular on exoplanets -- planets orbiting other stars than the Sun. ""By studying the climate on other planets, one of our strongest motivations is to determine their potential to host life,"" indicates Émeline Bolmont, assistant professor and director of the UNIGE Life in the Universe Center (LUC), and co-author of the study.

The LUC leads state-of-the-art interdisciplinary research projects regarding the origins of life on Earth, and the quest for life elsewhere in our solar system and beyond, in exoplanetary systems. ""After the previous studies, we suspected already the existence of a water vapor threshold, but the appearance of this cloud pattern is a real surprise!"" discloses Émeline Bolmont. ""We have also studied in parallel how this cloud pattern could create a specific signature, or ''fingerprint'', detectable when observing exoplanet atmospheres. The upcoming generation of instruments should be able to detect it,"" unveils Martin Turbet. The team is also not aiming to stop there, Guillaume Chaverot having received a research grant to continue this study at the ""Institut de Planétologie et d'Astrophysique de Grenoble"" (IPAG). This new step of the research project will focus on the specific case of the Earth.
A planet Earth in a fragile equilibrium
With their new climate models, the scientists have calculated that a very small increase of the solar irradiation -- leading to an increase of the global Earth temperature, of only a few tens of degrees -- would be enough to trigger this irreversible runaway process on Earth and make our planet as inhospitable as Venus. One of the current climate goals is to limit global warming on Earth, induced by greenhouse gases, to only 1.5 degrees by 2050. One of the questions of Guillaume Chaverot's research grant is to determine if greenhouse gases can trigger the runaway process as a slight increase of the Sun luminosity might do. If so, the next question will be to determine if the treshold temperatures are the same for both processes.
The Earth is thus not so far from this apocalyptical scenario. ""Assuming this runaway process would be started on Earth, an evaporation of only 10 meters of the oceans' surface would lead to a 1 bar increase of the atmospheric pressure at ground level. In just a few hundred years, we would reach a ground temperature of over 500°C. Later, we would even reach 273 bars of surface pressure and over 1 500°C, when all of the oceans would end up totally evaporated,"" concludes Guillaume Chaverot.

","score: 13.655818077054032, grade_level: '14'","score: 14.267155391425057, grade_levels: ['college_graduate'], ages: [24, 100]",10.1051/0004-6361/202346936,"While their detections remain challenging at present, observations of small terrestrial planets will become easier in a near future thanks to continuous improvements of detection and characterisation instruments. In this quest, climate modeling is a key step to understanding their characteristics, atmospheric composition, and possible histories. If a surface water reservoir is present on such a terrestrial planet, an increase in insolation may lead to a dramatic positive feedback induced by water evaporation: the runaway greenhouse. The resulting rise in the global surface temperature leads to the evaporation of the entire water reservoir, separating two very different population of planets: 1) temperate planets with a surface water ocean and 2) hot planets with a puffed atmosphere dominated by water vapor. Therefore, the understanding of the runaway greenhouse is pivotal to assess the different evolution of Venus and the Earth, as well as every similar terrestrial exoplanet. In this work, we use a 3D General Circulation Model (GCM), the Generic-PCM, to study the runaway greenhouse transition, linking temperate and post-runaway states. Our simulations were comprised of two phases. First, assuming initially a liquid surface ocean, there is an evaporation phase, which enriches the atmosphere with water vapor. Second, when the ocean is considered to be entirely evaporated, there is a dry transition phase for which the surface temperature increases dramatically. Finally, the evolution ends with a hot and stable post-runaway state. By describing in detail the evolution of the climate over these two steps, we show a rapid transition of the cloud coverage and of the wind circulation from the troposphere to the stratosphere. By comparing our result to previous studies using 1D models, we discuss the effect of intrinsically 3D processes such as the global dynamics and the clouds, which are key to understanding the runaway greenhouse. We also explore the potential reversibility of the runaway greenhouse that is limited by its radiative unbalance."
"
Scientists looking to uncover the mysteries of the underwater world have more valuable information at their fingertips thanks to an international team that has produced an inventory of species confirmed or expected to produce sound underwater.

Led by Audrey Looby from the University of Florida Department of Fisheries and Aquatic Sciences, the Global Library of Underwater Biological Sounds working group collaborated with the World Register of Marine Species to document 729 aquatic mammals, other tetrapods, fishes, and invertebrates that produce active or passive sounds. In addition, the inventory includes another 21,911 species that are considered to likely produce sounds.
With more than 70% of the Earth's surface covered in water, most of the planet's habitats are aquatic, and there is a misconception that most aquatic organisms are silent. The newly published comprehensive digital database on what animals are known to make sounds is the first of its kind and can revolutionize marine and aquatic science, the researchers said.
""Eavesdropping on underwater sounds can reveal a plethora of information about the species that produce them and is useful for a variety of applications, ranging from fisheries management, invasive species detection, improved restoration outcomes, and assessing human environmental impacts,"" said Looby, who also co-created FishSounds, which offers a comprehensive, global inventory of fish sound production research.
The team's research, ""Global Inventory of Species Categorized by Known Underwater Sonifery,"" was published Monday in Scientific Data and involved 19 authors from six countries, funding from the Richard Lounsbery Foundation and centuries of scientific effort to document underwater sounds.
""Understanding how marine species interact with their environments is of global importance, and this data being freely available is a major step toward that goal,"" said Kieran Cox, a member of the research team and a National Science and Engineering Research Council of Canada fellow.
Most people are familiar with whale or dolphin sounds but are often surprised to learn that many fishes and invertebrates use sounds to communicate, too, Looby said.
""Our dataset helps demonstrate how widespread underwater sound production really is across a variety of animals, but also that we still have a lot to learn,"" she said.

","score: 19.875392265193373, grade_level: '20'","score: 22.39284530386741, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41597-023-02745-4,"A working group from the Global Library of Underwater Biological Sounds effort collaborated with the World Register of Marine Species (WoRMS) to create an inventory of species confirmed or expected to produce sound underwater. We used several existing inventories and additional literature searches to compile a dataset categorizing scientific knowledge of sonifery for 33,462 species and subspecies across marine mammals, other tetrapods, fishes, and invertebrates. We found 729 species documented as producing active and/or passive sounds under natural conditions, with another 21,911 species deemed likely to produce sounds based on evaluated taxonomic relationships. The dataset is available on both figshare and WoRMS where it can be regularly updated as new information becomes available. The data can also be integrated with other databases (e.g., SeaLifeBase, Global Biodiversity Information Facility) to advance future research on the distribution, evolution, ecology, management, and conservation of underwater soniferous species worldwide."
"
Some coral species can be resilient to marine heat waves by ""remembering"" how they lived through previous ones, research by Oregon State University scientists suggests.

The study, funded by the National Science Foundation, also contains evidence that the ecological memory response is likely linked to the microbial communities that dwell among the corals.
The findings, published today in Global Change Biology, are important because coral reefs, crucial to the functioning of planet Earth, are in decline from a range of human pressures including climate change, said the study's lead author, Alex Vompe.
""It is vital to understand how quickly reefs can adapt to ever more frequent, repeated disturbances such as marine heat waves,"" said Vompe, a doctoral student who works in the lab of microbiology professor Rebecca Vega Thurber. ""The microbiomes living within their coral hosts might be a key component of rapid adaptation.""
Heat waves are likely to increase in frequency and severity because of climate change, he added. Slowing down the rate of coral cover and species loss is a major conservation goal, and predicting and engineering heat tolerance are two important tools.
Knowing the role microbes play in adaptation can inform coral gardening and planting efforts, Vompe said. A deeper understanding of the microbial processes, and the organisms responsible for ecological memory, can also aid in developing probiotics and/or monitoring protocols to assess and act on the quality of ecological memory of individual coral colonies.
Coral reefs are found in less than 1% of the ocean but are home to nearly one-quarter of all known marine species. They also help regulate the sea's carbon dioxide levels and are a crucial source for scientists searching for new medicines.

Corals are made up of interconnected animal hosts called polyps that house microscopic algae inside their cells. Corals also house functionally and taxonomically diverse bacteria, viruses, archaea and microeukaryotes. The community of bacteria and archaea living within corals are referred to as the coral microbiome.
Symbiosis is the foundation of the coral reef ecosystem as these microbes benefit coral hosts by assisting in carbon, nitrogen and sulfur cycling, essential vitamin supplementation, and protection against pathogens. The coral polyps in turn provide nutrition and protection to the algae and bacteria.
Climate change is threatening coral reefs in part because some of the relationships between coral and their microbes can be stressed by warming oceans to the point of dissolution -- a collapse of the host-microbe partnerships, which results in a phenomenon known as coral bleaching.
""But Acropora retusa, a prevalent coral species in the Mo'orean coral reef that we studied, appears to have a powerful ecological memory response to heat waves that the microbiome seems to play a role in,"" Vompe said. ""This means some coral species may be more resilient to climate change than previously thought.""
Vompe, Vega Thurber and colleagues at OSU, the University of California, Santa Barbara, Arizona State University, and the University of Essex spent five years studying 200 coral colonies at a reef on the north shore of Mo'orea, French Polynesia. Mo'orea is an island in the South Pacific, roughly halfway between Australia and South America.
Because of the reef's recent history, it presented a unique opportunity to examine heat wave response, the researchers said.

In 2010, crown-of-thorns starfish and a cyclone destroyed more than 99% of the corals, effectively hitting the reset button on the reef. Corals reestablished and went through comparatively minor heat wave events in 2016 and 2017 before experiencing the area's most severe marine heat wave in recorded history between December 2018 and July 2019.
The second-most severe heat wave soon followed, between February and July of 2020.
""We observed that some species of coral seem to remember exposure to past marine heat waves and maintain a higher level of health in subsequent heat waves,"" Vega Thurber said. ""And Acropora retusa's memory response was strongly linked to changes in its microbiome, supporting the idea that the microbial community has a part in this process.""
Cauliflower corals in the genus Pocillopora stayed in good health through the heat events, and their microbiomes also showed an ecological memory response, she noted. They were perturbed by the initial 2019 heat wave but recovered to their predisturbance state despite the second heat wave in 2020.
""Members of coral microbial communities have unique biological features that make them more adaptable and responsive to environmental change -- short generation cycles, large population sizes and diverse metabolic potential,"" Vega Thurber said. ""In two of the three coral species we focused on, we identified initial microbiome resilience, host and microbiome acclimatization, or developed microbiome resistance to repeated heat stress. The latter two patterns are consistent with the concept of ecological memory.""
Other Oregon State researchers involved the research were Thomas Sharpton, Hannah Epstein and Emily Schmeltzer. Sharpton is an associate professor of microbiology and statistics; Epstein was a postdoctoral researcher during the study and is now a lecturer at the University of Essex; Schmeltzer, a doctoral student in Vega Thurber's lab, has graduated and is working as a biologist with the U.S. Geological Survey.
The Gordon and Betty Moore Foundation also supported this research.

","score: 14.906327653997376, grade_level: '15'","score: 15.803255242463955, grade_levels: ['college_graduate'], ages: [24, 100]",10.1111/gcb.17088,"Microbiomes are essential features of holobionts, providing their hosts with key metabolic and functional traits like resistance to environmental disturbances and diseases. In scleractinian corals, questions remain about the microbiome's role in resistance and resilience to factors contributing to the ongoing global coral decline and whether microbes serve as a form of holobiont ecological memory. To test if and how coral microbiomes affect host health outcomes during repeated disturbances, we conducted a large‐scale (32 exclosures, 200 colonies, and 3 coral species sampled) and long‐term (28 months, 2018–2020) manipulative experiment on the forereef of Mo'orea, French Polynesia. In 2019 and 2020, this reef experienced the two most severe marine heatwaves on record for the site. Our experiment and these events afforded us the opportunity to test microbiome dynamics and roles in the context of coral bleaching and mortality resulting from these successive and severe heatwaves. We report unique microbiome responses to repeated heatwaves in Acropora retusa, Porites lobata, and Pocillopora spp., which included: microbiome acclimatization in A. retusa, and both microbiome resilience to the first marine heatwave and microbiome resistance to the second marine heatwave in Pocillopora spp. Moreover, observed microbiome dynamics significantly correlated with coral species‐specific phenotypes. For example, bleaching and mortality in A. retusa both significantly increased with greater microbiome beta dispersion and greater Shannon Diversity, while P. lobata colonies had different microbiomes across mortality prevalence. Compositional microbiome changes, such as changes to proportions of differentially abundant putatively beneficial to putatively detrimental taxa to coral health outcomes during repeated heat stress, also correlated with host mortality, with higher proportions of detrimental taxa yielding higher mortality in A. retusa. This study reveals evidence for coral species‐specific microbial responses to repeated heatwaves and, importantly, suggests that host‐dependent microbiome dynamics may provide a form of holobiont ecological memory to repeated heat stress."
"
A warmer environment could mean more mosquitoes as it becomes harder for their predators to control the population, according to a recent study led by Virginia Commonwealth University researchers.

As the cover feature in Ecology, a journal published by the Ecological Society of America, the study -- ""Warming and Top-Down Control of Stage-Structured Prey: Linking Theory to Patterns in Natural Systems"" -- found that rising temperatures, often linked to climate change, can make predators of mosquito larvae less effective at controlling mosquito populations. Warmer temperatures accelerate development time of larvae, leading to a smaller window of time that dragonflies could eat them.
This means there could be nearly twice as many mosquito larvae that make it to adulthood in the study area. The researchers looked at riverine rock pools at Belle Isle along the James River in Richmond and found that warmer temperature pools had more aquatic mosquito larvae, even when their predators that naturally control the populations were present.
The native rock pool mosquito is not an important disease vector, but it is one of the few local mosquitoes that doesn't have to feed as an adult to lay eggs. So the findings might apply to similar taxa, like the invasive Asian rock pool mosquito.
""We might see larger populations of everyone's least favorite bug, mosquitoes. While the mosquito larvae we studied here [are] the North American rock pool mosquito, these findings likely apply to species of mosquito that do act as vectors for diseases like West Nile or even Zika virus,"" said Andrew T. Davidson, Ph.D., lead researcher on the study. He conducted the research through the Ph.D. program in VCU's Center for Integrative Life Sciences Education.
Predators help stabilize ecosystems and food webs, and the study looked at predator-prey interaction between dragonfly nymphs and mosquito larvae. Before field work, the research was rooted in concepts of thermal physiology and short-term lab experiments that yielded predictive models of the relationship between predators, prey and temperature in the field. The field study then tested the models in a complete natural environment.
The study builds on Davidson's earlier research in Functional Ecology as well as work by lab mate C. Ryland Stunkle and the rest of the VCU rock pool team. The team also acknowledges the collaborative support of professor Brian Byrd of Western Carolina University's College of Health and Human Sciences.
The recent work led by Davidson was part of a larger National Science Foundation grant that has involved scientists from VCU, the University of Richmond, Radford University, Western Carolina University and Eastern Carolina University. The collaborative award of nearly $1 million has included nearly $400,000 for VCU.
Contributors to the new study include Stunkle, Joshua T. Armstrong and James R. Vonesh of VCU; Elizabeth A. Hamman of St. Mary's College of Maryland; and Michael W. McCoy of Florida Atlantic University.

","score: 15.336257554625757, grade_level: '15'","score: 16.284597861459787, grade_levels: ['college_graduate'], ages: [24, 100]",10.1002/ecy.4213,"Warming has broad and often nonlinear impacts on organismal physiology and traits, allowing it to impact species interactions like predation through a variety of pathways that may be difficult to predict. Predictions are commonly based on short‐term experiments and models, and these studies often yield conflicting results depending on the environmental context, spatiotemporal scale, and the predator and prey species considered. Thus, the accuracy of predicted changes in interaction strength, and their importance to the broader ecosystems they take place in, remain unclear. Here, we attempted to link one such set of predictions generated using theory, modeling, and controlled experiments to patterns in the natural abundance of prey across a broad thermal gradient. To do so, we first predicted how warming would impact a stage‐structured predator–prey interaction in riverine rock pools between Pantala spp. dragonfly nymph predators and Aedes atropalpus mosquito larval prey. We then described temperature variation across a set of hundreds of riverine rock pools (n = 775) and leveraged this natural gradient to look for evidence for or against our model's predictions. Our model's predictions suggested that warming should weaken predator control of mosquito larval prey by accelerating their development and shrinking the window of time during which aquatic dragonfly nymphs could consume them. This was consistent with data collected in rock pool ecosystems, where the negative effects of dragonfly nymph predators on mosquito larval abundance were weaker in warmer pools. Our findings provide additional evidence to substantiate our model‐derived predictions while emphasizing the importance of assessing similar predictions using natural gradients of temperature whenever possible."
"
In a new analysis of data spanning more than three decades in the eastern United States, a team of scientists found a concerning trend -- an increasing number of wildfires across a large swath of America.

""It's a serious issue that people aren't paying enough attention to: We have a rising incidence of wildfires across several regions of the U.S., not only in the West,"" said Victoria Donovan, lead author of the study and an assistant professor of forest management at the UF/IFAS West Florida Research and Education Center. ""We're allocating the majority of resources to fire suppression in the western part of the country, but we have evidence that other areas are going to need resources, too.""
The team used data from the federal Monitoring Trends in Burn Severity Database from the years 1984 to 2020, the most recently available dataset at the time, to quantify the characteristics of large wildfires -- each burning over 200 hectares, or 490 acres. This included identifying which regions during that period had the largest fires, most land burned and seasonality factors.
Their findings indicated increasing wildfire risk across the southern and eastern portions of what's known as the Eastern Temperate Forests, an area that roughly bisects the country from Michigan in the north to the eastern half of Texas in the south.
""The eastern U.S. has the most expansive wildland-urban interface in the country and thus is at high risk from wildfire,"" Donovan said. ""The thought behind this research was that if there are signals that wildfires are increasing, we need to understand what those changes look like.""
Particularly in the Southeast, Donovan said, the data showed increasing trends in wildfire numbers, the size of the events, total hectares burned, shifts in seasonality, and overall increases in the annual probability of large wildfires.
""We didn't see the same trends in the northern part of the region we examined, but that doesn't mean there were no wildfires there,"" she said. ""This data only included large wildfires, so smaller fires may have been occurring in that time.""
Three major factors influence wildfire regimes, which is the term fire scientists use to refer to an overall pattern in which fires naturally occur in a given ecosystem.

""The first is the ignition pattern, or what starts the fires,"" Donovan said. ""The second is changes in vegetation or fuel patterns, and then you have climate characteristics. We don't address the drivers of the trends in this study, but it's hard to talk about changing conditions without considering that climate may be a factor.""
As far as ignition patterns are concerned, Donovan said human ignitions still make up the majority of wildfires; but this is unsurprising in these areas, especially, being home to the majority of the country's population. This category also accounts for human-created infrastructure, like power lines.
""It's not necessarily human ignitions driving the trend of increasing wildfire patterns,"" she said. ""In other words, we're not seeing an indication that there are proportionally more human-caused ignitions than there have been in the past. In the Southern Coastal Plain, which includes much of Florida, lightning ignitions played an important role, too, contributing more to the total area burned in the ecoregion despite being a less frequent cause of large wildfire ignition.""
Overall, Donovan says the study highlights the need for proactive management of forested land and individual preparedness for people living in the eastern United States.
""We don't have the expansive wildfire problem that the western U.S. does yet, so this is also an opportunity to get ahead of the problem and prepare for shifting wildfire patterns before we start seeing the frequent destructive fires that we're seeing in the West,"" Donovan said. ""We're hopeful that this study will spur more research into understanding changing wildfire patterns in the east, but also that it will help to support an increase in prescribed fire management and motivate people to better prepare for wildfire in their location by hardening their homes and making escape plans.""

","score: 14.821945212364913, grade_level: '15'","score: 16.7687094747424, grade_levels: ['college_graduate'], ages: [24, 100]",10.1029/2023GL107051,"Large wildfires are increasing across numerous regions of the globe. While the West has remained a primary focus of wildfire research and resources in the U.S., recent signals suggest that wildfire risk is increasing in the eastern U.S. as well. We conducted an in‐depth assessment of large (>200 ha) wildfire regime characteristics (size, number, total hectares burned, seasonality, probability of occurrence, and ignition source) over a 36‐year period across the Eastern Temperate Forests of the U.S. to quantify geographic patterns in large‐wildfire regime and identify changing spatio‐temporal large wildfire patterns. We found increases in large wildfire size, occurrence, number, and total hectares burned in the southern and eastern regions of the Eastern Temperate Forests. In contrast, large wildfires declined or were minimal in northern ecoregions. We demonstrate increasing large wildfires across some of the most populated regions of the United States."
"
In a first-of-its-kind study of aardvarks, Oregon State University researchers spent months in sub-Saharan Africa collecting poop from the animal and concluded that aridification of the landscape is isolating them, which they say could have implications for their long-term survival.

""Everyone had heard of aardvarks and they are considered very ecologically important but there has been little study of them,"" said Clint Epps, a wildlife biologist at Oregon State. ""We wanted to see if we could collect enough data to begin to understand them.""
In a just-published paper in Diversity and Distribution, the researchers used genetic information gleaned from 104 aardvark poop samples to begin to understand the range of where they live.
""During times of rapid environmental change, evaluating and describing changes in the landscape where a species lives is important for informed conservation and management decisions,"" said Rachel Crowhurst, a wildlife geneticist who works with Epps and co-authored the paper.
Aardvarks are nocturnal, burrowing mammals that can weigh up to 180 pounds. They have long snouts, similar to that of a pig, that they use, along with their claws, to locate and dig out ant and termite hills. They are found over much of the southern two-thirds of Africa.
Despite often being compared to pigs and South American anteater, aardvarks are not related to them. Aardvarks are the only living member of the order Tubulidentata, and their most closely living relatives include golden moles, elephants and manatees.
The International Union for the Conservation of Nature lists aardvarks in the category of ""least concern,"" in part due to the broad range of ecosystems they inhabit. However, little is known about current population trends or their actual distribution across the landscape. The Oregon State researchers believe the species is understudied because they are nocturnal, hard to trap and exist in low densities across large and often remote landscapes.

Those factors led Epps to undertake the first study of the genetics of aardvarks in the wild and to develop noninvasive methods to do so. People have examined aardvark DNA in the past for studies of mammalian evolution, but never across wild populations.
Epps learned to recognize aardvark tracks and poop (which they bury) when working as post-doctoral researcher nearly 20 years ago in Tanzania. In 2016, during a sabbatical, he returned to Africa for six weeks to see if he could spot aardvark digging signs, track them through the bush and find their buried droppings.
""I wanted to work on a system that was understudied, where anything I learned would likely be truly new information to the scientific community,"" Epps said. ""I also wanted to work over large landscapes, on foot, alone or with a friend or with guards when needed, in protected areas, with minimal logistical support and little cost.""
He learned how to find their feces on that 2016 trip and returned for shorter trips in 2017 with a graduate student Rob Spaan and in 2018 with Crowhurst.
They surveyed eight protected and four privately owned areas in South Africa, two protected areas in Eswatini (formerly Swaziland) and a location in Kenya. They collected 253 fecal samples and analyzed 104 that were of the highest quality for genetic information.
They then used the genetic information to infer aardvark distribution and movement across the landscape. For example, if genetic testing showed that fecal samples collected in different spots came from the same aardvark, they then used this information to determine the scale of individual movements.

In South Africa, the genetic information they gathered suggested three regional divisions of aardvarks, indicating that animals in western, central and eastern regions of South Africa were somewhat isolated from each other. Individuals were detected at multiple locations separated by up to 7 kilometers, indicating that home ranges could be larger than previously determined, particularly in arid areas where food resources may be low.
Closely related individuals were detected as far as 44 km apart, and individuals less than 55 km were more genetically similar. Thus, they found aardvarks may disperse up to 55 km from where they are born. Across the study area, genetic differentiation between individuals was greater when intervening landscapes were more arid, suggesting that movement through those areas is restricted.
The researchers plan to continue the work and hope to perform genomic analysis on new samples and conduct field work across a wider area of sub-Saharan Africa.
""Our initial findings suggest that climate change will increase habitat fragmentation and limit gene flow for aardvarks, particularly where precipitation is expected to decrease and temperature increase,"" Epps said. ""With aridity expected to increase in southern-most Africa under most climate change scenarios, the need for further research is clear.""
Epps other hope for further research: to see an aardvark in the wild. That goal has eluded him.
The paper was also co-authored by Spaan and Matt Weldy of Oregon State and Hannah Tavalire of the University of Oregon.

","score: 13.737332185886405, grade_level: '14'","score: 14.641082616178998, grade_levels: ['college_graduate'], ages: [24, 100]",10.1111/ddi.13792,"As climate change accelerates, assessing how climate shapes gene flow and neutral and adaptive genetic differentiation on landscapes is increasingly important. Aardvarks (Orycteropus afer) are ecologically important in sub‐Saharan Africa but are sensitive to human pressures and increasing aridity. We used individual, population and landscape genetic approaches to infer the influence of landscape, climate and potential adaptive differences on gene flow. We surveyed 8 protected and 4 privately owned areas in South Africa, 2 protected areas in Eswatini and one location in Kenya during 2016–2018. We developed microsatellite markers and methods for DNA extraction from faeces, collected and genotyped faecal samples from focal areas and estimated genetic structure. We inferred space use from individual redetections, tested for close relatives and estimated genetic neighbourhood distance. We applied individual‐based landscape genetic analyses at multiple scales to test hypotheses about genetic differentiation by landscape resistance and potential adaptive differences. We developed 19 variable microsatellite loci and collected 253 faecal samples from 13 focal areas. We genotyped 104 samples successfully at ≥8 loci as needed for individual identification. The genetic structure suggested 3 regional divisions in South Africa. We detected individuals at locations ≤7.3 km distant and closely related individuals at ≤44 km; genetic neighbourhood distance was <55 km. Lower precipitation increased landscape resistance and strongly predicted genetic differentiation at most spatial scales. Temperature differences at sampling sites also influenced structure, suggesting climate‐associated adaptive differences. The genetic structure of aardvarks is strongly shaped by climate, with arid areas limiting gene flow and reflects apparent isolation by adaptation associated with temperature. Dispersal distances likely are <55 km. The markers we developed will facilitate studies of space use, dispersal, population density or survival. Aridification will increase fragmentation and we recommend monitoring aardvark presence as an indicator of ecosystem change associated with aridification."
"
As climate change redistributes terrestrial ecosystems across the globe, the world's natural capital is expected to decrease, causing a 9% loss of ecosystem services by 2100. That's according to a study of natural capital published today in the journal Nature led by scientists at the University of California, Davis, and Scripps Institution of Oceanography at UC San Diego.

Breathable air, clean water, healthy forests and biodiversity all contribute to people's well-being in ways that can be very difficult to quantify. ""Natural capital"" is the concept scientists, economists and policymakers use to represent the current and future flow of benefits the world's natural resources bring to people.
""The big question is what do we lose when we lose an ecosystem?"" said lead author Bernardo Bastien-Olvera, a doctoral student at UC Davis when the study was conducted and currently a postdoctoral fellow at Scripps. ""Flipping the question: What do we gain if we are able to limit climate change and avoid some of its impacts on natural systems? This study helps us better consider damages not usually accounted for. It also reveals an overlooked, yet startling dimension of climate change effects on natural systems -- its capacity to exacerbate global economic inequality.""
Profound inequalities
When countries lose natural capital, their economies suffer. The study found that, by 2100, climate change-induced changes to vegetation, rainfall patterns and higher CO2 result in an average 1.3% reduction in gross domestic product, or GDP, across all the countries analyzed. It further found profound inequalities in the distribution of these impacts.
""Our research found that the world's poorest 50% of countries and regions are expected to bear a staggering 90% of the GDP damages,"" Bastien-Olvera said. ""In sharp contrast, the losses for the wealthiest 10% might be limited to just 2%.""
The authors said this is largely because lower-income countries tend to rely more on natural resources for their economic production, and a larger fraction of their wealth is in the form of natural capital.

Natural and economic values
For the study, the authors used global vegetation models, climate models and World Bank estimates of natural capital values to estimate the consequences of climate shifts on countries' ecosystem services, economic production and natural capital stocks.
These estimates may be conservative, as the analysis considered only land-based systems -- primarily forests and grasslands. Bastien-Olvera plans to address marine ecosystem impacts in future research. The study also didn't account for disturbances like wildfires or insect-driven tree mortality.
Accounting for nature
The overall findings underscore the importance of creating climate policies that account for the particular values each country derives from its natural systems.
""With this study, we're embedding natural systems and human well-being within an economic framework,"" said senior author Frances C. Moore, an associate professor in the UC Davis Department of Environmental Science and Policy. ""Our economy and well-being depend on these systems, and we should recognize and account for these overlooked damages when we consider the cost of a changing climate.""
The study was funded by the National Science Foundation.
""Thanks to the efforts of this research team, we now know that damage to ecosystems impacts human well-being in ways that are both measurable and wildly disproportionate across populations,"" said Jeffrey Mantz, NSF program officer. ""The results will be critical to abating economic losses in the coming decades.""

","score: 14.108009138840071, grade_level: '14'","score: 14.824428822495605, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41586-023-06769-z,"Ecosystems generate a wide range of benefits for humans, including some market goods as well as other benefits that are not directly reflected in market activity1. Climate change will alter the distribution of ecosystems around the world and change the flow of these benefits2,3. However, the specific implications of ecosystem changes for human welfare remain unclear, as they depend on the nature of these changes, the value of the affected benefits and the extent to which communities rely on natural systems for their well-being4. Here we estimate country-level changes in economic production and the value of non-market ecosystem benefits resulting from climate-change-induced shifts in terrestrial vegetation cover, as projected by dynamic global vegetation models (DGVMs) driven by general circulation climate models. Our results show that the annual population-weighted mean global flow of non-market ecosystem benefits valued in the wealth accounts of the World Bank will be reduced by 9.2% in 2100 under the Shared Socioeconomic Pathway SSP2-6.0 with respect to the baseline no climate change scenario and that the global population-weighted average change in gross domestic product (GDP) by 2100 is −1.3% of the baseline GDP. Because lower-income countries are more reliant on natural capital, these GDP effects are regressive. Approximately 90% of these damages are borne by the poorest 50% of countries and regions, whereas the wealthiest 10% experience only 2% of these losses."
"
When plants are infected by pathogens, suffer from a lack of water or have to react to other external stimuli, the first thing they do is increase the proton and calcium concentration in the affected cells. The protons and calcium ions then act like messenger substances that trigger further reactions in the cell.

The interactions between protons and calcium ions in this process were previously largely unknown. An article in the journal Science by a team led by biophysicist Professor Rainer Hedrich from Julius-Maximilians-Universität (JMU) Würzburg in Bavaria, Germany, has now shed new light on this subject.
Using a sophisticated optogenetic approach, the researchers have discovered a previously unknown endogenous acid sensor in plant cells. And they have discovered in the guard cells of leaves that there is a calcium store that plays an important role in processing proton signals in cellular responses.
Why Such Simple Elements as Protons and Calcium Ions Act as Signals
In the course of evolution, cells have designed their metabolism to utilise energy-rich phosphates. This results in a problem: at the predominantly neutral cellular pH value, the valuable phosphates can be bound by calcium ions (Ca2+) and converted into an insoluble and therefore no longer usable form (calcium dihydrogen phosphate).
To avoid this, cells keep their internal calcium level very low; in their environment, however, it is 10,000 times higher. Outside the cells, the concentration of protons (H+) and therefore the acidity is also much higher. Due to this concentration gradient, both types of ions have a strong urge to flow into the cells -- making them ideal for use as messenger substances.
""The stimulus-dependent opening of calcium and proton channels in the cell membrane results in a temporary intracellular increase in both messenger ions,"" explains Rainer Hedrich. ""The cells understand this as a signal, which they translate into a biological reaction using calcium- and proton-binding enzymes.""
Light Switch Controls the Flow of Protons Into the Cell

How do plant cells react to the influx of protons and the associated acidification of their cell plasma? Until now, this could only be investigated with great experimental effort and even then only indirectly.
This is now much easier thanks to an appropriately equipped thale cress (Arabidopsis thaliana), which Hedrich's team has developed using optogenetic methods: A light-sensitive proton channel from a fungus, the channelrhodopsin KCR2, was optimised for use in plant cells. This means that protons can now be specifically sent into the cells in response to a light pulse.
Furthermore, they expressed KCR2 together with the genetically encoded pH reporter pHuji. This makes it very easy to measure the current pH value in the cell upon KCR2 activation.
Shouguang Huang, the first author of the Science publication, next scrutinised the guard cells of the new Arabidopsis mutant. ""When I stimulated them with blue light for a second, they depolarised, just as I had expected from a light-activated proton channel,"" says the researcher. During the subsequent experiments, the Würzburg ion channel specialists made a far-reaching discovery.
KCR2 Activation Acidifies the Cell and Causes Calcium to Rise
Their electrophysiological studies on guard cells showed that when the light stimulation began, the membrane potential immediately depolarised and the pH reporter pHuji signalled an acidification of the cell interior.

""However, we were astonished when the depolarisation and acidification continued for a good minute after the end of the light pulse,"" says Hedrich. ""This could only mean that the light activation of KCR2 and the acidification had activated the sphincter cell's own ion channels."" These are the long-known guard cell anion channels SLAC1 and SLAH3, whose activation, however, also requires the presence of calcium.
Endoplasmic Reticulum as a Calcium Store
""Taking all the facts together, it could be assumed that the proton currents carried by KCR2 and the associated acidification of the cell interior must also have generated a calcium signal,"" summarises the JMU professor.
His team was able to prove that the rapid acidification of the guard cells is followed by a calcium signal that lasts for 150 to 200 seconds. And they discovered that this calcium does not come from outside the cell, but is released from an endogenous store, the endoplasmic reticulum. This is a network of membrane tubes and cisterns that run through the cytoplasm.
Future studies will now focus on analysing the molecular nature of the H+-sensitive calcium channel of the endoplasmic reticulum and investigating its proton-activated on/off switch. Overall, these studies are important in order to better understand how plant cells react to external stimuli such as infections or drought.

","score: 14.08044871794872, grade_level: '14'","score: 14.977769230769233, grade_levels: ['college_graduate'], ages: [24, 100]",10.1126/science.adj9696,"Although there has been long-standing recognition that stimuli-induced cytosolic pH alterations coincide with changes in calcium ion (Ca 2+ ) levels, the interdependence between protons (H + ) and Ca 2+ remains poorly understood. We addressed this topic using the light-gated channelrhodopsin Hc KCR2 from the pseudofungus Hyphochytrium catenoides , which operates as a H + conductive, Ca 2+ impermeable ion channel on the plasma membrane of plant cells. Light activation of Hc KCR2 in Arabidopsis guard cells evokes a transient cytoplasmic acidification that sparks Ca 2+ release from the endoplasmic reticulum. A H + -induced cytosolic Ca 2+ signal results in membrane depolarization through the activation of Ca 2+ -dependent SLAC1/SLAH3 anion channels, which enabled us to remotely control stomatal movement. Our study suggests a H + -induced Ca 2+ release mechanism in plant cells and establishes Hc KCR2 as a tool to dissect the molecular basis of plant intracellular pH and Ca 2+ signaling."
"
The word ""drought"" typically conjures images of parched soil, dust-swept prairies, depleted reservoirs, and dry creek beds, all the result of weeks or seasons of persistently dry atmospheric conditions.

In the sun-soaked islands in the Caribbean, however, drought conditions can occur much more rapidly, with warning signs appearing too late for mediation strategies to limit agriculture losses or prevent stresses on infrastructure systems that provide clean water to communities.
Such occurrences -- known as flash droughts -- are the focus of a new paper authored by Assistant Professor Craig Ramseyer of the College of Natural Resources and Environment and published in the Journal of Hydrometeorology. The paper's finding is that Caribbean Islands are uniquely susceptible to sudden droughts, and Ramseyer advocates for alternative methodologies to more accurately measure dry conditions in the region.
""The tropics have extremely intense solar radiation, so atmospheric processes tend to be expedited,"" said Ramseyer, who teaches in the Department of Geography. ""Despite often receiving daily rainfall, island ecosystems are particularly vulnerable to drought conditions.""
Ramseyer, whose research focuses on tropical rainfall and severe weather impacts in the Caribbean, utilized a new drought index that considers the atmospheric demand for moisture to identify drought risk conditions instead of more traditional soil moisture measurements.
""This new drought index is really developed to try to identify the first trigger of drought by focusing on evaporative demand,"" said Ramseyer, who collaborated on the paper with Paul Miller '12, M.S. '14, an assistant professor at Louisiana State University. ""Evaporative demand is a measure of how thirsty the atmosphere is and how much moisture it can collect from soil or plant matter.""
Ramseyer, who received funding for this research through a grant from the National Oceanic and Atmospheric Administration's Climate Program Office, stressed that identifying drying conditions earlier is a key step to limiting the impacts of droughts.

""A lot of drought observation is based on soil moisture, but in tropical environments, a decline in soil moisture is a response to other things that have already happened so you're further down in the chain of events,"" he said. ""We can mitigate a lot of losses in, say, agriculture, by being able to forecast sudden, anomalous increases in evaporative demand.""
The impacts of drought conditions extend beyond agriculture: Tropical ecosystems are also strongly impacted by dry atmospheric weather conditions, and access to fresh water is a necessity for both communities in the region and a tourism industry that is a central driver for economies in the Caribbean.
A new position for atmospheric research
To better understand how that interplay of meteorological patterns impacts drought conditions, Ramseyer utilized 40 years of data from a long-term ecological research project in the El Yunque National Forest. He found that flash droughts have routinely occurred in the Caribbean and that occurrences of drought are not limited to traditional dry seasons on the island.
""In terms of climate, Puerto Rico is situated at a crossroads, buffered on the west by the El Niño southern oscillation and by the cooler North Atlantic oscillation on the east,"" said Ramseyer. ""Because of that, Puerto Rico has a unique geography for researching atmospheric changes.""
The looming concerns over global warming have only accelerated the need for meteorologists to better understand drought occurrences in the Caribbean and enhance monitoring of moisture conditions in the region.

""A warming planet results in more moisture available in the atmosphere overall, which means that the kinds of short-term precipitation events common to the Caribbean will increase in intensity,"" said Ramseyer. ""Meanwhile, droughts are becoming higher in magnitude, so climate change is altering both extremes.""
Ramseyer, who helped secure Virginia Tech's membership in the University Corporation for Atmospheric Research this year, said developing clearer criteria for flash drought conditions is an important first step toward addressing the infrastructure challenges that Caribbean communities are likely to face.
""The key current and future issue for the Caribbean is all about finding a way to capture rainfall successfully and draw it out slowly to mitigate evaporation losses,"" said Ramseyer. ""Puerto Rico and all of the Caribbean have water infrastructure challenges that must be addressed to accommodate these trends.""
Geography department chair Tom Crawford said Ramseyer's paper reflects a utilization of big data in tackling climate and meteorological challenges.
""Dr. Ramseyer's research applies advanced computing and geospatial science to make significant contributions to the problem of flash droughts and precipitation variability broadly,"" said Crawford. ""In addition to his research impact, his course on Climate Data Analysis and Programming is training the next generation of researchers on cutting edge computational techniques applied to the changing climate.""
Ramseyer advocates for additional research into understanding the relationship between flash drought events and economic losses and how future drought events can be better communicated to stakeholders and communities.

","score: 17.213402850191176, grade_level: '17'","score: 18.964483837330555, grade_levels: ['college_graduate'], ages: [24, 100]",10.1175/JHM-D-22-0226.1,"Despite the intensifying interest in flash drought both within the United States and globally, moist tropical landscapes have largely escaped the attention of the flash drought community. Because these ecozones are acclimatized to receiving regular, near-daily precipitation, they are especially vulnerable to rapid-drying events. This is particularly true within the Caribbean Sea basin where numerous small islands lack the surface and groundwater resources to cope with swiftly developing drought conditions. This study fills the tropical flash drought gap by examining the pervasiveness of flash drought across the pan-Caribbean region using a recently proposed criterion based on the evaporative demand drought index (EDDI). The EDDI identifies 46 instances of widespread flash drought “outbreaks” in which significant fractions of the pan-Caribbean encounter rapid drying over 15 days and then maintain this condition for another 15 days. Moreover, a self-organizing maps (SOM) classification reveals a tendency for flash drought to assume recurring typologies concentrated in one of the Central American, South American, or Greater Antilles coastlines, although a simultaneous, Caribbean-wide drought is never observed within the 40-yr (1981–2020) period examined. Furthermore, three of the six flash drought typologies identified by the SOM initiate most often during Phase 2 of the Madden–Julian oscillation. Collectively, these findings motivate the need to more critically examine the transferability of flash drought definitions into the global tropics, particularly for small water-vulnerable islands where even island-wide flash droughts may only occupy a few pixels in most reanalysis datasets. The purpose of this study is to understand if flash drought occurs in tropical environments, specifically the Caribbean. Flash droughts represent a quickly evolving drought, which have particularly acute impacts on agriculture and often catch stakeholders by surprise as conditions evolve rapidly from wet to dry conditions. Our results indicate that flash droughts occur with regular periodicity in the Caribbean. Expansive flash droughts tend to occur in coherent subregional clusters. Future studies will further investigate the drivers of these flash droughts to create early warning systems for flash drought."
"
It has long been known that exposure to pesticide sprays is harmful to honey bees. In a new study, researchers have uncovered the effect of such sprays on the sense of smell in bees, which could disrupt their social signals.

Honey bees live in dynamic communities and constantly communicate with each other using chemicals that serve as social cues. For example, nurse bees -- that are responsible for taking care of larvae that ultimately become queens and worker bees -- constantly monitor the larvae using in the dark using pheromones. The larvae emit brood pheromones to indicate that they need food. There are also alarm pheromones that workers produce to warn the other bees of danger. If these cues are dampened or not perceived properly, the colony may fail to thrive.
Since 2007, scientists have known that honey bees have been in trouble. One of the stressors that have raised concerns are insecticides, which affect honey bee health. Because these are usually used in combination with other chemicals, the resulting mixture can become unexpectedly toxic to bees.
""For many years, it was assumed that fungicides do not have an adverse impact on insects because they are designed for fungal targets,"" said May Berenbaum (GEGC/IGOH), a professor of entomology. ""Surprisingly, in addition to insecticides, fungicides also have an adverse effect on bees and combining the two can disrupt colony function.""
For more than a decade, reports originating from almond orchards, where two-thirds of the U.S. honey bees are transported every year when the flowers are in bloom, implicated pesticide spray mixtures. In particular, the problem lies in the use of supposedly inactive chemicals called adjuvants, which increases the ""stickiness"" of the insecticide so it stays on the plants.
Because adjuvants have long been considered to be biologically benign, they are not subject to the same level of safety testing as other insecticidal agents. ""Recently, researchers have shown that adjuvants alone or when used in combination with fungicides and insecticides are toxic to bees,"" Berenbaum said.
Nurse bees are especially vulnerable to these combinations. ""The health of the queens is paramount,"" Berenbaum said. ""If healthy queens are not produced, the colony can suffer.""
To understand how combinations affect nurse bees, the researchers tested their effect on the olfactory system of honey bees using the adjuvant Dyne-Amic, the fungicide Tilt, and the insecticide Altacor.

The researchers divided bees into four groups of ten bees and for a week exposed them to either untreated commercial pollen or to pollen that had been treated with either Dyne-Amic, or Tilt and Altacor, or all three together. The bees were then anesthetized on ice and one antenna was carefully removed from each bee. The researchers then exposed the antenna to chemical mimics of brood and alarm pheromones and recorded the antenna's response using a technique called electroantennography.
With this method, Ling-Hsiu Liao, a research scientist, and Wen-Yen Wu, a graduate student, in the Berenbaum lab, found that when nurse bees had consumed pollen contaminated by the three chemicals, their antennal responses to some brood pheromones and alarm pheromones were altered. Their finding suggests that these commonly-used pesticides can interfere with honey bee communication.
How these chemicals interact and influence the bees is still unclear. ""There are many possible explanations for how consuming these chemicals can affect the sensory responses of bees,"" Liao said. ""The antenna detects and triggers the response to olfactory signals. In this study we did not look at what other changes are triggered, particularly changes in behavior.""
In addition to parsing out the underlying molecular pathways that are affected, the researchers are also interested in testing other mixtures of commonly used pesticides as well as looking at the response of bees in other populations. They hope that their work can help beekeepers rethink how they manage and protect their colonies.

","score: 12.545509598258462, grade_level: '13'","score: 13.221267069067885, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41598-023-41818-7,"Exposure to agrochemical sprays containing pesticides and tank-mix adjuvants has been implicated in post-bloom mortality, particularly of brood, in honey bee colonies brought into California almond orchards for pollination. Although adjuvants are generally considered to be biologically inert, some adjuvants have exhibited toxicity and sublethal effects, including decreasing survival rates of next-generation queens. Honey bees have a highly developed olfactory system to detect and discriminate among social signals. To investigate the impact of pesticide-adjuvant combinations on honey bee signal perception, we performed electroantennography assays to assess alterations in their olfactory responsiveness to the brood ester pheromone (BEP), the volatile larval pheromone β-ocimene, and the alarm pheromone 2-heptanone. These assays aimed to uncover potential mechanisms underlying changes in social behaviors and reduced brood survival after pesticide exposure. We found that combining the adjuvant Dyne-Amic with the fungicide Tilt (propiconazole) and the insecticide Altacor (chlorantraniliprole) synergistically enhanced olfactory responses to three concentrations of BEP and as well exerted dampening and compensatory effects on responses to 2-heptanone and β-ocimene, respectively. In contrast, exposure to adjuvant alone or the combination of fungicide and insecticide had no effect on olfactory responses to BEP at most concentrations but altered responses to β-ocimene and 2-heptanone. Exposure to Dyne-Amic, Altacor, and Tilt increased BEP signal amplitude, indicating potential changes in olfactory receptor sensitivity or sensilla permeability to odorants. Given that, in a previous study, next-generation queens raised by nurses exposed to the same treated pollen experienced reduced survival, these new findings highlight the potential disruption of social signaling in honey bees and its implications for colony reproductive success."
"
In their rapid characterization of the magnitude 6.8 Al Haouz earthquake in Morocco, researchers from the U.S. Geological Survey's National Earthquake Information Center (NEIC) suggest that the earthquake ruptured roughly 25 kilometers deep beneath the surface.

The USGS source modeling, published in The Seismic Record, shows a compact source for the earthquake with slip occurring between 15 and 35 kilometers deep -- deeper than might be commonly expected for earthquakes in this region.
However, the 8 September earthquake took place in a region with very few historically recorded earthquakes. ""In the region there's not a high rate of seismic activity, so we don't have a great idea of what the 'common' characteristics are of large earthquakes in the Atlas Mountains,"" said U.S. Geological Survey seismologist William Yeck.
Seismologists use information about the depth of earthquake slip to help understand the seismic hazard in a particular region. The Al Haouz earthquake rupture did not break the surface and few aftershocks were recorded, both of which make it difficult to confirm which faults were involved in the earthquake, Yeck noted.
The earthquake is named after the Moroccan province most impacted by the shaking, with violent shaking near its epicenter and very strong shaking in the city of Marrakesh. Nearly 3,000 deaths and 5,500 injuries, along with widespread damage to structures, were reported in the weeks after the earthquake.
The USGS researchers relied on teleseismic data -- seismic wave data collected from stations around the globe -- for their analysis. The nearest seismic station is 100 kilometers away from the earthquake epicenter, and there are only three other stations within 500 kilometers with openly available data available in real time. The researchers combined the teleseismic data with InSAR satellite data, which captures changes in ground deformation, to create several models of the earthquake source.
""We use a lot of tools to characterize these earthquakes and each one gives us specific constraints,"" Yeck explained. ""Waveform modeling gives us the best estimate of the seismic centroid, the depth rupture, whereas InSAR can give us the precise location on the surface. It's really combining these data sets that gives us the most complete picture of the earthquake.""
The modeling shows that the earthquake occurred in the lower crust about 25 kilometers beneath the western Moroccan High Atlas Mountains and was a blind rupture, meaning it did not reach the surface.

""We have some sense of the[area's] large surface faults and some sense of how they dip and how they extend at depth, but their shape can change at depth, which makes it difficult to pin this to a fault on the surface,"" said Yeck.
Only five aftershocks were captured in the NEIC database. ""To understand the faulting in the region you want to have a good picture of the aftershocks, because then you can actually image which faults slipped,"" Yeck noted. ""But there can still be some ambiguity where slip occurred without those aftershocks.""
The researchers say that their study illustrates the benefits of regional and national networks following international data exchange standards to share real-time seismic data with the global seismic monitoring community, especially for earthquakes in remote regions with sparse seismic coverage.

","score: 13.931985239852398, grade_level: '14'","score: 16.234852398523984, grade_levels: ['college_graduate'], ages: [24, 100]",10.1785/0320230040,"The U.S. Geological Survey (USGS) National Earthquake Information Center (NEIC) estimates source characteristics of significant damaging earthquakes, aiming to place events within their seismotectonic framework. Contextualizing the 8 September 2023, Mw 6.8 Al Haouz, Morocco, earthquake is challenging, because it occurred in an enigmatic region of active surface faulting, and low seismicity yet produced significant damage and loss of life. Here, we present the rapid earthquake source products produced by the USGS NEIC, describing how the source model was derived using both seismic and geodetic observations. Our analysis indicates that the earthquake was the result of oblique-reverse faulting in the lower crust on either a steeply north-dipping fault or a moderately south-dipping fault. Finite-slip models using seismic and geodetic data reveal a compact source, with slip occurring at depths of 15–35 km. The causative fault is not apparent, because the rupture did not break the surface, and it is not possible to definitively attribute the earthquake to a known structure. The earthquake centroid depth of 25 km is noteworthy, because it shows slip extending beyond common estimates of seismogenic depth. This earthquake highlights that the seismogenic processes associated with mountain building in this wide plate boundary region are poorly understood."
"
An international team of scientists led by geneticists and disease biologists from the University of Oxford and LMU Munich have used ancient DNA to trace the evolution of Marek's Disease Virus (MDV). This global pathogen causes fatal infections in unvaccinated chickens and costs the poultry industry over $1 billion per year. The findings, published today in the journal Science, show how viruses evolve to become more virulent and could lead to the development of better ways to treat viral infections.

The team, which includes archaeologists and biologists, recovered and reconstructed ancient MDV sequences from archaeological chickens spanning the past 1,000 years. By comparing viral genomes derived from both modern and ancient birds, they were able to pinpoint the genetic alterations responsible for the increased virulence of the modern virus.
Based on the ancient genetic sequences, they were also able to resurrect ancient biological processes using cellular assays, demonstrating that ancient strains were significantly milder than their modern counterparts.
This breakthrough not only sheds light on the evolutionary history of MDV, but also holds promise for the development of more effective therapies against this devastating poultry disease.
This new study is based on DNA isolated from chicken bones that were excavated from 140 archaeological sites in Europe and the Near East. These ancient genomes revealed that MDV was widespread in European chickens at least 1,000 years before the disease was first described in 1907. This highlights the importance of preserving archaeological remains, especially given their power to reveal valuable insights into the evolution of virulence.
When first described, this disease only led to mild symptoms in older chickens. As chicken consumption dramatically increased in the 1950s and 1960s, MDV has continued to evolve and has become increasingly aggressive despite the development of several vaccines.
First author Dr Steven Fiddaman (Department of Biology, University of Oxford) said: ""Our findings not only unravel the evolutionary history of the Marek's Disease Virus but also provide a foundation for enhancing our current understanding of pathogen virulence. By combining ancient DNA techniques with modern genomics, we've opened a window into the past that can guide future strategies in managing viral diseases.""
Professor Naomi Sykes (University of Exeter), lead archaeologist on the study, stated: ""This study underscores the profound significance of biological material preserved in archaeological and museum collections since we cannot foresee how their investigation might possess transformative applications in the future.""

Professor Laurent Frantz (LMU Munich), co-senior author of the study stated: ""Our work highlights the power of interdisciplinary collaboration, bringing together paleogeneticists, virologists, archaeologists, and biologists to unravel the complex evolutionary history of a pathogen with significant economic and agricultural implications.""
Professor Greger Larson (University of Oxford), co-senior author commented: ""We have seen how mitigating diseases often creates a selection pressure that increases the virulence of the virus. Being able to watch this process take place by sequencing ancient virus genomes shows just how dramatically the virulence of MDV has increased in the past century.""
Professor Adrian Smith (Department of Biology, University of Oxford), co-senior author said: ""Ancient DNA has provided us with a unique perspective on the emergence of MDV as a deadly chicken virus and may teach us lessons that are applicable to the control of other viral infections of medical and veterinary importance.""
Professor Venugopal Nair, Scientist Emeritus at The Pirbright Institute, said: ""Findings from this paper on the origins of virulence, particularly associated with the genetic sequences of the ancient Marek's disease viruses, will provide great scientific opportunities to explore the molecular mechanisms of increasing virulence of this virus that coincided with the intensification of poultry farming from the 1960s.""

","score: 18.6161045751634, grade_level: '19'","score: 20.590784313725493, grade_levels: ['college_graduate'], ages: [24, 100]",10.1126/science.adg2238,"The pronounced growth in livestock populations since the 1950s has altered the epidemiological and evolutionary trajectory of their associated pathogens. For example, Marek’s disease virus (MDV), which causes lymphoid tumors in chickens, has experienced a marked increase in virulence over the past century. Today, MDV infections kill >90% of unvaccinated birds, and controlling it costs more than US$1 billion annually. By sequencing MDV genomes derived from archeological chickens, we demonstrate that it has been circulating for at least 1000 years. We functionally tested the Meq oncogene, one of 49 viral genes positively selected in modern strains, demonstrating that ancient MDV was likely incapable of driving tumor formation. Our results demonstrate the power of ancient DNA approaches to trace the molecular basis of virulence in economically relevant pathogens."
"
Drones flying along miles of rivers in the steep, mountainous terrain of central Taiwan and mapping the rock properties have revealed new clues about how water helps shape mountains over geological time, according to a team led by Penn State scientists.

The researchers found a link between the size of boulders in the rivers and the steepness of the rivers. The link shows how rock properties can influence the relationship between tectonic processes happening deep underground and how mountainous landscapes change shape. They reported in the journal Science Advances.
""Over the course of a mountain belt developing, we're seeing differences in how rivers incise, or cut down into the bedrock, in the younger and older sections,"" said Julia Carr, lead author of the study who earned her doctorate in geosciences from Penn State in 2022. ""It means that as a mountain belt evolves, erosion is changing at the surface.""
As tectonic plates collide and form mountain ranges, rocks that were previously buried in the Earth's crust are pushed to the surface in a process called uplift. The temperature and pressure that these rocks experience leads to variability in rock properties -- like rock hardness or the spacing and orientation of fractures -- that then affect how easily they are eroded by elements at the surface, the scientists said.
In Taiwan, the scientists found the main signature of rock strength of the mountains was the size of boulders in rivers, which were larger and stronger in locations where rocks had been buried deeper in Earth's crust. And the size of boulders correlated with the steepness of the rivers, which must be powerful enough to move these boulders downstream before eroding the mountain, the scientists said.
""When the boulders in the channels are larger, the river needs to steepen to be able to erode at the same rate,"" said Roman DiBiase, associate professor of geosciences at Penn State and co-author of the study. ""This is because in order to erode rock, the sediment covering a river channel needs to move out of the way. The larger the boulders in the channel, the steeper the channel needs to be to move them.""
Models can account for how things like storms and floods impact erosion rates, but it's harder to factor the role of rock strength on the process, the scientists said.

""Determining the controls on river incision into rock is important for understanding how mountain ranges evolve over geologic time,"" DiBiase said. ""But some key parameters for testing models of river incision, such as flow depth and sediment cover, are difficult to measure at large scales.""
The researchers turned to drones to avoid obstacles like hazardous river crossings and waterfalls to collect data. During these surveys, the scientists collected hundreds of thousands of measurements of river channel morphology and more than 22,000 measurements of boulders along roughly 18 miles of rivers.
""That's where it's really unprecedented -- something of this scale is really unusual,"" said Carr, who conducted the research at Penn State and is now a postdoctoral fellow at Simon Fraser University in British Columbia. ""It's exciting to be able to survey at this scale -- it helps us see patterns we really would otherwise never see. If you just went into the field and surveyed the few spots you could get to easily, you would not observe this pattern.""
Taiwan's central mountain range is one of the steepest landscapes on Earth and has one of the highest erosion rates of any place outside glaciated or human-influenced areas, Carr said. In addition, the tectonic setting of Taiwan is well known and has systematic burial depth patterns that can be used to evaluate the connection between subsurface history of rocks and their current condition at the surface.
""It's this great unique place because unlike somewhere like the Himalayas or the Alps, where there's so many complex tectonic histories, Taiwan can be a relatively simple landscape to study because the same collision forces that created it millions of years ago are still active today,"" Carr said. ""And these lessons learned from Taiwan can help inform erosion models that are applied to other mountain ranges with fewer constraints.""
Because of how the range formed, younger rocks are found in the south and west, while older rocks that were buried deeper -- up to 24 miles underground -- are found further east and north, the scientists said.

In the younger sections, rivers have fewer, smaller boulders that cover less of the area of the channels. And as you travel toward the older sections, the boulders increase to a median size of more than six feet, the scientists said.
These boulders aren't sitting in the rivers waiting to be broken down over time, according to the researchers. Instead, boulders in each of the sections of rivers were close to the threshold of mobility -- meaning the water was nearly powerful enough to move them downstream. During high flows after storms, these boulders may be fully mobile, and as they move, they help incise the river.
""One way you can think about how rivers incise long term -- you need to be able to move sediment, and once you cross over some threshold, you can incise the river,"" Carr said. ""If we apply this, it implies this primary rock strength signal controlling boulder size is setting river incision in the landscape. And that matches with the local steepness of the rivers.""
Also contributing were Donald Fisher, professor of geosciences at Penn State; En-Chao Yeh, associate professor at National Taiwan Normal University; and Eric Kirby, professor at University of North Carolina at Chapel Hill.
The National Science Foundation supported this work.
Link to drone video footage: https://youtu.be/uER7H-zm1yE 

","score: 12.500167096557139, grade_level: '13'","score: 14.555834080968935, grade_levels: ['college_graduate'], ages: [24, 100]",10.1126/sciadv.adg6794,"Feedbacks between surface and deep Earth processes in collisional mountain belts depend on how erosion and topographic relief vary in space and time. One outstanding unknown lies in how rock strength influences bedrock river morphology and thus mountain relief. Here, we quantify boulder cover and channel morphology using uncrewed aerial vehicle surveys along 30 kilometers of bedrock-bound river corridors throughout the Taiwan Central Range where regional gradients in rock properties relate to tectonic history. We find that boulder size systematically increases with increasing metamorphic grade and depth of exhumation. Boulder size correlates with reach-scale channel steepness but does not explain observations of highly variable channel width. Transport thresholds indicate that rivers are adjusted to mobilize boulders and are well in excess of the threshold to transport gravel and cobbles, as previously assumed. The linkage between metamorphic history, boulder size, and channel steepness reveals how rock properties can influence feedbacks between tectonics and topography throughout the life span of a mountain range."
"
In the frigid seas halfway between mainland Norway and the North Pole, two types of animals browse the palatable vegetation of a high-tundra archipelago, munching on thick moss, cropped grasses and low-lying shrubs. New research from a group led by Matteo Petit Bon from the Quinney College of Natural Resources is working to untangle the ecosystem impacts that two major players -- geese and reindeer -- have on a changing and vulnerable Arctic system.

Reindeer have been year-round residents on the islands of Svalbard for thousands of years, but at one point were almost completely gone. Svalbard reindeer, unlike their southern cousins, tend to be docile and extremely sedentary, making them easy targets for hunting. Miners, trappers and overwintering sailing expeditions relied on reindeer for food, and by 1900 the animals were more or less locally extirpated, although a few isolated areas had small populations persist, according to Mathilde Le Moullec, a coauthor on the study from the Centre for Biodiversity Dynamics at the Norwegian University of Science and Technology.
Those few reindeer were important, however, because they provided a population to slowly recolonize after the Norwegian government extended full protection to the animals. Now reindeer populations on Svalbard have expanded to over 20,000 animals.
Barnacle geese make a temporary but essential home on Svalbard at the end of their long migratory journey, said Karen Beard, coauthor on the research from the Department of Wildland Resources. Their time on the islands is fundamental to their life cycle, as they take advantage of the seasonal 24-hour light and nutrient-rich vegetation to raise young during a hospitable season in an otherwise very tough environment.
These two heavy-hitting herbivores influence the composition of vegetation on Svalbard in both direct and indirect ways -- from removal of certain plant species to natural fertilization and the compaction of soil through trampling. The way these animals interact with their landscape has complex repercussions for how the ecosystem will respond to climate change in the future, she said.
The Arctic climate is shifting faster than other places under global climate change, and Svalbard is one of the most rapidly warming regions on Earth, said Petit Bon. But life over the last half-century has been relatively good for populations of Svalbard geese. Conservation policies in their overwintering home in Scotland and changes in climate have led to impressive population expansion -- from under 3,000 birds in 1960 to more than 40,000 birds today. Since this growing population has the potential to change the face of the landscape in Svalbard, it's important to understand the ways the geese can impact the land, he said.
Geese are voracious eaters and maintain very low levels of plant biomass where they graze in concentrated patches. The impact that geese exert on the Svalbard tundra appears to be increasing over time -- in 2018 peak summer grass biomass in goose-grazed tundra was five times lower than researchers found in 2008, according to the research.
The geese, who live and eat in concentrated groups, had a bigger impact on the ecosystem than did reindeer, who are much larger animals but more widely dispersed. This likely reflects inherent differences in both habitat sensitivity and the way that the habitats are used by the different herbivores, as well as the way they move and eat, said Petit Bon.
Experimental long-term removal of reindeer from a portion of the island had little evident effect on the ecosystem's health, said the researchers, but the experimental exclusion of geese made a big difference for vegetation and soils. Although geese are obviously considerably smaller in size and are temporary residents, the way they use the landscape creates a heavy-weight impact -- bigger even than populations of reindeer.
This research provides important information to aid projections about the effects of shifting herbivore populations on ecosystem functioning, said Beard, and helps to refine predictions on whether and where these shifts are likely to mitigate or further amplify the impact of climate change on Arctic ecosystems.

","score: 15.878494059132361, grade_level: '16'","score: 18.420259740259738, grade_levels: ['college_graduate'], ages: [24, 100]",10.1111/1365-2745.14200,"Given the current rates of climate change, with associated shifts in herbivore population densities, understanding the role of different herbivores in ecosystem functioning is critical for predicting ecosystem responses. Here, we examined how migratory geese and resident, non‐migratory reindeer—two dominating yet functionally contrasting herbivores—control vegetation and ecosystem processes in rapidly warming Arctic tundra. We collected vegetation and ecosystem carbon (C) flux data at peak plant growing season in the two longest running, fully replicated herbivore removal experiments found in high‐Arctic Svalbard. Experiments had been set up independently in wet habitat utilised by barnacle geese Branta leucopsis in summer and in moist‐to‐dry habitat utilised by wild reindeer Rangifer tarandus platyrhynchus year‐round. Excluding geese induced vegetation state transitions from heavily grazed, moss‐dominated (only 4 g m−2 of live above‐ground vascular plant biomass) to ungrazed, graminoid‐dominated (60 g m−2 after 4‐year exclusion) and horsetail‐dominated (150 g m−2 after 15‐year exclusion) tundra. This caused large increases in vegetation C and nitrogen (N) pools, dead biomass and moss‐layer depth. Alterations in plant N concentration and CN ratio suggest overall slower plant community nutrient dynamics in the short‐term (4‐year) absence of geese. Long‐term (15‐year) goose removal quadrupled net ecosystem C sequestration (NEE) by increasing ecosystem photosynthesis more than ecosystem respiration (ER). Excluding reindeer for 21 years also produced detectable increases in live above‐ground vascular plant biomass (from 50 to 80 g m−2; without promoting vegetation state shifts), as well as in vegetation C and N pools, dead biomass, moss‐layer depth and ER. Yet, reindeer removal did not alter the chemistry of plants and soil or NEE. Synthesis. Although both herbivores were key drivers of ecosystem structure and function, the control exerted by geese in their main habitat (wet tundra) was much more pronounced than that exerted by reindeer in their main habitat (moist‐to‐dry tundra). Importantly, these herbivore effects are scale dependent, because geese are more spatially concentrated and thereby affect a smaller portion of the tundra landscape compared to reindeer. Our results highlight the substantial heterogeneity in how herbivores shape tundra vegetation and ecosystem processes, with implications for ongoing environmental change. Given the current rates of climate change, with associated shifts in herbivore population densities, understanding the role of different herbivores in ecosystem functioning is critical for predicting ecosystem responses. Here, we examined how migratory geese and resident, non‐migratory reindeer—two dominating yet functionally contrasting herbivores—control vegetation and ecosystem processes in rapidly warming Arctic tundra. We collected vegetation and ecosystem carbon (C) flux data at peak plant growing season in the two longest running, fully replicated herbivore removal experiments found in high‐Arctic Svalbard. Experiments had been set up independently in wet habitat utilised by barnacle geese Branta leucopsis in summer and in moist‐to‐dry habitat utilised by wild reindeer Rangifer tarandus platyrhynchus year‐round. Excluding geese induced vegetation state transitions from heavily grazed, moss‐dominated (only 4 g m−2 of live above‐ground vascular plant biomass) to ungrazed, graminoid‐dominated (60 g m−2 after 4‐year exclusion) and horsetail‐dominated (150 g m−2 after 15‐year exclusion) tundra. This caused large increases in vegetation C and nitrogen (N) pools, dead biomass and moss‐layer depth. Alterations in plant N concentration and CN ratio suggest overall slower plant community nutrient dynamics in the short‐term (4‐year) absence of geese. Long‐term (15‐year) goose removal quadrupled net ecosystem C sequestration (NEE) by increasing ecosystem photosynthesis more than ecosystem respiration (ER). Excluding reindeer for 21 years also produced detectable increases in live above‐ground vascular plant biomass (from 50 to 80 g m−2; without promoting vegetation state shifts), as well as in vegetation C and N pools, dead biomass, moss‐layer depth and ER. Yet, reindeer removal did not alter the chemistry of plants and soil or NEE. Synthesis. Although both herbivores were key drivers of ecosystem structure and function, the control exerted by geese in their main habitat (wet tundra) was much more pronounced than that exerted by reindeer in their main habitat (moist‐to‐dry tundra). Importantly, these herbivore effects are scale dependent, because geese are more spatially concentrated and thereby affect a smaller portion of the tundra landscape compared to reindeer. Our results highlight the substantial heterogeneity in how herbivores shape tundra vegetation and ecosystem processes, with implications for ongoing environmental change."
"
Hurricanes and other extreme weather events often affect disadvantaged communities more severely, and extended power outages are some of the most harmful effects. Concerns over the intensification of hurricanes has led to new environmental justice policies that aim to mitigate the unequal impacts of major storms. Now, policy experts and engineers are directing their attention toward illuminating the causes.

Researchers at the Georgia Institute of Technology sought to investigate whether socioeconomically vulnerable households experienced longer power outage durations after extreme weather events. The team analyzed data from the top eight major Atlantic hurricanes between 2017 and 2020 that knocked out power for over 15 million customers in nine states across the southeastern U.S. The team found that people in lower socioeconomic tiers wait significantly longer to have power restored after a major storm -- nearly three hours longer on average.
The interdisciplinary research team consists of Chuanyi Ji, an associate professor in the School of Electrical and Computer Engineering; Scott Ganz, a policy researcher at Georgetown University and a former Georgia Tech faculty member; and Chenghao Duan, a Ph.D. student in Ji's lab.
Their research paper, titled ""Socioeconomic Vulnerability and Differential Impact of Severe Weather-Induced Power Outages,"" was published in the journal PNAS Nexus.
""Not only do extreme weather events impact disadvantaged communities more harshly, but power disruption can be dangerous and even life-threatening in certain contexts,"" Ji said. ""Those with fewer resources are limited in their ability to evacuate from severe weather situations, and for individuals with electric medical equipment, an extended power outage can be disastrous.""
Ji, who specializes in large-scale data analytics for power grid resilience, has done previous work on power restoration procedures involving infrastructure and utility services, but wanted to expand the work into the realm of communities. The team hypothesized that disadvantaged communities likely wait longer for power to be restored, but to get a realistic picture of the mechanisms at play, the team needed to analyze troves of data.
They obtained weather data for eight major hurricanes between 2017 and 2020 from the National Oceanic and Atmospheric Administration and additional flood databases. They also examined power failure data for 15 million customers for the same time period, which spanned nine states, 588 counties, and 108 utility service regions in the Southeast.

The team used spatial data analytics to model weather impact across regions. They then measured customers' socioeconomic status by using the social vulnerability index, a tool produced by the Centers for Disease Control that considers indicators related to poverty, housing costs, education, health insurance, and other factors to determine socioeconomic status. Duan and Ji designed the models and estimates, and then analyzed the results to reveal the underlying relationship between customers' socioeconomic status and their power outage durations.
Their results show that, when comparing affluent communities and poor communities given the same kind of impact from weather events, poor communities experienced power outages that average 170 minutes longer. Specifically, they found that a one-decile drop in socioeconomic status is associated with a 6.1% longer outage duration. Their results indicate that there is a statistically significant relationship between socioeconomic vulnerability and the duration of time that elapses before power is restored.
""Our study also tries to rule out some possible explanations for why socioeconomically disadvantaged people take longer to get their power back on,"" Ganz said. ""For example, our study controls for population density in a county and the peak number of outages in that county, and we still observe that socioeconomically disadvantaged communities experience longer outages.""
He theorized that the ""primary cause is that poorer communities are also likely to be more distant from critical infrastructure or require more significant repairs to power lines, but these are important questions for future research.""
The results can have important implications for policymakers, pointing to the necessity of reexamining post-storm recovery and resource allocation policies. Service and utility providers approach power recovery by adhering to procedures and regulations that are policy-driven. Current research shows that the standard procedures for restoring power following big storms, while procedurally fair, may contribute to unequal outcomes. A greater focus on communities could help to correct the issue.
""Power grid resilience is not just about the infrastructure and utility companies -- it's also about the people they serve,"" Ji said. ""Success in achieving policy goals depends on our ability to identify the features that contribute most to these unequal impacts, which can in turn help us design appropriate interventions to improve outcomes.""

","score: 17.335289782549363, grade_level: '17'","score: 18.096986625420797, grade_levels: ['college_graduate'], ages: [24, 100]",10.1093/pnasnexus/pgad295,"In response to concerns about increasingly intense Atlantic hurricanes, new federal climate and environmental justice policies aim to mitigate the unequal impact of environmental disasters on economically and socially vulnerable communities. Recent research emphasizes that standard procedures for restoring power following extreme weather could be one significant contributor to these divergent outcomes. Our paper evaluates the hypothesis that more economically and socially vulnerable communities experience longer-duration power outages following hurricanes than less vulnerable communities do, conditional on the severity of the impact of the storm itself. Using data from eight major Atlantic hurricanes that made landfall between January 2017 and October 2020 and induced power outages for over 15 million customers in 588 counties in the Southeast, we demonstrate a significant relationship between socioeconomic vulnerability and the duration of time that elapses before power is restored for 95% of customers in a county. Specifically, a one-decile change in the socioeconomic status theme in the Social Vulnerability Index, a measure of vulnerability produced by the Centers for Disease Control and Prevention and the Agency for Toxic Substances and Disease Registry, produces a 6.1% change in expected outage duration in a focal county. This is equivalent to a 170-min average change in the period of time prior to power restoration."
"
With a severe shortage of affordable housing in the United States, renters living along the East and Gulf coasts are uniquely vulnerable to hurricane disasters. Two new studies based on data from 2009 to 2018 show that renters living along the East and Gulf coasts of the United States face rent increases, higher eviction rates, and a lack of affordable housing in the aftermath of a hurricane. The research will be presented in December at the annual meeting of the 2023 Society for Risk Analysis Annual Conference in Washington, D.C.

Both analytical studies are based on 10 years of data (2009 to 2018) on housing, hurricane disasters, and socioeconomic factors at the county level in 19 coastal states -- from Maine to Texas. The time period includes devastating hurricanes such as Irma (2011), Sandy (2012), and Matthew (2016).
The Impacts of a Hurricane on Rent Affordability 
Dr. Kelsea Best of The Ohio State University and her colleagues analyzed how the frequency and intensity of a hurricane correspond to changes in median rent and rental housing affordability over time. They found that median rents rise in the year following more intense hurricanes due to declines in housing availability. Their results also suggest that the occurrence of a hurricane in any given year (or in the previous year) reduces affordable rental housing. This was especially true for counties with a higher percentage of renters and people of color.
More than one-third of the American population (44 million households) live in rental dwellings. Renters have less access to post-disaster government aid programs and to benefits from federal mitigation programs such as home buyouts. In addition, people of renter status are more likely to be underinsured, with only 57% having insurance policies as of 2022 (Insurance Information Institute).
""Most federal post-disaster assistance programs are targeted to homeowners,"" says Best. ""Our study shows that deliberate attention must be given to renters -- especially low-income and minority renters -- in recovery efforts immediately following a disaster event and in subsequent years.""
She suggests that future local, state, and federal policies should provide explicit protections and support to renters after disasters. These could include eviction moratoria, limiting late fees on rent payments, increasing access to emergency rental assistance, and freezing rent increases. Additionally, efforts that prioritize affordable and stable housing supply with up-to-date market rent price monitoring could provide a critical reference for policymakers to understand and respond to renters' struggles, especially during post-disaster periods.

""Without such deliberate consideration of rent and renters, disaster recovery risks exacerbate the affordable housing crisis for some of the most vulnerable populations,"" says Best.
Hurricanes and Eviction Risk 
Another threat that renters may face following a disaster is eviction due to either loss of income or the lack of effective rental assistance when the housing supply tightens during the recovery phase.
Dr. Qian He of Rowan University and her colleagues investigated how disasters and post-disaster federal aid contribute to renters' eviction risks. They found that hurricanes corresponded to higher eviction filings and eviction threats by inflating market rent the year of and one year after the hurricane. Counties receiving higher amounts of aggregated federal aid (both post-disaster and hazard mitigation aid) were associated with lower eviction filings and eviction threats two years after the disaster.
According to He, this suggests that post-disaster federal aid programs can help mitigate renters' housing vulnerability during disaster recovery. ""Our findings indicate that coordinated public policies and renter aid programs, specifically after disaster events, can become crucial to ensure that at-risk communities have access to sufficient financial resources and legal support to help renters avoid eviction,"" says He.
For example, during the height of the Covid-19 pandemic, the Centers for Disease Control (CDC) issued a national eviction moratorium. This act provided immediate relief for over 6.5 million renter households across the country who were behind on their rent payment and those who were at an increased risk of eviction. ""Similar eviction moratoria after a climate-related disaster, potentially as part of federal recovery aid and efforts, could provide valuable protection to renters in affected communities,"" says He.

","score: 15.994410041391173, grade_level: '16'","score: 17.10095737246681, grade_levels: ['college_graduate'], ages: [24, 100]",10.1111/risa.14224,"Climate change is expected to increase the frequency and intensity of natural hazards such as hurricanes. With a severe shortage of affordable housing in the United States, renters may be uniquely vulnerable to disaster‐related housing disruptions due to increased hazard exposure, physical vulnerability of structures, and socioeconomic disadvantage. In this work, we construct a panel dataset consisting of housing, socioeconomic, and hurricane disaster data from counties in 19 states across the East and Gulf Coasts of the United States from 2009 to 2018 to investigate how the frequency and intensity of a hurricane correspond to changes in median rent and housing affordability (the interaction between rent prices and income) over time. Using a two‐stage least square random‐effects regression model, we find that more intense prior‐year hurricanes correspond to increases in median rents via declines in housing availability. The relationship between hurricanes and rent affordability is more complex, though the occurrence of a hurricane in a given year or the previous year reduces affordable rental housing, especially for counties with higher percentages of renters and people of color. Our results highlight the multiple challenges that renters are likely to face following a hurricane, and we emphasize that disaster recovery in short‐ and medium‐term should focus on providing safe, stable, and affordable rental housing assistance."
"
Using questionnaires created to determine whether a particular earthquake is natural or induced by human activity, a panel of experts concluded that the November 2022 magnitude 5.2 Peace River earthquake sequence in Alberta, Canada was likely to be induced.

The case study published in Seismological Research Letters was a serendipitous test of two recent questionnaire-based frameworks established to distinguish natural and induced earthquakes, the latter of which are mostly caused by hydraulic fracturing or injected water disposal wells associated with oil and gas recovery.
The Peace River sequence, which began in November 2022, was originally called a natural earthquake event by the Alberta Energy Regulator. But four months later, a new study came to the opposite conclusion. The 10-person expert panel described in SRL completed their work before the release of the study suggesting the Peace River sequence was induced.
The first reports indicated that the earthquakes were too deep to be induced and were not close to any hydraulic fracturing operations, which typically cause induced earthquakes in the region. However, later research found that the Peace River mainshock was shallower than previously determined, and that there was a disposal well less than a kilometer away.
""There was so much controversy on the day of the event on whether it was natural or induced,"" said Rebecca Salvage of the Geological Survey of Canada (previously the University of Calgary), especially because the 30 November 2022 magnitude 5.2 mainshock was one of the largest seismic events recorded in Alberta.
""If it's natural, that affects our seismic hazard models for Alberta, which then affects things like building codes and regulations,"" Salvage explained. ""If it's induced, it's still the largest event we think we've seen in Alberta, and that has implications for industry regulations.""
If the Peace River sequence was the result of the nearby disposal wells, where injections had taken place for decades without seismic incident, that could also affect how researchers think about the timing of induced events, Salvage added.

""It could have ramifications for other long-term storage technologies such as carbon capture, utilization and storage solutions or enhanced geothermal energy,"" she said.
To test the two questionnaire frameworks, Salvage and her colleagues gathered ten researchers -- from emeritus professor to Ph.D. student -- with a range of expertise in natural intraplate seismicity, injection-induced seismicity, and computational geophysics outside of the Alberta geographical region.
To answer the two framework questionnaires, the panel received data on the temporal and spatial evolution of the event's seismicity detected using public sensors, publicly available monthly fluid injection information from industry, and information on past regional seismicity. The questionnaires are roughly similar but differ in how they measure uncertainty and present results.
The experts agreed that the Peace River sequence was likely induced, but the study results suggest that the data to definitively distinguish induced from natural earthquakes were not always conclusive or available for the experts.
Ideally, these questionnaires would be distributed as soon as possible after an earthquake sequence and provide more timely data on fluid injection, Salvage said, but she and her team were only able to collect public data three months after the November 2022 earthquake.
""We could have done expert panels sooner, but there is so little public information out there, and the time lag [for injection data] is several months,"" she explained. In Alberta, industry operators are required to report monthly injection data, although Salvage said they likely do collect information over shorter time intervals.

The lag could have contributed to the expert panel's uncertainty, she noted. ""There's a lot of uncertainty in the data, so you can't make a fully informed decision if your data aren't complete:""
Salvage and her colleagues noted a few issues with administering the questionnaires. For instance, terms such as ""regional"" and ""near to"" -- which are important in defining an earthquake's relationship to industry activity -- were interpreted by the experts in different ways.
Salvage said the authors behind the two frameworks deliberately left the language in them ""generic, so they could be applied to as many scenarios as possible, and that's great, but maybe when it comes to administering these surveys in the future, there needs to be a checklist up at the front about what needs to be defined.""
The study also found that questions related to earthquake focal mechanisms were not useful in distinguishing the likely origin of earthquakes, but that aftershock characteristics might be helpful, although more research is needed to test such aftershock criteria.

","score: 17.75216535433071, grade_level: '18'","score: 19.49700787401575, grade_levels: ['college_graduate'], ages: [24, 100]",10.1785/0220230289,"Based on information available at the time, several questionnaire-based schemes have been developed to provide a qualitative assessment of whether a specific earthquake (or earthquake sequence) was likely induced by anthropogenic activities or is inferred to be natural. From a pragmatic perspective, the value of this assessment is arguably the greatest in the immediate aftermath of an event (hours to days), because it could then better serve to guide regulatory response. However, necessary information is often incomplete or uncertain, and there remains a lack of scientific consensus on the most distinctive attributes of induced (vs. natural) earthquake sequences. We present a case study of the Mw 5.2 Peace River earthquake sequence (Alberta, Canada), evaluated using two published frameworks for origin interpretation. The Alberta Energy Regulator initially considered the sequence to be natural, but a study published ~4 mo later came to the opposite interpretation. Prior to this publication, we convened a panel of experts who completed questionnaires as set out by the frameworks; results using both schemes indicate that experts believe the sequence was likely induced. Based on these expert responses, we critically evaluate information that was available publicly in the weeks to months following the mainshock on 30 November 2022; reassess the relative importance of various components of the questionnaires from a parsimonious, rapid-response perspective; and consider other types of information that could be critical for near-real-time assessment of whether an event was induced or natural."
"
Beef operations that keep cattle on lifelong grass-based diets may have an overall higher carbon footprint than those that switch cattle to grain-based diets partway through their lives. Daniel Blaustein-Rejto of the Breakthrough Institute, USA, and colleagues present these findings in the open-access journal PLOS ONE on December 13.

Cattle on lifelong grass diets are known as ""pasture finished,"" while those that switch from grass to grain before slaughter are ""grain finished."" Prior research has suggested that pasture-finished beef operations have a higher carbon footprint than grain-finished operations. However, most studies have limited their focus to the amount of greenhouse gases emitted directly by beef production without considering other factors that may affect the overall carbon footprint.
To help deepen understanding, Blaustein-Rejto and colleagues calculated and compared the carbon footprint of 100 beef operations located in 16 countries. In addition to direct greenhouse gas emissions, their calculations incorporated soil carbon sequestration -- the capture and long-term storage in pasture soils of atmospheric carbon, often in the form of dead plants and cattle waste. They also accounted for the carbon opportunity cost -- the carbon that would have been sequestered if the land had native ecosystems instead of being used for beef production.
Extensive statistical analysis showed that the pasture-finished operations produce 20 percent more greenhouse gases than grain-finished operations -- in line with prior studies. However, after incorporating soil carbon sequestration and carbon opportunity cost, the total carbon footprint of pasture-finished operations was 42 percent higher, likely due to its more intense usage of land.
Further analysis suggested that an increase in land use intensity is indeed strongly associated with a bigger overall carbon footprint for beef operations. The calculations also suggest that, averaging across all operations in the study, carbon opportunity costs may contribute even more to an operation's overall carbon footprint than its direct greenhouse gas emissions.
The researchers say their findings emphasize the need for climate mitigation efforts to account for carbon opportunity costs of beef production. With pasture-finished beef often being seen as more premium, carbon footprint data may also provide important additional information to aid consumer choice.
The authors add: ""Our research reveals that the carbon cost of land use accounts for the largest part of beef's carbon footprint. Therefore, there is an even larger carbon cost than typically found to land-intensive beef operations, such as many grass-fed systems, even when taking into account potential carbon sequestration due to grazing.""

","score: 16.022549019607847, grade_level: '16'","score: 18.29860294117647, grade_levels: ['college_graduate'], ages: [24, 100]",10.1371/journal.pone.0295035,"Beef production accounts for the largest share of global livestock greenhouse gas emissions and is an important target for climate mitigation efforts. Most life-cycle assessments comparing the carbon footprint of beef production systems have been limited to production emissions. None also consider potential carbon sequestration due to grazing and alternate uses of land used for production. We assess the carbon footprint of 100 beef production systems in 16 countries, including production emissions, soil carbon sequestration from grazing, and carbon opportunity cost—the potential carbon sequestration that could occur on land if it were not used for production. We conduct a pairwise comparison of pasture-finished operations in which cattle almost exclusively consume grasses and forage, and grain-finished operations in which cattle are first grazed and then fed a grain-based diet. We find that pasture-finished operations have 20% higher production emissions and 42% higher carbon footprint than grain-finished systems. We also find that more land-intensive operations generally have higher carbon footprints. Regression analysis indicates that a 10% increase in land-use intensity is associated with a 4.8% increase in production emissions, but a 9.0% increase in carbon footprint, including production emissions, soil carbon sequestration and carbon opportunity cost. The carbon opportunity cost of operations was, on average, 130% larger than production emissions. These results point to the importance of accounting for carbon opportunity cost in assessing the sustainability of beef production systems and developing climate mitigation strategies."
"
This holiday season brings surprising news about your Christmas tree. Scientists just discovered that globally, trees growing in wetter regions are more sensitive to drought. That means if your tree hails from a more humid clime, it's likely been spoiled for generations.

Scientists have long debated whether arid conditions make trees more or less resilient to drought. It seems intuitive that trees living at their biological limits will be most vulnerable to climate change, since even just a little extra stress could tip them past the brink. On the other hand, these populations have adapted to a harsher setting, so they might be more capable of withstanding a drought.
According to a new study in the journal Science by researchers at UC Santa Barbara and UC Davis, greater water availability could ""spoil"" trees by reducing their adaptations to drought. ""And that's really critical to understand when we're thinking about the global vulnerability of forest carbon stocks and forest health,"" said ecologist Joan Dudney, an assistant professor at UCSB's Bren School of Environmental Science & Management and in the Environmental Studies Program. ""You don't want to be a 'spoiled' tree when facing a major drought.""
Dudney and her co-authors expected trees growing in the most arid regions to be more sensitive to drought, since they're already living at the edge of their limits. What's more, climate change models predict that these regions will experience more rapid drying than wetter regions. This shift in climate could expose trees to conditions beyond their adaptive capacity.
To measure drought sensitivity, the authors analyzed 6.6 million tree ring samples from 122 species worldwide. For each year, they measured whether the tree grew faster or slower than average based on its ring width. They linked these trends with historic climate data, including precipitation and temperature.
The team then compared drought responses across different regions. ""As you move to the drier edge of a species' range, trees become less and less sensitive to drought,"" said lead author Robert Heilmayr, an environmental economist also in the Environmental Studies Program and at the Bren School. ""Those trees are actually quite resilient.""
Dudney, Heilmayr and their co-author Frances Moore were inspired, in part, by the work of UCSB professor Tamma Carleton on the effects climate change has on human populations. ""This paper highlights the value of cross-disciplinary scientific work,"" added Moore, an associate professor at UC Davis. ""We were able to adapt methods from economics originally developed to study how people and businesses adjust to a changing climate and apply them to the ecological context to study forest sensitivity to drought.""
""A heatwave is likely to kill more people in a cool place like Seattle than in hotter cities like Phoenix,"" Heilmayr said. The Southwest is already quite hot, so heatwaves there are scorching. But the region's cities are adapted to an extreme climate, he points out. Now we know that forests display similar trends.

Unfortunately, warmer regions are slated to get disproportionately drier in the coming decades. ""There is a pretty large portion of species' ranges that are going to face a completely novel climate, something that those species don't see anywhere in their range today,"" Heilmayr explained. The authors found that 11% of an average species' range in 2100 will be drier than the driest parts of their historic range. This increases to over 50% for some species.
""Broadly, our research highlights that very few forests will be unaffected by climate change,"" Dudney said. ""Even wetter forests are more threatened than we thought.""
But there is a flip side of the coin. Species have a reservoir of drought-hearty stock in the drier parts of their range that could bolster forests in wetter areas. Previous research out of UCSB revealed that many species do have the capacity to adapt to environmental change. However, those researchers also found that trees migrate slowly from one generation to the next. That means human intervention -- such as assisted migration -- may be necessary in order to take advantage of this genetic diversity.
Whether your Christmas trees grow in a dry or wet region, they'll likely experience growth declines in the future. But understanding how trees will respond to climate change can help ensure the future of the Tannenbaum and its wild counterparts.

","score: 10.622085048010977, grade_level: '11'","score: 11.79609703270522, grade_levels: ['12'], ages: [17, 18]",10.1126/science.adi1071,"Climate change is shifting the structure and function of global forests, underscoring the critical need to predict which forests are most vulnerable to a hotter and drier future. We analyzed 6.6 million tree rings from 122 species to assess trees’ sensitivity to water and energy availability. We found that trees growing in wetter portions of their range exhibit the greatest drought sensitivity. To test how these patterns of drought sensitivity influence vulnerability to climate change, we predicted tree growth through 2100. Our results suggest that drought adaptations in arid regions will partially buffer trees against climate change. By contrast, trees growing in the wetter, hotter portions of their climatic range may experience unexpectedly large adverse impacts under climate change."
"
Even in the precipitation-heavy Pacific Northwest, more frequent heatwaves are threatening a key source of water supply.

A Washington State University study that intended to look at snow melting under a single, extreme event, the 2021 ""heat dome,"" instead revealed an alarming, longer-term rising trend of successive heatwaves melting snowpack earlier in the year.
The findings have implications for many areas worldwide that are dependent on snow-capped mountains to provide summer water since heatwaves have been on the rise globally.
""Short-term events like heatwaves have had an under-appreciated impact on accelerating snow melt, and cumulatively, they can amplify each other,"" said Luke Reyes, a doctoral student in WSU's School of the Environment, and lead author of the study published in npj Climate and Atmospheric Science.
Heat domes, rare events that occur when the atmosphere traps hot ocean air, caused record temperatures nearing 122 degrees Fahrenheit across the Pacific Northwest in late June 2021. Yet, the researchers found that by the time the dome arrived, a lot of the region's snowpack had already melted.
Their analysis of high-resolution snowpack and temperature data revealed that high-elevation snow had started melting during a series of heatwaves in April, May and early June -- when temperatures were 7.2 to 12.6 degrees Fahrenheit above normal.
Even more concerning, when the researchers looked back at temperature records spanning from 1940 to 2021, they saw that these springtime heatwaves have doubled in frequency, intensity or both since the mid-1990s.

""The data suggests that we don't necessarily need to be worried about a very rare event like the heat dome, but that heat waves are becoming far more prevalent and are more likely to be driving a lot of snowpack loss in the future,"" said co-author Marc Kramer, a WSU associate professor of environmental chemistry.
The effect of short-term heatwaves on snowpack has been understudied because historically researchers looked at snowpack levels on April 1 and average monthly temperatures to estimate climate change impacts on snowpack loss. Those averages might show single-digit temperature increases but obscure the impact of heat spikes that may last just a couple days.
Also, for many years, mountainous snowpack was thought to be resilient to short-term high temperatures in spring because it remained sufficiently cold at high elevations. The WSU study revealed that this buffering capacity appears to have diminished in the face of more frequent and intense heatwaves.
In 2021, the combined result of heatwaves and the heat dome meant the snowpack melted about three weeks earlier than usual, with most snow cover gone by late June. Normally, snowmelt provides the Pacific Northwest with water well into August.
This extreme early melting occurred even though 2021 was a La Niña year, a global weather phenomenon that typically means a deeper snowpack. In fact, Pacific Northwest snowpack that spring was 135% of normal for the higher-elevation snow zone and its 18-year record examined by the study. By the end of June, though, that was gone.
This rapid melt does not bode well for the coming year, 2024, which is expected to have drier weather brought by El Niño.
""We have a coming El Niño year and next year, and there may be some amplification effects,"" said Kramer. ""If we have less snow to begin with, the snowpack is going to be all that much more vulnerable to these heat anomalies earlier in the season.""
The study is part of the Kramer lab's broader research into heat wave impacts on ecosystems and agriculture. This research is supported by a grant from the U.S. Department of Agriculture's National Institute of Food and Agriculture.

","score: 13.639080541696366, grade_level: '14'","score: 15.604787598004279, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41612-023-00521-0,"A heatwave in June 2021 exposed Pacific Northwest (PNW) snowpack to record temperatures, allowing us to probe seasonal snowpack response to short-term heat extremes. Using high-resolution contiguous snowpack and temperature datasets (daily 1 km2 SNODAS, 4 km2 PRISM), we examined daily snowmelt in cooler, higher-elevation zones during this event, contrasted with the prior 18 years (2004–2021). We found that multiple early season (spring) heatwaves, concluding with the 2021 heat dome itself, resulted in dramatic early season melt including the most persistent fraction of PNW snowpack. Using longer-term station records (1940–2021), we show that springtime +5 °C daily anomalies were historically rare but since the mid-1990s have doubled in frequency and/or intensity, now potentially affecting typically cool La Niña periods (2021). Collectively, these results indicate that successive heat extremes drive rapid snowmelt, and these extremes may increasingly threaten previously resilient fractions of seasonal snowpack."
"
The transition to a society without fossil fuels means that the need for batteries is increasing at a rapid pace. At the same time, the increase will mean a shortage of the metals lithium and cobalt, which are key components in the most common battery types. One option is a sodium-ion battery, where table salt and biomass from the forest industry make up the main raw materials. Now, researchers from Chalmers University of Technology, Sweden, show that these sodium-ion batteries have an equivalent climate impact as their lithium-ion counterparts -- without the risk of running out of raw materials.

""The materials we use in the batteries of the future will be important in order to be able to switch to renewable energy and a fossil-free vehicle fleet,"" says Rickard Arvidsson, Associate Professor of Environmental Systems Analysis at Chalmers.
According to the European Commission's Critical Raw Materials Act, the demand for critical raw battery materials is expected to increase exponentially as EU countries transition to renewable energy systems and electric vehicles. The green transition will also require more local production of batteries and other new fossil-free technologies, and a steady supply of raw materials is needed to meet demand. At the same time, such production carries a high risk of supply disruptions, due to the limited number of sources for raw materials. ""Lithium-ion batteries are becoming a dominant technology in the world and they are better for the climate than fossil-based technology is, especially when it comes to transport. But lithium poses a bottleneck. You can't produce lithium-based batteries at the same rate as you want to produce electric cars, and the deposits risk being depleted in the long term,"" says Rickard Arvidsson. In addition to this, critical battery materials, such as lithium and cobalt, are largely mined in just a few places in the world, posing a risk to the supply.
Sodium-ion batteries offer promising technology
The development of new battery technologies is moving fast in the quest for the next generation of sustainable energy storage -- which should preferably have a long lifetime, have a high energy density and be easy to produce. The research team at Chalmers chose to look at sodium-ion batteries, which contain sodium -- a very common substance found in common sodium chloride -- instead of lithium. In a new study, they have carried out a so-called life cycle assessment of the batteries, where they have examined their total environmental and resource impact during raw material extraction and manufacturing. ""We came to the conclusion that sodium-ion batteries are much better than lithium-ion batteries in terms of impact on mineral resource scarcity, and equivalent in terms of climate impact. Depending on which scenario you look at, they end up at between 60 and just over 100 kilogrammes of carbon dioxide equivalents per kilowatt hour theoretical electricity storage capacity, which is lower than previously reported for this type of sodium-ion battery. It's clearly a promising technology,"" says Rickard Arvidsson.
The researchers also identified a number of measures with the potential to further reduce climate impact, such as developing an environmentally better electrolyte, as it accounted for a large part of the battery's total impact.
Green energy requires energy storage
Today's sodium-ion batteries are already expected to be used for stationary energy storage in the electricity grid, and with continued development, they will probably also be used in electric vehicles in the future. ""Energy storage is a prerequisite for the expansion of wind and solar power. Given that the storage is done predominantly with batteries, the question is what those batteries will be made from? Increased demand for lithium and cobalt could be an obstacle to this development,"" says Rickard Arvidsson.

The major advantage of the technology is that the materials in the sodium-ion batteries are abundant and can be found all over the world. One electrode in the batteries -- the cathode -- has sodium ions as a charge carrier, and the other electrode -- the anode -- consists of hard carbon, which in one of the examples the Chalmers researchers have investigated can be produced from biomass from the forest industry. In terms of production processes and geopolitics, sodium-ion batteries are also an alternative that can accelerate the transition to a fossil-free society. ""Batteries based on abundant raw materials could reduce geopolitical risks and dependencies on specific regions, both for battery manufacturers and countries,"" says Rickard Arvidsson.
More about the study
The study is a prospective life cycle assessment of two different sodium-ion battery cells where the environmental and resource impact is calculated from cradle to gate, i.e. from raw material extraction to the manufacture of a battery cell. The functional unit of the study is 1 kWh theoretical electricity storage capacity at the cell level. Both types of battery cells are mainly based on abundant raw materials. The anode is made up of hard carbon from either bio-based lignin or fossil raw materials, and the cathode is made up of so-called ""Prussian white"" (consisting of sodium, iron, carbon and nitrogen). The electrolyte contains a sodium salt. The production is modelled to correspond to a future, large-scale production. For example, the actual production of the battery cell is based on today's large-scale production of lithium-ion batteries in gigafactories.
Two different electricity mixes were tested, as well as two different types of so-called allocation methods -- that is, allocation of resources and emissions. One where the climate and resource impact is distributed between coproducts based on mass, and one method where all impact is allocated to the main product (the sodium-ion battery and its components and materials).
The study was funded by the Swedish Energy Agency through the Battery Fund Program.

","score: 14.92613277133825, grade_level: '15'","score: 15.604429038877491, grade_levels: ['college_graduate'], ages: [24, 100]",10.1111/jiec.13452,"Batteries are enablers for reducing fossil‐fuel dependency and climate‐change impacts. In this study, a prospective life cycle assessment (LCA) of large‐scale production of two different sodium‐ion battery (SIB) cells is performed with a cradle‐to‐gate system boundary. The SIB cells modeled have Prussian white cathodes and hard carbon anodes based only on abundant elements and thus constitute potentially preferable options to current lithium‐ion battery (LIB) cells from a mineral resource scarcity point of view. The functional unit was 1 kWh theoretical electricity storage capacity, and the specific energy density of the cells was 160 Wh/kg. Data for the cathode active material come from a large‐scale facility under construction and data for the SIB cell production is based on a large‐scale LIB cell gigafactory. For other SIB cell materials, prospective inventory data was obtained from a generic eight‐step procedure developed, which can be used by other LCA practitioners. The results show that both SIB cells indeed have considerably lower mineral resource scarcity impacts than nickel‐manganese‐cobalt (NMC)‐type LIB cells in a cradle‐to‐gate perspective, while their global warming impacts are on par. Main recommendations to SIB manufacturers are to source fossil‐free electricity for cell production and use hard carbon anodes based on lignin instead of phenolic resin. Additionally, since none of the assessed electrolytes had clearly lower cradle‐to‐gate impacts than any other, more research into SIB electrolyte materials with low environmental and resource impacts should be prioritized. An improvement of the SIB cell production model would be to obtain large‐scale production data specific to SIB cells."
"
Strong precipitation may cause natural disasters, such as flooding or landslides. Global climate models are required to forecast the frequency of these extreme events, which is expected to change as a result of climate change. Researchers of Karlsruhe Institute of Technology (KIT) have now developed a first method based on artificial intelligence (AI), by means of which the precision of coarse precipitation fields generated by global climate models can be increased. The researchers succeeded in improving spatial resolution of precipitation fields from 32 to two kilometers and temporal resolution from one hour to ten minutes. This higher resolution is required to better forecast the more frequent occurrence of heavy local precipitation and the resulting natural disasters in future.

Many natural disasters, such as flooding or landslides, are directly caused by extreme precipitation. Researchers expect that increasing average temperatures will cause extreme precipitation events to further increase. To adapt to a changing climate and prepare for disasters at an early stage, precise local and global data on the current and future water cycle are indispensable. ""Precipitation is highly variable in space and time and, hence, difficult to forecast, in particular on the local level,"" says Dr. Christian Chwala from the Atmospheric Environmental Research Division of KIT's Institute of Meteorology and Climate Research (IMK-IFU), KIT's Campus Alpine in Garmisch-Partenkirchen."" For this reason, we want to enhance the resolution of precipitation fields generated e.g. by global climate models and improve their classification as regards possible threats, such as floodings.""
Higher Resolution for More Precise Regional Climate Models 
Currently used global climate models are based on a grid that is not fine enough to precisely present the variability of precipitation. Highly resolved precipitation maps can only be produced with computationally expensive and, hence, spatially or temporally limited models. ""For this reason, we have developed an AI-based generative neural network, called GAN, and trained it with high-resolution radar precipitation fields. In this way, the GAN learns how to generate realistic precipitation fields and derive their temporal sequence from coarsely resolved data,"" says Luca Glawion from IMK-IFU. ""The network is able to generate highly resolved radar precipitation films from very coarsely resolved maps."" These refined radar maps not only show how rain cells develop and move, but precisely reconstruct local rain statistics and the corresponding extreme value distribution.
""Our method serves as a basis to increase the resolution of coarsely grained precipitation fields, such that the high spatial and temporal variability of precipitation can be reproduced adequately and local effects can be studied,"" says Julius Polz from IMK-IFU. ""Our deep learning method is quicker by several orders of magnitude than the calculation of such highly resolved precipitation fields with numerical weather models usually applied to regionally refine data of global climate models."" The researchers point out that their method also generates an ensemble of different potential precipitation fields. This is important, as a multitude of physically plausible highly resolved solutions exists for each coarsely resolved precipitation field. Similar to a weather forecast, an ensemble allows for a more precise determination of the associated uncertainty.
Higher Resolution for Better Forecasts under Climate Change
The results show that the AI model and methodology developed by the researchers will enable future use of neural networks to improve the spatial and temporal resolution of precipitation calculated by climate models. This will allow for a more precise analysis of the impacts and developments of precipitation in a changing climate.
""In a next step, we will apply the method to global climate simulations that transfer specific large-scale weather situations to a future world with a changed climate, e.g. to the year of 2100. The higher resolution of precipitation events simulated with our method will allow for a better estimation of the impacts the weather conditions that caused the flooding of the river Ahr in 2021 would have had in a world warmer by 2 degrees,"" Glawion explains. Such information is of decisive importance to develop climate adaptation methods.

","score: 16.00619047619048, grade_level: '16'","score: 16.475, grade_levels: ['college_graduate'], ages: [24, 100]",10.1029/2023EA002906,"Climate models face limitations in their ability to accurately represent highly variable atmospheric phenomena. To resolve fine‐scale physical processes, allowing for local impact assessments, downscaling techniques are essential. We propose spateGAN, a novel approach for spatio‐temporal downscaling of precipitation data using conditional generative adversarial networks. Our method is based on a video super‐resolution approach and trained on 10 years of country‐wide radar observations for Germany. It simultaneously increases the spatial and temporal resolution of coarsened precipitation observations from 32 to 2 km and from 1 hr to 10 min. Our experiments indicate that the ensembles of generated temporally consistent rainfall fields are in high agreement with the observational data. Spatial structures with plausible advection were accurately generated. Compared to trilinear interpolation and a classical convolutional neural network, the generative model reconstructs the resolution‐dependent extreme value distribution with high skill. It showed a high fractions skill score of 0.6 (spatio‐temporal scale: 32 km and 1 hr) for rainfall intensities over 15 mm h−1 and a low relative bias of 3.35%. A power spectrum analysis confirmed that the probabilistic downscaling ability of our model further increased its skill. We observed that neural network predictions may be interspersed by recurrent structures not related to rainfall climatology, which should be a known issue for future studies. We were able to mitigate them by using an appropriate model architecture and model selection process. Our findings suggest that spateGAN offers the potential to complement and further advance the development of climate model downscaling techniques, due to its performance and computational efficiency."
"
Leipzig. Plants emit odours for a variety of reasons, such as to communicate with each other, to deter herbivores or to respond to changing environmental conditions. An interdisciplinary team of researchers from Leipzig University, the Leibniz Institute for Tropospheric Research (TROPOS) and the German Centre for Integrative Biodiversity Research (iDiv) carried out a study to investigate how biodiversity influences the emission of these substances. For the first time, they were able to show that species-rich forests emit less of these gases into the atmosphere than monocultures. It was previously assumed that species-rich forests release more emissions. The Leipzig team has now been able to disprove this assumption experimentally. Their study has been published in the journal Communications Earth & Environment.

Plant odours penetrate the atmosphere
Plants produce a variety of organic compounds to communicate with each other and with their environment. These are known as biogenic volatile organic compounds (BVOCs), such as terpenes, which give plants their characteristic scent and help to repel pests. As well as acting as chemical signals, these substances play a role in regulating climate, air quality and atmospheric chemistry. This is because these BVOCs emitted by plants form biogenic secondary organic aerosols (BSOAs) in the air, i.e. particles in the atmosphere. These aerosols in turn affect air quality, cloud formation and the climate.
MyDiv Experiment: Measurements in plots with different tree species
But how do emissions and concentrations of aerosols in the air change as biodiversity declines or plants are stressed by drought? The interdisciplinary team led by scientists Dr Anvar Sanaei and Professor Alexandra Weigelt from Leipzig University and other researchers from TROPOS and iDiv investigated this question. The scientists collected the data at the MyDiv tree diversity experimental site. The site, near Bad Lauchstädt in Saxony-Anhalt, covers around two hectares and has 80 plots with ten tree species growing together in monocultures or mixtures of different species. For the study, the team spent almost two weeks collecting air samples from ten of the 11x11 metre plots, which grow four tree species -- rowan, wild cherry, common ash and sycamore -- in different combinations.
Fewer plant odours, fewer risks
""In the field, we measured BVOCs and BSOA compounds in ten plots of varying tree diversity. Our results show that the amount of BVOCs decreases with increasing biodiversity in most cases,"" says Dr Anvar Sanaei, first author of the study and postdoctoral researcher at the Institute of Biology at Leipzig University. It is estimated that global BVOC emissions from vegetation will increase by around a third as a result of climate change and higher temperatures. ""There are considerable uncertainties here: these precursor gases can form particles, which in turn can become cloud droplets. Whether BVOCs ultimately cool or warm the atmosphere depends on many factors and is difficult to predict. However, more biodiversity and fewer BVOCs would reduce the changes in the atmosphere and thus also the risks of climate change -- including changes in precipitation,"" adds Professor Hartmut Herrmann from TROPOS. The second part of the study shows how difficult it is to investigate these complex processes in the field: the team was unable to establish any clear correlations for BSOAs, which could be partly due to environmental influences, as the conversion of BVOC gases into BSOA particles takes a certain amount of time. At just under two weeks, the measurement campaign was also comparatively short. That is why the team wants to continue the research -- not least because many questions remain unanswered.

More stress, more plant odours?
Previously, it was thought that species-rich forests and grasslands released more gases into the atmosphere than species-poor ones. The reason for this was thought to be that species-rich systems produce more biomass because they can utilise resources such as light, water and nutrients more efficiently. More biomass then also means more leaf surface area from which the gases can be emitted. ""Our new results, however, suggest that the situation may be due to the fact that plants in species-rich forests and grasslands are under less stress. Compared to monocultures, they face fewer herbivores and less heat and drought. But this is just a hypothesis for now. Much more research is needed to better understand how biodiversity affects the atmosphere, where we need to look more closely at the microclimate, above- and below-ground stress on plants, and many other factors in long-term experiments,"" says Professor Nico Eisenhauer from iDiv.
Biology + climate research + chemistry = A team fit for the future
What made this study so special was that different disciplines worked together, combining atmospheric and biological measurements. ""Only with knowledge from biology, climate research and atmospheric chemistry can we decipher how plant emissions are linked to biodiversity and the atmosphere. Our study highlights the need for experiments at the local and regional scales and the development of models to improve our understanding of biosphere-atmosphere interactions,"" says senior author Professor Alexandra Weigelt from the Institute of Biology. She adds that this is a prime example of the Breathing Nature research project, for which the University submitted a draft proposal back in May as part of the Excellence Strategy. After all, answers to the pressing questions of our time can only be found by transcending disciplinary and institutional boundaries.

","score: 13.246444191343965, grade_level: '13'","score: 14.731161731207287, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s43247-023-01113-9,"Climate extremes in tandem with biodiversity change affect plant emissions of biogenic volatile organic compounds, as a result, the formation of biogenic secondary organic aerosols. The resulting biogenic secondary organic aerosols can have a wide variety of impacts, such as on Earth’s radiative balance or cloud- and precipitation formation. However, at present, it is unclear how changing biodiversity will lead to changes in biogenic volatile organic compound emissions, biogenic secondary organic aerosols and their corresponding effects. We present a conceptual framework of the relationships between biodiversity and biogenic volatile organic compound emissions based on our current mechanistic understanding and combining knowledge from the fields of biology and atmospheric chemistry. Parts of this framework are tested in a case study using a tree diversity experiment. The relative differences in tree monocultures and mixtures show that the overall concentration of biogenic volatile organic compounds decreases with increasing biodiversity, but results for biogenic secondary organic aerosols are mixed and overall non-significant. A deeper understanding of how changing biodiversity influences biogenic organic compound emissions and biogenic secondary organic aerosol formation requires in-depth investigations of microclimate conditions, accurate monitoring of above- and below-ground biotic and abiotic stress, and manipulating stress conditions across long-term biodiversity experiments."
"
A verdant forest is one of the most iconic symbols of the power of nature, from the abundance of plant and animal life that shelters among its thick vegetation to the positive impact it has on Earth's climate, thanks in part to photosynthesis, which removes carbon dioxide from the air, thereby mitigating the effects of global warming. Cutting down tropical evergreen forests has played a significant role in exacerbating the climate crisis, and many environmental initiatives focus on rehabilitating destroyed forests or planting new trees. The problem is that, even if we were to cover the entire surface of the planet with trees, the resultant massive photosynthetic force would still not suffice to absorb the huge surplus of carbon dioxide -- the major greenhouse gas -- that has been pumped into the atmosphere during the past 150 years of human activity.

There is another way of dealing with the climate crisis, which, unlike the forests, is neither natural nor green, at least not in the literal sense of the word. This artificial solution consists of erecting fields of dark-colored solar panels. Obviously, the production of electricity from solar power has a positive impact on climate balance, since it replaces power stations that use fossil fuels such as coal and gas, thereby reducing harmful emissions of greenhouse gases that accumulate at increasing concentrations in the atmosphere.
But both the green, natural forest and the artificial, dark ""solar forest"" produce other effects, some of which can be problematic from a climate perspective. They are both relatively dark, which means that they absorb a large proportion of the radiation from the Sun (making them ""low albedo"" surfaces in the professional jargon) and, as a result, they heat up. Some of this energy is used for photosynthesis in natural forests or to produce electricity in solar ""forests"" -- but most returns to the atmosphere as fluxes of energy, heating it up. In contrast, the light-colored desert soil, for example, reflects a significant portion of the sunlight back into space, which does not add to the accumulated heat in the atmosphere. (Such soil is known as a surface with a ""high albedo."")
What, then, would be the most effective use of a certain plot of land in terms of the climate crisis: planting a forest, which is a natural means of absorbing carbon dioxide from the atmosphere, or erecting fields of solar panels, which reduce the emission of carbon dioxide into the atmosphere? This dilemma has long been debated by decision-makers around the world.
Now, for the first time -- based on findings from arid areas and on comprehensive measurements of the energy flow exchanged between the ground and the atmosphere -- we may have an answer to this question, thanks to a new study led by Dr. Rafael Stern, Dr. Jonathan Muller and Dr. Eyal Rotenberg from Prof. Dan Yakir's lab at the Earth and Planetary Sciences Department of the Weizmann Institute of Science. The study, published today in PNAS Nexus, was coauthored by Madi Amer, also from Prof. Yakir's lab, and Dr. Lior Segev of Weizmann's Physics Core Facilities Department.
A century of photosynthesis
The first stage of the study involved comparing the impact of a forest situated on the border of an arid area to that of a field of solar panels, or a solar farm, in an arid environment. Arid areas are characterized by a large amount of sunlight and a relative paucity of plant diversity and biomass, which makes them especially suited for large solar farms. Such fields already exist in Israel in the Arava and the Negev, and the government has plans to erect more in Jordan through an international collaboration. Elsewhere in the world, huge solar projects are under way, for example, in the deserts of China, and the European Union has long discussed plans to build solar farms in the Sahara. The Weizmann researchers traveled down to the Arava in a truck carrying a mobile measuring station, specially designed by Yakir and Rotenberg. They began by placing this measuring station close to the solar panel field to measure the flux of energy between the ground and the atmosphere -- as it occurs in an arid area without solar panels. Then they placed the station inside the solar panel field itself; this required overcoming operational and safety challenges stemming from the sensitivity of the panels, which had interfered with such measurements in the past. At both locations, the experiments were repeated during different seasons of the year. Finally, to compare their results to the similar process occurring in a forest, the scientists relied on data that Yakir and Rotenberg had collected over the past 20 years in Yatir Forest -- the largest of the forests planted in Israel by the Jewish National Fund -- on the northern edge of the arid Negev Desert.

The researchers discovered that the albedo effect of both of these ""forests"" was similar, but the absorption or prevention of carbon emissions was very different, favoring the solar forest. To complete the comparison, they calculated the equilibrium points at which the opposing effects on the Earth's climate -- heating from both forests' dark color and cooling from reduced atmospheric carbon dioxide -- balance out one another, ultimately lowering the concentration of greenhouse gases in the atmosphere as a result of the natural forest's photosynthesis or the solar forest's reduced electricity-production emissions. It turns out that it takes two and a half years for the heat emitted by solar farms to be offset by the carbon emissions that are averted thanks to the energy they generate. This even takes into account the carbon emissions from the manufacture, transportation and operation of the panels, as well as of batteries used for electricity storage. In the case of a forest of similar size, it would take more than 100 years of photosynthesis to offset its heating effect.
The researchers also wanted to establish how the heating and cooling ratio changed in other climates. Using data from similar measurements collected from satellites and databases, they found that in more humid environments such as the tropics or in temperate grassland regions like Europe, the heating effect of planting large numbers of trees is smaller. This is because the ground there is darker to begin with, which means that the albedo-related effect is smaller, and the carbon capture rate by trees is higher, so the break-even point is reached within 15 to 18 years. With that, they note, it must be kept in mind that less open space is available in these areas for planting new forests.
Stern and Muller explain: ""Our study unequivocally shows that in arid environments, where most of the open land reserves exist, building solar farms is far more effective than planting forests when it comes to dealing with the climate crisis. In this environment, erecting solar panels on areas that are far smaller than forests (up to one hundredth of the size) will offset exactly the same quantity of carbon emissions. Having said that, forests currently absorb close to one-third of humanity's annual carbon emissions, so it's of paramount importance to safeguard this capability and prevent the kind of widescale deforestation that takes place in tropical regions. Moreover, forests play a vital role in the global rain cycle, in maintaining biodiversity and in many other environmental and social contexts. Therefore, the conclusion from our study is that we must protect the Earth's forests, and that the most appropriate solution to the climate crisis is to combine the planting and rehabilitation of forests in humid regions with erecting fields of solar panels in arid regions.""
Prof. Dan Yakir's research is supported by the Helen Kimmel Center for Planetary Science and the Schwartz Reisman Collaborative Science Program. 
Prof. Yakir is the incumbent of the Hilda and Cecil Lewis Professorial Chair.

","score: 16.555545977011494, grade_level: '17'","score: 18.24482183908046, grade_levels: ['college_graduate'], ages: [24, 100]",10.1093/pnasnexus/pgad352,"Suppression of carbon emissions through photovoltaic (PV) energy and carbon sequestration through afforestation provides complementary climate change mitigation (CCM) strategies. However, a quantification of the “break-even time” (BET) required to offset the warming impacts of the reduced surface reflectivity of incoming solar radiation (albedo effect) is needed, though seldom accounted for in CCM strategies. Here, we quantify the CCM potential of PV fields and afforestation, considering atmospheric carbon reductions, solar panel life cycle analysis (LCA), surface energy balance, and land area required across different climatic zones, with a focus on drylands, which offer the main remaining land area reserves for forestation aiming climate change mitigation (Rohatyn S, Yakir D, Rotenberg E, Carmel Y. Limited climate change mitigation potential through forestation of the vast dryland regions. 2022. Science 377:1436–1439). Results indicate a BET of PV fields of ∼2.5 years but &gt;50× longer for dryland afforestation, even though the latter is more efficient at surface heat dissipation and local surface cooling. Furthermore, PV is ∼100× more efficient in atmospheric carbon mitigation. While the relative efficiency of afforestation compared with PV fields significantly increases in more mesic climates, PV field BET is still ∼20× faster than in afforestation, and land area required greatly exceeds availability for tree planting in a sufficient scale. Although this analysis focusing purely on the climatic radiative forcing perspective quantified an unambiguous advantage for the PV strategy over afforestation, both approaches must be combined and complementary, depending on climate zone, since forests provide crucial ecosystem, climate regulation, and even social services."
"
Manatees are endangered species volatile to the environment. Because of their voracious appetites, they often spend up to eight hours a day grazing for food within shallow waters, making them vulnerable to environmental changes and other risks.

Accurately counting manatee aggregations within a region is not only biologically meaningful in observing their habit, but also crucial for designing safety rules for boaters and divers as well as scheduling nursing, intervention, and other plans. Nevertheless, counting manatees is challenging.
Because manatees tend to live in herds, they often block each other when viewed from the surface. As a result, small manatees are likely to be partially or completely blocked from view. In addition, water reflections tend to make manatees invisible, and they also can be mistaken for other objects such as rocks and branches.
While aerial survey data are used in some regions to count manatees, this method is time-consuming and costly, and the accuracy depends on factors such as observer bias, weather conditions and time of day. Moreover, it is crucial to have a low-cost method that provides a real-time count to alert ecologists of threats early to enable them to act proactively to protect manatees.
Artificial intelligence is used in a wide spectrum of fields, and now, researchers from Florida Atlantic University's College of Engineering and Computer Science have harnessed its powers to help save the beloved manatee. They are among the first to use a deep learning-based crowd counting approach to automatically count the number of manatees in a designated region, using images captured from CCTV cameras, which are readily available, as input.
This pioneering study, published Scientific Reports, not only addresses the technical challenges of counting in complex outdoor environments but also offers potential ways to aid endangered species.
To determine manatee densities and calculate their numbers, researchers used generic images captured from surveillance videos from the water surface. They then used a unique design matching to manatees' shape -- Anisotropic Gaussian Kernel (AGK) -- to transform the images into manatee customized density maps, representing manatees' unique body shapes.

Although many methods exist for counting, most of the existing counting methods are applied to crowds to count the number of people, due to their relevance to important applications such as urban planning and public safety.
To save labeling costs, researchers used line-label based annotation with a single straight line to mark each manatee. The goal of the study was to learn to count the number of objects within a scene and obtain labels to support counting.
Results of the study reveal that the FAU-developed method outperformed other baselines, including the traditional Gaussian kernel-based approach. Transitioning from dot to line labeling also improved wheat head counting accuracy, an important role in crop yield estimation, suggesting broader applications for convex-shaped objects in diverse contexts. This approach worked particularly well when the image had a high density of manatees in a complicated background.
By formatting manatee counting as a deep neural network density estimation learning task, this approach balanced the labeling costs vs. counting efficiency. As a result, this method delivers a simple and high throughput solution for manatee counting that requires very little labeling efforts. A direct impact is that state parks can leverage this method to understand the number of manatees in different regions, by using their existing CCTV cameras, in real time.
""There are many ways to use computational methods to help save endangered species, such as detecting the presence of the species and counting them to collect information about numbers and density,"" said Xingquan (Hill) Zhu, Ph.D., senior author, an IEEE Fellow and a professor in FAU's Department of Electrical Engineering and Computer Science. ""Our method considered distortions caused by the perspective between the water space and the image plane. Since the shape of the manatee is closer to an ellipse than a circle, we used AGK to best represent the manatee contour and estimate manatee density in the scene. This allows density map to be more accurate, in terms of mean absolute errors and root mean square error, than other alternatives in estimating manatees' numbers.""
To validate their method and facilitate further research in this domain, the researchers developed a comprehensive manatee counting dataset, along with their source code, published through GitHub for public access at github.com/yeyimilk/deep-learning-for-manatee-counting.

""Manatees are one of the wildlife species being affected by human-related threats. Therefore, calculating their numbers and gathering patterns in real time is vital for understanding their population dynamics,"" said Stella Batalama, Ph.D., dean, FAU College of Engineering and Computer Science. ""The methodology developed by professor Zhu and our graduate students provides a promising trajectory for broader applications, especially for convex-shaped objects, to improve counting techniques that may foretell better ecological results from management decisions.""
Manatees can be found from Brazil to Florida and all the way around the Caribbean islands. Some species including the Florida Manatee are considered endangered by the International Union for Conservation of Nature.
Study co-authors are FAU graduate students Zhiqiang Wang; Yiran Pang; and Cihan Ulus, also a teaching assistant, all within the Department of Electrical Engineering and Computer Science.
The research was sponsored by the United States National Science Foundation.

","score: 15.749930056087102, grade_level: '16'","score: 16.917132959419334, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41598-023-45507-3,"Manatees are aquatic mammals with voracious appetites. They rely on sea grass as the main food source, and often spend up to eight hours a day grazing. They move slow and frequently stay in groups (i.e. aggregations) in shallow water to search for food, making them vulnerable to environment change and other risks. Accurate counting manatee aggregations within a region is not only biologically meaningful in observing their habit, but also crucial for designing safety rules for boaters, divers, etc., as well as scheduling nursing, intervention, and other plans. In this paper, we propose a deep learning based crowd counting approach to automatically count number of manatees within a region, by using low quality images as input. Because manatees have unique shape and they often stay in shallow water in groups, water surface reflection, occlusion, camouflage etc. making it difficult to accurately count manatee numbers. To address the challenges, we propose to use Anisotropic Gaussian Kernel (AGK), with tunable rotation and variances, to ensure that density functions can maximally capture shapes of individual manatees in different aggregations. After that, we apply AGK kernel to different types of deep neural networks primarily designed for crowd counting, including VGG, SANet, Congested Scene Recognition network (CSRNet), MARUNet etc. to learn manatee densities and calculate number of manatees in the scene. By using generic low quality images extracted from surveillance videos, our experiment results and comparison show that AGK kernel based manatee counting achieves minimum Mean Absolute Error (MAE) and Root Mean Square Error (RMSE). The proposed method works particularly well for counting manatee aggregations in environments with complex background."
"
A new study finds that precipitation levels during nesting season are not related to reproductive success for wild turkeys, which runs counter to the conventional wisdom regarding the role that rainfall plays in wild turkey nesting success. The findings shed new light on how climate change may affect wild turkey populations.

""We wanted to know how weather influences nesting success right now, and then use that data to assess how climate change may influence wild turkey populations in the future,"" says Wesley Boone, corresponding author of a paper on the work and a postdoctoral researcher at North Carolina State University.
""Wild turkeys are fairly tolerant of a wide range of conditions, but there are a host of factors that can affect their reproductive success,"" says Chris Moorman, co-author of the study and a professor of forestry and environmental resources at NC State. ""This work focused on two of those conditions, precipitation and temperature, and how they may influence nest survival during the incubation period.""
For the study, researchers focused on daily nest survival, which is whether the eggs in the nest survive any given 24-hour period. Over the course of eight years, researchers monitored 715 turkey nests and collected daily precipitation and temperature data for each nest during the entire incubation period. For temperature, the researchers looked specifically at the extent to which temperatures at each nest varied from historical averages.
The researchers analyzed all of this data to determine the extent to which precipitation and temperature were associated with daily nest survival.
""The most surprising finding was that precipitation during nesting was not a good predictor of daily nest survival,"" Moorman says. ""It had been widely believed that particularly rainy weather made it more likely that eggs wouldn't survive.""
""We also found that temperatures which were higher than historical averages were associated with higher rates of daily nest survival during incubation,"" says Boone. ""Peak nesting season is generally in April, so we're talking about warmer than average spring weather.""
""Taken by itself, this might suggest that climate change could benefit turkey reproductive success and, by extension, turkey populations,"" Moorman says. ""However, we also looked at precipitation and temperature data for the months leading up to nesting season, and at the overall likelihood that a turkey nest will successfully hatch at least one egg. And when we looked at both of those datasets, things get a lot less clear.""
""For example, the data suggest that more precipitation in January -- long before nesting season -- is associated with greater nest survival,"" Boone says. ""The data also suggest that higher temperatures in January are associated with worse nesting survival. But there is so much uncertainty related to those findings that it's not clear whether there's a real relationship there, or if it's an anomaly. However, it does temper any enthusiasm we might have about the likelihood that climate change will benefit turkey populations.""
The research was done with support from the U.S. Geological Survey's Southeast Climate Adaptation Science Center, which is headquartered at NC State; and from the National Institute of Food and Agriculture, under McIntire Stennis Project Number 7001494. Additional support was provided by the Georgia Department of Natural Resources-Wildlife Resources Division, the Louisiana Department of Wildlife and Fisheries, the South Carolina Department of Natural Resources, the North Carolina Wildlife Resources Commission, the National Wild Turkey Federation, the United States Department of Agriculture's Forest Service, the Warnell School of Forestry and Natural Resources at the University of Georgia and the School of Renewable Natural Resources at Louisiana State University.

","score: 15.76330903139414, grade_level: '16'","score: 17.224334176461838, grade_levels: ['college_graduate'], ages: [24, 100]",10.1002/jwmg.22524,"Temperature and precipitation have been identified as factors that potentially influence eastern wild turkey (Meleagris gallopavo silvestris) reproduction, but robust analyses testing the relationship between weather parameters and turkey nest success are lacking. Therefore, we assessed how weather influenced turkey daily nest survival using 8 years of data collected from 715 nests across the southeastern United States. We also conducted exploratory analyses investigating if weather conditions during or prior to nesting best predicted nest success. We then assessed the possible implications of climate change through 2041–2060 for future eastern wild turkey daily nest survival and nest success for variables determined significant in analyses. During incubation, positive anomalies of minimum daily temperature were associated with greater daily nest survival. Precipitation during nesting was not a good predictor of daily nest survival. Exploratory analyses unexpectedly indicated that weather conditions in January prior to incubation were more important to nest success than weather conditions during incubation. In January, negative anomalies of minimum temperature and greater average daily precipitation were associated with greater nest success. Projections of future nest success or daily nest survival based on these relationships with the predictive covariates, and informed by climate models, suggest that nest success may increase as January precipitation increases and that daily nest survival may increase as temperature during incubation increases. These positive associations could be offset by a negative association between nest success and the expected increases in January minimum average temperature. Additional research is needed to investigate causes of these relationships and assess the implications of climate change for eastern wild turkey poult survival."
"
Likening it to providing more runways at busy airports, researchers at North Carolina State University found in a new study that adding protruding rocks to restored streams can help attract female aquatic insects that lay their eggs on the rock bottoms or sides.

More eggs that hatch into larval insects is great news for stream restoration because the re-establishment of organisms, such as insects, is often slower than expected in restored streams, says Brad Taylor, associate professor of applied ecology at NC State and corresponding author of a paper describing the research. A thriving population of stream insects generally portends good water quality, overall stream health, and provides food for fish, amphibians, reptiles, and even birds, he adds.
Most stream insects use rocks protruding above the water as runways to land on, then crawl underwater and attach their eggs to the underside of the rocks. Because restored streams sometimes fail to regain their abundance of aquatic insects even decades following restoration, researchers were interested in testing whether increasing egg-laying habitat the rock landing areas would increase the abundance and diversity of insect eggs and larvae.
Taylor and NC State graduate student Samantha Dilworth selected 10 restored streams in northwestern North Carolina and added protruding rocks gathered near the streams to five of them; the other five restored streams did not receive additional rocks.
The results showed that the streams with added protruding rocks had almost twice as many egg masses compared to the untreated streams. After adding rocks, egg numbers in treated streams were similar to those in undisturbed streams downstream of state parks and national forests.
""This study was a successful proof of concept: adding rocks to restored streams enhances the abundance and diversity of stream insect eggs,"" Taylor said.
The researchers also discovered that some rocks were more attractive to females and received most of the eggs; many rocks receive few or no eggs.

""The insects are really putting all their eggs in one basket, so to speak, or onto a few rocks,"" Taylor says. ""A future goal is to figure out how and why females select specific rocks, so that any rocks added to streams get used, because moving large rocks around a stream is hard work.""
The study also showed that the response of the larval stages of streams insects was weaker than expected based on the increase in eggs in restored streams.
""We can restore aquatic insect eggs by adding rocks, but we didn t see a consistent increase of the larval insect stages,"" Taylor said. ""The second year of the study was one of the wettest in recent years, so a lot of the rocks rolled. Insects in restored streams with no added rocks also declined, while those with added rocks did not change or increased, but not as much as expected.
""We observed that many rocks rolled, which could have caused eggs to be crushed or exposed to air where they would dry out and die. These sources of egg mortality may explain the weaker increase of larval insects, as some eggs may have never hatched into larvae. Also, higher larval mortality in restored streams could have been caused by factors unrelated to disturbance of the rock egg-laying runways. For example, priority or primacy effects by established insects or other organisms could be excluding newly arriving insects, food resources could be limiting larval insects, or other disturbances of larval habitats could be occurring.""
Taylor plans to follow up with research on how best to stabilize protruding rocks using hydrologic modeling to determine the best locations. To extend the runway metaphor, what will make rocks more like O'Hare International Airport runways than a local grass or gravel airstrip?
""If we can get this information to restoration practitioners, they can add rocks that are both more attractive to female insects and stable,"" Taylor said. ""This will make restoration efforts more cost efficient and effective. There are miles of restored streams, so there are lots of rocks needed. We want to make sure every rock gets eggs.""
The study appears in Ecological Applications. The study was funded by the North Carolina Dept. of Environmental Quality.

","score: 12.66183174370262, grade_level: '13'","score: 14.061385179750552, grade_levels: ['college_graduate'], ages: [24, 100]",10.1002/eap.2939,"Recruitment limitation is known to influence species abundances and distributions. Recognition of how and why it occurs both in natural and in designed environments could improve restoration. Aquatic insects, for instance, rarely reestablish in restored streams to levels comparable to reference streams even years after restoration. We experimentally increased oviposition habitat in five out of 10 restored streams in western North Carolina to test whether insect egg‐laying habitat was limiting insect populations in restored streams. A main goal was to test whether adding oviposition habitat in the form of rocks that partially protrude above the water surface could be used to increase the abundance and richness of stream insect eggs and larval insects in restored streams. Adding egg‐laying habitat enhanced several response variables (e.g., protruding rocks, number of eggs, egg masses, egg morphotype richness, and oviposition habitat stability) to levels similar to those found in reference streams. Following the addition of protruding rocks, egg mass abundance increased by 186% and richness by 77% in restored‐treated streams. Densities of larval insects that attached their eggs to protruding rocks showed an overall pattern consistent with treatment effects due to the combination of nonsignificant and significant increases of several taxa and not just one taxon. Our results indicate that these stream insect populations are limited by oviposition habitat and that adding egg‐laying habitat alleviated this component of recruitment limitation. However, the weaker larval response indicates that additional post‐recruitment factors, such as egg or larval mortality, may still be limiting a full recovery of larval insect abundances in these restored streams. This study shows the importance of integrating information from animal life histories, ecology, and geomorphology into restoration practices to improve the recovery of aquatic insects, which are commonly used to assess water quality and the biological efficacy of stream restoration."
"
In devasting cases dotting the globe in recent years, climate warming has led to an increase in the number and severity of destructive wildfires. Climate change projections indicate that environmental and economic damage from wildfires will spread and escalate in the years ahead.

While studies have analyzed impacts on land, new research from the University of California San Diego and other institutions indicates that aquatic ecosystems are also undergoing rapid changes as a result of wildfires.
Led by School of Biological Sciences Professor Jonathan Shurin's laboratory, the researchers compared how aquatic systems change with the input of burnt plant matter, including effects on food webs. Their results are featured in two research studies published in the journal Global Change Biology.
Among the findings emerging from the research, scientists show that fire chemically transforms plant debris and changes the role of aquatic ecosystems as key players in the carbon cycle. The shifts point to a fundamental change in the way these aquatic systems store, process and emit carbon.
The findings also are important since aquatic ecosystems serve as sinks that capture water flows and store carbon in their sediments.
""The effects of wildfires are not limited to terrestrial systems,"" said Postdoctoral Scholar Chris Wall, a member of Shurin's group and first author of one of the studies. ""When we think about wildfires increasing, especially in the West, it's important to remember that burned materials flow directly into waterways that are vital for people and wildlife. We're now recognizing that wildfires can greatly influence ecosystem health, with implications for water resources, like aquifers and recreational fishing.""
The findings emerged from a series of experiments conducted at UC San Diego, and carry implications for aquatic ecosystems in areas such as the Sierra Nevada mountains -- where Shurin's group conducts research -- and other regions.

""We've seen the impact that these huge fires have had on watersheds, so we're working in these natural systems to understand how different components of climate change are altering the ecosystems,"" said Shurin, a faculty member in the Department of Ecology, Behavior and Evolution.
Many normally functioning lake and pond ecosystems tend to emit more carbon dioxide than they absorb since they receive carbon into their system from neighboring sources. The new study showed that this relationship could change with the increased input of burned wildfire materials. The study found that ponds receiving burned materials had overall less carbon dioxide emissions relative to unburned material, indicating a shift toward greater carbon storage.
""Burned plant matter fuels the biological carbon pump of lakes, allowing them to soak up more CO2 from the atmosphere,"" said Shurin. ""However, this capacity for increased carbon storage was lost as the amount of burned material increased, with treatments receiving the greatest amounts of burned plant material exhibiting highest CO2 export to the atmosphere.""
""More frequent and intense wildfire may alter the capacity of aquatic systems to store, transform and exchange carbon with the atmosphere,"" the researchers conclude in the paper. They note that in the future, forecasts of climate change should include integrative models that account for feedbacks between aquatic and terrestrial ecosystems in order to fully understand changes to the global carbon cycle.
The study was conducted on experimental pond systems across a 90-day testing period. The researchers tested various amounts of burned and unburned plant matter at 10, 31, 59 and 89 days. As part of their studies, the researchers fertilized sage plants with nitrogen so they could track the chemical's movement from plant leaves into the food web and into hosts such as plankton. This labeling allowed them to track the path dead plants follow through plankton and other aquatic species and determine how this transfer of nitrogen differed in response to burning.
""By using the nitrogen tracer in plant materials, we found less burned plant-derived nitrogen was being incorporated by zooplankton, indicating that burning reduced the transfer of nitrogen to higher organisms,"" said Wall. ""This agreed with other findings, which showed burned treatments had lower carbon dioxide concentrations, greater oxygenation and higher rates of photosynthesis relative to unburned treatments.""
""Burning changes the chemistry of leaves and that affects their cycling through freshwater ecosystems,"" said Shurin.

As the influence of burnt matter rose, the experimental ponds shifted in the makeup of their inhabitants. Unburned test ponds displayed species characteristic of aquatic systems such as zooplankton. Ponds with heavy loads of burned material, on the other hand, transformed into havens for insects such as mosquitoes.
""These impacts were shifted by fire treatment,"" the researchers noted in their report. ""Burning increased the elemental and organic composition of detritus, with cascading effects on ecosystem function.""
Authors of the study include: Christopher Wall (UC San Diego postdoctoral scholar), Cody Spiegel (UC San Diego master's student), Evelyn Diaz (UC San Diego undergraduate student), Cindy Tran (UC San Diego undergraduate student), Alexia Fabiani(UC San Diego PhD student), Taryn Broe(UC San Diego PhD student), Elisabet Perez-Coronel (former UC San Diego postdoctoral scholar), Sara Jackrel (UC San Diego assistant professor), Natalie Mladenov (San Diego State University professor), Celia Symons (UC Irvine professor) and Jonathan Shurin (UC San Diego professor).
The study was supported by the National Science Foundation (award number 2018058).

","score: 15.100180995475117, grade_level: '15'","score: 16.995475113122176, grade_levels: ['college_graduate'], ages: [24, 100]",10.1111/gcb.17058,"Fire can lead to transitions between forest and grassland ecosystems and trigger positive feedbacks to climate warming by releasing CO2 into the atmosphere. Climate change is projected to increase the prevalence and severity of wildfires. However, fire effects on the fate and impact of terrestrial organic matter (i.e., terrestrial subsidies) in aquatic ecosystems are unclear. Here, we performed a gradient design experiment in freshwater pond mesocosms adding 15 different amounts of burned or unburned plant detritus and tracking the chronology of detritus effects at 10, 31, 59, and 89 days. We show terrestrial subsidies had time‐ and mass‐dependent, non‐linear impacts on ecosystem function that influenced dissolved organic carbon (DOC), ecosystem metabolism (net primary production and respiration), greenhouse gas concentrations (carbon dioxide [CO2], methane [CH4]), and trophic transfer. These impacts were shifted by fire treatment. Burning increased the elemental concentration of detritus (increasing %N, %P, %K), with cascading effects on ecosystem function. Mesocosms receiving burned detritus had lower [DOC] and [CO2] and higher dissolved oxygen (DO) through Day 59. Fire magnified the effects of plant detritus on aquatic ecosystem metabolism by stimulating photosynthesis and respiration at intermediate detritus‐loading through Day 89. The effect of loading on DO was similar for burned and unburned treatments (Day 10); however, burned‐detritus in the highest loading treatments led to sustained hypoxia (through Day 31), and long‐term destabilization of ecosystem metabolism through Day 89. In addition, fire affected trophic transfer by increasing autochthonous nitrogen source utilization and reducing the incorporation of 15N‐labeled detritus into plankton biomass, thereby reducing the flux of terrestrial subsidies to higher trophic levels. Our results indicate fire chemically transforms plant detritus and alters the role of aquatic ecosystems in processing and storing carbon. Wildfire may therefore induce shifts in ecosystem functions that cross the boundary between aquatic and terrestrial habitats."
"
Brigham researchers' findings support developing public health interventions that incorporate components of environmental health literacy alongside cancer screening efforts.

The world's climate crisis has wide ranging implications for human health. But how do our perceptions about climate change influence our intentions when it comes to personal health? A new study by investigators from Brigham and Women's Hospital, a founding member of the Mass General Brigham healthcare system, used the National Cancer Institute's annual Health Information and National Trends Survey (HINTS) to analyze adults' views on climate change and their interest in cancer screening. The study found that individuals who saw climate change as a personal health threat were also more likely to endorse interest in cancer screening. The results are published in the Journal of the National Cancer Institute. 
""Our findings suggest that individuals who are more aware of the potential health impacts of climate change may also be more overall health conscious, which may drive their interest in preventive healthcare measures, such as cancer screening,"" said senior author Alexander P. Cole, MD, assistant professor in the Department of Urology and the Center for Surgery and Public Health at Brigham and Women's Hospital. ""Awareness of climate change impacts could also prompt individuals to take more proactive steps to protect their health in the face of environmental threats.""
Previous studies have uncovered both direct and indirect links between cancer and climate change. Certain climate change consequences, such as ozone depletion and the emergence of more environmental carcinogens, can cause cancer. And extreme weather events can disrupt cancer care as well as access to health care in general.
In the current study, Cole and colleagues analyzed responses from adults who completed the HINTS survey in 2021. Each respondent's perceived risk of climate change to their personal health was categorized as ""no harm"" or ""little harm"" versus ""some harm or ""a lot of harm."" Researchers also assessed each respondent's interest in getting screened for cancer the following year, categorized as ""not at all"" and ""a little"" versus ""somewhat"" and ""very.""
The study found that 54 percent of survey respondents felt that climate change would cause ""some"" or ""a lot"" of harm to their health. Respondents also showed 73 percent higher odds of being interested in cancer screening when they felt that climate change posed ""some"" risk to their health and 84 percent higher odds when they perceived that climate change could harm their health ""a lot.""
Other findings showed that respondents who were younger, female, and more educated were more likely to perceive climate change as a health threat. And that non-Hispanic Black and Hispanic populations expressed a higher interest in cancer screening than non-Hispanic White populations.

""We see room for improvement in climate change awareness overall as well as some racial disparities underlying some of the differences in awareness,"" said first author Zhiyu Qian, MD, a urology resident in the Department of Urology and research fellow at the Center for Surgery and Public Health at Brigham and Women's Hospital. ""There are a lot of ways to interpret this and one could be that more vulnerable populations are already feeling more of the impacts from climate change.""
The authors note that their study is retrospective and does not prove causation, but future studies could explore whether climate change awareness directly impacts screening behavior. The research team is continuing to investigate specific ways that cancer care can be impacted by climate change, such as in the relationship between extreme weather events and cancer care. They are also working to improve sustainability in cancer care and adaptability of the healthcare system, such as by increasing the accessibility and quality of telehealth.
Moreover, addressing the root causes of climate change can yield advantages in lowering cancer risk. A prominent example is the reduction of meat consumption, which not only diminishes greenhouse gas emissions but also mitigates a significant cancer risk factor. Additionally, lifestyle modifications like active commuting and preserving greenspace have been associated with decreased cancer risks, while also supporting environmental health.
""Cancer care is multidisciplinary and extremely complex. You need a well-functioning healthcare system for it to work,"" said Cole. ""Raising awareness is a big piece as is changing the healthcare system. There are so many exciting opportunities to do this through public awareness, healthcare delivery, and lifestyle and diet modifications where you can do things that are great for planetary health as well as for patients' health and cancer risk.""
Authorship: Additional authors include Edoardo Beatrici, Quoc-Dien Trinh, Adam S. Kibel, Stacy Loeb, and Hari S. Iyer.
Disclosures: QDT reports personal fees from Astellas, Bayer, and Janssen, outside the submitted work. QDT reports research funding from the American Cancer Society, the Defense Health Agency, and Pfizer Global Medical Grants. APC reports research funding from the American Cancer Society and Pfizer Global Medical Grants.
Funding: APC reports research funding from the American Cancer Society and Pfizer Global Medical Grants (Prostate Cancer Disparities #63354905), by the Bruce A. Beal and Robert L. Beal surgical fellowship of the Brigham and Women's Hospital Department of Surgery and a Physician Research Award from the Department of Defense (#PC220342).

","score: 14.418571428571429, grade_level: '14'","score: 16.098687651331723, grade_levels: ['college_graduate'], ages: [24, 100]",10.1093/jnci/djad237,"As the climate crisis deepens, its adverse effects on human health are becoming evident, including impacts on cancer pathogenesis and treatment. This study explored the link between individuals’ awareness of the health impacts of climate change and interest in cancer screening. Using the 2021 Health Information National Trends Survey, our study demonstrated a statistically significant association between recognition of climate change as a personal health threat and interest in cancer screening. Although the study’s retrospective nature and self-reported data pose some limitations, these findings signal a promising avenue for future research on the intersection of climate and cancer risk. This research supports the development of public health interventions that incorporate components of environmental health literacy alongside cancer screening efforts."
"
A climate policy that raises the price of carbon-intensive products across the entire U.S. economy would yield a side benefit of reducing nitrate groundwater contamination throughout the Mississippi River Basin.

The Gulf of Mexico, an important U.S. fishery, also would see modest benefits from the nitrate reductions. These were among the conclusions of a recent study published in the Proceedings of the National Academy of Sciences (PNAS).
The study, led by four early career researchers, three of them from Purdue University, combined four scientific models to simulate how different aspects of climate policy, agricultural economics and the environment would interact. A major feature of the study traced how nitrogen that humans have converted into other forms, called reactive nitrogen, flows through the environment.
""Food production is generating a lot of the reactive nitrogen. Some comes from energy as well,"" said study co-author Thomas Hertel, Distinguished Professor of Agricultural Economics at Purdue. ""This is a big problem. The reactive nitrogen ends up in rural groundwater. Some of it ends up in the atmosphere. And of course, it ends up in the streams and eventually the Gulf of Mexico.""
Excessive fertilizer use creates a growing number of water-quality concerns, said Shan Zuidema, a research scientist at the University of New Hampshire's Earth Systems Research Center. ""Our models showed that with this climate policy, U.S. carbon emissions could significantly decline, which would translate into about a 3% to 4% reduction of the Gulf of Mexico dead zone in an average year.""
One of the study's four linked models analyzed various issues related to climate change economics. Another model linked local policies to national and international agriculture prices, land use and the environment. A third produced an estimate of nitrate leaching in the entire Mississippi River Basin based on changes in fertilization rates and land cover. The fourth simulated vertical water exchange between the ground and atmosphere and horizontal transport through runoff and stream networks.
Central to the study was the ""nitrogen cascade"" concept that the University of Virginia's James Galloway and his co-authors published in 2003. The concept states that a single nitrogen atom may trigger a cascade of effects in the atmosphere, terrestrial ecosystems, freshwater and marine systems, and on human health.

The PNAS study documented how much nitrogen gets harvested with the remainder getting put into the environment.
Galloway, UVA's Sidman P. Poole Professor Emeritus of Environmental Sciences, who was not a co-author of the PNAS study, noted that for more than 100 years, an abundant supply of nitrogen has fueled agricultural production.
""Tracking these nitrogen losses, and trying to mitigate them, requires an approach that links economics, agroecology and hydrology to ensure that actions at one point in the chain do not have unintended consequences at another point."" The PNAS paper ""is an excellent example of the type of work that is needed,"" Galloway said.
The study included an assessment of policy outcomes that assigned the social cost of carbon to estimates ranging from $51 to $152 per ton of carbon dioxide equivalents.
""Fertilizer is most affected because ammonia fertilizer is largely converted natural gas,"" Hertel said. The highest carbon price in the models reduced U.S. carbon emission by almost 50%. Coupled with an increase in nitrogen fertilizer prices, this reduced fertilizer applications by about 15% for corn production across the Mississippi River Basin.
The team also considered a scenario that restored wetlands in the central U.S. Corn Belt without a climate policy. The farmers in the area grew less corn and applied less fertilizer. But that prompted increased fertilizer applications in the untreated regions, resulting in negative spillover.
""It seems like every environmental policy we look at has spillovers. They haven't been considered before, but when you think about the economics and what we call market-mediated spillovers, they're pervasive,"" said Hertel, who founded and is executive director of Purdue's Global Trade Analysis Project (GTAP). Hertel and co-authors include a discussion of spillovers in a recent special issue of Environmental Research Letters that focuses on global-to-local-to-global sustainability analysis challenges.
The PNAS study looked only at carbon pricing and wetlands, but many other food system interventions lend themselves to similar analyses, including how a change in our diets could affect the nitrogen cascade. ""How about reducing food waste, or reducing ethanol production? Or improving nitrogen use efficiency?"" Hertel asked.
In addition to Hertel, the Purdue co-authors are: Jing Liu, research economist in agricultural economics;Maksym Chepeliev, principal research economist at GTAP; David Johnson, associate professor of industrial engineering and political science; and Uris Baldos, research associate professor of agricultural economics.

","score: 13.998724489795919, grade_level: '14'","score: 14.0037869226638, grade_levels: ['college_graduate'], ages: [24, 100]",10.1073/pnas.2302087120,"We utilize a coupled economy–agroecology–hydrology modeling framework to capture the cascading impacts of climate change mitigation policy on agriculture and the resulting water quality cobenefits. We analyze a policy that assigns a range of United States government’s social cost of carbon estimates ($51, $76, and $152/ton of CO 2 -equivalents) to fossil fuel–based CO 2 emissions. This policy raises energy costs and, importantly for agriculture, boosts the price of nitrogen fertilizer production. At the highest carbon price, US carbon emissions are reduced by about 50%, and nitrogen fertilizer prices rise by about 90%, leading to an approximate 15% reduction in fertilizer applications for corn production across the Mississippi River Basin. Corn and soybean production declines by about 7%, increasing crop prices by 6%, while nitrate leaching declines by about 10%. Simulated nitrate export to the Gulf of Mexico decreases by 8%, ultimately shrinking the average midsummer area of the Gulf of Mexico hypoxic area by 3% and hypoxic volume by 4%. We also consider the additional benefits of restored wetlands to mitigate nitrogen loading to reduce hypoxia in the Gulf of Mexico and find a targeted wetland restoration scenario approximately doubles the effect of a low to moderate social cost of carbon. Wetland restoration alone exhibited spillover effects that increased nitrate leaching in other parts of the basin which were mitigated with the inclusion of the carbon policy. We conclude that a national climate policy aimed at reducing greenhouse gas emissions in the United States would have important water quality cobenefits."
"
A 20-year experiment in the Sierra Nevada confirms that different forest management techniques -- prescribed burning, restoration thinning or a combination of both -- are effective at reducing the risk of catastrophic wildfire in California.

These treatments also improve forest health, making trees more resilient to stressors like drought and bark beetles, and they do not negatively impact plant or wildlife biodiversity within individual tree stands, the research found. The findings of the experiment, called the Fire Surrogate Study, were published today in the journal Ecological Applications.
""The research is pretty darn clear that these treatments are effective -- very effective,"" said study lead author Scott Stephens, a professor of fire science at the University of California, Berkeley.""I hope this lets people know that there is great hope in doing these treatments at scale, without any negative consequences.""
Last year, California announced a strategic plan for expanding the use of prescribed fire to 400,000 acres annually by 2025. However, the use of beneficial fire continues to be hindered by multiple factors, including the lack of a trained workforce, the need for specific weather conditions for burning, and fears about potential risks.
This study shows that restoration thinning is also a viable option for forest management and can be used in tandem with beneficial fire without harming forest health or biodiversity.
""Our findings show that there's not just one solution -- there are multiple things that you can do to impact the risk of catastrophic fire,"" said study co-author Ariel Roughton, research station manager at Berkeley Forests. ""Folks can choose from different combinations of treatments that might fit their needs, and we can show them how those treatments might impact things like wildfire behavior, tree growth and carbon holding in their forests.""
Surrogates to wildfire 
Over the past two decades, Stephens and other researchers at Berkeley Forests have used prescribed burning, restoration thinning or a combination of both to treat plots of land at Blodgett Forest Research Station, a 4,000-acre experimental forest located about 65 miles northeast of Sacramento on the unceded lands of the Nisenan peoples.

The Fire Surrogate Study was one of 13 studies across the U.S. first launched in 1999 with funding from the U.S. Joint Fire Science Program. Its aim was to study whether the two treatments could mimic the beneficial impacts of lightning fires and Indigenous burning practices on California's forests, which have become dense and overgrown after a century of logging and fire suppression.
""Prescribed fire and restoration thinning are both surrogates for wildfire, a key process that happened frequently in California before European colonization,"" Stephens said. ""The impetus of this study was: If you're going to implement these treatments at a large scale, is there anything that's going to be lost?""
The study created nine experimental plots and three control plots at Blodgett. Three of the experimental plots were managed only using prescribed burns; three burns occurred over the course of 20 years. Three other experimental plots were first thinned and then burned, and the final three were treated only with restoration thinning. The control plots were left to grow without human interference except continued fire suppression.
At the end of the 20-year period, the researchers surveyed the vegetation in each plot and used computational modeling to estimate how many trees were likely to survive wildfire. They found that all three types of experimental plots were significantly more resilient to wildfire than the control plots, showing an 80% likelihood that at least 80% of trees would survive.
They also calculated the ""index of competition,"" a measure of how strongly trees must compete for resources like sunlight, water and soil nutrients. By removing excess trees and vegetation, thinning and burning both limited the amount of competition between trees, making them less vulnerable to stressors, like drought and bark beetles.
However, the plots that were treated with a combination of thinning and fire had the best index of competition, suggesting that they would be the most resilient to the impacts of climate change.

""When you combine thinning with fire, you're able to modify all different levels of the forest structure,and it speeds up the timeline for achieving a more resilient structure,"" Roughton said.
Restoration thinning can also provide financial benefits: Often, larger trees can be sold to sawmills, and the proceeds can be used to help offset the cost of forest management. Over the course of 20 years, the treatments at Blodgett were entirely paid for by revenue from timber.
""When I go to Sacramento and talk about [forest management] with legislators, the first question they always ask is about cost,"" Stephens said. ""People in the state government are telling us that they can't be the sole source support for this work. That's why the economicsare so important.""
Trial by fire
In September 2022, the forests at Blodgett were subjected to a real-life test: On the morning of Sept. 9, 2022, the Mosquito Fire breached the north side of the property, burning approximately 300 acres before it was contained two days later.
One of the study's control plots was located directly in the path of the blaze, and more than 60% of the trees in this plot were completely scorched. However, neighboring experimental plots that had been treated with prescribed burns served as ""fuel breaks,"" burning less hot than the control and acting as staging areas for firefighters.
""We think that, overall, our management actions, coupled with the weather, did have a pretty big impact on the behavior of the fire,"" Roughton said.
The researchers have received a four-year grant from the Joint Fire Science Program to continue the Fire Surrogate Project. With the help of the grant, they have established a new control plot to replace the one that burned and plan to apply a fourth fire to the experimental burn-only plots.
They are also collaborating with the United Auburn Indian Community to reestablish Indigenous cultural burning at Blodgett.
""We want to be part of the solution, and that's part of our mission at Blodgett,"" Roughton said. ""We hope that by doing these studies and bringing folks here to see the effects of the different treatments, they will take that back and apply it to the land that they're going to be managing.""
Additional co-authors of the study include Daniel E. Foster, John J. Battles, Alexis A. Bernal, Brandon M. Collins, Rachelle Hedges and Robert A. York of UC Berkeley and Jason J. Moghaddas of the Spatial Informatics Group. This project was originally funded by the U.S. Joint Fire Science Program, and it has received additional support from the California Fourth Climate Change Assessment, the McIntire-Stennis Program, the California Greenhouse Gas Reduction Fund, and the UC Office of the President's UC Laboratory Fees Research Program. Smart Practices and Architecture for Prescribed Fires in California was also important to keeping this long-term project active.

","score: 13.96990840421103, grade_level: '14'","score: 15.944240766073868, grade_levels: ['college_graduate'], ages: [24, 100]",10.1002/eap.2932,"Fire suppression and past selective logging of large trees have fundamentally changed frequent‐fire‐adapted forests in California. The culmination of these changes produced forests that are vulnerable to catastrophic change by wildfire, drought, and bark beetles, with climate change exacerbating this vulnerability. Management options available to address this problem include mechanical treatments (Mech), prescribed fire (Fire), or combinations of these treatments (Mech + Fire). We quantify changes in forest structure and composition, fuel accumulation, modeled fire behavior, intertree competition, and economics from a 20‐year forest restoration study in the northern Sierra Nevada. All three active treatments (Fire, Mech, Mech + Fire) produced forest conditions that were much more resistant to wildfire than the untreated control. The treatments that included prescribed fire (Fire, Mech + Fire) produced the lowest surface and duff fuel loads and the lowest modeled wildfire hazards. Mech produced low fire hazards beginning 7 years after the initial treatment and Mech + Fire had lower tree growth than controls. The only treatment that produced intertree competition somewhat similar to historical California mixed‐conifer forests was Mech + Fire, indicating that stands under this treatment would likely be more resilient to enhanced forest stressors. While Fire reduced modeled wildfire hazard and reintroduced a fundamental ecosystem process, it was done at a net cost to the landowner. Using Mech that included mastication and restoration thinning resulted in positive revenues and was also relatively strong as an investment in reducing modeled wildfire hazard. The Mech + Fire treatment represents a compromise between the desire to sustain financial feasibility and the desire to reintroduce fire. One key component to long‐term forest conservation will be continued treatments to maintain or improve the conditions from forest restoration. Many Indigenous people speak of “active stewardship” as one of the key principles in land management and this aligns well with the need for increased restoration in western US forests. If we do not use the knowledge from 20+ years of forest research and the much longer tradition of Indigenous cultural practices and knowledge, frequent‐fire forests will continue to be degraded and lost."
"
At the end of the last ice age, large herds of bison roamed across Europe. But by 1927, the European bison became extinct in the wild, with only about 60 individuals remaining in captivity. Scientists have long debated the exact causes of the grazers' near extinction, and how much humans were to blame.

A new study combines fossil evidence, ancient DNA, and modeling to disentangle the threats that forced the European bison's population decline. Rapid environmental change and hunting by humans were the main drivers, according to the study, published today in the journal Proceedings of the Royal Society B: Biological Sciences.
Since the near-extinction of the European bison, enormous conservation efforts have helped to restore wild populations, and its numbers are on the rise. However, the study authors argue that ensuring the species's long-term protection and recovery requires understanding why they nearly went extinct in the first place.
""Our study also suggests areas where rewilding attempts are most likely to be successful,"" said lead author July Pilowsky, currently a disease ecologist at Cary Institute of Ecosystem Studies. Pilowsky completed the research while working on their PhD at University of Adelaide and University of Copenhagen.
Reconstructing the bison's decline
""This study is one of the few attempts to reconstruct the process of species extinction,"" explained co-author Rafał Kowalczyk, who heads the Mammal Research Institute at the Polish Academy of Sciences. ""It is a fascinating story of a species that is the last remnant of the legendary megafauna that inhabited the European continent in the late Pleistocene, and a lesson in how easy it is to wipe out a species.""
To clarify which factors led to the European bison's decline, Pilowsky and colleagues built a detailed simulation that combined paleoclimate data, vegetation and habitat information, the population growth and expansion of Palaeolithic humans across Eurasia, and bison population and dispersal dynamics. Historical records, fossil evidence, and ancient DNA were used to independently test the model's accuracy.

Researchers ran 55,000 different simulations to explore how climate, hunting by humans, and land use change affected bison population and distribution across Europe. Toggling off different variables one at a time allowed the scientists to test the importance of each variable. If human removal of forests was turned down to zero, for example, and there was no change in bison abundance and range, then the scientists would conclude that land use change likely was not a factor in the species' demise.
After reconstructing 21,000 years of European bison range dynamics, the team concluded that the bison's range began collapsing around 14,700 years ago due to rapid warming of the climate and its effects on bison habitat. After that, the activities of a growing number of humans prevented the bison from bouncing back. Hunting caused range loss in the north and east of its distribution, while land use change was responsible for losses in the west and south. The arrival of firearms in the 1500s dramatically hastened the species's decline.
""The stories of the past are being repeated in the present,"" said Kowalczyk. Land use change, poaching, and climate change are the very same threats that imperil the European bison today. ""Learning from the past, and understanding the process of species extinction, can help us better protect the species.""
Maximizing conservation efforts
The European bison is a priority species for conservation because it serves an important role as an ecosystem engineer, restoring grassland habitat. Thanks to efforts to reestablish and rewild the species, there are approximately 7,300 free-ranging European bison today. However, rewilding has been done without a strong understanding of habitats and regions where bison once thrived. As a result, the species has been released at sites ranging from the coastal dunes of the Netherlands to the mountains of the French Alps and the Mediterranean climate of Spain, with mixed success. Of the 47 free-living European bison populations, only eight have more than 150 adults, and all eight depend on supplemental feeding due to poor habitat suitability.
By identifying areas where the European bison would be distributed today if hunting and land use change had not occurred, the study zeroes in on regions that are most suitable for the species's reintroduction. These include parts of Poland, Ukraine, and western Russia.

With habitat restoration, parts of the Balkans and Germany also have the potential to be good sites for bison reintroduction.
""I hope that the maps we've produced can help inform future efforts in terms of where reintroduction efforts should occur,"" said Pilowsky. ""It's especially crucial because of the war in Ukraine. Over 50 percent of all free-living bison are in Ukraine, and conservationists are really worried.""
Next steps
Pilowsky next hopes to adjust the simulations to identify sites that are not only suitable for European bison today, but also in the future under human-caused climate and environmental change.
The study's methodology could also be adapted to reconstruct the causes of population declines and range collapses of other large herbivores, including American bison, to improve awareness of past threats and enrich current conservation plans.
Lessons learned are informing new lines of inquiry for Pilowsky. At Cary Institute, they are translating the bison simulation code into new software that models disease transmission in wildlife. ""I literally have my bison code open on one monitor and my new code that I'm building on another monitor,"" they said. Instead of simulating bison abundance and range, the new software shows the prevalence and distribution of a disease in a species over time.

","score: 13.69798275862069, grade_level: '14'","score: 14.684331896551726, grade_levels: ['college_graduate'], ages: [24, 100]",10.1098/rspb.2023.1095,"European bison ( Bison bonasus ) were widespread throughout Europe during the late Pleistocene. However, the contributions of environmental change and humans to their near extinction have never been resolved. Using process-explicit models, fossils and ancient DNA, we disentangle the combinations of threatening processes that drove population declines and regional extinctions of European bison through space and across time. We show that the population size of European bison declined abruptly at the termination of the Pleistocene in response to rapid environmental change, hunting by humans and their interaction. Human activities prevented populations of European bison from rebounding in the Holocene, despite improved environmental conditions. Hunting caused range loss in the north and east of its distribution, while land use change was responsible for losses in the west and south. Advances in hunting technologies from 1500 CE were needed to simulate low abundances observed in 1870 CE. While our findings show that humans were an important driver of the extinction of the European bison in the wild, vast areas of its range vanished during the Pleistocene–Holocene transition because of post-glacial environmental change. These areas of its former range have been climatically unsuitable for millennia and should not be considered in reintroduction efforts."
"
Why did professional skateboarding arise in southern California in the 1970s? Was it a coincidence, or was it a perfect storm of multiple factors?

It's fairly well-known that a drought in southern California in the mid-1970s led to a ban on filling backyard swimming pools, and these empty pools became playgrounds for freestyle skateboarders in the greater Los Angeles area. But a new cross-disciplinary study from the University of Cambridge shows that beyond the drought, it was the entanglement of environmental, economic and technological factors that led to the explosive rise of professional skateboarding culture in the 1970s.
The authors say that professional skateboarding could not have started anywhere else, at any other point in time. Their study, reported in the journal PNAS Nexus, shows how small environmental changes can have profound effects on human behaviour, and stimulate cultural and technical innovation. Even the rise of popular pastimes such as skateboarding are the result of the deep relationships between humans and the climate.
""We often look into the past to make associations between climate and society, but as we go further back in time, the evidence for those associations gets thinner,"" said lead author Professor Ulf Büntgen from Cambridge's Department of Geography. ""We wanted to find a more modern example that can show how climate affects human behaviour, where we had lots of data to look at, which is how we ended up studying skateboarding.""
California has a highly variable climate, and in the 1970s, it experienced a period of prolonged drought. Although this period of drought was not exceptional when looking at conditions over a thousand-year period, it was exceptional in the short-term: 1977 was California's driest year of the 20th century. The Colorado and Sacramento Rivers, which are vital to the state's water supply, were both at exceptionally low levels.
The drought resulted in estimated losses of $3 billion in California's massive agricultural sector, and the state's reservoir storage reached a record low in 1977. The state's water agencies responded by mandating severe cuts, including a ban on filling backyard swimming pools.
While swimming pools are almost synonymous with California today, in the 1970s, they were still relatively new to many families. The widespread economic prosperity of post-World War II America, combined with radical changes in urban planning regulations, resulted in the construction of more than 150,000 swimming pools in California during the 1960s.

Kidney-shaped pools were particularly trendy during this period: up to 20,000 of these pools were installed per year in the greater Los Angeles region, accompanying a housing boom of single-family home construction. Soon, the new, curved-walled swimming pools in the suburbs of LA accounted for 60% of all pools in California.
When the California drought took hold in the 1970s, many of these kidney-shaped pools were empty, making them ideal playgrounds for freestyle skateboarders in the LA area. Skateboarding had been a hobby for teens since the 1950s and 1960s, but in the 1970s, the freestyle scene exploded in popularity. Freestyle borrowed much of its style from surfing, and California was the epicentre of US surf culture.
""The popularity and influence of surf culture was completely vital to the rise of skateboard culture, which is why it could have only happened in southern California,"" said Büntgen. ""You could have had the same drought, the same pools in somewhere like Phoenix, but since Phoenix doesn't have an embedded surf culture, professional skateboarding couldn't have originated there.""
Another ingredient in the rise of professional skateboarding came from the chemical industry, which began industrial production of polyurethane in the 1950s, underpinning a revolution in sports equipment. By the 1970s, polyurethane was being used for skateboard wheels, giving them excellent grip and toughness. The polyurethane wheels allowed skaters to make faster turns at higher speeds than they could with earlier steel wheels.
The combination of improved equipment, the influence of surf culture, and empty kidney-shaped pools, among many other factors, led to the rise of what was initially referred to as 'vertical' skating. The empty pools allowed skaters to land gravity-defying tricks, and professional skating teams such as the Zephyr Competition team, known as the Z-boys, began to emerge.
By the end of the decade, the sport had professionalised considerably. The pages of Skateboarder magazine showed the extraordinary pace of change in the skating community between 1975 and 1979. Major skateboard companies were founded in this period and the sport began to influence music and fashion.

""California is well-known for its entrepreneurial culture, which also helped popularise skateboarding,"" said Büntgen. ""These kids took risks and started businesses, some of which are still major players in the industry today.""
In the 1980s, skateboarding became a global industry. The development of hand-held consumer video cameras enabled commercial skateboard videography, which helped make the sport popular worldwide. Skateboard movies such as Bones Brigade (1984) and Shackle Me Not (1988) were made, and skaters such as Tony Hawk, Danny Way and Tony Magnusson became global media stars. Almost 50 years after it started in earnest, skateboarding is now a multibillion-dollar industry, and led to the rise of snowboarding, another multibillion-dollar industry.
""We often think about the negative effects of climate change, and those are deeply worrying when we look at current trends,"" said Büntgen. ""But our study shows that local climate triggers can have unexpected, and major, impacts on human society, and not all of them are negative -- in this case, a drought in California led to the development of a huge industry.
""When you look a bit deeper, it confirms our thinking that climate and environmental factors deeply influence society. These developments are not random -- in the case of skateboarding, you needed each one of the ingredients to exist in the same place and time. It couldn't have happened ten years earlier, ten years later, or a few hundred miles away.""

","score: 13.176839826839828, grade_level: '13'","score: 14.84771428571429, grade_levels: ['college_graduate'], ages: [24, 100]",10.1093/pnasnexus/pgad395,"In 1977 California, authorities responded to an extreme drought with an unprecedented state order to drastically reduce domestic water usage and leave countless newly built swimming pools empty. These curved pools became “playgrounds” for inspired surfers to develop professional vertical skateboarding in the Los Angeles area. Industrial production of polyurethane, and the advent of digital photography, laser printing, and high gloss mass media further contributed to the explosive popularization of skateboarding, creating a global subculture and multibillion-dollar industry that still impacts music, fashion, and lifestyle worldwide. Our interdisciplinary investigation demonstrates that neither the timing nor the location of the origin of professional skateboarding was random. This modern case study highlights how environmental changes can affect human behavior, transform culture, and engender technical innovation in the Anthropocene."
"
When it comes to greenhouse gases, methane is one the biggest contributors. Not only is it massively abundant -- it's about 25 times more potent than carbon dioxide at trapping heat in the atmosphere.

That makes tracking methane emissions critically important, and nowhere more so than in the Arctic, which is now the fastest warming part of the globe. A new study conducted at Brown University helps shed light on the actual atmospheric methane emissions from Arctic lakes and wetlands, which are major producers of the gas but remain largely unmapped.
Using unprecedented high-resolution satellite and airborne imagery from NASA -- harnessing the technology to overcome barriers posed by the region's sheer size and numerous natural land formations that are major methane producers -- a pair of researchers produced new estimates and found that these unmapped lakes are not the great methane emitters that previous research has made them out to be. Instead of contributing about 40% of the region's methane emissions, small unmapped lakes contribute only about 3%, according to the study.
""What the research has shown is that these smaller lakes are the greater emitters of methane on a per area basis, which means even that though they take up a small amount of the landscape they have a disproportionate level of emissions,"" said Ethan D. Kyzivat, who led the study as part of his Ph.D. at Brown. ""Traditionally, we haven't had a good picture of how much area they take up, but this new high-resolution dataset helped us scale it up to finally make those estimations much more accurately.""
These new findings, described in Geophysical Research Letters, contradict close to 15 years of research based on older datasets with much lower resolution quality. In the older data, the number of small lakes that could be seen were statistically extrapolated to produce estimates for the region on the total number of small unmapped lakes and how much methane they emitted.
The new analysis of the aerial imagery showed the researchers, including Brown professor Laurence C. Smith, that there are far fewer small, unmapped lakes than were previously estimated, greatly reducing the cumulative amount of methane they were thought to emit.
The study focuses on small lakes that are about a tenth of a square kilometer or smaller, which is equivalent to about 20 football fields. Kyzivat, who's now a postdoctoral researcher at Harvard University, spent more than two years working on the study, initially compiling the data while working on his master's degree and analyzing and writing the paper while working on his Ph.D. at Brown.

The project started as an effort to look for hidden lakes in the Arctic but morphed as the researchers examined the data more closely. The effort, which involved combining the high-resolution airborne data with a global map of lakes in the Arctic region, also unearthed a couple of unexpected but welcome results.
The work, for instance, shows that a lot of small and large lakes are still being double-counted as wetlands. This double-counting pumps methane emission estimates for the region, but taken with the new findings of fewer unmapped small lakes, the researchers believe the problem is on a smaller magnitude than what was previously thought.
Another unexpected result comes in methodology.
In the methane modeling field, there are two widely held trains of thought. One is ""bottom up"" estimates, which model methane emissions based on maps of the Earth, as the researchers do here. The other method is ""top down"" estimates, which model methane based on atmospheric measurements. For more than a decade, there has been a perplexing discrepancy between the figures produced by these two methods, according to Smith.
The new figures from the analysis may help reconcile the two opposing viewpoints by closing the difference between them.
""It's probably been 15 to 20 years of loggerheads, but the takeaway is the satellite resolution is now there for the 'bottom-up' community to take a much better look at how much methane is actually being emitted,"" said Smith, who is a professor of environmental studies and of earth, environmental and planetary sciences. ""We can now actually see the very smallest of these water bodies and they're not as abundant as we were extrapolating. The end result of all of this is going to reduce the bottom-up estimates to make them fall more in line with the top-down estimates. It's going to unify these two communities.""
Because of this, Kyzivat and Smith see the work from the NASA-funded study as a proof of concept and now look to expand their methane modeling technique to other parts of the world.
""The next step is to go global,"" Kyzivat said.

","score: 14.026440715003716, grade_level: '14'","score: 15.210064245443817, grade_levels: ['college_graduate'], ages: [24, 100]",10.1029/2023GL104825,"Lake methane emissions are commonly upscaled from lake area, with recognition that smaller, non‐inventoried lakes emit more per unit area. There is also growing awareness of the importance of lake aquatic vegetation and potential “double‐counting” with wetlands, but lack of consensus on which is most impactful. Here, we combine high‐resolution data with the comprehensive lake inventory HydroLAKES to rank these three variables based on emissions sensitivity. Including non‐inventoried small lakes <0.1 km2 (+30 [range: 9.0 to 82]% change) is greatest, followed by double‐counting (−20 [−11 to −34]%) and lake aquatic vegetation (+14 [2.7 to 43]%). Significantly, emissions from non‐inventoried lakes contribute far less than the ∼40% previously determined globally through statistical area extrapolation. We produce a first pan‐Arctic estimate of lake aquatic vegetation in 1.37 million km2 of lakes, but after correcting for persistent double‐counting, its net effect is to decrease emissions estimates by 9%. Thus, previous global emissions estimates are likely too high."
"
Until the early morning of February 24th, 2022, Ukrainian scientist Olena Iarmosh did not believe there would be a Russian invasion of Ukraine. Iarmosh grew up and had settled in Kharkiv, her beloved city in Eastern Ukraine and only 40 km away from the Russian border, where she worked for more than 16 years as a lecturer in higher education before fleeing to Switzerland. At approximately 5AM, she awoke to the sounds of bombing, hoping that they were merely the loud sounds of technical maintenance at the local power plant.

""My city looks worse now after the bombing than after two occupations by German troops,"" says Iarmosh. Iarmosh remained steadfast in her apartment throughout the bombardment for nine days before fleeing westward, first in western Ukraine, until the bombing started there too. She then fled to Switzerland, all the while ensuring her teaching duties online, and eventually landed a temporary position at EPFL with Gaétan de Rassenfosse.
In the meantime, de Rassenfosse and his team set out to quantify the impact of the war's influence on Ukrainian research, with one of the most extensive surveys yet, analyzing the responses from roughly 2500 Ukrainian scientists in autumn 2022. The results are published in Humanities and Social Sciences Communications.
""Our survey shows that Ukraine has lost almost 20% of top scientists, like Olena,"" explains de Rassenfosse of EPFL's College of Management of Technology, who was able to hire Iarmosh to work in his lab as a visting professor.
""Many of these emigrant scientists are under precarious contracts at their host institutions. Of the scientists who stay in Ukraine, if still alive, about 15% have left research, and others have little time to devote to research given the circumstances of war.""
The EPFL researchers found that research capacity in Ukraine, that is time directly devoted to research activity, is down 20%. The study reports that 23.5% of scientists still in Ukraine have lost access to critical input for their research, and 20.8% cannot physically access their institution. de Rassenfosse and his colleagues highlight in the study that ""the provision of more and longer scholarships emerges as a paramount concern"" for migrant scientists. As for scientists still in Ukraine, the study suggests that ""institutions across Europe and beyond can offer a host of support programs, such as remote visiting programs, access to digital libraries and computing resources, as well as collaborative research grants.""
""From a purely academic perspective, moving abroad may actually be an opportunity to improve as a scientist, as our survey shows that being abroad means exposure to novelty,"" continues de Rassenfosse.

Now at UNIL on a temporary contract, Iarmosh is living day-to-day in Switzerland, trying to juggle constraints imposed by employer contracts and her temporary Swiss permit. ""In Ukraine, with my level of education, there were many more options that I could choose from. In Switzerland, I am less picky about the job and know that each opportunity will be a positive experience for me.""
Iarmosh continues, ""Despite the war, Ukraine is doing a lot to keep researchers and scientists employed. Education in eastern and southern Ukraine is fully online. Ukrainian universities still want to keep us. They invite us to activities, ask us to supervise and continue research. It is a great privilege for all lecturers and researchers. They are trying to maintain a university education for youth.""
""More generally, our study shows that Ukrainian scientists are getting more and more disconnected from the Ukrainian scientific community, and this is dangerous for the future of Ukraine and Ukrainian research,"" warns de Rassenfosse. ""Policymakers must anticipate the renewal of the Ukrainian research system in order for scientists to return, and to train the next generation of Ukrainian scientists.""
""I am the biggest patriot of my city,"" concludes Iarmosh. ""Kharkiv is beautiful, the people, the mentality, the architecture, it is clean. I love Kharkiv. But the human loss has been colossal. Physically and mentally strong, patriotic, open-minded men stayed behind, fighting to protect Ukraine. We can rebuild buildings. It takes many years to build a new generation.""

","score: 11.737088277371036, grade_level: '12'","score: 12.439840949255235, grade_levels: ['college'], ages: [18, 24]",10.1057/s41599-023-02346-x,"The ongoing war in Ukraine has profoundly impacted the Ukrainian scientific community. Numerous researchers have either emigrated or transitioned to alternate professions. For those who remain in research, the destruction of civil infrastructure and psychological stress may dramatically slow down research progress. There is limited knowledge concerning the war’s influence on Ukrainian research. This study presents the results of a representative survey of over 2500 Ukrainian scientists. The data suggest that by the Fall of 2022, about 18.5% of the population of Ukrainian scientists fled the country. Notably, these emigrant scientists were amongst the most research-active in Ukraine. However, a significant portion of these migrant scientists are under precarious contracts at their host institutions. Of the scientists who stayed in Ukraine, about 15% have left research, and the others experience a marked reduction in research time. A large number of stayers have lost access to critical input for their research (23.5%) or cannot physically access their institution (20.8%). Finally, should the war stop today, it seems that Ukraine has already lost about seven percent of its scientists. These observations bear significant policy implications. In light of the vulnerable position of migrant scientists, the provision of more and longer scholarships emerges as a paramount concern for this group of scientists. Concerning stayers, institutions across Europe and beyond can offer a host of support programs, such as remote visiting programs, access to digital libraries and computing resources, as well as collaborative research grants."
"
Dr. Dewi Langlet, a scientist at the Evolution, Cell Biology and Symbiosis Unit at the Okinawa Institute of Science and Technology (OIST), studies foraminifera, single-cell organisms with shells made of calcium carbonate. He and his collaborators have shown for the first time that the burrowing of single-celled organisms in marine ecosystems affects oxygen distribution and bacterial diversity in sea sediments. Their findings have been published in the journal Biogeosciences.

Foraminifera are mostly marine organisms that have been around for about 550 million years and when they die, their shells accumulate on the ocean floor and become part of the ocean sediment.
While they are microscopic organisms (between 63 and 500 micrometers in diameter) they are still 'big' compared to other single-cell organisms, with each species having a unique shape. They live and are abundant in all marine sediments, from estuaries to the deep sea.
Geologists have studied them for a long time because their shells can fossilize, but we do not know much about their biology. Dr. Langlet is trying to understand how they move in the sediment and how this affects the entire sea bottom ecosystem.
Bioturbation occurs when organisms disturb the sediment by moving in it and creating burrows, which affects the mixing of the sediment particles. This affects the size of the particles, regulates water through the sediment, and changes the chemical composition of the sediment.
Bioturbators, often referred to as 'tillers of the soil', play an important role in determining nutrient availability and providing food and shelter for many species. They also significantly contribute to many natural processes and outputs, collectively known as 'ecosystem services', that we humans greatly benefit from.
""Typically, at the surface of marine sediments oxygen is consumed by the organisms living in the sediment, and gradually the oxygen decreases as you go deeper. We asked the question, 'Does foraminifera affect the oxygen distribution in the sediment when they move or when they create burrows?'"" said Dr. Langlet.

""It was hypothesized for a long time that they affect the oxygenation and chemistry of the sediment, but it was never proven because they are so small that their impact is very difficult to detect."" Previous studies have shown that larger multicellular organisms, such as worms, increase the oxygen penetration in the sediment by creating burrows, but this has never been shown for single-cell organisms such as foraminifera.
By creating burrows, foraminifera are engineering their entire ecosystem at a small-scale, allowing them to live deeper in the sediment where there is usually no oxygen. The scientists show that through their burrowing, they affect not just oxygenation but organic matter, bacterial diversity and ultimately how much food there is available in the sediment.
Finding enough foraminifera for the experiment was a challenge. ""For a big aquarium, we need to study many foraminifera and it is very time consuming to isolate them to know how many there are, so we must work with small amounts of sediment. It's all about miniaturization, working with small systems,"" Dr. Langlet explained.
The effect that these organisms have on their environment is relatively small because of their very small size, so the scientists needed very precise instruments, called microsensors, to accurately measure the distribution of oxygen in the sediment.
In his lab at OIST, Dr. Langlet worked with tiny sediment samples, each about 1 cm wide, placed in a tank filled with water. He added foraminifera to these samples and every few days measured how the oxygen levels changed at different depths. He found that with their burrows, foraminifera allow oxygen to go deeper into the sediment, increasing the amount of oxygen by 15 to 20 per cent. This causes a decrease in organic matter which leads to reduced bacterial abundance, which ultimately decreases the movement of oxygen from the water into the sediment.
Dr. Langlet's future research will explore the interactions between foraminifera and other organisms of similar size, as well as larger animals such as worms. ""Are foraminifera interacting with these worm-created burrows? Could their presence potentially amplify the effects of these burrows?"" he asks. These questions will guide his future investigations.

","score: 13.71782060518732, grade_level: '14'","score: 13.879960374639772, grade_levels: ['college_graduate'], ages: [24, 100]",10.5194/bg-20-4875-2023,"Abstract. Bioturbation processes influence particulate (sediment reworking) and dissolved (bioirrigation) fluxes at the sediment–water interface. Recent works showed that benthic foraminifera largely contribute to sediment reworking in intertidal mudflats, yet their role in bioirrigation processes remains unknown. In a laboratory experiment, we showed that foraminifera motion behaviour increased the oxygen penetration depth and decreased the total organic content. Their activity in the top 5 mm of the sediment also affected prokaryotic community structure. Indeed, in bioturbated sediment, bacterial richness was reduced, and sulfate-reducing taxa abundance in deeper layers was also reduced, probably inhibited by the larger oxygen penetration depth. Since foraminifera can modify both particulate and dissolved fluxes, their role as bioturbators can no longer be neglected. They are further able to mediate the prokaryotic community, suggesting that they play a major role in the benthic ecosystem functioning and may be the first described single-celled eukaryotic ecosystem engineers."
"
Nutrient runoff from agricultural production is a significant source of water pollution in the U.S., and climate change that produces extreme weather events is likely to exacerbate the problem. A new study from the University of Illinois Urbana-Champaign looks at how extreme rainfall impacts runoff and suggests possible mitigation strategies.

""We look at more than a decade of precipitation events in the state of Wisconsin and quantify the increase in nutrient runoff right around the event and at the end of the growing season. Climate models predict that we'll continue to see an increase in extreme events, and our works speaks to the challenging relationship between nutrient use and water quality,"" said Marin Skidmore, assistant professor in the Department of Agricultural and Consumer Economics, part of the College of Agricultural, Consumer and Environmental Sciences (ACES) at U. of I. Skidmore is lead author of the study with coauthors Jeremy Foltz from University of Wisconsin-Madison and Tihitina Andarge from the University of Massachusetts-Amherst.
""Our focus on a single state allows us to accurately measure farm locations and practices, while keeping statewide regulation constant, in a way that would be difficult in a national study,"" Skidmore added.
Livestock manure and crop fertilizer are major causes of nonpoint source pollution from agriculture. Wisconsin has a large dairy industry, where most farms are below the federal definition of concentrated animal feeding operations (CAFOs) and therefore not regulated under the Clean Water Act. Instead, they are subject to a patchwork of local regulations.
The researchers studied water quality across nearly 50 watersheds in Wisconsin from 2008 to 2020. They correlated ammonia and phosphorus concentration data from the Water Quality Portal with the location of livestock farms and crop acreages, and they determined nutrient levels after ½ inch, 1 inch, and 2 inches of rainfall.
They found spikes in nutrient concentrations immediately after extreme precipitation events, and the effect increased with the amount of precipitation. For example, within five days of an inch of precipitation, ammonia was 49% higher and phosphorus was 24% higher. If there was at least one day in a month with over an inch of precipitation, monthly ammonia was 28% higher and monthly phosphorus was 15% higher.
""We observe a significant interaction between rainfall, agricultural production, and runoff. It is not just a short-term spike in nutrient levels; at the end of the season, we still see persistent increases in phosphorus and ammonia attributed to those extreme precipitation events months earlier,"" Skidmore stated.

However, the researchers found that agricultural management practices can help mitigate the effects.
""Our results show that cover crops planted in the winter can lower the amount of nutrients in the water. Areas with cover crops have significantly lower spikes in ammonia and phosphorus, and the effect persists until the end of the growing season. We already know cover crops are great for soils and nutrient management, but this is additional empirical evidence showing that cover crops are climate-smart practices that can help agriculture be resilient into the future,"" Skidmore said.
The researchers also observed the presence of legacy nutrients, which are left behind from agricultural practices decades or even centuries ago.
""There is a direct impact of extreme precipitation on runoff that is unexplained by current activities. We attribute this to sedimented nutrients that remain in the soil from previous activities,"" Skidmore noted. ""One of the best ways to deal with legacy nutrients is to ensure soils are healthy. By preventing soil erosion, you keep the legacy nutrients in the soil and out of surface water. These findings further support the use of management practices such as conservation tillage, vegetative buffer strips, and cover crops.""
Wisconsin watersheds feed into North America's two largest river systems, the Mississippi and Great Lakes/St. Lawrence. Nutrient pollution can have acute local impacts, such as green algal blooms, which can be toxic to humans and animals. If people can't enjoy recreational activities like swimming or fishing, it leads to losses for local economies. Furthermore, downstream impacts continue along the Mississippi River into the Gulf of Mexico where nutrients contribute to a growing dead zone.
Finding solutions to dealing with nutrient pollution benefits the environment and society in general, Skidmore noted.
""Conservation strategies are not necessarily cost-effective for producers, so we must ensure there are policies in place to support their implementation. As we're approaching the next Farm Bill, there are discussions around how to allocate funds from the Inflation Reduction Act for climate-smart and conservation ag practices. It's important that such practices continue to receive funding so farmers can facilitate those benefits for all of us,"" she concluded.

","score: 14.245865782932892, grade_level: '14'","score: 15.215498983204036, grade_levels: ['college_graduate'], ages: [24, 100]",10.1002/jaa2.90,"Agriculture remains a leading source of water pollution in the United States, and climate change may exacerbate this relationship. We quantify increases in nutrient runoff following extreme precipitation events, showing that spikes are higher in regions with crop and livestock production. The spike per head is smaller around large‐scale livestock production, relative to small‐scale, and cover crops mitigate runoff. Legacy nutrients enter the surface water following extreme precipitation, regardless of current activity. We shed light on practices to protect water quality with more frequent extreme precipitation events and demonstrate the importance of including these events in empirical models of water quality."
"
Train accidents could be caused by solar storms switching signalling from red to green according to new research examining the impact of space weather.

Solar storms can trigger powerful magnetic disturbances on Earth, creating geomagnetically induced currents (GICs) which could potentially interfere with electricity transmission and distribution grids.
A team led by PhD researcher Cameron Patterson and Professor Jim Wild from Lancaster University modelled how GICs flowed through the track circuits of AC electrified lines powered with overhead cables.
Using two routes -- the Preston to Lancaster section of the West Coast Main Line, and the Glasgow to Edinburgh line -- the team modelled how GICs induced in the rails could cause rail signalling to malfunction.
There are more than 50,000 signalling track circuits in the UK, where the signal is controlled by an electrical circuit between the rails.
Physics PhD researcher Cameron Patterson said: ""Crucially, our research suggests that space weather is able to flip a signal in either direction, turning a red signal green or a green signal red. This is obviously very significant from a safety perspective.
""By building a computer model of the signalling track circuits using realistic specifications for the various components of the system, we found that space weather events capable of triggering faults in these track circuits are expected in the UK every few decades.""
Cameron's earlier research in the journal Space Weather explored what is known in the industry as ""right side"" failures, where the signal is switched from green to red.

This is a fail-safe scenario but the converse ""wrong side"" failures -- when the signal goes from red to green -- are much more hazardous.
This latest study, also in Space Weather, shows that ""wrong side"" failures could occur at a lower geoelectric field strength than for ""right side"" failures, meaning a weaker geomagnetic storm could more easily trigger ""wrong side"" failures.
It was estimated that, for the lines studied, ""wrong side"" failures could occur due to a geomagnetic storm with a frequency of about one or two decades.
The analysis was also performed for once-in-a-century extreme event, and it was shown that it could potentially cause many malfunctions of both typs throughout the lines in both directions of travel, depending on the number of trains on the line at that time.
Cameron said: ""When we experience severe space weather which happens every few decades or extreme space weather seen every century or two, then there is a potential for significant signalling misoperation, which has an obvious safety impact.""
There have been several examples of space weather impacting power grids in the last few decades, including power outages affecting millions across the Canadian province of Quebec in 1989 and the Swedish city of Malmo in 2003.

There are also historic examples of space weather interfering with railway signalling as far back as the nineteenth century. And in 1859, a massive solar eruption triggered a geomagnetic storm that disrupted telegraph lines across the world. The team also assessed the impact of a solar storm of the size of the 1859 event, predicting that it would cause widespread problems with signalling on both lines studied.
Cameron said: ""Our research shows that space weather poses a serious, if relatively rare, risk to the rail signalling system, which could cause delays or even have more critical, safety implications. This natural hazard needs to be taken seriously. By their nature, high-impact, low-frequency events are hard to plan for, but ignoring them is rarely the best way forward.
Jim Wild, Professor of Space Physics at Lancaster University said: ""Other industries such as aviation, electricity generation and transmission, and the space sector are considering the risks to their operations, and exploring how these might be mitigated. It's important that the rail sector is included in this planning.""
""As our understanding of the space weather hazard improves, it's possible to consider how to reduce the risks. In future, we could see space weather forecasting being used make decisions about limiting railway operations if an extreme event is expected, just as meteorological forecasts are used currently.
Severe space weather is included in the UK Government's National Risk Register for Civil Emergencies which lists the risk posed to the UK's economy and society as ""significant.""

","score: 14.64733956651084, grade_level: '15'","score: 16.420397365065874, grade_levels: ['college_graduate'], ages: [24, 100]",10.1029/2023SW003625,"The majority of studies into space weather impacts on ground‐based systems focus on power supply networks and oil and gas pipelines. The effects on railway signaling infrastructure remain a sparsely covered aspect even though these systems are known to have experienced adverse effects in the past as a result of geomagnetic activity. This study extends recent modeling of geomagnetic effects on DC signaling for AC‐electrified railways in the UK that analyzed “right side” failures in which green signals are turned to red. The extended model reported here allows the study of “wrong side” failures where red signals are turned green: a failure mode that is potentially more dangerous. Railway lines using track circuit signaling, like those modeled in this study, are separated into a number of individual blocks. This study shows that a relay is most susceptible to “wrong side” failure when a train is at the end of a track circuit block. Assuming that each train is positioned at the end of the block it is occupying, the results show that the geoelectric field threshold at which “wrong side” failures can occur is lower than for “right side” failures. This misoperation field level occurs on a timescale of once every 10 or 20 years. We also show that the estimated electric field caused by a 1‐in‐100 years event could cause a significant number of “wrong side” failures at multiple points along the railway lines studied, although this depends on the number of trains on the line at that time."
"
An international team of scientists led by Dr Xin LIU, Assistant Professor of the Department of Earth Sciences, The University of Hong Kong (HKU), along with seismologists from the USA and China, has recently introduced a new method called ambient noise differential adjoint tomography, which allows researchers to visualise rocks with fluids better, leading to potential advancements in the discovery of water and oil resources, as well as applications in urban geologic hazard and early warning systems for tsunamis and the understanding of the water cycle. Their findings have been published in the journal Nature Communications.

The method utilises a portable instrument called 'seismometer' to record the Earth's natural vibrations, making it a cost-effective and easy way to study areas in cities and oceans. Seismometers record ground motion in three dimensions: up-down, north-south, and east-west. In the study, 42 seismometers were placed along a line across the Los Angeles basin from Long Beach to Whittier Narrows.
Researchers found that rocks about 1-2 km beneath the surface near the Newport-Inglewood Fault, a fault that causes earthquakes, contain a significant amount of fluids. These rocks have tiny holes filled with fluids, which may explain the occurrence of small earthquakes in Long Beach, California. The abundance of fluids within these tiny holes reduces friction along the fault plane, allowing the two rock blocks on either side to slide past each other more easily and generate small earthquakes.
The paper suggests that ambient noise differential adjoint tomography can be used to find water and oil resources without the need for expensive drilling. This novel method generates images of the ground covered by seismometers, revealing how fast seismic waves travel in soils and rocks. In some locations, the seismic wave travels much slower compared to other regions at the same depth, indicating the presence of fluid. As water and oil are fluids in rocks, this method can identify rocks containing such fluids.
""Previously, groundwater aquifers or deep fluid reservoirs were difficult to find without drilling multiple expensive wells or costly seismic surveys with loud artificial sound that are not environmentally friendly on land or ocean. Using just weak seismic noise recordings by two dozen seismometers on land or seafloor, our new technique can create images containing fluid information within rocks, and pinpoint the location and depth of fluid-rich rocks,"" said Dr Liu, who is also the first author of the journal paper.
Additionally, this innovative method can be used to create detailed images of the ground in urban areas and the deep ocean, serving various purposes such as assessing urban geologic hazards, implementing early warning systems for tsunamis and enhancing our understanding of the water cycle under the seafloor. In urban settings, a series of land seismometers can be deployed over the area of interest. In the ocean, a line of Ocean Bottom Seismometers (OBS) can be installed on the seafloor to record background vibrations. In both cases, a detailed image is created right underneath the line of seismometers, providing information about the locations of loose soil/sediments and fluid-bearing rocks that directly relate to regions with slow seismic wave velocity.
""In conclusion, this innovative method has the potential to revolutionise our approach to discovering and utilising water and oil resources, enhancing urban safety measures, and deepening our understanding of the environment. Its direct impact on our daily lives spans from efficient resource exploration to effective disaster preparedness and promoting sustainable environmental management practices. This scientific breakthrough holds excellent promise in shaping a better future for us all,"" Dr Liu added.

","score: 15.766190476190477, grade_level: '16'","score: 18.083551251008878, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41467-023-42536-4,"Water scarcity is a pressing issue in California. We develop ambient noise differential adjoint tomography that improves the sensitivity to fluid-bearing rocks by canceling bias caused by noise sources. Here we image the shallow S-wave velocity structure using this method beneath a linear seismic array (LASSIE) in Los Angeles Basin, which shows significant velocity reduction marking a major regional water producer, the Silverado aquifer, along with other fluid-bearing structures. Based on the S-wave tomography and previous P-wave studies, we derive the porosity in Long Beach and discover that the rock from 1-2 km depth surrounding the Newport-Inglewood Fault contains abundant fluids with pore-fluid fraction ~0.33. The high-porosity rock around the fault coincides with previously observed week-long shallow seismicity south of LASSIE array in Long Beach. The imaged S-wave velocity in the top layer shows a similar trend in the geotechnical layer Vs 30, suggesting additional applications to ground motion prediction."
"
Fairy circles, a nearly hexagonal pattern of bare-soil circular gaps in grasslands, initially observed in Namibia and later in other parts of the world, have fascinated and baffled scientists for years. Theories for their appearance range from spatial self-organization induced by scale-dependent water-vegetation feedback to pre-existing patterns of termite nests.

Prof. Ehud Meron of Ben-Gurion University of the Negev has been studying the Namibian fairy circles as a case study for understanding how ecosystems respond to water stress. He believes that all theories to date have overlooked the coupling between two robust mechanisms essential for understanding ecosystem response: phenotypic plasticity at the level of a single plant, and spatial self-organization in vegetation patterns at the level of a plant population. Phenotypic plasticity is the plant's ability to change its own traits in response to environmental stresses.
Prof. Meron, together with his postdoctoral fellows, Jamie Bennett, Bidesh Bera, and Michel Ferré, and his colleagues, Profs. Hezi Yizhaq and Stephan Getzin, propose a novel model that captures both spatial patterning by a scale-dependent water-vegetation feedback and phenotypic changes involving deep-root growth to reach a moister soil layer. By comparing model predictions with empirical observations, they show that the coupling between these two mechanisms is the key to resolving two outstanding puzzles that the classical theory of vegetation pattern formation fails to explain: the appearance of multi-scale fairy-circle patterns, where the matrix in between the fairy circles consists of small-scale vegetation spots, and the absence of stripe and spot patterns, besides gap patterns, along the rainfall gradient, as the classical theory predicts.
Furthermore, they find that the combination of plant-level phenotypic changes and population-level spatial patterning can result in many additional pathways of ecosystem response to water stress, resulting in different multi-scale patterns, all of which are significantly more resilient to water stress than those involving a single phenotype.
Their findings were published last week in the Proceedings of the National Academy of Sciences (PNAS).
""Identifying these alternative pathways is essential for shifting fragile ecosystems on tracks to collapse to pathways of resilience,"" explains Prof. Meron, who recently won an ERC Synergy Grant to study resilience pathways in drylands and other biomes. ""This study highlights the importance of considering more elements of ecosystem complexity when addressing how to evade tipping to dysfunctional ecosystem states as warmer and drier climates develop,"" Prof. Meron concludes.
The Israel Science Foundation supported this research under grants no. 1053/17 and 2167/21.

","score: 17.665496665496665, grade_level: '18'","score: 20.46306072306072, grade_levels: ['college_graduate'], ages: [24, 100]",10.1073/pnas.2311528120,"Regular spatial patterns of vegetation are a common sight in drylands. Their formation is a population-level response to water stress that increases water availability for the few via partial plant mortality. At the individual level, plants can also adapt to water stress by changing their phenotype. Phenotypic plasticity of individual plants and spatial patterning of plant populations have extensively been studied independently, but the likely interplay between the two robust mechanisms has remained unexplored. In this paper, we incorporate phenotypic plasticity into a multi-level theory of vegetation pattern formation and use a fascinating ecological phenomenon, the Namibian “fairy circles,” to demonstrate the need for such a theory. We show that phenotypic changes in the root structure of plants, coupled with pattern-forming feedback within soil layers, can resolve two puzzles that the current theory fails to explain: observations of multi-scale patterns and the absence of theoretically predicted large-scale stripe and spot patterns along the rainfall gradient. Importantly, we find that multi-level responses to stress unveil a wide variety of more effective stress-relaxation pathways, compared to single-level responses, implying a previously underestimated resilience of dryland ecosystems."
"
An international team of experts undertaking fundamental research has developed a way of using polyethylene waste (PE) as a feedstock and converted it into valuable chemicals, via light-driven photocatalysis.

The University of Adelaide's Professor Shizhang Qiao, Chair of Nanotechnology, and Director, Centre for Materials in Energy and Catalysis, at the School of Chemical Engineering, led the team which published their findings in the journal Science Advances.
""We have upcycled polyethylene plastic waste into ethylene and propionic acid with high selectivity using atomically dispersed metal catalysts,"" said Professor Qiao.
""An oxidation-coupled room-temperature photocatalysis method was used to convert the waste into valuable products with high selectivity.
""Nearly 99 per cent of the liquid product is propionic acid, alleviating the problems associated with complex products that then require separation.
""Renewable solar energy was used rather than industrial processes that consume fossil fuel and emit greenhouse gases.
""This waste-to-value strategy is primarily implemented with four components, including plastic waste, water, sunlight and non-toxic photocatalysts that harness solar energy and boost the reaction. A typical photocatalyst is titanium dioxide with isolated palladium atoms on its surface.""
Most of the plastics used today end up being discarded and accumulated in landfills. PE is the most widely used plastic in the world. Daily food packaging, shopping bags and reagent bottles are all made from PE. It is also the largest proportion of all plastic waste and primarily ends up in landfills, posing a threat to global environment and ecology.

""Plastic waste is an untapped resource that can be recycled and processed into new plastics and other commercial products,"" said Professor Qiao.
""Catalytic recycling of PE waste is still in early development and is practically challenging because of chemical inertness of polymers and side reactions arising from structural complexities of reactant molecules.""
Current chemical recycling for PE waste is operated at high temperatures greater than 400 degrees centigrade that yield complex product compositions.
Ethylene is an important chemical feedstock that can be further processed into a variety of industrial and daily products, while propionic acid is also in high demand owing to its antiseptic and antibacterial properties.
The team's work aims to address contemporary environmental and energy challenges, contributing to a circular economy. It will be of use in further scientific research, waste management and chemical manufacturing.
""Our fundamental research provides a green and sustainable solution to simultaneously reduce plastic pollution and produce valuable chemicals from waste for a circular economy,"" said Professor Qiao.
""It will inspire the rational design of high-performance photocatalysts for solar energy utilisation and benefit the development of solar-driven waste upcycling technology.""

","score: 16.31311711711712, grade_level: '16'","score: 16.455472972972977, grade_levels: ['college_graduate'], ages: [24, 100]",10.1126/sciadv.adk2407,"Current chemical recycling of bulk synthetic plastic, polyethylene (PE), operates at high temperature/pressure and yields a complex mixture of products. PE conversion under mild conditions and with good selectivity toward value-added chemicals remains a practical challenge. Here, we demonstrate an atomic engineering strategy to modify a TiO 2 photocatalyst with reversible Pd species for the selective conversion of PE to ethylene (C 2 H 4 ) and propionic acid via dicarboxylic acid intermediates under moderate conditions. TiO 2 -supported atomically dispersed Pd species exhibits C 2 H 4 evolution of 531.2 μmol g cat −1 hour −1 , 408 times that of pristine TiO 2 . The liquid product is a valuable chemical propanoic acid with 98.8% selectivity. Plastic conversion with a C 2 hydrocarbon yield of 0.9% and a propionic acid yield of 6.3% was achieved in oxidation coupled with 3 hours of photoreaction. In situ spectroscopic studies confirm a dual role of atomic Pd species: an electron acceptor to boost charge separation/transfer for efficient photoredox, and a mediator to stabilize reaction intermediates for selective decarboxylation."
"
The shimmering green, red and purple curtains of the northern and southern lights -- the auroras -- may be the best-known phenomena lighting up the nighttime sky, but the most mysterious are the mauve and white streaks called Steve and their frequent companion, a glowing green ""picket fence.""

First recognized in 2018 as distinct from the common auroras, Steve -- a tongue-in-cheek reference to the benign name given a scary hedge in a 2006 children's movie -- and its associated picket fence were nevertheless thought to be caused by the same physical processes. But scientists were left scratching their heads about how these glowing emissions were produced.
Claire Gasque, a University of California, Berkeley, graduate student in physics, has now proposed a physical explanation for these phenomena that is totally different from the processes responsible for the well-known auroras. She has teamed up with researchers at the campus's Space Sciences Laboratory (SSL) to propose that NASA launch a rocket into the heart of the aurora to find out if she's correct.
Vibrant auroras and glowing phenomena such as Steve and the picket fence are becoming more common as the sun enters the active period of its 11-year cycle, and November was a good month for Steve observations in the northern latitudes. Because all these transient luminous phenomena are triggered by solar storms and coronal mass ejections from the sun, the approaching solar maximum is an ideal time to study rare events like Steve and the picket fence.
Gasque described the physics behind the picket fence in a paper published last month in the journal Geophysical Research Letters and will discuss the results on Dec. 14 in an invited talk at the American Geophysical Union meeting in San Francisco.
She calculated that in a region of the upper atmosphere farther south than that in which auroras form, electric fields parallel to Earth's magnetic field could produce the color spectrum of the picket fence. If correct, this unusual process has implications for how physicists understand energy flow between Earth's magnetosphere, which surrounds and protects Earth from the solar wind, and the ionosphere at the edge of space.
""This would upend our modeling of what creates light and the energy in the aurora in some cases,"" Gasque said.

""The really interesting thing about Claire's paper is that we've known for a couple of years now that the Steve spectrum is telling us there's some very exotic physics going on. We just didn't know what it was,"" said Brian Harding, a co-author of the paper and an SSL assistant research physicist. ""Claire's paper showed that parallel electric fields are capable of explaining this exotic spectrum.""
The paper was a side project from Gasque's Ph.D. thesis, which is focused on the connection between events like volcanoes on Earth's surface and phenomena in the ionosphere 100 kilometers or more above our heads.
But after hearing about Steve -- which has now become an acronym for Strong Thermal Emission Velocity Enhancement -- at a conference in 2022, she couldn't resist looking into the physics behind Steve and the picket fence.
""It's really cool,"" she said. ""It's one of the biggest mysteries in space physics right now.""
The physics of Steve and picket fence
The common auroras are produced when the solar wind energizes particles in Earth's magnetosphere, often at altitudes higher than 1,000 kilometers above the surface. These energized particles spiral around Earth's magnetic field lines toward the poles, where they crash into and excite oxygen and nitrogen molecules in the upper atmosphere. When those molecules relax, oxygen emits specific frequencies of green and red light, while nitrogen generates a bit of red, but primarily a blue, emission line.

The colorful, shimmering curtains that result can extend for thousands of kilometers across the northern or southern latitudes.
Steve, however, displays not individual emission lines, but a broad range of frequencies centered around purple or mauve. And unlike auroras, neither Steve nor the picket fence emit blue light, which is generated when the most energetic particles hit and ionize nitrogen. Steve and the picket fence also occur at lower latitudes than the aurora, potentially even as far south as the equator.
Some researchers proposed that Steve is caused by ion flows in the upper atmosphere, referred to as subauroral ion drift, or SAID, though there's no well accepted physical explanation for how SAID could generate the colorful emissions.
Gasque's interest was sparked by suggestions that the picket fence's emissions could be generated by low-altitude electric fields parallel to Earth's magnetic field, a situation thought to be impossible because any electric field aligned with the magnetic field should quickly short out and disappear.
Using a common physical model of the ionosphere, Gasque subsequently showed that a moderate parallel electric field -- around 100 millivolts per meter -- at a height of about 110 km could accelerate electrons to an energy that would excite oxygen and nitrogen and generate the spectrum of light observed from the picket fence. Unusual conditions in that area, such as a lower density of charged plasma and more neutral atoms of oxygen and nitrogen, could potentially act as insulation to keep the electric field from shorting out.
""If you look at the spectrum of the picket fence, it's much more green than you would expect. And there's none of the blue that's coming from the ionization of nitrogen,"" Gasque said. ""What that's telling us is that there's only a specific energy range of electrons that can create those colors, and they can't be coming from way out in space down into the atmosphere, because those particles have too much energy.""
Instead, she said, ""the light from the picket fence is being created by particles that have to be energized right there in space by a parallel electric field, which is a completely different mechanism than any of the aurora that we've studied or known before.""
She and Harding suspect that Steve itself may be produced by related processes. Their calculations also predict the type of ultraviolet emissions that this process would produce, which can be checked to verify the new hypothesis about the picket fence.
Though Gasque's calculations don't directly address the on-off glow that makes the phenomenon look like a picket fence, it's likely due to wavelike variations in the electric field, she said. And while the particles that are accelerated by the electric field are probably not from the sun, the scrambling of the atmosphere by solar storms probably triggers Steve and the picket fence, as it does the common aurora.
Enhanced auroras exhibit a picket fence-like glow
The next step, Harding said, is to launch a rocket from Alaska through these phenomena and measure the strength and direction of the electric and magnetic fields. SSL scientists specialize in designing and building instruments that do just that. Many of these instruments are on spacecraft now orbiting Earth and the sun.
Initially, the target would be what's known as an enhanced aurora, which is a normal aurora with picket fence-like emissions embedded in it.
""The enhanced aurora is basically this bright layer that's embedded in the normal aurora. The colors are similar to the picket fence in that there's not as much blue in them, and there's more green from oxygen and red from nitrogen. The hypothesis is that these are also created by parallel electric fields, but they are a lot more common than the picket fence,"" Gasque said.
The plan is not only ""to fly a rocket through that enhanced layer to actually measure those parallel electric fields for the first time,"" she said, but also send a second rocket up to measure the particles at higher altitudes, ""to distinguish the conditions from those that cause the auroras."" Eventually, she hopes for a rocket that will fly directly through Steve and the picket fence.
Harding, Gasque and colleagues proposed just such a sounding rocket campaign to NASA this fall and expect to hear back regarding its selection in the first half of 2024. Gasque and Harding consider the experiment an important step in understanding the chemistry and physics of the upper atmosphere, the ionosphere and Earth's magnetosphere, and a proposal in line with the Low Cost Access to Space (LCAS) program sponsored by NASA for projects like this.
""It's fair to say that there's going to be a lot of study in the future about how those electric fields got there, what waves they are or aren't associated with, and what that means for the larger energy transfer between Earth's atmosphere and space,"" Harding said. ""We really don't know. Claire's paper is the first step in the chain of that understanding.""
Gasque expressed appreciation for the input from people who study the middle ionosphere, or mesosphere, and the stratosphere, whose ideas helped her puzzle out the solution.
""With this collaboration, we were able to make some really cool progress in this field,"" she said. ""Honestly, it was just following our nose and being excited about it.""
In addition to Harding, her other co-authors are Reza Janalizadeh of Pennsylvania State University in University Park, Justin Yonker of the Applied Physics Laboratory at Johns Hopkins University in Laurel, Maryland, and D. Megan Gillies of the University of Calgary in Alberta, Canada.
Partial support for this work was provided by the National Science Foundation (AGS-2010088), National Aeronautics and Space Administration (80NSSC21K1386) and Robert P. Lin Fellowship at UC Berkeley.

","score: 14.410093323761664, grade_level: '14'","score: 16.165648779612347, grade_levels: ['college_graduate'], ages: [24, 100]",10.1029/2023GL106073,"Recent studies suggest that, despite its aurora‐like appearance, the picket fence may not be driven by magnetospheric particle precipitation but instead by local electric fields parallel to Earth's magnetic field. Here, we evaluate the parallel electric fields hypothesis by quantitatively comparing picket fence spectra with the emissions generated in a kinetic model driven by local parallel electric fields energizing ambient electrons in a realistic neutral atmosphere. We find that, at a typical picket fence altitude of 110 km, parallel electric fields between 40 and 70 Td (∼80–150 mV/m at 110 km) energize ambient electrons sufficiently so that, when they collide with neutrals, they reproduce the observed ratio of N2 first positive to atomic oxygen green line emissions, without producing first negative emissions. These findings establish a quantitative connection between ionospheric electrodynamics and observable picket fence emissions, offering verifiable targets for future models and experiments."
"
Record breaking marine heatwaves will cause devastating mass coral bleaching worldwide in the next few years, according to a University of Queensland coral reef scientist.

The alarming finding is the result of an international study led by UQ's Professor Ove Hoegh-Guldberg of UQ's School of the Environment, who is currently attending the COP28 climate change meetings in Dubai.
""We were shocked to find heat stress conditions started as much as 12 weeks ahead of previously recorded peaks and were sustained for much longer in the eastern tropical Pacific and wider Caribbean,"" Professor Hoegh-Guldberg said.
""Historical data suggests the current marine heatwaves will likely be the precursor to a global mass coral bleaching and mortality event over the next 12 to 24 months, as the El Niño phase of El Niño-Southern Oscillation or ENSO continues.
""Across July 2023, Earth experienced its warmest days on record since 1910, as well as the warmest month ever recorded for sea surface temperatures.
""This puts immense pressure on vital but fragile tropical ecosystems, such as coral reefs, mangrove forests, and seagrass meadows.
""For example, a coral reef in the Florida Keys called Newfound Harbor Key accumulated heat stress almost 3 times the previous record and it occurred 6 weeks ahead of previous peaks.""
Professor Hoegh-Guldberg said the findings come at a critical point in protecting global biodiversity, with commitment to climate change mitigation slipping in many nations.

""The latest environmental information indicates that we're well off-track when it comes to keeping global surface temperatures from reaching a very dangerous condition by mid to late this century,"" he said.
""Frankly, we're hurtling in the opposite direction.
""Compounding this is the fact these devastating impacts appear to be rolling into a vast record-breaking global event.""
Professor Hoegh-Guldberg said that without serious and swift action, the persistence of coral reefs beyond the next few decades is in serious jeopardy.
""Our study shows that ENSO is a major determinant of the fate of the world's coral reefs,"" he said.
""Rising sea temperatures, coupled with other stressors such as ocean acidification and pollution, have severely weakened their resilience.

""This puts coral reefs and a quarter of the ocean's biodiversity at serious risk of annihilation.""
Professor Hoegh-Guldberg said efforts to introduce of heat-tolerance genes into the natural coral population have shown promise, but the reality of scaling these efforts remains logistically challenging.
""Given the complex and interconnected nature of marine ecosystems such as coral reefs, a comprehensive approach is necessary for mitigating the impacts of changing oceanic conditions,"" he said.
""The importance of reducing our emissions is underscored in our findings, where massive changes to oceanic warming are set to destroy coral reefs and many other ecosystems.
""With this in mind, there are extremely tough discussions underway at the COP28 climate meetings.""

","score: 14.693092783505154, grade_level: '15'","score: 15.854291915355397, grade_levels: ['college_graduate'], ages: [24, 100]",10.1126/science.adk4532,Climate change and its impacts on coral reefs have reached uncharted territory
"
In the ocean, organic particles from the sunlit surface water sink to the bottom. This process is an important part of the biological carbon pump. It binds carbon dioxide (CO2) from the atmosphere and provides energy to deep-sea ecosystems. Previous research has suggested that the biological pump is particularly efficient in areas with an oxygen minimum zone (OMZ) below the surface. Due to the lack of oxygen, particle consumers such as zooplankton could not effectively reduce the export of carbon to the deep sea. Researchers from the GEOMAR Helmholtz Centre for Ocean Research Kiel challenge this assumption in a new study published in the journal Communications Earth & Environment. Using the tropical upwelling system off the coast of Peru as an example, they were able to show the influence that zooplankton species can have on the efficiency of the biological carbon pump in these areas.

The researchers were able to show that zooplankton species that are tolerant to oxygen deficiency consume sinking particles and thus attenuate the export fluxes of carbon to the deep sea. They investigated the carbon fluxes and their attenuation off the coast of Peru in an area with a millennia-old permanent oxygen minimum zone. To this end, the scientists collected samples in the Humboldt Current on two expeditions with the research vessel METEOR (M136 and M138) in April and June 2017. They deployed drifting sediment traps six times to collect and chemically analyse sinking particles in the OMZ at depths of between 50 and 600 metres. The team also used vertical Multi-Net hauls, which collect plankton samples from different depths, and an Underwater Vision Profiler (UVP5), a pressure-resistant underwater camera, to obtain high-resolution data on the occurrence of various zooplankton species.
""Based on several independent methods, our results provide consistent evidence that zooplankton interact with sinking particles even in low-oxygen systems, thereby reducing carbon export to the deep sea. The species found off Peru are well adapted to living under low-oxygen conditions. Low-oxygen zones that have recently developed, e.g. due to eutrophication, are generally avoided by animals,"" says the corresponding author of the study, Dr Anja Engel, Professor of Biological Oceanography and Head of the Marine Biogeochemistry Research Unit at GEOMAR.
The results challenge the previously prevailing assumption of a uniformly efficient biological carbon pump in oxygen minimum zones. ""Zooplankton once again proves to be a key player in the regulation of the carbon pump,"" explains Dr Engel. In view of their findings, the authors call for a significant increase in observations and knowledge of deep-sea ecosystems in order to make reliable predictions about the marine carbon cycle.

","score: 14.40080091533181, grade_level: '14'","score: 15.021144164759725, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s43247-023-01140-6,"In the ocean, downward flux of particles produced in sunlit surface waters is the major component of the biological carbon pump, which sequesters atmospheric carbon dioxide and fuels deep-sea ecosystems. The efficiency of downward carbon transfer is expected to be particularly high in tropical upwelling systems where hypoxia occurring beneath the productive surface waters is thought to hamper particle consumption. However, observations of both particle feeders and carbon export in low-oxygen waters are scarce. Here, we provide evidence that hypoxia-tolerant zooplankton feed on sinking particles in the extensive Oxygen Minimum Zone (OMZ) off Peru. Using several arrays of drifting sediment traps and in situ imaging, we show geochemical and morphological transformations of sinking particles and substantial control of carbon export by zooplankton. Our findings challenge the assumption of a consistently efficient biological carbon pump in OMZs and further demonstrate the need to consider mesopelagic organisms when studying oceanic carbon sequestration."
"
A recent study from researchers in Canada and Germany has revealed that an unlikely event, occurring over 12 million years ago played an important role in shaping one of Canada's most damaging invasive species.

Zebra and quagga mussels, belonging to the Dreissenid family, are widespread freshwater invasive species throughout North America that present a significant danger to native ecosystems by competing for resources. Using a fibrous anchor called a byssus, Dreissenid mussels contribute to biofouling on surfaces and obstruct intake structures in power stations and water treatment plants.
""This new study, which looks into the way these mussels stick to surfaces, may help improve strategies against biofouling, a problem causing millions in damages in Canada alone"" shares co-author and lead McGill Professor, Matthew Harrington.
Surprisingly, researchers discovered that a previously undocumented event contributed to Dreissenid mussel's resilience as a species. University of Göttingen Professor and co-author Daniel J. Jackson explains, ""More than 12 million years ago, it is likely that a single bacterium transferred genetic material into a single mussel endowing its descendants with the ability to make these fibers. Given their crucial role in mussel attachment in freshwater habitats, this horizontal gene transfer event supported the harmful global expansion of these mussels.""
This research, marking important progress in the understanding of invasive mussels and their attachment mechanisms, could offer potential solutions to mitigate their environmental and economic impact in Canada.
The study also sheds light on how mussel fibres could inspire the development of sustainable materials.
Sustainable materials inspired by mussel biology
""This research not only advances our understanding of mussel evolution and biofouling, but also presents an exciting opportunity for the development of novel materials,"" said Harrington who is also co-director of McGill Institute of Advanced Materials. ""Dreissenid byssus fibers, which resemble spider silk structurally, could inspire future development of tough polymer fibers, contributing to more durable and sustainable materials typically used in textiles and technical plastics.""
""We found that the building blocks of the fibres are massive coiled-coil proteins, the largest ever found,"" Harrington said. These proteins, structurally similar to those found in human hair, were found to transform into silk-like beta crystallites through simple application of stretching forces during formation. This method of fiber fabrication is much simpler than spider silk formation, potentially offering an easier route toward biotechnological manufacture of sustainable fibers -- an industry currently dominated by artificial spider silks.

","score: 18.449276895943566, grade_level: '18'","score: 20.480211640211643, grade_levels: ['college_graduate'], ages: [24, 100]",10.1073/pnas.2311901120,"Zebra and quagga mussels ( Dreissena spp. ) are invasive freshwater biofoulers that perpetrate devastating economic and ecological impact. Their success depends on their ability to anchor onto substrates with protein-based fibers known as byssal threads. Yet, compared to other mussel lineages, little is understood about the proteins comprising their fibers or their evolutionary history. Here, we investigated the hierarchical protein structure of Dreissenid byssal threads and the process by which they are fabricated. Unique among bivalves, we found that threads possess a predominantly β -sheet crystalline structure reminiscent of spider silk. Further analysis revealed unexpectedly that the Dreissenid thread protein precursors are mechanoresponsive α -helical proteins that are mechanically processed into β -crystallites during thread formation. Proteomic analysis of the byssus secretory organ and byssus fibers revealed a family of ultrahigh molecular weight (354 to 467 kDa) asparagine-rich (19 to 20%) protein precursors predicted to form α -helical coiled coils. Moreover, several independent lines of evidence indicate that the ancestral predecessor of these proteins was likely acquired via horizontal gene transfer. This chance evolutionary event that transpired at least 12 Mya has endowed Dreissenids with a distinctive and effective fiber formation mechanism, contributing significantly to their success as invasive species and possibly, inspiring new materials design."
