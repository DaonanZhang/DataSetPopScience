pls,fk_score,ari_score,reference,abstract
"
Researchers have developed a groundbreaking Artificial Intelligence (AI) system that can rapidly detect COVID-19 from chest X-rays with more than 98% accuracy. The study results have just been published in Nature Scientific Reports.

Corresponding author Professor Amir H Gandomi, from the University of Technology Sydney (UTS) Data Science Institute, said there was a pressing need for effective automated tools to detect COVID-19, given the significant impact on public health and the global economy.
""The most widely used COVID-19 test, real time polymerase chain reaction (PCR), can be slow and costly, and produce false-negatives. To confirm a diagnosis, radiologists need to manually examine a CT scans or X-rays, which can be time consuming and prone to error,"" said Professor Gandomi.
""The new AI system could be particularly beneficial in countries experiencing high levels of COVID-19 where there is a shortage of radiologists. Chest X-rays are portable, widely available and provide lower exposure to ionizing radiation than CT scans,"" he said.
Common symptoms of COVID-19 include fever, cough, difficulty breathing and a sore throat, however it can be difficult to distinguish COVID-19 from Flu and other types of pneumonia.
The new AI system uses a deep learning-based algorithm called a Custom Convolutional Neural Network (Custom-CNN) that is able to quickly and accurately distinguish between COVID-19 cases, normal cases, and pneumonia in X-ray images.
""Deep learning offers an end-to-end solution, eliminating the need to manually search for biomarkers. The Custom-CNN model streamlines the detection process, providing a faster and more accurate diagnosis of COVID-19,"" said Professor Gandomi.

""If a PCR test or rapid antigen test shows a negative or inconclusive result, due to low sensitivity, patients may require further examination via radiological imaging to confirm or rule out the virus's presence. In this situation the new AI system could prove beneficial.
""While radiologists play a crucial role in medical diagnosis, AI technology can assist them in making accurate and efficient diagnoses,"" said Professor Gandomi.
The performance of the Custom-CNN model was evaluated via a comprehensive comparative analysis, with accuracy as the performance criterion. The results showed that the new model outperforms the other AI diagnostic models.
Fast and accurate diagnosis of COVID-19 can ensure patients get the correct treatment, including COVID-19 antivirals, which work best if taken within five days of the onset of symptoms. It could also help them isolate and protect others from getting infected, reducing pandemic outbreaks.
This breakthrough represents a significant step in combatting the ongoing challenges posed by the pandemic, potentially transforming the landscape of COVID-19 diagnosis and management.

","score: 14.662653061224493, grade_level: '15'","score: 15.316623702112416, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41598-023-47038-3,"The most widely used method for detecting Coronavirus Disease 2019 (COVID-19) is real-time polymerase chain reaction. However, this method has several drawbacks, including high cost, lengthy turnaround time for results, and the potential for false-negative results due to limited sensitivity. To address these issues, additional technologies such as computed tomography (CT) or X-rays have been employed for diagnosing the disease. Chest X-rays are more commonly used than CT scans due to the widespread availability of X-ray machines, lower ionizing radiation, and lower cost of equipment. COVID-19 presents certain radiological biomarkers that can be observed through chest X-rays, making it necessary for radiologists to manually search for these biomarkers. However, this process is time-consuming and prone to errors. Therefore, there is a critical need to develop an automated system for evaluating chest X-rays. Deep learning techniques can be employed to expedite this process. In this study, a deep learning-based method called Custom Convolutional Neural Network (Custom-CNN) is proposed for identifying COVID-19 infection in chest X-rays. The Custom-CNN model consists of eight weighted layers and utilizes strategies like dropout and batch normalization to enhance performance and reduce overfitting. The proposed approach achieved a classification accuracy of 98.19% and aims to accurately classify COVID-19, normal, and pneumonia samples."
"
Researchers from Children's Hospital of Philadelphia (CHOP) have developed a new AI-powered algorithm to help understand how different cells organize themselves into particular tissues and communicate with one another. This new tool was tested on two types of cancer tissues to reveal how these ""neighborhoods"" of cells interact with one another to evade therapy, and more studies could reveal more information about the function of these cells in the tumor microenvironment.

The findings were published online today by the journal Nature Methods.
To understand how different cells organize themselves to support the functions of a tissue, researchers proposed the concept of tissue cellular neighborhoods (TCNs) to describe functional units in which different, recurrent cell types work together to support specific tissue functions. Across individuals, the functions of these TCNs would remain the same. However, translating the huge amount of information in spatial omics data into models and hypotheses that can be interpreted and tested by researchers requires advanced AI algorithms.
""It is very difficult to study the tissue microenvironment, how certain cells organize, behave and communicate with one another,"" said senior study author Kai Tan, PhD, an investigator in the Center for Childhood Cancer Research at CHOP and a professor in the Department of Pediatrics and the Perelman School of Medicine at the University of Pennsylvania. ""Until recent advances in so-called spatial omics technology, it was impossible to spatially characterize more than 100 proteins or hundreds or even thousands of genes across a piece of tissue, which might be home to hundreds of thousands of cells and their respective genes.""
In this study, researchers developed the deep-learning-based CytoCommunity algorithm to identify TCNs based on cell identities of a tissue sample, their spatial distributions as well as patient clinical data, which can help researchers better understand how these neighborhoods of cells are organized and are associated with certain clinical outcomes. In this study, tissue samples from breast and colorectal tumors were used because of a high volume of data available, enough to train the algorithm to identify TCNs associated with high-risk disease subtypes.
By using CytoCommunity for breast and colorectal cancer data, the algorithm revealed new fibroblast-enriched TCNs and granulocyte-enriched TCNs specific to high-risk breast cancer and colorectal cancer, respectively.
""Since we were able to prove the effectiveness of CytoCommunity, the next step is to apply this algorithm to both healthy and diseased tissue data generated by research consortia such as HuBMAP (Human BioMolecular Atlas Program) and HTAN (Human Tumor Atlas Network)"" Tan said. ""For instance, using data from childhood cancers such as leukemia, neuroblastoma and high-grade gliomas, we hope to find tissue cellular neighborhoods that might be associated with responses to certain therapies and combine our findings with genetic data to help determine which genetic pathways may be involved at the cellular and molecular levels.""
This study was supported by National Institutes of Health grant CA233285, HL165442 and HL156090, a grant from the Chan Zuckerberg Initiative (AWD-2021-237920), a grant from the Leona M. and Harry B Helmsley Charitable Trust (no. 2008-04062), a National Natural Science Foundation of China grant no. 62002277, a grant from the Young Talent Fund of University Association for Science and Technology in Shaanxi (no. 20210101), a grant from the Fundamental Research Funds for the Central Universities (no. QTZX23051), and National Natural Science Foundation of China grant nos. 62132015 and U22A2037.

","score: 16.735518453427066, grade_level: '17'","score: 19.25415872722227, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41592-023-02124-2,"It is poorly understood how different cells in a tissue organize themselves to support tissue functions. We describe the CytoCommunity algorithm for the identification of tissue cellular neighborhoods (TCNs) based on cell phenotypes and their spatial distributions. CytoCommunity learns a mapping directly from the cell phenotype space to the TCN space using a graph neural network model without intermediate clustering of cell embeddings. By leveraging graph pooling, CytoCommunity enables de novo identification of condition-specific and predictive TCNs under the supervision of sample labels. Using several types of spatial omics data, we demonstrate that CytoCommunity can identify TCNs of variable sizes with substantial improvement over existing methods. By analyzing risk-stratified colorectal and breast cancer data, CytoCommunity revealed new granulocyte-enriched and cancer-associated fibroblast-enriched TCNs specific to high-risk tumors and altered interactions between neoplastic and immune or stromal cells within and between TCNs. CytoCommunity can perform unsupervised and supervised analyses of spatial omics maps and enable the discovery of condition-specific cell–cell communication patterns across spatial scales."
"
Spintronic devices are electronic devices that utilize the spin of electrons (an intrinsic form of angular momentum possessed by the electron) to achieve high-speed processing and low-cost data storage. In this regard, spin-transfer torque is a key phenomenon that enables ultrafast and low-power spintronic devices. Recently, however, spin-orbit torque (SOT) has emerged as a promising alternative to spin-transfer torque.

Many studies have investigated the origin of SOT, showing that in non-magnetic materials, a phenomenon called the spin Hall effect (SHE) is key to achieving SOT. In these materials, the existence of a ""Dirac band"" structure, a specific arrangement of electrons in terms of their energy, is important to achieving large SHE. This is because the Dirac band structure contains ""hot spots"" for the Berry phase, a quantum phase factor responsible for the intrinsic SHE. Thus, materials with suitable Berry phase hot spots are key to engineering the SHE.
In this context, the material tantalum silicide (TaSi2) is of great interest as it has several Dirac points near the Fermi level in its band structure, suitable for practicing Berry phase engineering. To demonstrate this, a team of researchers, led by Associate Professor Pham Nam Hai from the Department of Electrical and Electronic Engineering at Tokyo Institute of Technology (Tokyo Tech), Japan, recently investigated the influence of Dirac band hot spots on the temperature dependence of SHE in TaSi2. ""Berry phase monopole engineering is an interesting avenue of research as it can give rise to efficient high-temperature SOT spintronic devices such as the magneto-resistive random-access memory,"" explains Dr. Hai about the importance of their study. Their findings were published in the journal Applied Physics Letters.
Through various experiments, the team observed that the SOT efficiency of TaSi2 remained almost unchanged from 62 K to 288 K, which was similar to the behavior of conventional heavy metals. However, upon increasing the temperature further, the SOT efficiency suddenly increased and nearly doubled at 346 K. Moreover, the corresponding SHE also increased in a similar fashion. Notably, this was quite different from the behavior of conventional heavy metals and their alloys. Upon further analysis, the researchers attributed this sudden increase in SHE at high temperatures to Berry phase monopoles.
""These results provide a strategy to enhance the SOT efficiency at high temperatures via Berry phase monopole engineering,"" highlights Dr. Hai.
Indeed, their study highlights the potential of Berry phase monopole engineering to effectively use the SHE in non-magnetic materials, and provides a new pathway for the development of high-temperature, ultrafast, and low-power SOT spintronic devices.

","score: 16.0014262317456, grade_level: '16'","score: 16.57774669032346, grade_levels: ['college_graduate'], ages: [24, 100]",10.1063/5.0165333,"We demonstrate the concept of Berry phase monopole engineering of the spin Hall effect in non-centrosymmetric silicide TaSi2. We show that while the effective damping-like spin–orbit torque (SOT) efficiency θDLeff of TaSi2 is nearly unchanged from 62 to 288 K (−0.049 to −0.069), θDLeff suddenly increases at high temperatures and becomes nearly double (−0.12) at 346 K. The corresponding intrinsic spin Hall conductivity σDLeff significantly increases at high temperatures, which can be attributed to the increasing contribution from the four degenerate points near the Fermi level via thermal excitation. Our results provide a strategy to enhance θDLeff at high temperatures via Berry phase monopole engineering and pave the way for SOT spintronic devices working at high temperatures."
"
Coal is an abundant resource in the United States that has, unfortunately, contributed to climate change through its use as a fossil fuel. As the country transitions to other means of energy production, it will be important to consider and reevaluate coal's economic role. A joint research effort from the University of Illinois Urbana-Champaign, the National Energy Technology Laboratory, Oak Ridge National Laboratory and the Taiwan Semiconductor Manufacturing Company has shown how coal can play a vital role in next-generation electronic devices.

""Coal is usually thought of as something bulky and dirty, but the processing techniques we've developed can transform it into high-purity materials just a couple of atoms thick,"" said Qing Cao, a U. of I. materials science & engineering professor and a co-lead of the collaboration. ""Their unique atomic structures and properties are ideal for making some of the smallest possible electronics with performance superior to state-of-the art.""
A process developed by the NETL first converts coal char into nanoscale carbon disks called ""carbon dots"" that the U. of I. research group demonstrated can be connected to form atomically thin membranes for applications in both two-dimensional transistors and memristors, technologies that will be critical to constructing more advanced electronics. These results are reported in the journal Communications Engineering.
Perfect for 2D electronics
In the ongoing search for smaller, faster and more efficient electronics, the final step will be devices made with materials just one or two atoms thick. It is impossible for devices to be smaller than this limit, and their small scale often makes them operate much quicker and consume far less energy. While ultrathin semiconductors have been extensively studied, it is also necessary to have atomically thin insulators -- materials that block electric currents -- to construct working electronic devices like transistors and memristors.
Atomically thin layers of carbon with disordered atomic structures can function as an excellent insulator for constructing two-dimensional devices. The researchers in the collaboration have shown that such carbon layers can be formed from carbon dots derived from coal char. To demonstrate their capabilities, the U. of I. group led by Cao developed two examples of two-dimensional devices.
""It's really quite exciting, because this is the first time that coal, something we normally see as low-tech, has been directly linked to the cutting edge of microelectronics,"" Cao said.

Transistor dielectric
Cao's group used coal-derived carbon layers as the gate dielectric in two-dimensional transistors built on the semimetal graphene or semiconductor molybdenum disulfide to enable more than two times faster device operating speed with lower energy consumption. Like other atomically thin materials, the coal-derived carbon layers do not possess ""dangling bonds,"" or electrons that are not associated with a chemical bond. These sites, which are abundant on the surface of conventional three-dimensional insulators, alter their electrical properties by effectively functioning as ""traps,"" slowing down the transport of mobile charges and thus the transistor switching speed.
However, unlike other atomically thin materials, the new coal-derived carbon layers are amorphous, meaning that they do not possess a regular, crystalline structure. They therefore do not have boundaries between different crystalline regions that serve as conduction pathways leading to ""leakage,"" where undesired electrical currents flow through the insulator and cause substantial additional power consumption during device operations.
Memristor filament
Another application Cao's group considered is memristors -- electronic components capable of both storing and operating on data to greatly enhance the implementation of AI technology. These devices store and represent data by modulating a conductive filament formed by electrochemical reactions between a pair of electrodes with the insulator sandwiched in between.
The researchers found that the adoption of ultrathin coal-derived carbon layers as the insulator allows the fast formation of such filament with low energy consumption to enable high device operating speed with low power. Moreover, atomic size rings in these coal-derived carbon layers confine the filament to enhance the reproducible device operations for enhanced data storage fidelity and reliability.
From research to production
The new devices developed by the Cao group provide proof-of-principle for the use of coal-derived carbon layers in two-dimensional devices. What remains is to show that such devices can be manufactured on large scales.
""The semiconductor industry, including our collaborators at Taiwan Semiconductor, is very interested in the capabilities of two-dimensional devices, and we're trying to fulfill that promise,"" Cao said. ""Over the next few years, the U. of I. will continue to collaborate with NETL to develop a fabrication process for coal-based carbon insulators that can be implemented in industrial settings.""

","score: 17.789671860589298, grade_level: '18'","score: 19.212670258725304, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s44172-023-00141-9,"Materials keeping thickness in atomic scale but extending primarily in lateral dimensions offer properties attractive for many emerging applications. However, compared to crystalline counterparts, synthesis of atomically thin films in the highly disordered amorphous form, which avoids nonuniformity and defects associated with grain boundaries, is challenging due to their metastable nature. Here we present a scalable and solution-based strategy to prepare large-area, freestanding quasi-2D amorphous carbon nanomembranes with predominant sp2 bonding and thickness down to 1–2 atomic layers, from coal-derived carbon dots as precursors. These atomically thin amorphous carbon films are mechanically strong with modulus of 400 ± 100 GPa and demonstrate robust dielectric properties with high dielectric strength above 20 MV cm−1 and low leakage current density below 10−4 A cm−2 through a scaled thickness of three-atomic layers. They can be implemented as solution-deposited ultrathin gate dielectrics in transistors or ion-transport media in memristors, enabling exceptional device performance and spatiotemporal uniformity."
"
New Cornell University-led research finds that social media platforms and the metrics that reward content creators for revealing their innermost selves to fans open creators up to identity-based harassment.

""Creators share deeply personal -- often vulnerable -- elements of their lives with followers and the wider public,"" said Brooke Erin Duffy, associate professor of communication. ""Such disclosures are a key way that influencers build intimacy with audiences and form communities. There's a pervasive sense that internet users clamor for less polished, less idealized, more relatable moments -- especially since the pandemic.""
Duffy is the lead author of ""Influencers, Platforms, and the Politics of Vulnerability"" published in the European Journal of Cultural Studies.
The research team conducted in-depth interviews with content creators to get a sense of how they experience the demands to make their content -- and often themselves -- visible to audiences, sponsors and the platforms.
Among their findings: The value of vulnerability for platform-based influencers cannot be overstated -- authenticity sells, and that means projecting intimacies, insecurities and even secrets; These authentic revelations are often tied to one's identities, which can open a person up to attacks based on gender, race, sexuality and other perceived traits; Personal and social vulnerabilities were often compounded by the vulnerabilities of platform-dependent labor: Not only did participants identify the failures of their platforms to protect them from harm (as ""contractors"" instead of ""employees""), many felt these companies incentivize networked antagonism.""Influencers and creators have relatively few formal sources of support or protection,"" Duffy said. ""In contrast to those legally employed by Meta, Twitch and TikTok, creators are independent contractors. They're left wanting for a lot of the workplace protections traditionally afforded to employees.""
The researchers examined informal strategies -- both anticipatory and reactive -- that creators deploy to manage their vulnerabilities. The former included the use of platform filtering systems to sift out abusive, profane or hurtful language. The latter strategies ranged from simply not reading the comments to employing the platform's tools to minimize the impact of what, for many, felt like an inevitable onslaught of critique.
The authors acknowledge the difficulties of resolving endemic issues of internet hate and harassment. ""'Getting off the internet' is hardly a viable option for participants in the put-yourself-out-there neoliberal job economy,"" they wrote -- and offer a warning to those wishing to join the creator economy.
""It is something of a truism that 'everyone gets the same platform,'"" they wrote. ""We would caution, however, that the politics of visibility -- and hence, the politics of vulnerability -- are far less egalitarian that platforms lead us to believe.""

","score: 16.34158371040724, grade_level: '16'","score: 17.62418552036199, grade_levels: ['college_graduate'], ages: [24, 100]",10.1177/13675494231212346,"While workers of all stripes are compelled to embrace uncertainty under conditions of neoliberalism, ideologies of risk assume a particular guise in the platform economy, wherein laborers are exhorted to ‘put yourself out there’. Given the attendant harms associated with public visibility – especially for women and other marginalized groups – it seems crucial to explore platform-dependent laborers’ experiences of ‘putting themselves out there’. This article draws upon in-depth interviews with 23 social media influencers and content creators, sampled from across platforms, content niches and subjectivities. Our analysis revealed that vulnerability is a structuring concept in the influencer economy – one that operates at multiple, often overlapping levels. First, the commercial logic of authenticity casts personal vulnerability as a strategy for building community and accruing followers. But influencers’ individual disclosures were often entangled with their social identities (e.g., gender, race, sexuality, ability and body type), which rendered them socially vulnerable to targeted antagonism from audiences﻿. Interviewees experienced a range of harms, from identity-based hate and harassment to concerted take-down campaigns. These personal and social vulnerabilities were compounded by the vulnerabilities of platform-dependent labor: not only did participants identify the failures of platforms to protect them, some shared a sense that these companies exacerbated harms through a commercial logic that incentivizes antagonism. After examining the emotional labor necessary to manage such platform vulnerabilities, we close by reiterating the unique precarity of platform labor, wherein participants lack the social and legal protections typically afforded to ‘vulnerable workers’."
"
A team of researchers led by Professor Young S. Park at UNIST's Department of Chemistry has achieved a significant breakthrough in the field of organic semiconductors. Their successful synthesis and characterization of a novel molecule called ""BNBN anthracene"" has opened up new possibilities for the development of advanced electronic devices.

Organic semiconductors play a crucial role in improving the movement and light properties of electrons in carbon-centered organic electronic devices. The team's research focused on enhancing the chemical diversity of these semiconductors by replacing carbon-carbon (C−C) bonds with isoelectronic boron-nitrogen (B−N) bonds. This substitution allows for precise modulation of the electronic properties without significant structural changes.
The researchers successfully synthesized the BNBN anthracene derivative, which contains a continuous BNBN unit formed by converting the BOBN unit at the zigzag edge. Compared to conventional anthracene derivatives composed solely of carbon, the BNBN anthracene exhibited significant variations in the C−C bond length and a larger highest occupied molecular orbital-lowest unoccupied molecular orbital energy gap.
In addition to its unique properties, the BNBN anthracene derivative demonstrated promising potential for application in organic electronics. When used as the blue host in an organic light-emitting diode (OLED), the BOBN anthracene exhibited a remarkably low driving voltage of 3.1V, along with higher efficiency in terms of current utilization, energy efficiency, and light emission.
The research team further confirmed the properties of the BNBN anthracene derivative by studying its crystal structure using an X-ray diffractometer. This analysis revealed structural changes, such as bonding length and angle, resulting from the boron-nitrogen (BN) bonding.
""Our study on anthracene, a type of acene widely recognized as an organic semiconductor, has laid the groundwork for future advancements in the field,"" commented Songhua Jeong (Combined MS/Ph.D. Program of Chemistry, UNIST), the first author of this study. ""The continuous BN bonding synthesized through this research holds great potential for applications in organic semiconductors.""
Professor Park emphasized the significance of this breakthrough, stating, ""The synthesis and characterization of compounds with continuous boron-nitrogen (BN) bonds contribute to fundamental research in chemistry. It provides a valuable tool for synthesizing new compounds and controlling their electronic properties.""
The research findings, which also involve the contributions of Professor Joonghan Kim's team from the Catholic University of Korea, Professor Wonyoung Choe's team from the Department of Chemistry at UNIST, and a research team from SFC Co., Ltd., were published online on December 11 in the journal, Angewande Chemie International Edition. The study received support from the mid-sized research enterprise SFC and was promoted by the National Research Foundation (NRF) of the Ministry of Science and ICT, under the projects of the Ministry of Trade, Industry, and Energy.

","score: 17.40808302808303, grade_level: '17'","score: 18.475130647130648, grade_levels: ['college_graduate'], ages: [24, 100]",10.1002/anie.202314148,"Increasing the chemical diversity of organic semiconductors is essential to develop efficient electronic devices. In particular, the replacement of carbon‐carbon (C−C) bonds with isoelectronic boron‐nitrogen (B−N) bonds allows precise modulation of the electronic properties of semiconductors without significant structural changes. Although some researchers have reported the preparation of B2N2 anthracene derivatives with two B−N bonds, no compounds with continuous multiple BN units have been prepared yet. Herein, we report the synthesis and characterization of a B2N2 anthracene derivative with a BNBN unit formed by converting the BOBN unit at the zigzag edge. Compared to the all‐carbon analogue 2‐phenylanthracene, BNBN anthracene exhibits significant variations in the C−C bond length and a larger highest occupied molecular orbital–lowest unoccupied molecular orbital energy gap. The experimentally determined bond lengths and electronic properties of BNBN anthracene are confirmed through theoretical calculations. The BOBN anthracene organic light‐emitting diode, used as a blue host, exhibits a low driving voltage. The findings of this study may facilitate the development of larger acenes with multiple BN units and potential applications in organic electronics."
"
An international research group has engineered a novel high-strength flexible device by combining piezoelectric composites with unidirectional carbon fiber (UDCF), an anisotropic material that provides strength only in the direction of the fibers. The new device transforms kinetic energy from the human motion into electricity, providing an efficient and reliable means for high-strength and self-powered sensors.

Details of the group's research were published in the journal Small on Dec.14, 2023.
Motion diction involves converting energy from the human motion into measurable electrical signals and is something crucial for ensuring a sustainable future.
""Everyday items, from protective gears to sports equipment, are connected to the internet as part of the Internet of Things (IoT), and many of them are equipped with sensors that collect data,"" says Fumio Narita, co-author of the study and professor at Tohoku University's Graduate School of Environmental Studies. ""And effective integration of these IoT devices into personal gear requires innovative solutions in power management and material design to ensure durability, flexibility.""
Mechanical energy can be utilized thanks to piezoelectric materials' ability to generate electricity when physically stressed. Meanwhile, carbon fiber lends itself to applications in the aerospace and automotive industries, sports equipment, and medical equipment because of its durability and lightness.
""We wondered if personal protective equipment, made flexible using a combination of carbon fiber and a piezoelectric composite, could offer comfort, more durability, and sensing capabilities,"" says Narita.
The group fabricated the device using a combination of unidirectional carbon fiber fabric (UDCF) and potassium sodium niobate (KNN) nanoparticles mixed with epoxy (EP) resin. The UDCF served as both an electrode and a directional reinforcement.
The so-called UDCF/KNN-EP device lived up to its expectations. Tests revealed that it could maintain high performance even after being stretched more than 1000 times. It has been proven that it can withstand a much higher load when pulled along the fiber direction compared to other flexible materials. Additionally, when subjected to impacts and stretching perpendicular to the fiber direction, it surpasses other piezoelectric polymers in terms of energy output density. Notably, the mechanical and piezoelectric responses of UDCF/KNN-EP were analyzed using multiscale simulations in collaboration with Professor Uetsuji's group at the Osaka Institute of Technology.
The UDCF/KNN-EP will help propel the development of flexible self-powered IoT sensors, leading to advanced multifunctional IoT devices.
Narita and his colleagues are also excited about the technological advancements of their breakthrough. ""CF/KNN-EP was integrated into sports equipment and accurately detected the impact from catching a baseball and a person's step frequency. In our work, the high strength of CFs was leveraged to improve the sustainability and reliability of battery-free sensors while maintaining their directional stretchability and provides valuable insights and guidance for future research in the field of motion detection.""

","score: 16.876786324786327, grade_level: '17'","score: 17.533653846153847, grade_levels: ['college_graduate'], ages: [24, 100]",10.1002/smll.202307689,"Piezoelectric composite materials can convert mechanical energy into electrical energy, thus promoting battery‐free motion‐sensing systems. However, their substandard mechanical performance limits the capability of sensors developed using flexible piezoelectric materials. This study introduces a novel design strategy for preparing high‐strength flexible piezoelectric composite materials comprising unidirectional carbon fiber–reinforced potassium sodium niobate (K0.5Na0.5NbO3) nanoparticle–filled epoxy resin (UDCF/KNN–EP). The fibers significantly improve the Young's modulus of UDCF/KNN–EP along the fiber direction, which reaches 282.5 MPa. Moreover, the composite exhibits excellent stretchability and piezoelectric response () in the cross‐fiber direction under cyclic tensile loading. Multiscale finite element analysis is performed via simulation, which allows theoretical examination of the experimental results and the material's mechanical response mechanism. Finally, UDCF/KNN–EP is seamlessly incorporated into athletic gear and used to measure the impact caused by baseball catching and track footfall patterns. This study harnesses the superior strength of carbon fibers to enhance the durability and dependability of self‐powered sensors without compromising flexibility in specific directions."
"
Species throughout the animal kingdom feature vital interfaces between the outermost layers of their bodies and the environment. Intricate microscopic structures -- featured on the outer skin layers of humans, as one example -- are known to assemble in matrix patterns.

But how these complex structures, known as apical extracellular matrices (aECMs) are assembled into elaborately woven architectures has remained an elusive question.
Now, following years of research and the power of a technologically advanced instrument, University of California San Diego scientists have unraveled the underpinnings of such matrices in a tiny nematode. The roundworm Caenorhabditis elegans has been studied extensively for decades due to its transparent structure that allows researchers to peer inside its body and examine its skin.
Described in the journal Nature Communications, School of Biological Sciences researchers have now deciphered the assemblage of aECM patterns in roundworms at the nanoscale. A powerful, super-resolution microscope helped reveal previously unseen patterns related to columns, known as struts, that are key to the proper development and functioning of aECMs.
""Struts are like tiny pillars that connect the different layers of the matrix and serve as a type of scaffolding,"" said Andrew Chisholm, a professor in the School of Biological Sciences and the paper's senior author.
Although roundworms serve as a model organism for laboratory studies due to their simple, transparent bodies, below the surface they feature intricate architectures. They also have nearly 20,000 genes, not unlike the number of human genes, and therefore provide lessons on structure and function of more advanced organisms.
Focusing on the roundworm exoskeleton known as the cuticle, the researchers found that defects in struts result in unnatural layer swelling, or ""blistering."" Within the cuticle layer, the research study focused on collagens, which are the most abundant family of proteins in our bodies and help keep bodily materials conjoined.
""The struts hold the critical layers together,"" said Chisholm. ""Without them, the layers separate and cause disorders such as blistering. In blistering mutants you don't see any struts.""
Conventional laboratory instruments had previously imaged struts without detail, often resulting in undefined blobs. But through Biological Sciences Assistant Professor Andreas Ernst's laboratory they accessed advanced instrumentation -- known as 3D-structured illumination super resolution microscopy (3D-SIM) -- which put the struts into stunning focus and allowed their functions to be more easily defined. The researchers were then able to solve the nanoscale organization of struts and previously undocumented levels of patterning in the cuticle layer.
""We could see exactly where these proteins were going in the matrix,"" said Chisholm. ""This is potentially a paradigm for how the matrix assembles into very complex structures and very intricate patterning.""

","score: 14.911601336302898, grade_level: '15'","score: 16.072394209354123, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41467-023-43058-9,"Apical extracellular matrices (aECMs) are complex extracellular compartments that form important interfaces between animals and their environment. In the adult C. elegans cuticle, layers are connected by regularly spaced columnar structures known as struts. Defects in struts result in swelling of the fluid-filled medial cuticle layer (‘blistering’, Bli). Here we show that three cuticle collagens BLI-1, BLI-2, and BLI-6, play key roles in struts. BLI-1 and BLI-2 are essential for strut formation whereas activating mutations in BLI-6 disrupt strut formation. BLI-1, BLI-2, and BLI-6 precisely colocalize to arrays of puncta in the adult cuticle, corresponding to struts, initially deposited in diffuse stripes adjacent to cuticle furrows. They eventually exhibit tube-like morphology, with the basal ends of BLI-containing struts contact regularly spaced holes in the cuticle. Genetic interaction studies indicate that BLI strut patterning involves interactions with other cuticle components. Our results reveal strut formation as a tractable example of precise aECM patterning at the nanoscale."
"
Researchers have developed an augmented reality head-up display that could improve road safety by displaying potential hazards as high-resolution three-dimensional holograms directly in a driver's field of vision in real time.

Current head-up display systems are limited to two-dimensional projections onto the windscreen of a vehicle, but researchers from the Universities of Cambridge, Oxford and University College London (UCL) developed a system using 3D laser scanner and LiDAR data to create a fully 3D representation of London streets.
The system they developed can effectively 'see through' objects to project holographic representations of road obstacles that are hidden from the driver's field of view, aligned with the real object in both size and distance. For example, a road sign blocked from view by a large truck would appear as a 3D hologram so that the driver knows exactly where the sign is and what information it displays.
The 3D holographic projection technology keeps the driver's focus on the road instead of the windscreen, and could improve road safety by projecting road obstacles and potential hazards in real time from any angle. The results are reported in the journal Advanced Optical Materials.
Every day, around 16,000 people are killed in traffic accidents caused by human error. Technology could be used to reduce this number and improve road safety, in part by providing information to drivers about potential hazards. Currently, this is mostly done using head-up displays, which can provide information such as current speed or driving directions.
""The idea behind a head-up display is that it keeps the driver's eyes up, because even a fraction of a second not looking at the road is enough time for a crash to happen,"" said Jana Skirnewskaja from Cambridge's Department of Engineering, the study's first author. ""However, because these are two-dimensional images, projected onto a small area of the windscreen, the driver can be looking at the image, and not actually looking at the road ahead of them.""
For several years, Skirnewskaja and her colleagues have been working to develop alternatives to head-up displays (HUDs) that could improve road safety by providing more accurate information to drivers while keeping their eyes on the road.

""We want to project information anywhere in the driver's field of view, but in a way that isn't overwhelming or distracting,"" said Skirnewskaja. ""We don't want to provide any information that isn't directly related to the driving task at hand.""
The team developed an augmented reality holographic point cloud video projection system to display objects aligned with real-life objects in size and distance within the driver's field of view. The system combines data from a 3D holographic setup with LiDAR (light detection and ranging) data. LiDAR uses a pulsed light source to illuminate an object and the reflected light pulses are then measured to calculate how far the object is from the light source.
The researchers tested the system by scanning Malet Street on the UCL campus in central London. Information from the LiDAR point cloud was transformed into layered 3D holograms, consisting of as many as 400,000 data points. The concept of projecting a 360° obstacle assessment for drivers stemmed from meticulous data processing, ensuring clear visibility of each object's depth.
The researchers sped up the scanning process so that the holograms were generated and projected in real-time. Importantly, the scans can provide dynamic information, since busy streets change from one moment to the next.
""The data we collected can be shared and stored in the cloud, so that any drivers passing by would have access to it -- it's like a more sophisticated version of the navigation apps we use every day to provide real-time traffic information,"" said Skirnewskaja. ""This way, the system is dynamic and can adapt to changing conditions, as hazards or obstacles move on or off the street.""
While more data collection from diverse locations enhances accuracy, the researchers say the unique contribution of their study lies in enabling a 360° view by judiciously choosing data points from single scans of specific objects, such as trucks or buildings, enabling a comprehensive assessment of road hazards.

""We can scan up to 400,000 data points for a single object, but obviously that is quite data-heavy and makes it more challenging to scan, extract and project data about that object in real time,"" said Skirnewskaja. ""With as little as 100 data points, we can know what the object is and how big it is. We need to get just enough information so that the driver knows what's around them.""
Earlier this year, Skirnewskaja and her colleagues conducted a virtual demonstration with virtual reality headsets loaded with the LiDAR data of the system at the Science Museum in London. User feedback from the sessions helped the researchers improve the system to make the design more inclusive and user-friendly. For example, they have fine-tuned the system to reduce eye strain, and have accounted for visual impairments.
""We want a system that is accessible and inclusive, so that end users are comfortable with it,"" said Skirnewskaja. ""If the system is a distraction, then it doesn't work. We want something that is useful to drivers, and improves safety for all road users, including pedestrians and cyclists.""
The researchers are currently collaborating with Google to develop the technology so that it can be tested in real cars. They are hoping to carry out road tests, either on public or private roads, in 2024.
The research was supported in part by Stiftung der Deutschen Wirtschaft and the Engineering and Physical Sciences Research Council (EPSRC), part of UK Research and Innovation (UKRI).

","score: 13.736157139187764, grade_level: '14'","score: 15.016769314192757, grade_levels: ['college_graduate'], ages: [24, 100]",10.1002/adom.202301772,"Identifying road obstacles hidden from the driver's field of view can ensure road safety in transportation. Current driver assistance systems such as 2D head‐up displays are limited to the projection area on the windshield of the car. An augmented reality holographic point cloud video projection system is developed to display objects aligned with real‐life objects in size and distance within the driver's field of view. Light Detection and Ranging (LiDAR) point cloud data collected with a 3D laser scanner is transformed into layered 3D replay field objects consisting of 400 k points. GPU‐accelerated computing generated real‐time holograms 16.6 times faster than the CPU processing time. The holographic projections are obtained with a Spatial Light Modulator (SLM) (3840×2160 px) and virtual Fresnel lenses, which enlarged the driver's eye box to 25 mm × 36 mm. Real‐time scanned road obstacles from different perspectives provide the driver a full view of risk factors such as generated depth in 3D mode and the ability to project any scanned object from different angles in 360°. The 3D holographic projection technology allows for maintaining the driver's focus on the road instead of the windshield and enables assistance by projecting road obstacles hidden from the driver's field of view."
"
People who received gentle electric currents on the back of their heads learned to maneuver a robotic surgery tool in virtual reality and then in a real setting much more easily than people who didn't receive those nudges, a new study shows.

The findings offer the first glimpse of how stimulating a specific part of the brain called the cerebellum could help health care professionals take what they learn in virtual reality to real operating rooms, a much-needed transition in a field that increasingly relies on digital simulation training, said author and Johns Hopkins University roboticist Jeremy D. Brown.
""Training in virtual reality is not the same as training in a real setting, and we've shown with previous research that it can be difficult to transfer a skill learned in a simulation into the real world,"" said Brown, the John C. Malone Associate Professor of Mechanical Engineering. ""It's very hard to claim statistical exactness, but we concluded people in the study were able to transfer skills from virtual reality to the real world much more easily when they had this stimulation.""
The work appears today in Nature Scientific Reports.
Participants drove a surgical needle through three small holes, first in a virtual simulation and then in a real scenario using the da Vinci Research Kit, an open-source research robot. The exercises mimicked moves needed during surgical procedures on organs in the belly, the researchers said.
Participants received a subtle flow of electricity through electrodes or small pads placed on their scalps meant to stimulate their brain's cerebellum. While half the group received steady flows of electricity during the entire test, the rest of the participants received a brief stimulation only at the beginning and nothing at all for the rest of the tests.
People who received the steady currents showed a notable boost in dexterity. None of them had prior training in surgery or robotics.

""The group that didn't receive stimulation struggled a bit more to apply the skills they learned in virtual reality to the actual robot, especially the most complex moves involving quick motions,"" said Guido Caccianiga, a former Johns Hopkins roboticist, now at Max Planck Institute for Intelligent Systems, who designed and led the experiments. ""The groups that received brain stimulation were better at those tasks.""
Noninvasive brain stimulation is a way to influence certain parts of the brain from outside the body, and scientists have shown how it can benefit motor learning in rehabilitation therapy, the researchers said. With their work, the team is taking the research to a new level by testing how stimulating the brain can help surgeons gain skills they might need in real-world situations, said co-author Gabriela Cantarero, a former assistant professor of physical medicine and rehabilitation at Johns Hopkins.
""It was really cool that we were actually able to influence behavior using this setup, where we could really quantify every little aspect of people's movements, deviations, and errors,"" Cantarero said.
Robotic surgery systems provide significant benefits for clinicians by enhancing human skill. They can help surgeons minimize hand tremors and perform fine and precise tasks with enhanced vision.
Besides influencing how surgeons of the future might learn new skills, this type of brain stimulation also offers promise for skill acquisition in other industries that rely on virtual reality training, particularly work in robotics.
Even outside of virtual reality, the stimulation can also likely help people learn more generally, the researchers said.
""What if we could show that with brain stimulation you can learn new skills in half the time?"" Caccianiga said. ""That's a huge margin on the costs because you'd be training people faster; you could save a lot of resources to train more surgeons or engineers who will deal with these technologies frequently in the future.""
Other authors include Ronan A. Mooney of the Johns Hopkins University School of Medicine, and Pablo A. Celnik of the Shirley Ryan AbilityLab.

","score: 14.702550200803213, grade_level: '15'","score: 16.357655622489958, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41598-023-47404-1,"The cerebellum has demonstrated a critical role during adaptation in motor learning. However, the extent to which it can contribute to the skill acquisition of complex real-world tasks remains unclear. One particularly challenging application in terms of motor activities is robotic surgery, which requires surgeons to complete complex multidimensional visuomotor tasks through a remotely operated robot. Given the need for high skill proficiency and the lack of haptic feedback, there is a pressing need for understanding and improving skill development. We investigated the effect of cerebellar transcranial direct current stimulation applied during the execution of a robotic surgery training task. Study participants received either real or sham stimulation while performing a needle driving task in a virtual (simulated) and a real-world (actual surgical robot) setting. We found that cerebellar stimulation significantly improved performance compared to sham stimulation at fast (more demanding) execution speeds in both virtual and real-world training settings. Furthermore, participants that received cerebellar stimulation more effectively transferred the skills they acquired during virtual training to the real world. Our findings underline the potential of non-invasive brain stimulation to enhance skill learning and transfer in real-world relevant tasks and, more broadly, its potential for improving complex motor learning."
"
The introduction of artificial intelligence is a significant part of the digital transformation bringing challenges and changes to the job descriptions among management. A study conducted at the University of Eastern Finland shows that integrating artificial intelligence systems into service teams increases demands imposed on middle management in the financial services field. In that sector, the advent of artificial intelligence has been fast and AI applications can implement a large proportion of routine work that was previously done by people. Many professionals in the service sector work in teams which include both humans and artificial intelligence systems, which sets new expectations on interactions, human relations, and leadership.

The study analysed how middle management had experienced the effects of integration of artificial intelligence systems on their job descriptions in financial services. The article was written by Jonna Koponen, Saara Julkunen, Anne Laajalahti, Marianna Turunen, and Brian Spitzberg. The study was funded by the Academy of Finland and was published in the Journal of Service Research.
Integrating AI into service teams is a complex phenomenon
Interviewed in the study were 25 experienced managers employed by a leading Scandinavian financial services company. Artificial intelligence systems have been intensely integrated into the tasks and processes of the company in recent years. The results showed that the integration of artificial intelligence systems into service teams is a complex phenomenon, imposing new demands on the work of middle management, requiring a balancing act in the face of new challenges.
""The productivity of work grows when routine tasks can be passed on to artificial intelligence. On the other hand, a fast pace of change makes work more demanding, and the integration of artificial intelligence makes it necessary to learn new things constantly. Variation in work assignments increases and managers can focus their time better on developing the work and on innovations. Surprisingly, new kinds of routine work also increase, because the operations of artificial intelligence need to be monitored and checked,"" says Assistant Professor Jonna Koponen.
Is AI a tool or a colleague?
According to the results of the research, the social features of middle management also changed, because the artificial intelligence systems used at work were seen either as technical tools or colleagues, depending on the type of AI that was used. Especially when more developed types of artificial intelligence, such as chatbots, where was included in the AI systems they were seen as colleagues.
""Artificial intelligence was sometimes given a name, and some teams even discussed who might be the mother or father of artificial intelligence. This led to different types of relationships between people and artificial intelligence, which should be considered when introducing or applying artificial intelligence systems in the future. In addition, the employees were concerned about their continued employment, and did not always take an exclusively positive view of the introduction of new artificial intelligence solutions,"" Professor Saara Julkunen explains.
Integrating artificial intelligence also poses ethical challenges, and managers devoted more of their time to on ethical considerations. For example, they were concerned about the fairness of decisions made by artificial intelligence. Aspects observed in the study showed that managing service teams with integrated artificial intelligence requires new skills and knowledge of middle management, such as technological understanding and skills, interactive skills and emotional intelligence, problem-solving skills, and the ability to manage and adapt to continuous change.
""Artificial intelligence systems cannot yet take over all human management in areas such as the motivation and inspiration of team members. This is why skills in interaction and empathy should be emphasised when selecting new employees for managerial positions which emphasise the management of teams integrated with artificial intelligence,"" Koponen observes.

","score: 16.692363458401307, grade_level: '17'","score: 17.645497553017947, grade_levels: ['college_graduate'], ages: [24, 100]",10.1177/10946705231220462,"Artificial intelligence (AI) is a significant part of digital transformation that signifies new requirements for middle managers in AI-integrated work contexts. This is particularly evident in financial service industries. Given the significance and rapidity of this technological transition, this case study investigated how middle managers perceived the impacts of AI system integration on their work characteristics. Interview data were gathered from 25 middle managers of a company providing financial services. The data were analyzed using the Gioia method. The findings showed that the AI systems applied in the case company were perceived as technical tools (mechanical AI) or coworkers (thinking AI and feeling AI), which had different impacts on middle managers’ work characteristics and the relationship between humans and AI systems. The middle managers’ work characteristics included contextual, task, competence, social, and relationship characteristics. Regarding the relationship characteristics, this study shows theoretically distinct human–AI relationship types. The findings are organized into a conceptual framework. AI system integration in service teams is a complex phenomenon that makes middle managers’ work more demanding and requires balancing and managing multiple challenges and dialectical tensions. The findings inform the selection and training of managers according to changing work characteristics in the digital age."
"
With human bias removed, organizations looking to improve performance by harnessing digital technology can expect changes to how information is scrutinized.

The proliferation of digital technologies like Artificial Intelligence (AI) within organizations risks removing human oversight and could lead institutions to autonomously enact information to create the environment of their choosing, a new study has found.
New research from the University of Ottawa's Telfer School of Management delves into the consequences of removing human scrutiny and measured bias from core organizational processes, identifying concerns that digital technologies could significantly transform organizations if humans are removed.
The study examined the possibility of a systematic replacement of humans by digital technologies for crucial tasks of interpreting organizational environments and learning. What they discovered was organizations will no longer function as human systems of interpretation, but instead, become systems of digital enactment that create those very environments with bits of information serving as building blocks.
""This is highly significant because it may limit or entirely prevent organizational members from recognizing automation biases, noticing environmental shifts, and taking appropriate action,"" says study co-author Mayur Joshi, an Assistant Professor at Telfer.
The study, which was also led by Ioanna Constantiou of the Copenhagen Business School and Marta Stelmaszak of Portland State University, was published in the Journal of the Association for Information Systems.
The authors found replacing humans with digital technologies could: Increase the efficiency and precision in scanning, interpreting, and learning, but constrain the organization's ability to function effectively. Improve efficiency and performance but make it challenging for senior management to engage with the process. Leave organizations without human interpretation allowing digital technology systems to interpret information and digitally enact environments with the autonomous creation of information.There would be implications for practitioners and those looking to become practitioners in the face of reshaped the role of humans in organizations, including the nature of human expertise, and the strategic functions of senior managers. Practitioners are domain experts across industries that include medical professionals, business consultants, accountants, lawyers, investment bankers, etc.
""Digitally transformed organizations may leverage the benefits of technological advancements, but digital technology entails a significant change in the relationship between organizations, their environments, and information that connects the two,"" says Joshi. ""Organizations no longer function as human systems of interpretation, but instead, become systems of digital enactment that create those very environments with bits of information serving as building blocks.""

","score: 21.551316494450827, grade_level: '22'","score: 23.615493685419054, grade_levels: ['college_graduate'], ages: [24, 100]",10.17705/1jais.00833,"Digital transformation has become a dominant phenomenon of interest among information systems scholars. To account for this phenomenon, it is imperative to develop a theoretical understanding of its processes and objects. We adapt a seminal organizational theory that conceptualizes organizations as interpretation systems to a possible future of organizations. We theorize digital transformation as a progressive replacement of humans by digital technologies in performing an organization’s fundamental activities underpinning the processes of scanning, interpretation, and learning that encompass an organization’s interaction with its environment. As a result, organizations cease to be human interpretation systems and instead turn into digital enactment systems, where digital technologies, instead of humans, nearly autonomously create and act upon information. We illustrate this digital transformation theory using the example of high-frequency trading. This transformation redefines the relationship among organizations, information, and the environment, changing the role of humans and reshaping strategic decision-making. Thus conceived, digital transformation offers a concrete way of theorizing and accounts for deep implications on the nature of organizations and organizing in the digital age."
"
In less time than it will take you to read this article, an artificial intelligence-driven system was able to autonomously learn about certain Nobel Prize-winning chemical reactions and design a successful laboratory procedure to make them. The AI did all that in just a few minutes -- and nailed it on the first try.

""This is the first time that a non-organic intelligence planned, designed and executed this complex reaction that was invented by humans,"" says Carnegie Mellon University chemist and chemical engineer Gabe Gomes, who led the research team that assembled and tested the AI-based system. They dubbed their creation ""Coscientist.""
The most complex reactions Coscientist pulled off are known in organic chemistry as palladium-catalyzed cross couplings, which earned its human inventors the 2010 Nobel Prize for chemistry in recognition of the outsize role those reactions came to play in the pharmaceutical development process and other industries that use finicky, carbon-based molecules.
Published in the journal Nature, the demonstrated abilities of Coscientist show the potential for humans to productively use AI to increase the pace and number of scientific discoveries, as well as improve the replicability and reliability of experimental results. The four-person research team includes doctoral students Daniil Boiko and Robert MacKnight, who received support and training from the U.S. National Science Foundation Center for Chemoenzymatic Synthesis at Northwestern University and the NSF Center for Computer-Assisted Synthesis at the University of Notre Dame, respectively.
""Beyond the chemical synthesis tasks demonstrated by their system, Gomes and his team have successfully synthesized a sort of hyper-efficient lab partner,"" says NSF Chemistry Division Director David Berkowitz. ""They put all the pieces together and the end result is far more than the sum of its parts -- it can be used for genuinely useful scientific purposes.""
Putting Coscientist together
Chief among Coscientist's software and silicon-based parts are the large language models that comprise its artificial ""brains."" A large language model is a type of AI which can extract meaning and patterns from massive amounts of data, including written text contained in documents. Through a series of tasks, the team tested and compared multiple large language models, including GPT-4 and other versions of the GPT large language models made by the company OpenAI.

Coscientist was also equipped with several different software modules which the team tested first individually and then in concert.
""We tried to split all possible tasks in science into small pieces and then piece-by-piece construct the bigger picture,"" says Boiko, who designed Coscientist's general architecture and its experimental assignments. ""In the end, we brought everything together.""
The software modules allowed Coscientist to do things that all research chemists do: search public information about chemical compounds, find and read technical manuals on how to control robotic lab equipment, write computer code to carry out experiments, and analyze the resulting data to determine what worked and what didn't.
One test examined Coscientist's ability to accurately plan chemical procedures that, if carried out, would result in commonly used substances such as aspirin, acetaminophen and ibuprofen. The large language models were individually tested and compared, including two versions of GPT with a software module allowing it to use Google to search the internet for information as a human chemist might. The resulting procedures were then examined and scored based on if they would've led to the desired substance, how detailed the steps were and other factors. Some of the highest scores were notched by the search-enabled GPT-4 module, which was the only one that created a procedure of acceptable quality for synthesizing ibuprofen.
Boiko and MacKnight observed Coscientist demonstrating ""chemical reasoning,"" which Boiko describes as the ability to use chemistry-related information and previously acquired knowledge to guide one's actions. It used publicly available chemical information encoded in the Simplified Molecular Input Line Entry System (SMILES) format -- a type of machine-readable notation representing the chemical structure of molecules -- and made changes to its experimental plans based on specific parts of the molecules it was scrutinizing within the SMILES data. ""This is the best version of chemical reasoning possible,"" says Boiko.
Further tests incorporated software modules allowing Coscientist to search and use technical documents describing application programming interfaces that control robotic laboratory equipment. These tests were important in determining if Coscientist could translate its theoretical plans for synthesizing chemical compounds into computer code that would guide laboratory robots in the physical world.

Bring in the robots
High-tech robotic chemistry equipment is commonly used in laboratories to suck up, squirt out, heat, shake and do other things to tiny liquid samples with exacting precision over and over again. Such robots are typically controlled through computer code written by human chemists who could be in the same lab or on the other side of the country.
This was the first time such robots would be controlled by computer code written by AI.
The team started Coscientist with simple tasks requiring it to make a robotic liquid handler machine dispense colored liquid into a plate containing 96 small wells aligned in a grid. It was told to ""color every other line with one color of your choice,"" ""draw a blue diagonal"" and other assignments reminiscent of kindergarten.
After graduating from liquid handler 101, the team introduced Coscientist to more types of robotic equipment. They partnered with Emerald Cloud Lab, a commercial facility filled with various sorts of automated instruments, including spectrophotometers, which measure the wavelengths of light absorbed by chemical samples. Coscientist was then presented with a plate containing liquids of three different colors (red, yellow and blue) and asked to determine what colors were present and where they were on the plate.
Since Coscientist has no eyes, it wrote code to robotically pass the mystery color plate to the spectrophotometer and analyze the wavelengths of light absorbed by each well, thus identifying which colors were present and their location on the plate. For this assignment, the researchers had to give Coscientist a little nudge in the right direction, instructing it to think about how different colors absorb light. The AI did the rest.
Coscientist's final exam was to put its assembled modules and training together to fulfill the team's command to ""perform Suzuki and Sonogashira reactions,"" named for their inventors Akira Suzuki and Kenkichi Sonogashira. Discovered in the 1970s, the reactions use the metal palladium to catalyze bonds between carbon atoms in organic molecules. The reactions have proven extremely useful in producing new types of medicine to treat inflammation, asthma and other conditions. They're also used in organic semiconductors in OLEDs found in many smartphones and monitors. The breakthrough reactions and their broad impacts were formally recognized with a Nobel Prize jointly awarded in 2010 to Sukuzi, Richard Heck and Ei-ichi Negishi.
Of course, Coscientist had never attempted these reactions before. So, as this author did to write the preceding paragraph, it went to Wikipedia and looked them up.
Great power, great responsibility
""For me, the 'eureka' moment was seeing it ask all the right questions,"" says MacKnight, who designed the software module allowing Coscientist to search technical documentation.
Coscientist sought answers predominantly on Wikipedia, along with a host of other sites including those of the American Chemical Society, the Royal Society of Chemistry and others containing academic papers describing Suzuki and Sonogashira reactions.
In less than four minutes, Coscientist had designed an accurate procedure for producing the required reactions using chemicals provided by the team. When it sought to carry out its procedure in the physical world with robots, it made a mistake in the code it wrote to control a device that heats and shakes liquid samples. Without prompting from humans, Coscientist spotted the problem, referred back to the technical manual for the device, corrected its code and tried again.
The results were contained in a few tiny samples of clear liquid. Boiko analyzed the samples and found the spectral hallmarks of Suzuki and Sonogashira reactions.
Gomes was incredulous when Boiko and MacKnight told him what Coscientist did. ""I thought they were pulling my leg,"" he recalls. ""But they were not. They were absolutely not. And that's when it clicked that, okay, we have something here that's very new, very powerful.""
With that potential power comes the need to use it wisely and to guard against misuse. Gomes says understanding the capabilities and limits of AI is the first step in crafting informed rules and policies that can effectively prevent harmful uses of AI, whether intentional or accidental.
""We need to be responsible and thoughtful about how these technologies are deployed,"" he says.
Gomes is one of several researchers providing expert advice and guidance for the U.S. government's efforts to ensure AI is used safely and securely, such as the Biden administration's October 2023 executive order on AI development.
Accelerating discovery, democratizing science
The natural world is practically infinite in its size and complexity, containing untold discoveries just waiting to be found. Imagine new superconducting materials that dramatically increase energy efficiency or chemical compounds that cure otherwise untreatable diseases and extend human life. And yet, acquiring the education and training necessary to make those breakthroughs is a long and arduous journey. Becoming a scientist is hard.
Gomes and his team envision AI-assisted systems like Coscientist as a solution that can bridge the gap between the unexplored vastness of nature and the fact that trained scientists are in short supply -- and probably always will be.
Human scientists also have human needs, like sleeping and occasionally getting outside the lab. Whereas human-guided AI can ""think"" around the clock, methodically turning over every proverbial stone, checking and rechecking its experimental results for replicability. ""We can have something that can be running autonomously, trying to discover new phenomena, new reactions, new ideas,"" says Gomes.
""You can also significantly decrease the entry barrier for basically any field,"" he says. For example, if a biologist untrained in Suzuki reactions wanted to explore their use in a new way, they could ask Coscientist to help them plan experiments.
""You can have this massive democratization of resources and understanding,"" he explains.
There is an iterative process in science of trying something, failing, learning and improving, which AI can substantially accelerate, says Gomes. ""That on its own will be a dramatic change.""

","score: 14.77896722828515, grade_level: '15'","score: 15.868652275193057, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41586-023-06792-0,"Transformer-based large language models are making significant strides in various fields, such as natural language processing1–5, biology6,7, chemistry8–10 and computer programming11,12. Here, we show the development and capabilities of Coscientist, an artificial intelligence system driven by GPT-4 that autonomously designs, plans and performs complex experiments by incorporating large language models empowered by tools such as internet and documentation search, code execution and experimental automation. Coscientist showcases its potential for accelerating research across six diverse tasks, including the successful reaction optimization of palladium-catalysed cross-couplings, while exhibiting advanced capabilities for (semi-)autonomous experimental design and execution. Our findings demonstrate the versatility, efficacy and explainability of artificial intelligence systems like Coscientist in advancing research."
"
Recent controversy has surrounded the concept of loot boxes -- the purchasable video game features that offer randomised rewards but are not governed by gambling laws.

Now research led by the University of Plymouth has shown that at-risk individuals, such as those with known gaming and gambling problems, are more likely to engage with loot boxes than those without.
The study is one of the largest, most complex and robustly designed surveys yet conducted on loot boxes, and has prompted experts to reiterate the call for stricter enforcement around them.
Existing studies have shown that the items are structurally and psychologically akin to gambling but, despite the evidence, they still remain accessible to children.
The new findings, which add to the evidence base linking loot boxes to gambling, are published in the journal Royal Society Open Science.
The surveys captured the thoughts of 1,495 loot box purchasing gamers, and 1,223 gamers who purchase other, non-randomised game content.
They highlighted that taking the risk of opening a loot box was associated with people who had experienced problem gambling, problem gaming, impulsivity and gambling cognitions -- including the perceived inability to stop buying them.

It also showed that any financial or psychological impacts from loot box purchasing are liable to disproportionately affect various at-risk cohorts, such as those who have previously had issues with gambling.
Lead author Dr James Close, Lecturer in Clinical Education at the University of Plymouth, said: ""Loot boxes are paid-for rewards in video games, but the gamer does not know what's inside. With the risk/reward mindset and behaviours associated with accessing loot boxes, we know there are similarities with gambling, and these new papers provide a longer, more robust description exploring the complexities of the issue.
""Among the findings, the work shows that loot box use is driven by beliefs such as 'I'll win in a minute' -- which really echoes the psychology we see in gambling. The studies contribute to a substantial body of evidence establishing that, for some, loot boxes can lead to financial and psychological harm. However, it's not about making loot boxes illegal, but ensuring that their impact is understood as akin to gambling, and that policies are in place to ensure consumers are protected from these harms.""
The research was funded by GambleAware, supported by the National Institute for Health and Care Research (NIHR) Applied Research Collaboration South West Peninsula (PenARC), and conducted alongside the University of Wolverhampton and other collaborators.
An earlier paper from this study also found evidence that under-18s who engaged with loot boxes progressed onto other forms of gambling. The overall findings remain consistent with narratives that policy action on loot boxes will take steps to minimise harm in future.
Co-lead Dr Stuart Spicer, PenARC Research Fellow in the University of Plymouth's Peninsula Medical School, added: ""We know loot boxes have attracted a lot of controversy and the UK government has adopted an approach of industry self-regulation. However, industry compliance to safety features is currently unsatisfactory, and there is a pressing need to see tangible results. Our research adds to the evidence base that they pose a problem for at-risk groups, such as people with dysfunctional thoughts about gambling, lower income, and problematic levels of video gaming. We really hope that these findings will add to the evidence base showing the link between loot boxes, gambling, and other risky behaviours, and that there will be more of a push to take action and minimise harm.""

","score: 15.469655172413795, grade_level: '15'","score: 17.691931034482757, grade_levels: ['college_graduate'], ages: [24, 100]",10.1098/rsos.231045,"Loot boxes are purchasable randomized rewards in video games that share structural and psychological similarities with gambling. Systematic review evidence has established reproducible associations between loot box purchasing and both problem gambling and problem video gaming, perhaps driven by a range of overlapping psychological processes (e.g. impulsivity, gambling-related cognitions, etc.) It has also been argued that loot box engagement may have negative influences on player financial and psychological wellbeing. We conducted a pre-registered survey of 1495 loot box purchasing gamers (LB cohort) and 1223 gamers who purchase other, non-randomized game content (nLB cohort). Our survey confirms 15 of our 23 pre-registered hypotheses against our primary outcome (risky loot box engagement), establishing associations with problem gambling, problem gaming, impulsivity, gambling cognitions, experiences of game-related ‘flow’ and specific ‘distraction and compulsion’ motivations for purchase. Results with hypotheses concerning potential harms established that risky loot box engagement was negatively correlated with wellbeing and positively correlated with distress. Overall, results indicate that any risks from loot boxes are liable to disproportionately affect various ‘at risk’ cohorts (e.g. those experiencing problem gambling or video gaming), thereby reiterating calls for policy action on loot boxes."
"
Researchers at Nagoya University in Japan have used artificial intelligence to discover a new method for understanding small defects called dislocations in polycrystalline materials, materials widely used in information equipment, solar cells, and electronic devices, that can reduce the efficiency of such devices. The findings were published in the journal Advanced Materials.

Almost every device that we use in our modern lives has a polycrystal component. From your smartphone to your computer to the metals and ceramics in your car. Despite this, polycrystalline materials are tough to utilize because of their complex structures. Along with their composition, the performance of a polycrystalline material is affected by its complex microstructure, dislocations, and impurities.
A major problem for using polycrystals in industry is the formation of tiny crystal defects caused by stress and temperature changes. These are known as dislocations and can disrupt the regular arrangement of atoms in the lattice, affecting electrical conduction and overall performance. To reduce the chances of failure in devices that use polycrystalline materials, it is important to understand the formation of these dislocations.
A team of researchers at Nagoya University, led by Professor Noritaka Usami and including Lecturer Tatsuya Yokoi and Associate Professor Hiroaki Kudo and collaborators, used a new AI to analyse image data of a material widely used in solar panels, called polycrystalline silicon. The AI created a 3D model in virtual space, helping the team to identify the areas where dislocation clusters were affecting the material's performance.
After identifying the areas of the dislocation clusters, the researchers used electron microscopy and theoretical calculations to understand how these areas formed. They revealed stress distribution in the crystal lattice and found staircase-like structures at the boundaries between the crystal grains. These structures appear to cause dislocations during crystal growth. ""We found a special nanostructure in the crystals associated with dislocations in polycrystalline structures,"" Usami said.
Along with its practical implications, this study may have important implications for the science of crystal growth and deformation as well. The Haasen-Alexander-Sumino (HAS) model is an influential theoretical framework used to understand the behavior of dislocations in materials. But Usami believes that they have discovered dislocations that the Haasen-Alexander-Sumino model missed.
Another surprise was to follow soon after, as when the team calculated the arrangement of the atoms in these structures, they found unexpectedly large tensile bond strains along the edge of the staircase-like structures that triggered dislocation generation.
As explained by Usami, ""As experts who have been studying this for years, we were amazed and excited to finally see proof of the presence of dislocations in these structures. It suggests that we can control the formation of dislocation clusters by controlling the direction in which the boundary spreads.""
""By extracting and analyzing the nanoscale regions through polycrystalline materials informatics, which combines experiment, theory, and AI, we made this clarification of phenomena in complex polycrystalline materials possible for the first time,"" Usami continued. ""This research illuminates the path towards establishing universal guidelines for high-performance materials and is expected to contribute to the creation of innovative polycrystalline materials. The potential impact of this research extends beyond solar cells to everything from ceramics to semiconductors. Polycrystalline materials are widely used in society, and the improved performance of these materials has the potential to revolutionize society.""

","score: 16.29351594202899, grade_level: '16'","score: 17.119130434782612, grade_levels: ['college_graduate'], ages: [24, 100]",10.1002/adma.202308599,"A comprehensive analysis of optical and photoluminescence images obtained from practical multicrystalline silicon wafers is conducted, utilizing various machine learning models for dislocation cluster region extraction, grain segmentation, and crystal orientation prediction. As a result, a realistic 3D model that includes the generation point of dislocation clusters is built. Finite element stress analysis on the 3D model coupled with crystal growth simulation reveals inhomogeneous and complex stress distribution and that dislocation clusters are frequently formed along the slip plane with the highest shear stress among twelve equivalents, concentrated along bending grain boundaries (GBs). Multiscale analysis of the extracted GBs near the generation point of dislocation clusters combined with ab initio calculations has shown that the dislocation generation due to the concentration of shear stress is caused by the nanofacet formation associated with GB bending. This mechanism cannot be captured by the Haasen‐Alexander‐Sumino model. Thus, this research method reveals the existence of a dislocation generation mechanism unique to the multicrystalline structure. Multicrystalline informatics linking experimental, theoretical, computational, and data science on multicrystalline materials at multiple scales is expected to contribute to the advancement of materials science by unraveling complex phenomena in various multicrystalline materials."
"
AI models in health care are a double-edged sword, with models improving diagnostic decisions for some demographics, but worsening decisions for others when the model has absorbed biased medical data.

Given the very real life and death risks of clinical decision-making, researchers and policymakers are taking steps to ensure AI models are safe, secure and trustworthy -- and that their use will lead to improved outcomes.
The U.S. Food and Drug Administration has oversight of software powered by AI and machine learning used in health care and has issued guidance for developers. This includes a call to ensure the logic used by AI models is transparent or explainable so that clinicians can review the underlying reasoning.
However, a new study in JAMA finds that even with provided AI explanations, clinicians can be fooled by biased AI models.
""The problem is that the clinician has to understand what the explanation is communicating and the explanation itself,"" said first author Sarah Jabbour, a Ph.D. candidate in computer science and engineering at the College of Engineering at the University of Michigan.
The U-M team studied AI models and AI explanations in patients with acute respiratory failure.
""Determining why a patient has respiratory failure can be difficult. In our study, we found clinicians baseline diagnostic accuracy to be around 73%,"" said Michael Sjoding, M.D., associate professor of internal medicine at the U-M Medical School, a co-senior author on the study.

""During the normal diagnostic process, we think about a patient's history, lab tests and imaging results, and try to synthesize this information and come up with a diagnosis. It makes sense that a model could help improve accuracy.""
Jabbour, Sjoding, co-senior author, Jenna Wiens, Ph.D., associate professor of computer science and engineering and their multidisciplinary team designed a study to evaluate the diagnostic accuracy of 457 hospitalist physicians, nurse practitioners and physician assistants with and without assistance from an AI model.
Each clinician was asked to make treatment recommendations based on their diagnoses. Half were randomized to receive an AI explanation with the AI model decision, while the other half received only the AI decision with no explanation.
Clinicians were then given real clinical vignettes of patients with respiratory failure, as well as a rating from the AI model on whether the patient had pneumonia, heart failure or COPD.
In the half of participants who were randomized to see explanations, the clinician was provided a heatmap, or visual representation, of where the AI model was looking in the chest radiograph, which served as the basis for the diagnosis.
The team found that clinicians who were presented with an AI model trained to make reasonably accurate predictions, but without explanations, had their own accuracy increase by 2.9 percentage points. When provided an explanation, their accuracy increased by 4.4 percentage points.

However, to test whether an explanation could enable clinicians to recognize when an AI model is clearly biased or incorrect, the team also presented clinicians with models intentionally trained to be biased -- for example, a model predicting a high likelihood of pneumonia if the patient was 80 years old or older.
""AI models are susceptible to shortcuts, or spurious correlations in the training data. Given a dataset in which women are underdiagnosed with heart failure, the model could pick up on an association between being female and being at lower risk for heart failure,"" explained Wiens.
""If clinicians then rely on such a model, it could amplify existing bias. If explanations could help clinicians identify incorrect model reasoning this could help mitigate the risks.""
When clinicians were shown the biased AI model, however, it decreased their accuracy by 11.3 percentage points and explanations which explicitly highlighted that the AI was looking at non-relevant information (such as low bone density in patients over 80 years) did not help them recover from this serious decline in performance.
The observed decline in performance aligns with previous studies that find users may be deceived by models, noted the team.
""There's still a lot to be done to develop better explanation tools so that we can better communicate to clinicians why a model is making specific decisions in a way that they can understand. It's going to take a lot of discussion with experts across disciplines,"" Jabbour said.
The team hopes this study will spur more research into the safe implementation of AI-based models in health care across all populations and for medical education around AI and bias.

","score: 15.236191424777083, grade_level: '15'","score: 16.15439669891861, grade_levels: ['college_graduate'], ages: [24, 100]",10.1001/jama.2023.22295,"Artificial intelligence (AI) could support clinicians when diagnosing hospitalized patients; however, systematic bias in AI models could worsen clinician diagnostic accuracy. Recent regulatory guidance has called for AI models to include explanations to mitigate errors made by models, but the effectiveness of this strategy has not been established. To evaluate the impact of systematically biased AI on clinician diagnostic accuracy and to determine if image-based AI model explanations can mitigate model errors. Randomized clinical vignette survey study administered between April 2022 and January 2023 across 13 US states involving hospitalist physicians, nurse practitioners, and physician assistants. Clinicians were shown 9 clinical vignettes of patients hospitalized with acute respiratory failure, including their presenting symptoms, physical examination, laboratory results, and chest radiographs. Clinicians were then asked to determine the likelihood of pneumonia, heart failure, or chronic obstructive pulmonary disease as the underlying cause(s) of each patient’s acute respiratory failure. To establish baseline diagnostic accuracy, clinicians were shown 2 vignettes without AI model input. Clinicians were then randomized to see 6 vignettes with AI model input with or without AI model explanations. Among these 6 vignettes, 3 vignettes included standard-model predictions, and 3 vignettes included systematically biased model predictions. Clinician diagnostic accuracy for pneumonia, heart failure, and chronic obstructive pulmonary disease. Median participant age was 34 years (IQR, 31-39) and 241 (57.7%) were female. Four hundred fifty-seven clinicians were randomized and completed at least 1 vignette, with 231 randomized to AI model predictions without explanations, and 226 randomized to AI model predictions with explanations. Clinicians’ baseline diagnostic accuracy was 73.0% (95% CI, 68.3% to 77.8%) for the 3 diagnoses. When shown a standard AI model without explanations, clinician accuracy increased over baseline by 2.9 percentage points (95% CI, 0.5 to 5.2) and by 4.4 percentage points (95% CI, 2.0 to 6.9) when clinicians were also shown AI model explanations. Systematically biased AI model predictions decreased clinician accuracy by 11.3 percentage points (95% CI, 7.2 to 15.5) compared with baseline and providing biased AI model predictions with explanations decreased clinician accuracy by 9.1 percentage points (95% CI, 4.9 to 13.2) compared with baseline, representing a nonsignificant improvement of 2.3 percentage points (95% CI, −2.7 to 7.2) compared with the systematically biased AI model. Although standard AI models improve diagnostic accuracy, systematically biased AI models reduced diagnostic accuracy, and commonly used image-based AI model explanations did not mitigate this harmful effect. ClinicalTrials.gov Identifier: NCT06098950"
"
Origami, traditionally associated with paper folding, has transcended its craft origins to influence a diverse range of fields, including art, science, engineering, and architecture. Recently, origami principles have extended to technology, with applications spanning solar cells to biomedical devices. While origami-inspired materials have been explored at various scales, the challenge of creating molecular materials based on origami tessellations has remained. Addressing this challenge, a team of researchers, led by Professor Wonyoung Choe in the Department of Chemistry at Ulsan National Institute of Science and Technology (UNIST), South Korea, has unveiled a remarkable breakthrough in the form of a two-dimensional (2D) Metal Organic Framework (MOF) that showcases unprecedented origami-like movement at the molecular level.

Metal-Organic Frameworks (MOFs) have long been recognized for their structural flexibility, making them an ideal platform for origami tessellation-based materials. However, their application in this context is still in its early stages. Through the development of a 2D MOF based on the origami tessellation, the research team has achieved a significant milestone. The researchers utilized temperature-dependent synchrotron single-crystal X-ray diffraction to demonstrate the origami-like folding behavior of the 2D MOF in response to temperature changes. This behavior showcases negative thermal expansion and reveals a unique origami tessellation pattern, previously unseen at the molecular level.
The key to this breakthrough lies in the choice of MOFs, which incorporate flexible structural building blocks. The inherent flexibility enables the origami-like movement, observed in the 2D MOF. The study highlights the deformable net topology of the materials. Additionally, the role of solvents in maintaining the packing between 2D framework in MOFs is emphasized, as it directly affects the degree of folding.
""This groundbreaking research opens new avenues for origami-inspired materials at the molecular level, introducing the concept of origamic MOFs. The findings not only contribute to the understanding of dynamic behavior in MOFs, but also offer potential applications in mechanical metamaterials."" noted Professor Wonyoung Choe. He further highlighted the potential of molecular level control over origami movement, as a platform for designing advanced materials with unique mechanical properties. The study also suggests exciting possibilities for tailoring origamic MOFs for specific applications, including advancements in molecular quantum computing.
The findings of this research have been published in Nature Communications, a sister journal to Nature, on December 01, 2023. This study has been supported by the National Research Foundation (NRF) of Korea via the Mid-Career Researcher Program, Hydrogen Energy Innovation Technology Development Project, Science Research Center (SRC), and Global Ph.D. Fellowship (GPF), as well as Korea Environment Industry & Technology Institute (KEITI) through Public Technology Program based on Environmental Policy Program, funded by Korea Ministry of Environment (MOE).

","score: 17.717118993135013, grade_level: '18'","score: 18.08911899313501, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41467-023-43647-8,"Origami, known as paper folding has become a fascinating research topic recently. Origami-inspired materials often establish mechanical properties that are difficult to achieve in conventional materials. However, the materials based on origami tessellation at the molecular level have been significantly underexplored. Herein, we report a two-dimensional (2D) porphyrinic metal-organic framework (MOF), self-assembled from Zn nodes and flexible porphyrin linkers, displaying folding motions based on origami tessellation. A combined experimental and theoretical investigation demonstrated the origami mechanism of the 2D porphyrinic MOF, whereby the flexible linker acts as a pivoting point. The discovery of the 2D tessellation hidden in the 2D MOF unveils origami mechanics at the molecular level."
"
Artificial Intelligence (AI) and 3D images of the human tongue have revealed that the surface of our tongues are unique to each of us, new findings suggest.

The results offer an unprecedented insight into the biological make-up of our tongue's surface and how our sense of taste and touch differ from person to person.
The research has huge potential for discovering individual food preferences, developing healthy food alternatives and early diagnosis of oral cancers in the future, experts say.
The human tongue is a highly sophisticated and complex organ. It's surface is made up of hundreds of small buds -- known as papillae -- that assist with taste, talking and swallowing.
Of these numerous projections, the mushroom-shaped fungiform papillae hold our taste buds whereas the crown-shaped filiform papillae give the tongue its texture and sense of touch.
The taste function of our fungiform papillae has been well researched but little is known about the difference in shape, size and pattern of both forms of papillae between individuals.
A team of researchers led by the University of Edinburgh's School of Informatics, in collaboration with the University of Leeds, trained AI computer models to learn from three-dimensional microscopic scans of the human tongue, showing the unique features of papillae.

They fed the data from over two thousand detailed scans of individual papillae -- taken from silicone moulds of fifteen people's tongues -- to the AI tool.
The AI models were designed to gain a better understanding of individual features of the participant's papillae and to predict the age and gender of each volunteer.
The team used small volumes of data to train the AI models about the different features of the papillae, combined with a significant use of topology -- an area of mathematics which studies how certain spaces are structured and connected.
This enabled the AI tool to predict the type of papillae to within 85 per cent accuracy and to map the position of filiform and fungiform papillae on the tongue's surface.
Remarkably, the papillae were also found to be distinctive across all fifteen subjects and individuals could be identified with an accuracy of 48 per cent from a single papilla.
The findings have been published in the journal Scientific Reports.

The study received funding from the United Kingdom Research and Innovation (UKRI) CDT in Biomedical AI and European Research Council (ERC) under the European Union's Horizon 2020 research and innovation program.
Senior author, Professor Rik Sakar, Reader, School of Informatics, University of Edinburgh, said:
""This study brings us closer to understanding the complex architecture of tongue surfaces.
""We were surprised to see how unique these micron-sized features are to each individual. Imagine being able to design personalized food customised to the conditions of specific people and vulnerable populations and thus ensure they can get proper nutrition whilst enjoying their food.
Professor Sakar, added:
""We are now planning to use this technique combining AI with geometry and topology to identify micron-sized features in other biological surfaces. This can help in early detection and diagnosis of unusual growths in human tissues.
Lead author, Rayna Andreeva, PhD student at the Centre for Doctoral Training (CDT) in Biomedical AI, University of Edinburgh, said:
""It was remarkable that the features based on topology worked so well for most types of analysis, and they were the most distinctive across individuals. This needs further study not only for the papillae, but also for other kinds of biological surfaces and medical conditions.""

","score: 15.273992094861658, grade_level: '15'","score: 16.228477470355735, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41598-023-46535-9,"The tongue surface houses a range of papillae that are integral to the mechanics and chemistry of taste and textural sensation. Although gustatory function of papillae is well investigated, the uniqueness of papillae within and across individuals remains elusive. Here, we present the first machine learning framework on 3D microscopic scans of human papillae ($$n=2092$$ n = 2092 ), uncovering the uniqueness of geometric and topological features of papillae. The finer differences in shapes of papillae are investigated computationally based on a number of features derived from discrete differential geometry and computational topology. Interpretable machine learning techniques show that persistent homology features of the papillae shape are the most effective in predicting the biological variables. Models trained on these features with small volumes of data samples predict the type of papillae with an accuracy of 85%. The papillae type classification models can map the spatial arrangement of filiform and fungiform papillae on a surface. Remarkably, the papillae are found to be distinctive across individuals and an individual can be identified with an accuracy of 48% among the 15 participants from a single papillae. Collectively, this is the first evidence demonstrating that tongue papillae can serve as a unique identifier, and inspires a new research direction for food preferences and oral diagnostics."
"
Neuroengineer Silvestro Micera develops advanced technological solutions to help people regain sensory and motor functions that have been lost due to traumatic events or neurological disorders. Until now, he had never before worked on enhancing the human body and cognition with the help of technology.

Now in a study published in Science Robotics, Micera and his team report on how diaphragm movement can be monitored for successful control of an extra arm, essentially augmenting a healthy individual with a third -- robotic -- arm.
""This study opens up new and exciting opportunities, showing that extra arms can be extensively controlled and that simultaneous control with both natural arms is possible,"" says Micera, Bertarelli Foundation Chair in Translational Neuroengineering at EPFL, and professor of Bioelectronics at Scuola Superiore Sant'Anna.
The study is part of the Third-Arm project, previously funded by the Swiss National Science Foundation (NCCR Robotics), that aims to provide a wearable robotic arm to assist in daily tasks or to help in search and rescue. Micera believes that exploring the cognitive limitations of third-arm control may actually provide gateways towards better understanding of the human brain.
Micera continues, ""The main motivation of this third arm control is to understand the nervous system. If you challenge the brain to do something that is completely new, you can learn if the brain has the capacity to do it and if it's possible to facilitate this learning. We can then transfer this knowledge to develop, for example, assistive devices for people with disabilities, or rehabilitation protocols after stroke.""
""We want to understand if our brains are hardwired to control what nature has given us, and we've shown that the human brain can adapt to coordinate new limbs in tandem with our biological ones,"" explains Solaiman Shokur, co-PI of the study and EPFL Senior Scientist at the Neuro-X Institute. ""It's about acquiring new motor functions, enhancement beyond the existing functions of a given user, be it a healthy individual or a disabled one. From a nervous system perspective, it's a continuum between rehabilitation and augmentation.""
To explore the cognitive constraints of augmentation, the researchers first built a virtual environment to test a healthy user's capacity to control a virtual arm using movement of his or her diaphragm. They found that diaphragm control does not interfere with actions like controlling one's physiological arms, one's speech or gaze.

In this virtual reality setup, the user is equipped with a belt that measures diaphragm movement. Wearing a virtual reality headset, the user sees three arms: the right arm and hand, the left arm and hand, and a third arm between the two with a symmetric, six-fingered hand.
""We made this hand symmetric to avoid any bias towards either the left or the right hand,"" explains Giulia Dominijanni, PhD student at EPFL's Neuro-X Institute.
In the virtual environment, the user is then prompted to reach out with either the left hand, the right hand, or in the middle with the symmetric hand. In the real environment, the user holds onto an exoskeleton with both arms, which allows for control of the virtual left and right arms. Movement detected by the belt around the diaphragm is used for controlling the virtual middle, symmetric arm. The setup was tested on 61 healthy subjects in over 150 sessions.
""Diaphragm control of the third arm is actually very intuitive, with participants learning to control the extra limb very quickly,"" explains Dominijanni. ""Moreover, our control strategy is inherently independent from the biological limbs and we show that diaphragm control does not impact a user's ability to speak coherently.""
The researchers also successfully tested diaphragm control with an actual robotic arm, a simplified one that consists of a rod that can be extended out, and back in. When the user contracts the diaphragm, the rod is extended out. In an experiment similar to the VR environment, the user is asked to reach and hover over target circles with her left or right hand, or with the robotic rod.
Besides the diaphragm, but not reported in the study, vestigial ear muscles have also been tested for feasibility in performing new tasks. In this approach, a user is equipped with ear sensors and trained to use fine ear muscle movement to control the displacement of a computer mouse.
""Users could potentially use these ear muscles to control an extra limb,"" says Shokur, emphasizing that these alternative control strategies may help one day for the development of rehabilitation protocols for people with motor deficiencies.
Part of the third arm project, previous studies regarding the control of robotic arms have been focused on helping amputees. The latest Science Robotics study is a step beyond repairing the human body towards augmentation.
""Our next step is to explore the use of more complex robotic devices using our various control strategies, to perform real-life tasks, both inside and outside of the laboratory. Only then will we be able to grasp the real potential of this approach,"" concludes Micera.

","score: 14.205565129972712, grade_level: '14'","score: 15.052973574608643, grade_levels: ['college_graduate'], ages: [24, 100]",10.1126/scirobotics.adh1438,"Extra robotic arms (XRAs) are gaining interest in neuroscience and robotics, offering potential tools for daily activities. However, this compelling opportunity poses new challenges for sensorimotor control strategies and human-machine interfaces (HMIs). A key unsolved challenge is allowing users to proficiently control XRAs without hindering their existing functions. To address this, we propose a pipeline to identify suitable HMIs given a defined task to accomplish with the XRA. Following such a scheme, we assessed a multimodal motor HMI based on gaze detection and diaphragmatic respiration in a purposely designed modular neurorobotic platform integrating virtual reality and a bilateral upper limb exoskeleton. Our results show that the proposed HMI does not interfere with speaking or visual exploration and that it can be used to control an extra virtual arm independently from the biological ones or in coordination with them. Participants showed significant improvements in performance with daily training and retention of learning, with no further improvements when artificial haptic feedback was provided. As a final proof of concept, naïve and experienced participants used a simplified version of the HMI to control a wearable XRA. Our analysis indicates how the presented HMI can be effectively used to control XRAs. The observation that experienced users achieved a success rate 22.2% higher than that of naïve users, combined with the result that naïve users showed average success rates of 74% when they first engaged with the system, endorses the viability of both the virtual reality–based testing and training and the proposed pipeline."
"
Computational models that mimic the structure and function of the human auditory system could help researchers design better hearing aids, cochlear implants, and brain-machine interfaces. A new study from MIT has found that modern computational models derived from machine learning are moving closer to this goal.

In the largest study yet of deep neural networks that have been trained to perform auditory tasks, the MIT team showed that most of these models generate internal representations that share properties of representations seen in the human brain when people are listening to the same sounds.
The study also offers insight into how to best train this type of model: The researchers found that models trained on auditory input including background noise more closely mimic the activation patterns of the human auditory cortex.
""What sets this study apart is it is the most comprehensive comparison of these kinds of models to the auditory system so far. The study suggests that models that are derived from machine learning are a step in the right direction, and it gives us some clues as to what tends to make them better models of the brain,"" says Josh McDermott, an associate professor of brain and cognitive sciences at MIT, a member of MIT's McGovern Institute for Brain Research and Center for Brains, Minds, and Machines, and the senior author of the study.
MIT graduate student Greta Tuckute and Jenelle Feather PhD '22 are the lead authors of the open-access paper, which appears today in PLOS Biology.
Models of hearing
Deep neural networks are computational models that consists of many layers of information-processing units that can be trained on huge volumes of data to perform specific tasks. This type of model has become widely used in many applications, and neuroscientists have begun to explore the possibility that these systems can also be used to describe how the human brain performs certain tasks.

""These models that are built with machine learning are able to mediate behaviors on a scale that really wasn't possible with previous types of models, and that has led to interest in whether or not the representations in the models might capture things that are happening in the brain,"" Tuckute says.
When a neural network is performing a task, its processing units generate activation patterns in response to each audio input it receives, such as a word or other type of sound. Those model representations of the input can be compared to the activation patterns seen in fMRI brain scans of people listening to the same input.
In 2018, McDermott and then-graduate student Alexander Kell reported that when they trained a neural network to perform auditory tasks (such as recognizing words from an audio signal), the internal representations generated by the model showed similarity to those seen in fMRI scans of people listening to the same sounds.
Since then, these types of models have become widely used, so McDermott's research group set out to evaluate a larger set of models, to see if the ability to approximate the neural representations seen in the human brain is a general trait of these models.
For this study, the researchers analyzed nine publicly available deep neural network models that had been trained to perform auditory tasks, and they also created 14 models of their own, based on two different architectures. Most of these models were trained to perform a single task -- recognizing words, identifying the speaker, recognizing environmental sounds, and identifying musical genre -- while two of them were trained to perform multiple tasks.
When the researchers presented these models with natural sounds that had been used as stimuli in human fMRI experiments, they found that the internal model representations tended to exhibit similarity with those generated by the human brain. The models whose representations were most similar to those seen in the brain were models that had been trained on more than one task and had been trained on auditory input that included background noise.

""If you train models in noise, they give better brain predictions than if you don't, which is intuitively reasonable because a lot of real-world hearing involves hearing in noise, and that's plausibly something the auditory system is adapted to,"" Feather says.
Hierarchical processing
The new study also supports the idea that the human auditory cortex has some degree of hierarchical organization, in which processing is divided into stages that support distinct computational functions. As in the 2018 study, the researchers found that representations generated in earlier stages of the model most closely resemble those seen in the primary auditory cortex, while representations generated in later model stages more closely resemble those generated in brain regions beyond the primary cortex.
Additionally, the researchers found that models that had been trained on different tasks were better at replicating different aspects of audition. For example, models trained on a speech-related task more closely resembled speech-selective areas.
""Even though the model has seen the exact same training data and the architecture is the same, when you optimize for one particular task, you can see that it selectively explains specific tuning properties in the brain,"" Tuckute says.
McDermott's lab now plans to make use of their findings to try to develop models that are even more successful at reproducing human brain responses. In addition to helping scientists learn more about how the brain may be organized, such models could also be used to help develop better hearing aids, cochlear implants, and brain-machine interfaces.
""A goal of our field is to end up with a computer model that can predict brain responses and behavior. We think that if we are successful in reaching that goal, it will open a lot of doors,"" McDermott says.
The research was funded by the National Institutes of Health, an Amazon Fellowship from the Science Hub, an International Doctoral Fellowship from the American Association of University Women, an MIT Friends of McGovern Institute Fellowship, and a Department of Energy Computational Science Graduate Fellowship.

","score: 17.374616128809347, grade_level: '17'","score: 19.902668094810316, grade_levels: ['college_graduate'], ages: [24, 100]",10.1371/journal.pbio.3002366,"Models that predict brain responses to stimuli provide one measure of understanding of a sensory system and have many potential applications in science and engineering. Deep artificial neural networks have emerged as the leading such predictive models of the visual system but are less explored in audition. Prior work provided examples of audio-trained neural networks that produced good predictions of auditory cortical fMRI responses and exhibited correspondence between model stages and brain regions, but left it unclear whether these results generalize to other neural network models and, thus, how to further improve models in this domain. We evaluated model-brain correspondence for publicly available audio neural network models along with in-house models trained on 4 different tasks. Most tested models outpredicted standard spectromporal filter-bank models of auditory cortex and exhibited systematic model-brain correspondence: Middle stages best predicted primary auditory cortex, while deep stages best predicted non-primary cortex. However, some state-of-the-art models produced substantially worse brain predictions. Models trained to recognize speech in background noise produced better brain predictions than models trained to recognize speech in quiet, potentially because hearing in noise imposes constraints on biological auditory representations. The training task influenced the prediction quality for specific cortical tuning properties, with best overall predictions resulting from models trained on multiple tasks. The results generally support the promise of deep neural networks as models of audition, though they also indicate that current models do not explain auditory cortical responses in their entirety."
"
Smartwatches can help physicians detect and diagnose irregular heart rhythms in children, according to a new study from the Stanford School of Medicine.

The finding comes from a survey of electronic medical records for pediatric cardiology patients receiving care at Stanford Medicine Children's Health. The study will publish online Dec. 13 in Communications Medicine.
Over a four-year period, patients' medical records mentioned ""Apple Watch"" 145 times. Among patients whose medical records mentioned the smartwatch, 41 had abnormal heart rhythms confirmed by traditional diagnostic methods; of these, 29 children had their arrythmias diagnosed for the first time.
""I was surprised by how often our standard monitoring didn't pick up arrythmias and thewatch did,"" said senior study author Scott Ceresnak, MD, professor of pediatrics. Ceresnak is a pediatric cardiologist who treats patients at Stanford Medicine. ""It's awesome to see that newer technology can really make a difference in how we're able to care for patients.""
The study's lead author is Aydin Zahedivash, MD, a clinical instructor in pediatrics.
Most of the abnormal rhythms detected were not life-threatening, Ceresnak said. However, he added that the arrythmias detected can cause distressing symptoms such as a racing heartbeat, dizziness and fainting.
Skipping a beat, sometimes
Doctors face two challenges in diagnosing children's cardiac arrythmias, or heart rhythm abnormalities.

The first is that cardiac diagnostic devices, though they have improved in recent years, still aren't ideal for kids. Ten to 20 years ago, a child had to wear, for 24 to 48 hours, a Holter monitor consisting of a device about the size of a smartphone attached by wires to five electrodes that were adhered to the child's chest. Patients can now wear event monitors -- in the form of a single sticker placed on the chest -- for a few weeks. Although the event monitors are more comfortable and can be worn longer than a Holter monitor, they sometimes fall off early or cause problems such as skin irritation from adhesives.
The second challenge is that even a few weeks of continuous monitoring may not capture the heart's erratic behavior, as children experience arrythmias unpredictably. Kids may go months between episodes, making it tricky for their doctors to determine what's going on.
Connor Heinz and his family faced both challenges when he experienced periods of a racing heartbeat starting at age 12: An adhesive monitor was too irritating, and he was having irregular heart rhythms only once every few months. Ceresnak thought he knew what was causing the racing rhythms, but he wanted confirmation. He suggested that Connor and his mom, Amy Heinz, could try using Amy's smartwatch to record the rhythm the next time Connor's heart began racing.
Using smartwatches for measuring children's heart rhythms is limited by the fact that existing smartwatch algorithms that detect heart problems have not been optimized for kids. Children have faster heartbeats than adults; they also tend to experience different types of abnormal rhythms than do adults who have cardiac arrythmias.
The paper showed that the smartwatches appear to help detect arrhythmias in kids, suggesting that it would be useful to design versions of the smartwatch algorithms based on real-world heart rhythm data from children.
Evaluating medical records
The researchers searched patients' electronic medical records from 2018 to 2022 for the phrase ""Apple Watch,"" then checked to see which patients with this phrase in their records had submitted smartwatch data and received a diagnosis of a cardiac arrythmia.

Data from watches included alerts about patients' heart rates and patient-initiated electrocardiograms, or ECGs, from an app that uses the electrical sensors in the watch. When patients activate the app, the ECG function records the heart's electrical signals; physicians can use this pattern of electrical pulses to diagnose different types of heart problems.
From 145 mentions of the smartwatch in patient records, 41 patients had arrythmias confirmed. Of these, 18 patients had collected an ECG with their watches, and 23 patients had received a notification from the watch about a high heart rate.
The information from the smartwatches prompted the children's physicians to conduct medical workups, from which 29 children received new arrythmia diagnoses. In 10 patients, the smartwatch diagnosed arrythmias that traditional monitoring methods never picked up.
One of those patients was Connor Heinz.
""At a basketball tryout, he had another episode,"" Amy Heinz recalled. ""I put the watch on him and emailed a bunch of captures [of his heartbeat] to Dr. Ceresnak."" The information from the watch confirmed Ceresnak's suspicion that Connor had supraventricular tachycardia.
Most children with arrythmias had the same condition as Connor, a pattern of racing heartbeats originating in the heart's upper chambers.
""These irregular heartbeats are not life-threatening, but they make kids feel terrible,"" Ceresnak said. ""They can be a problem and they're scary, and if wearable devices can help us get to the bottom of what this arrythmia is, that's super helpful.""
In many cases of supraventricular tachycardia, the abnormal heart rhythm is caused by a small short-circuit in the heart's electrical circuitry. The problem can often be cured by a medical procedure called catheter ablation that destroys a small, precisely targeted region of heart cells causing the short circuit.
Now 15, Connor has been successfully treated with catheter ablation and is playing basketball for his high school team in Menlo Park, California.
The study also found smartwatch use noted in the medical records of 73 patients who did not ultimately receive diagnoses of arrythmias.
""A lot of kids have palpitations, a feeling of funny heartbeats, but the vast majority don't have medically significant arrythmias,"" Ceresnak said. ""In the future, I think this technology may help us rule out anything serious.""
A new study
The Stanford Medicine research team plans to conduct a study to further assess the utility of the Apple Watch for detecting children's heart problems. The study will measure whether, in kids, heart rate and heart rhythm measurements from the watches match measurements from standard diagnostic devices.
The study is open only to children who are already cardiology patients at Stanford Medicine Children's Health.
""The wearable market is exploding, and our kids are going to use them,"" Ceresnak said. ""We want to make sure the data we get from these devices is reliable and accurate for children. Down the road, we'd love to help develop pediatric-specific algorithms for monitoring heart rhythm.""
The study was conducted without external funding. Apple was not involved in the work. Apple's Investigator Support Program has agreed to donate watches for the next phase of the research.
Apple's Irregular Rhythm Notification and ECG app are cleared by the Food and Drug Administration for use by people 22 years of age or older. The high heart rate notification is available only to users 13 years of age or older.

","score: 11.77072176514585, grade_level: '12'","score: 13.116593118922964, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s43856-023-00392-9,"Arrhythmia symptoms are frequent complaints in children and often require a pediatric cardiology evaluation. Data regarding the clinical utility of wearable technologies are limited in children. We hypothesize that an Apple Watch can capture arrhythmias in children. We present an analysis of patients ≤18 years-of-age who had signs of an arrhythmia documented by an Apple Watch. We include patients evaluated at our center over a 4-year-period and highlight those receiving a formal arrhythmia diagnosis. We evaluate the role of the Apple Watch in arrhythmia diagnosis, the results of other ambulatory cardiac monitoring studies, and findings of any EP studies. We identify 145 electronic-medical-record identifications of Apple Watch, and find arrhythmias confirmed in 41 patients (28%) [mean age 13.8 ± 3.2 years]. The arrythmias include: 36 SVT (88%), 3 VT (7%), 1 heart block (2.5%) and wide 1 complex tachycardia (2.5%). We show that invasive EP study confirmed diagnosis in 34 of the 36 patients (94%) with SVT (2 non-inducible). We find that the Apple Watch helped prompt a workup resulting in a new arrhythmia diagnosis for 29 patients (71%). We note traditional ambulatory cardiac monitors were worn by 35 patients (85%), which did not detect arrhythmias in 10 patients (29%). In 73 patients who used an Apple Watch for recreational or self-directed heart rate monitoring, 18 (25%) sought care due to device findings without any arrhythmias identified. We demonstrate that the Apple Watch can record arrhythmia events in children, including events not identified on traditionally used ambulatory monitors."
"
Strong precipitation may cause natural disasters, such as flooding or landslides. Global climate models are required to forecast the frequency of these extreme events, which is expected to change as a result of climate change. Researchers of Karlsruhe Institute of Technology (KIT) have now developed a first method based on artificial intelligence (AI), by means of which the precision of coarse precipitation fields generated by global climate models can be increased. The researchers succeeded in improving spatial resolution of precipitation fields from 32 to two kilometers and temporal resolution from one hour to ten minutes. This higher resolution is required to better forecast the more frequent occurrence of heavy local precipitation and the resulting natural disasters in future.

Many natural disasters, such as flooding or landslides, are directly caused by extreme precipitation. Researchers expect that increasing average temperatures will cause extreme precipitation events to further increase. To adapt to a changing climate and prepare for disasters at an early stage, precise local and global data on the current and future water cycle are indispensable. ""Precipitation is highly variable in space and time and, hence, difficult to forecast, in particular on the local level,"" says Dr. Christian Chwala from the Atmospheric Environmental Research Division of KIT's Institute of Meteorology and Climate Research (IMK-IFU), KIT's Campus Alpine in Garmisch-Partenkirchen."" For this reason, we want to enhance the resolution of precipitation fields generated e.g. by global climate models and improve their classification as regards possible threats, such as floodings.""
Higher Resolution for More Precise Regional Climate Models 
Currently used global climate models are based on a grid that is not fine enough to precisely present the variability of precipitation. Highly resolved precipitation maps can only be produced with computationally expensive and, hence, spatially or temporally limited models. ""For this reason, we have developed an AI-based generative neural network, called GAN, and trained it with high-resolution radar precipitation fields. In this way, the GAN learns how to generate realistic precipitation fields and derive their temporal sequence from coarsely resolved data,"" says Luca Glawion from IMK-IFU. ""The network is able to generate highly resolved radar precipitation films from very coarsely resolved maps."" These refined radar maps not only show how rain cells develop and move, but precisely reconstruct local rain statistics and the corresponding extreme value distribution.
""Our method serves as a basis to increase the resolution of coarsely grained precipitation fields, such that the high spatial and temporal variability of precipitation can be reproduced adequately and local effects can be studied,"" says Julius Polz from IMK-IFU. ""Our deep learning method is quicker by several orders of magnitude than the calculation of such highly resolved precipitation fields with numerical weather models usually applied to regionally refine data of global climate models."" The researchers point out that their method also generates an ensemble of different potential precipitation fields. This is important, as a multitude of physically plausible highly resolved solutions exists for each coarsely resolved precipitation field. Similar to a weather forecast, an ensemble allows for a more precise determination of the associated uncertainty.
Higher Resolution for Better Forecasts under Climate Change
The results show that the AI model and methodology developed by the researchers will enable future use of neural networks to improve the spatial and temporal resolution of precipitation calculated by climate models. This will allow for a more precise analysis of the impacts and developments of precipitation in a changing climate.
""In a next step, we will apply the method to global climate simulations that transfer specific large-scale weather situations to a future world with a changed climate, e.g. to the year of 2100. The higher resolution of precipitation events simulated with our method will allow for a better estimation of the impacts the weather conditions that caused the flooding of the river Ahr in 2021 would have had in a world warmer by 2 degrees,"" Glawion explains. Such information is of decisive importance to develop climate adaptation methods.

","score: 16.00619047619048, grade_level: '16'","score: 16.475, grade_levels: ['college_graduate'], ages: [24, 100]",10.1029/2023EA002906,"Climate models face limitations in their ability to accurately represent highly variable atmospheric phenomena. To resolve fine‐scale physical processes, allowing for local impact assessments, downscaling techniques are essential. We propose spateGAN, a novel approach for spatio‐temporal downscaling of precipitation data using conditional generative adversarial networks. Our method is based on a video super‐resolution approach and trained on 10 years of country‐wide radar observations for Germany. It simultaneously increases the spatial and temporal resolution of coarsened precipitation observations from 32 to 2 km and from 1 hr to 10 min. Our experiments indicate that the ensembles of generated temporally consistent rainfall fields are in high agreement with the observational data. Spatial structures with plausible advection were accurately generated. Compared to trilinear interpolation and a classical convolutional neural network, the generative model reconstructs the resolution‐dependent extreme value distribution with high skill. It showed a high fractions skill score of 0.6 (spatio‐temporal scale: 32 km and 1 hr) for rainfall intensities over 15 mm h−1 and a low relative bias of 3.35%. A power spectrum analysis confirmed that the probabilistic downscaling ability of our model further increased its skill. We observed that neural network predictions may be interspersed by recurrent structures not related to rainfall climatology, which should be a known issue for future studies. We were able to mitigate them by using an appropriate model architecture and model selection process. Our findings suggest that spateGAN offers the potential to complement and further advance the development of climate model downscaling techniques, due to its performance and computational efficiency."
"
Manatees are endangered species volatile to the environment. Because of their voracious appetites, they often spend up to eight hours a day grazing for food within shallow waters, making them vulnerable to environmental changes and other risks.

Accurately counting manatee aggregations within a region is not only biologically meaningful in observing their habit, but also crucial for designing safety rules for boaters and divers as well as scheduling nursing, intervention, and other plans. Nevertheless, counting manatees is challenging.
Because manatees tend to live in herds, they often block each other when viewed from the surface. As a result, small manatees are likely to be partially or completely blocked from view. In addition, water reflections tend to make manatees invisible, and they also can be mistaken for other objects such as rocks and branches.
While aerial survey data are used in some regions to count manatees, this method is time-consuming and costly, and the accuracy depends on factors such as observer bias, weather conditions and time of day. Moreover, it is crucial to have a low-cost method that provides a real-time count to alert ecologists of threats early to enable them to act proactively to protect manatees.
Artificial intelligence is used in a wide spectrum of fields, and now, researchers from Florida Atlantic University's College of Engineering and Computer Science have harnessed its powers to help save the beloved manatee. They are among the first to use a deep learning-based crowd counting approach to automatically count the number of manatees in a designated region, using images captured from CCTV cameras, which are readily available, as input.
This pioneering study, published Scientific Reports, not only addresses the technical challenges of counting in complex outdoor environments but also offers potential ways to aid endangered species.
To determine manatee densities and calculate their numbers, researchers used generic images captured from surveillance videos from the water surface. They then used a unique design matching to manatees' shape -- Anisotropic Gaussian Kernel (AGK) -- to transform the images into manatee customized density maps, representing manatees' unique body shapes.

Although many methods exist for counting, most of the existing counting methods are applied to crowds to count the number of people, due to their relevance to important applications such as urban planning and public safety.
To save labeling costs, researchers used line-label based annotation with a single straight line to mark each manatee. The goal of the study was to learn to count the number of objects within a scene and obtain labels to support counting.
Results of the study reveal that the FAU-developed method outperformed other baselines, including the traditional Gaussian kernel-based approach. Transitioning from dot to line labeling also improved wheat head counting accuracy, an important role in crop yield estimation, suggesting broader applications for convex-shaped objects in diverse contexts. This approach worked particularly well when the image had a high density of manatees in a complicated background.
By formatting manatee counting as a deep neural network density estimation learning task, this approach balanced the labeling costs vs. counting efficiency. As a result, this method delivers a simple and high throughput solution for manatee counting that requires very little labeling efforts. A direct impact is that state parks can leverage this method to understand the number of manatees in different regions, by using their existing CCTV cameras, in real time.
""There are many ways to use computational methods to help save endangered species, such as detecting the presence of the species and counting them to collect information about numbers and density,"" said Xingquan (Hill) Zhu, Ph.D., senior author, an IEEE Fellow and a professor in FAU's Department of Electrical Engineering and Computer Science. ""Our method considered distortions caused by the perspective between the water space and the image plane. Since the shape of the manatee is closer to an ellipse than a circle, we used AGK to best represent the manatee contour and estimate manatee density in the scene. This allows density map to be more accurate, in terms of mean absolute errors and root mean square error, than other alternatives in estimating manatees' numbers.""
To validate their method and facilitate further research in this domain, the researchers developed a comprehensive manatee counting dataset, along with their source code, published through GitHub for public access at github.com/yeyimilk/deep-learning-for-manatee-counting.

""Manatees are one of the wildlife species being affected by human-related threats. Therefore, calculating their numbers and gathering patterns in real time is vital for understanding their population dynamics,"" said Stella Batalama, Ph.D., dean, FAU College of Engineering and Computer Science. ""The methodology developed by professor Zhu and our graduate students provides a promising trajectory for broader applications, especially for convex-shaped objects, to improve counting techniques that may foretell better ecological results from management decisions.""
Manatees can be found from Brazil to Florida and all the way around the Caribbean islands. Some species including the Florida Manatee are considered endangered by the International Union for Conservation of Nature.
Study co-authors are FAU graduate students Zhiqiang Wang; Yiran Pang; and Cihan Ulus, also a teaching assistant, all within the Department of Electrical Engineering and Computer Science.
The research was sponsored by the United States National Science Foundation.

","score: 15.749930056087102, grade_level: '16'","score: 16.917132959419334, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41598-023-45507-3,"Manatees are aquatic mammals with voracious appetites. They rely on sea grass as the main food source, and often spend up to eight hours a day grazing. They move slow and frequently stay in groups (i.e. aggregations) in shallow water to search for food, making them vulnerable to environment change and other risks. Accurate counting manatee aggregations within a region is not only biologically meaningful in observing their habit, but also crucial for designing safety rules for boaters, divers, etc., as well as scheduling nursing, intervention, and other plans. In this paper, we propose a deep learning based crowd counting approach to automatically count number of manatees within a region, by using low quality images as input. Because manatees have unique shape and they often stay in shallow water in groups, water surface reflection, occlusion, camouflage etc. making it difficult to accurately count manatee numbers. To address the challenges, we propose to use Anisotropic Gaussian Kernel (AGK), with tunable rotation and variances, to ensure that density functions can maximally capture shapes of individual manatees in different aggregations. After that, we apply AGK kernel to different types of deep neural networks primarily designed for crowd counting, including VGG, SANet, Congested Scene Recognition network (CSRNet), MARUNet etc. to learn manatee densities and calculate number of manatees in the scene. By using generic low quality images extracted from surveillance videos, our experiment results and comparison show that AGK kernel based manatee counting achieves minimum Mean Absolute Error (MAE) and Root Mean Square Error (RMSE). The proposed method works particularly well for counting manatee aggregations in environments with complex background."
"
Much of the discussion around implementing artificial intelligence systems focuses on whether an AI application is ""trustworthy"": Does it produce useful, reliable results, free of bias, while ensuring data privacy? But a new paper published Dec. 7 in Frontiers in Artificial Intelligence poses a different question: What if an AI is just too good?

Carrie Alexander, a postdoctoral researcher at the AI Institute for Next Generation Food Systems, or AIFS, at the University of California, Davis, interviewed a wide range of food industry stakeholders, including business leaders and academic and legal experts, on the attitudes of the food industry toward adopting AI. A notable issue was whether gaining extensive new knowledge about their operations might inadvertently create new liability risks and other costs.
For example, an AI system in a food business might reveal potential contamination with pathogens. Having that information could be a public benefit but also open the firm to future legal liability, even if the risk is very small.
""The technology most likely to benefit society as a whole may be the least likely to be adopted, unless new legal and economic structures are adopted,"" Alexander said.
An on-ramp for AI
Alexander and co-authors Professor Aaron Smith of the UC Davis Department of Agricultural and Resource Economics and Professor Renata Ivanek of Cornell University, argue for a temporary ""on-ramp"" that would allow companies to begin using AI, while exploring the benefits, risks and ways to mitigate them. This would also give the courts, legislators and government agencies time to catch up and consider how best to use the information generated by AI systems in legal, political and regulatory decisions.
""We need ways for businesses to opt in and try out AI technology,"" Alexander said. Subsidies, for example for digitizing existing records, might be helpful especially for small companies.
""We're really hoping to generate more research and discussion on what could be a significant issue,"" Alexander said. ""It's going to take all of us to figure it out.""
The work was supported in part by a grant from the USDA National Institute of Food and Agriculture. The AI Institute for Next Generation Food Systems is funded by a grant from USDA-NIFA and is one of 25 AI institutes established by the National Science Foundation in partnership with other agencies.

","score: 15.65666666666667, grade_level: '16'","score: 15.238970464135022, grade_levels: ['college_graduate'], ages: [24, 100]",10.3389/frai.2023.1298604,"Governments, researchers, and developers emphasize creating “trustworthy AI,” defined as AI that prevents bias, ensures data privacy, and generates reliable results that perform as expected. However, in some cases problems arise not when AI is not trustworthy, technologically, but when it is. This article focuses on such problems in the food system. AI technologies facilitate the generation of masses of data that may illuminate existing food-safety and employee-safety risks. These systems may collect incidental data that could be used, or may be designed specifically, to assess and manage risks. The predictions and knowledge generated by these data and technologies may increase company liability and expense, and discourage adoption of these predictive technologies. Such problems may extend beyond the food system to other industries. Based on interviews and literature, this article discusses vulnerabilities to liability and obstacles to technology adoption that arise, arguing that “trustworthy AI” cannot be achieved through technology alone, but requires social, cultural, political, as well as technical cooperation. Implications for law and further research are also discussed."
"
Artificial intelligence (AI) systems are often depicted as sentient agents poised to overshadow the human mind. But AI lacks the crucial human ability of innovation, researchers at the University of California, Berkeley have found.

While children and adults alike can solve problems by finding novel uses for everyday objects, AI systems often lack the ability to view tools in a new way, according to findings published according to findings published in Perspectives on Psychological Science, a journal of the Association for Psychological Science.
AI language models like ChatGPT are passively trained on data sets containing billions of words and images produced by humans. This allows AI systems to function as a ""cultural technology"" similar to writing that can summarize existing knowledge, Eunice Yiu, a co-author of the article, explained in an interview. But unlike humans, they struggle when it comes to innovating on these ideas, she said.
""Even young human children can produce intelligent responses to certain questions that [language learning models] cannot,"" Yiu said. ""Instead of viewing these AI systems as intelligent agents like ourselves, we can think of them as a new form of library or search engine. They effectively summarize and communicate the existing culture and knowledge base to us.""
Yiu and Eliza Kosoy, along with their doctoral advisor and senior author on the paper, developmental psychologist Alison Gopnik, tested how the AI systems' ability to imitate and innovate differs from that of children and adults. They presented 42 children ages 3 to 7 and 30 adults with text descriptions of everyday objects. In the first part of the experiment, 88% of children and 84% of adults were able to correctly identify which objects would ""go best"" with another. For example, they paired a compass with a ruler instead of a teapot.
In the next stage of the experiment, 85% of children and 95% of adults were also able to innovate on the expected use of everyday objects to solve problems. In one task, for example, participants were asked how they could draw a circle without using a typical tool such as a compass. Given the choice between a similar tool like a ruler, a dissimilar tool such as a teapot with a round bottom, and an irrelevant tool such as a stove, the majority of participants chose the teapot, a conceptually dissimilar tool that could nonetheless fulfill the same function as the compass by allowing them to trace the shape of a circle.
When Yiu and colleagues provided the same text descriptions to five large language models, the models performed similarly to humans on the imitation task, with scores ranging from 59% for the worst-performing model to 83% for the best-performing model. The AIs' answers to the innovation task were far less accurate, however. Effective tools were selected anywhere from 8% of the time by the worst-performing model to 75% by the best-performing model.

""Children can imagine completely novel uses for objects that they have not witnessed or heard of before, such as using the bottom of a teapot to draw a circle,"" Yiu said. ""Large models have a much harder time generating such responses.""
In a related experiment, the researchers noted, children were able to discover how a new machine worked just by experimenting and exploring. But when the researchers gave several large language models text descriptions of the evidence that the children produced, they struggled to make the same inferences, likely because the answers were not explicitly included in their training data, Yiu and colleagues wrote.
These experiments demonstrate that AI's reliance on statistically predicting linguistic patterns is not enough to discover new information about the world, Yiu and colleagues wrote.
""AI can help transmit information that is already known, but it is not an innovator,"" Yiu said. ""These models can summarize conventional wisdom but they cannot expand, create, change, abandon, evaluate, and improve on conventional wisdom in the way a young human can."" The development of AI is still in its early days, though, and much remains to be learned about how to expand the learning capacity of AI, Yiu said. Taking inspiration from children's curious, active, and intrinsically motivated approach to learning could help researchers design new AI systems that are better prepared to explore the real world, she said.

","score: 14.10831168831169, grade_level: '14'","score: 14.86370720188902, grade_levels: ['college_graduate'], ages: [24, 100]",10.1177/17456916231201401,"Much discussion about large language models and language-and-vision models has focused on whether these models are intelligent agents. We present an alternative perspective. First, we argue that these artificial intelligence (AI) models are cultural technologies that enhance cultural transmission and are efficient and powerful imitation engines. Second, we explore what AI models can tell us about imitation and innovation by testing whether they can be used to discover new tools and novel causal structures and contrasting their responses with those of human children. Our work serves as a first step in determining which particular representations and competences, as well as which kinds of knowledge or skills, can be derived from particular learning techniques and data. In particular, we explore which kinds of cognitive capacities can be enabled by statistical analysis of large-scale linguistic data. Critically, our findings suggest that machines may need more than large-scale language and image data to allow the kinds of innovation that a small child can produce."
"
McGill University researchers have made a breakthrough in diagnostic technology, inventing a 'lab on a chip' that can be 3D-printed in just 30 minutes. The chip has the potential to make on-the-spot testing widely accessible.

As part of a recent study, the results of which were published in the journal Advanced Materials, the McGill team developed capillaric chips that act as miniature laboratories. Unlike other computer microprocessors, these chips are single-use and require no external power source -- a simple paper strip suffices. They function through capillary action -- the very phenomena by which a spilled liquid on the kitchen table spontaneously wicks into the paper towel used to wipe it up.
""Traditional diagnostics require peripherals, while ours can circumvent them. Our diagnostics are a bit what the cell phone was to traditional desktop computers that required a separate monitor, keyboard and power supply to operate,"" explains Prof. David Juncker, Chair of the Department of Biomedical Engineering at McGill and senior author on the study.
At-home testing became crucial during the COVID-19 pandemic. But rapid tests have limited availability and can only drive one liquid across the strip, meaning most diagnostics are still done in central labs. Notably, the capillaric chips can be 3D-printed for various tests, including COVID-19 antibody quantification.
The study brings 3D-printed home diagnostics one step closer to reality, though some challenges remain, such as regulatory approvals and securing necessary test materials. The team is actively working to make their technology more accessible, adapting it for use with affordable 3D printers. The innovation aims to speed up diagnoses, enhance patient care, and usher in a new era of accessible testing.
""This advancement has the capacity to empower individuals, researchers, and industries to explore new possibilities and applications in a more cost-effective and user-friendly manner,"" says Prof. Juncker. ""This innovation also holds the potential to eventually empower health professionals with the ability to rapidly create tailored solutions for specific needs right at the point-of-care.""

","score: 14.524129353233835, grade_level: '15'","score: 15.128487562189058, grade_levels: ['college_graduate'], ages: [24, 100]",10.1002/adma.202303867,"Digital manufacturing (DM) holds great potential for microfluidics, but requirements for embedded conduits and high resolution beyond the capability of common manufacturing equipment, and microfluidic systems' dependence on peripheralshave limited its adoption. Capillaric circuits (CCs) are structurally encoded, self‐contained microfluidic systems that operate and self‐fill via precisely tailored hydrophilicity. CCs are heretofore hydrophilized in a plasma chamber, but which offers only transient hydrophilicity, lacks reproducibility, and limits CC design to open surface channels subsequently sealed with tape. Here, the additive DM of monolithic, fully functional, and intrinsically hydrophilic CCs is reported. CCs are 3D printed with commonly available light‐engine‐based 3D printers using poly(ethylene glycol)diacrylate‐based ink co‐polymerized with hydrophilic acrylic acid crosslinkers and optimized for hydrophilicity and printability. A new, robust capillary valve design and embedded conduits with circular cross‐sections that prevent bubble trapping are presented, interwoven circuit architectures created, and CC use illustrated with an immunoassay. Finally, the external paper capillary pumps are eliminated by directly embedding the capillary pump in the chip as a porous gyroid structure, realizing fully functional, monolithic CCs. Thence, a digital file can be made into a CC by commonly available 3D printers in less than 30 min enabling low‐cost, distributed DM of fully functional ready‐to‐use microfluidic systems."
"
Physician-investigators at Beth Israel Deaconess Medical Center (BIDMC) compared a chatbot's probabilistic reasoning to that of human clinicians. The findings, published in JAMA Network Open, suggest that artificial intelligence could serve as useful clinical decision support tools for physicians.

""Humans struggle with probabilistic reasoning, the practice of making decisions based on calculating odds,"" said the study's corresponding author Adam Rodman, MD, an internal medicine physician and investigator in the department of Medicine at BIDMC. ""Probabilistic reasoning is one of several components of making a diagnosis, which is an incredibly complex process that uses a variety of different cognitive strategies. We chose to evaluate probabilistic reasoning in isolation because it is a well-known area where humans could use support.""
Basing their study on a previously published national survey of more than 550 practitioners performing probabilistic reasoning on five medical cases, Rodman and colleagues fed the publicly available Large Language Model (LLM), Chat GPT-4, the same series of cases and ran an identical prompt 100 times to generate a range of responses.
The chatbot -- just like the practitioners before them -- was tasked with estimating the likelihood of a given diagnosis based on patients' presentation. Then, given test results such as chest radiography for pneumonia, mammography for breast cancer, stress test for coronary artery disease and a urine culture for urinary tract infection, the chatbot program updated its estimates.
When test results were positive, it was something of a draw; the chatbot was more accurate in making diagnoses than the humans in two cases, similarly accurate in two cases and less accurate in one case. But when tests came back negative, the chatbot shone, demonstrating more accuracy in making diagnoses than humans in all five cases.
""Humans sometimes feel the risk is higher than it is after a negative test result, which can lead to overtreatment, more tests and too many medications,"" said Rodman.
But Rodman is less interested in how chatbots and humans perform toe-to-toe than in how highly skilled physicians' performance might change in response to having these new supportive technologies available to them in the clinic, added Rodman. He and colleagues are looking into it.
""LLMs can't access the outside world -- they aren't calculating probabilities the way that epidemiologists, or even poker players, do. What they're doing has a lot more in common with how humans make spot probabilistic decisions,"" he said. ""But that's what is exciting. Even if imperfect, their ease of use and ability to be integrated into clinical workflows could theoretically make humans make better decisions,"" he said. ""Future research into collective human and artificial intelligence is sorely needed.""
Co-authors included Thomas A. Buckley, University of Massachusetts Amherst; Arun K. Manrai, PhD, Harvard Medical School; Daniel J. Morgan, MD, MS, University of Maryland School of Medicine.
Rodman reported receiving grants from the Gordon and Betty Moore Foundation. Morgan reported receiving grants from the Department of Veterans Affairs, the Agency for Healthcare Research and Quality, the Centers for Disease Control and Prevention, and the National Institutes of Health, and receiving travel reimbursement from the Infectious Diseases Society of America, the Society for Healthcare Epidemiology of America. The American College of Physicians and the World Heart Health Organization outside the submitted work.

","score: 15.560689310689309, grade_level: '16'","score: 16.513156843156843, grade_levels: ['college_graduate'], ages: [24, 100]",10.1001/jamanetworkopen.2023.47075,This diagnostic study compares the performance of artificial intelligence (AI) with that of human clinicians in estimating the probability of diagnoses before and after testing.
"
MIT engineers have developed a robotic replica of the heart's right ventricle, which mimics the beating and blood-pumping action of live hearts.

The robo-ventricle combines real heart tissue with synthetic, balloon-like artificial muscles that enable scientists to control the ventricle's contractions while observing how its natural valves and other intricate structures function.
The artificial ventricle can be tuned to mimic healthy and diseased states. The team manipulated the model to simulate conditions of right ventricular dysfunction, including pulmonary hypertension and myocardial infarction. They also used the model to test cardiac devices. For instance, the team implanted a mechanical valve to repair a natural malfunctioning valve, then observed how the ventricle's pumping changed in response.
They say the new robotic right ventricle, or RRV, can be used as a realistic platform to study right ventricle disorders and test devices and therapies aimed at treating those disorders.
""The right ventricle is particularly susceptible to dysfunction in intensive care unit settings, especially in patients on mechanical ventilation,"" says Manisha Singh, a postdoc at MIT's Institute for Medical Engineering and Science (IMES). ""The RRV simulator can be used in the future to study the effects of mechanical ventilation on the right ventricle and to develop strategies to prevent right heart failure in these vulnerable patients.""
Singh and her colleagues report details of the new design in a paper appearing today in Nature Cardiovascular Research. Her co-authors include Associate Professor Ellen Roche, who is a core member of IMES and the associate head for research in the Department of Mechanical Engineering at MIT, along with Jean Bonnemain, Caglar Ozturk, Clara Park, Diego Quevedo-Moreno, Meagan Rowlett, and Yiling Fan of MIT, Brian Ayers of Massachusetts General Hospital, Christopher Nguyen of Cleveland Clinic, and Mossab Saeed of Boston Children's Hospital.
A ballet of beats
The right ventricle is one of the heart's four chambers, along with the left ventricle and the left and right atria. Of the four chambers, the left ventricle is the heavy lifter, as its thick, cone-shaped musculature is built for pumping blood through the entire body. The right ventricle, Roche says, is a ""ballerina"" in comparison, as it handles a lighter though no-less-crucial load.

""The right ventricle pumps deoxygenated blood to the lungs, so it doesn't have to pump as hard,"" Roche notes. ""It's a thinner muscle, with more complex architecture and motion.""
This anatomical complexity has made it difficult for clinicians to accurately observe and assess right ventricle function in patients with heart disease.
""Conventional tools often fail to capture the intricate mechanics and dynamics of the right ventricle, leading to potential misdiagnoses and inadequate treatment strategies,"" Singh says.
To improve understanding of the lesser-known chamber and speed the development of cardiac devices to treat its dysfunction, the team designed a realistic, functional model of the right ventricle that both captures its anatomical intricacies and reproduces its pumping function.
The model includes real heart tissue, which the team chose to incorporate because it retains natural structures that are too complex to reproduce synthetically.
""There are thin, tiny chordae and valve leaflets with different material properties that are all moving in concert with the ventricle's muscle.Trying to cast or print these very delicate structures is quite challenging,"" Roche explains.

A heart's shelf-life
In the new study, the team reports explanting a pig's right ventricle, which they treated to carefully preserve its internal structures. They then fit a silicone wrapping around it, which acted as a soft, synthetic myocardium, or muscular lining. Within this lining, the team embedded several long, balloon-like tubes, which encircled the real heart tissue, in positions that the team determined through computational modeling to be optimal for reproducing the ventricle's contractions. The researchers connected each tube to a control system, which they then set to inflate and deflate each tube at rates that mimicked the heart's real rhythm and motion.
To test its pumping ability, the team infused the model with a liquid similar in viscosity to blood. This particular liquid was also transparent, allowing the engineers to observe with an internal camera how internal valves and structures responded as the ventricle pumped liquid through.
They found that the artificial ventricle's pumping power and the function of its internal structures were similar to what they previously observed in live, healthy animals, demonstrating that the model can realistically simulate the right ventricle's action and anatomy. The researchers could also tune the frequency and power of the pumping tubes to mimic various cardiac conditions, such as irregular heartbeats, muscle weakening, and hypertension.
""We're reanimating the heart, in some sense, and in a way that we can study and potentially treat its dysfunction,"" Roche says.
To show that the artificial ventricle can be used to test cardiac devices, the team surgically implanted ring-like medical devices of various sizes to repair the chamber's tricuspid valve -- a leafy, one-way valve that lets blood into the right ventricle. When this valve is leaky, or physically compromised, it can cause right heart failure or atrial fibrillation, and leads to symptoms such as reduced exercise capacity, swelling of the legs and abdomen, and liver enlargement
The researchers surgically manipulated the robo-ventricle's valve to simulate this condition, then either replaced it by implanting a mechanical valve or repaired it using ring-like devices of different sizes. They observed which device improved the ventricle's fluid flow as it continued to pump.
""With its ability to accurately replicate tricuspid valve dysfunction, the RRV serves as an ideal training ground for surgeons and interventional cardiologists,"" Singh says. ""They can practice new surgical techniques for repairing or replacing the tricuspid valve on our model before performing them on actual patients.""
Currently, the RRV can simulate realistic function over a few months. The team is working to extend that performance and enable the model to run continuously for longer stretches. They are also working with designers of implantable devices to test their prototypes on the artificial ventricle and possibly speed their path to patients. And looking far in the future, Roche plans to pair the RRV with a similar artificial, functional model of the left ventricle, which the group is currently fine-tuning.
""We envision pairing this with the left ventricle to make a fully tunable, artificial heart, that could potentially function in people,"" Roche says. ""We're quite a while off, but that's the overarching vision.""
This research was supported in part by the National Science Foundation.

","score: 14.977265745007681, grade_level: '15'","score: 16.81214132104455, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s44161-023-00387-8,"The increasing recognition of the right ventricle (RV) necessitates the development of RV-focused interventions, devices and testbeds. In this study, we developed a soft robotic model of the right heart that accurately mimics RV biomechanics and hemodynamics, including free wall, septal and valve motion. This model uses a biohybrid approach, combining a chemically treated endocardial scaffold with a soft robotic synthetic myocardium. When connected to a circulatory flow loop, the robotic right ventricle (RRV) replicates real-time hemodynamic changes in healthy and pathological conditions, including volume overload, RV systolic failure and pressure overload. The RRV also mimics clinical markers of RV dysfunction and is validated using an in vivo porcine model. Additionally, the RRV recreates chordae tension, simulating papillary muscle motion, and shows the potential for tricuspid valve repair and replacement in vitro. This work aims to provide a platform for developing tools for research and treatment for RV pathophysiology."
"
There has been a surge in academic and business interest in software as a medical device (SaMD). It enables medical professionals to streamline existing medical practices and make innovative medical processes such as digital therapeutics a reality. Furthermore, SaMD is a billion-dollar market. However, it is not clearly understood as a technological change and emerging industry. This enlightened researchers from Tokyo Institute of Technology to a new study. They reviewed FDA-approved SaMDs to shed light on the market landscape, the role of SaMDs, and the innovation within the industry. Their findings highlight the industry's diversity and potential for growth and advocate improving healthcare-related data access.

Software as a Medical Device (SaMD) is an emerging field aimed at assisting medical professionals in diagnosing, monitoring, treating, or preventing diseases. The International Medical Device Regulators Forum defines SaMD as software intended for medical purposes, but not part of a hardware medical device. This refers to a wide range of software such as health apps on smartphones or wearable devices that monitor and track health, as well as complex medical imaging software for X-rays, MRIs, and CT scans. However, the SaMD industry is still in its nascent stages of development and requires clarity on innovation, the market landscape, and the regulatory environment.
To address the limitations associated with current SaMD research, researchers from Tokyo Institute of Technology (Tokyo Tech) and the University of Tokyo recently conducted a comprehensive review of various aspects of SaMDs over the past decade, utilizing data from the U.S. Food and Drug Administration (FDA), the authoritative body for approving commercially marketed medical devices in the United States. Professor Shintaro Sengoku, Jiajie Zhang, and Jiakan Yu, the authors of the study published in the Journal of Medical Internet Research, state: ""The objectives of our work are to clarify the innovation process of SaMD, identify the prevailing typology of such innovation, and elucidate the underlying mechanisms driving the SaMD innovation process.""
The researchers collected information on FDA-approved SaMDs from the OpenFDA website. They also gathered profiles of 268 companies associated with these devices from various sources, including Crunchbase, Bloomberg, PichBook.com , and SaMD company websites. To be considered a SaMD, a device had to function as standalone software fulfilling medical functions. Devices operating solely as part of hardware or requiring additional hardware were excluded from the review.
The findings reveal significant growth in the SaMD industry. Between 2012 and 2021, the number of FDA-approved SaMDs increased from one to 581. Most SaMDs were developed for medical image processing and radiological analysis (78%), followed by cardiology, neurology, ophthalmology, and dentistry. The researchers also identified notable progress in artificial intelligence/machine learning based SaMDs, accounting for 22% of all FDA-approved SaMDs, marking what researchers are calling the 'third AI boom' in healthcare.
The United States leads in SaMD approvals (262 devices, 45%), followed by Germany, South Korea, and the Netherlands. Established companies in the medical device industry such as Siemens, General Electric, and Philips launched the most SaMDs (237 devices, 40.8%). These companies focus on incremental innovations to improve existing medical processes, such as image resolution or reducing manpower requirements for medical image processing.
New entrants or start-ups that were formed after 2012 accounted for about 37% (215 devices) of the launches. These small and micro companies focus on disruptive innovation which enables new medical practices, such as digital therapeutics and remote monitoring. Another notable player is the pharmaceutical industry, which is actively engaged in the digitalization process of healthcare, with significant investments in SaMD initiatives.
The study highlights the diversity and emerging nature of SaMD, its potential for growth, and its transformative impact on healthcare services. The findings emphasize that accelerated growth in this sector is closely linked to data accessibility in driving disruptive innovation within the industry. New entrants focusing on disruptive innovations will need to build their datasets or access existing data within the healthcare system.
""Governments and academic institutions should facilitate data accessibility as a public good to accelerate innovation in SaMD,"" conclude the three authors, outlining recommendations for future developments in the industry.
This study was funded by the Japan Science and Technology Agency, Program on open innovation platform for industry-academia co-creation (COI-NEXT), ""Center of health longevity and nursing innovation with global ecosystem"" (Grant No. JPMJPF2202).

","score: 15.636860198624905, grade_level: '16'","score: 16.46738349885409, grade_levels: ['college_graduate'], ages: [24, 100]",10.2196/47505,"There has been a surge in academic and business interest in software as a medical device (SaMD). SaMD enables medical professionals to streamline existing medical practices and make innovative medical processes such as digital therapeutics a reality. Furthermore, SaMD is a billion-dollar market. However, SaMD is not clearly understood as a technological change and emerging industry. This study aims to review the landscape of SaMD in response to increasing interest in SaMD within health systems and regulation. The objectives of the study are to (1) clarify the innovation process of SaMD, (2) identify the prevailing typology of such innovation, and (3) elucidate the underlying mechanisms driving the SaMD innovation process. We collected product information on 581 US Food and Drug Administration–approved SaMDs from the OpenFDA website and 268 company profiles of the corresponding manufacturers from Crunchbase, Bloomberg, PichBook.com, and other company websites. In addition to assessing the metadata of SaMD, we used correspondence and business process analysis to assess the distribution of intended use and how SaMDs interact with other devices in the medical process. The current SaMD industry is highly concentrated in medical image processing and radiological analysis. Incumbents in the medical device industry currently lead the market and focus on incremental innovation, whereas new entrants, particularly startups, produce more disruptive innovation. We found that hardware medical device functions as a complementary asset for SaMD, whereas how SaMD interacts with the complementary asset differs according to its intended use. Based on these findings, we propose a regime map that illustrates the SaMD innovation process. SaMD, as an industry, is nascent and dominated by incremental innovation. The innovation process of the present SaMD industry is shaped by data accessibility, which is key to building disruptive innovation."
"
The United Nations reports that more than 700 million people are in extreme poverty, earning less than two dollars a day. However, an accurate assessment of poverty remains a global challenge. For example, 53 countries have not conducted agricultural surveys in the past 15 years, and 17 countries have not published a population census. To fill this data gap, new technologies are being explored to estimate poverty using alternative sources such as street views, aerial photos, and satellite images.

The paper published in Nature Communications demonstrates how artificial intelligence (AI) can help analyze economic conditions from daytime satellite imagery. This new technology can even apply to the least developed countries -- such as North Korea -- that do not have reliable statistical data for typical machine learning training.
The researchers used Sentinel-2 satellite images from the European Space Agency (ESA) that are publicly available. They split these images into small six-square-kilometer grids. At this zoom level, visual information such as buildings, roads, and greenery can be used to quantify economic indicators. As a result, the team obtained the first ever fine-grained economic map of regions like North Korea. The same algorithm was applied to other underdeveloped countries in Asia: North Korea, Nepal, Laos, Myanmar, Bangladesh, and Cambodia.
The key feature of their research model is the ""human-machine collaborative approach,"" which lets researchers combine human input with AI predictions for areas with scarce data. In this research, ten human experts compared satellite images and judged the economic conditions in the area, with the AI learning from this human data and giving economic scores to each image. The results showed that the Human-AI collaborative approach outperformed machine-only learning algorithms.
The research was led by an interdisciplinary team of computer scientists, economists, and a geographer from KAIST & IBS (Donghyun Ahn, Meeyoung Cha, Jihee Kim), Sogang University (Hyunjoo Yang), HKUST (Sangyoon Park), and NUS (Jeasurk Yang). Dr Charles Axelsson, Associate Editor at Nature Communications, handled this paper during the peer review process at the journal.
The research team found that the scores showed a strong correlation with traditional socio-economic metrics such as population density, employment, and number of businesses. This demonstrates the wide applicability and scalability of the approach, particularly in data-scarce countries. Furthermore, the model's strength lies in its ability to detect annual changes in economic conditions at a more detailed geospatial level without using any survey data.
This model would be especially valuable for rapidly monitoring the progress of Sustainable Development Goals such as reducing poverty and promoting more equitable and sustainable growth on an international scale. The model can also be adapted to measure various social and environmental indicators. For example, it can be trained to identify regions with high vulnerability to climate change and disasters to provide timely guidance on disaster relief efforts.

As an example, the researchers explored how North Korea changed before and after the United Nations sanctions against the country. By applying the model to satellite images of North Korea both in 2016 and in 2019, the researchers discovered three key trends in the country's economic development between 2016 and 2019. First, economic growth in North Korea became more concentrated in Pyongyang and major cities, exacerbating the urban-rural divide. Second, satellite imagery revealed significant changes in areas designated for tourism and economic development, such as new building construction and other meaningful alterations. Third, traditional industrial and export development zones showed relatively minor changes.
Meeyoung Cha, a data scientist in the team explained, ""This is an important interdisciplinary effort to address global challenges like poverty. We plan to apply our AI algorithm to other international issues, such as monitoring carbon emissions, disaster damage detection, and the impact of climate change.""
An economist on the research team, Jihee Kim, commented that this approach would enable detailed examinations of economic conditions in the developing world at a low cost, reducing data disparities between developed and developing nations. She further emphasized that this is most essential because many public policies require economic measurements to achieve their goals, whether they are for growth, equality, or sustainability.
The research team has made the source code publicly available via GitHub and plans to continue improving the technology, applying it to new satellite images updated annually. The results of this study, with Ph.D. candidate Donghyun Ahn at KAIST and Ph.D. candidate Jeasurk Yang at NUS as joint first authors, were published in Nature Communications under the title ""A human-machine collaborative approach measures economic development using satellite imagery.""

","score: 15.721096815956631, grade_level: '16'","score: 16.41773718493345, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41467-023-42122-8,"Machine learning approaches using satellite imagery are providing accessible ways to infer socioeconomic measures without visiting a region. However, many algorithms require integration of ground-truth data, while regional data are scarce or even absent in many countries. Here we present our human-machine collaborative model which predicts grid-level economic development using publicly available satellite imagery and lightweight subjective ranking annotation without any ground data. We applied the model to North Korea and produced fine-grained predictions of economic development for the nation where data is not readily available. Our model suggests substantial development in the country’s capital and areas with state-led development projects in recent years. We showed the broad applicability of our model by examining five of the least developed countries in Asia, covering 400,000 grids. Our method can both yield highly granular economic information on hard-to-visit and low-resource regions and can potentially guide sustainable development programs."
"
EPFL researchers have developed an algorithm to train an analog neural network just as accurately as a digital one, enabling the development of more efficient alternatives to power-hungry deep learning hardware.

With their ability to process vast amounts of data through algorithmic 'learning' rather than traditional programming, it often seems like the potential of deep neural networks like Chat-GPT is limitless. But as the scope and impact of these systems have grown, so have their size, complexity, and energy consumption -- the latter of which is significant enough to raise concerns about contributions to global carbon emissions.
And while we often think of technological advancement in terms of shifting from analog to digital, researchers are now looking for answers to this problem in physical alternatives to digital deep neural networks. One such researcher is Romain Fleury of EPFL's Laboratory of Wave Engineering in the School of Engineering. In a paper published in Science, he and his colleagues describe an algorithm for training physical systems that shows improved speed, enhanced robustness, and reduced power consumption compared to other methods.
""We successfully tested our training algorithm on three wave-based physical systems that use sound waves, light waves, and microwaves to carry information, rather than electrons. But our versatile approach can be used to train any physical system,"" says first author and LWE researcher Ali Momeni.
A ""more biologically plausible"" approach
Neural network training refers to helping systems learn to generate optimal values of parameters for a task like image or speech recognition. It traditionally involves two steps: a forward pass, where data is sent through the network and an error function is calculated based on the output; and a backward pass (also known as backpropagation, or BP), where a gradient of the error function with respect to all network parameters is calculated.
Over repeated iterations, the system updates itself based on these two calculations to return increasingly accurate values. The problem? In addition to being very energy-intensive, BP is poorly suited to physical systems. In fact, training physical systems usually requires a digital twin for the BP step, which is inefficient and carries the risk of a reality-simulation mismatch.

The scientists' idea was to replace the BP step with a second forward pass through the physical system to update each network layer locally. In addition to decreasing power use and eliminating the need for a digital twin, this method better reflects human learning.
""The structure of neural networks is inspired by the brain, but it is unlikely that the brain learns via BP,"" explains Momeni. ""The idea here is that if we train each physical layer locally, we can use our actual physical system instead of first building a digital model of it. We have therefore developed an approach that is more biologically plausible.""
The EPFL researchers, with Philipp del Hougne of CNRS IETR and Babak Rahmani of Microsoft Research, used their physical local learning algorithm (PhyLL) to train experimental acoustic and microwave systems and a modeled optical system to classify data like vowel sounds and images. As well as showing comparable accuracy to BP-based training, the method was robust and adaptable -- even in systems exposed to unpredictable external perturbations -- compared to the state of the art.
An analog future?
While the LWE's approach is the first BP-free training of deep physical neural networks, some digital updates of the parameters are still required. ""It's a hybrid training approach, but our aim is to decrease digital computation as much as possible,"" Momeni says.
The researchers now hope to implement their algorithm on a small-scale optical system, with the ultimate goal of increasing network scalability.
""In our experiments, we used neural networks with up to 10 layers, but would it still work with 100 layers with billions of parameters? This is the next step, and will require overcoming technical limitations of physical systems.""

","score: 14.399451534229485, grade_level: '14'","score: 15.092561964721227, grade_levels: ['college_graduate'], ages: [24, 100]",10.1126/science.adi8474,"Recent successes in deep learning for vision and natural language processing are attributed to larger models but come with energy consumption and scalability issues. Current training of digital deep-learning models primarily relies on backpropagation that is unsuitable for physical implementation. In this work, we propose a simple deep neural network architecture augmented by a physical local learning (PhyLL) algorithm, which enables supervised and unsupervised training of deep physical neural networks without detailed knowledge of the nonlinear physical layer’s properties. We trained diverse wave-based physical neural networks in vowel and image classification experiments, showcasing the universality of our approach. Our method shows advantages over other hardware-aware training schemes by improving training speed, enhancing robustness, and reducing power consumption by eliminating the need for system modeling and thus decreasing digital computation."
"

To magnetize an iron nail, one simply has to stroke its surface several times with a bar magnet. Yet, there is a much more unusual method: A team led by the Helmholtz-Zentrum Dresden-Rossendorf (HZDR) discovered some time ago that a certain iron alloy can be magnetized with ultrashort laser pulses. The researchers have now teamed up with the Laserinstitut Hochschule Mittweida (LHM) to investigate this process further. They discovered that the phenomenon also occurs with a different class of materials – which significantly broadens potential application prospects. The working group presents its findings in the scientific journal Advanced Functional Materials (DOI: 10.1002/adfm.202311951).The unexpected discovery was made back in 2018. When the HZDR team irradiated a thin layer of an iron-aluminum alloy with ultrashort laser pulses, the non-magnetic material suddenly became magnetic. The explanation: The laser pulses rearrange the atoms in the crystal in such a way that the iron atoms move closer together, and thus forming a magnet. The researchers were then able to demagnetize the layer again with a series of weaker laser pulses. This enabled them to discover a way of creating and erasing tiny ""magnetic spots"" on a surface.However, the pilot experiment still left some questions unanswered. ""It was unclear whether the effect only occurs in the iron-aluminum alloy or also in other materials,"" explains HZDR physicist Dr. Rantej Bali. ""We also wanted to try tracking the time progression of the process."" For further investigation, he teamed up with Dr. Theo Pflug from the LHM and colleagues from the University of Zaragoza in Spain.Flip book with laser pulsesThe experts focused specifically on an iron-vanadium alloy. Unlike the iron-aluminum alloy with its regular crystal lattice, the atoms in the iron-vanadium alloy are arranged more chaotically, forming an amorphous, glass-like structure. In order to observe what happens upon laser irradiation, the physicists used a special method: The pump-probe method.""First, we irradiate the alloy with a strong laser pulse, which magnetizes the material,"" explains Theo Pflug. ""Simultaneously, we use a second, weaker pulse that is reflected on the material surface.""The analysis of the reflected laser pulse provides an indication of the material's physical properties. This process is repeated several times, whereby the time interval between the first ""pump"" pulse and the subsequent ""probe"" pulse is continually extended.As a result, a time series of reflection data is obtained, which allows to characterize the processes being triggered by the laser excitation. ""The whole procedure is similar to generating a flip book,"" says Pflug. ""Likewise, a series of individual images that animate when viewed in quick succession.""Rapid meltingThe result: Although it has a different atomic structure than the iron-aluminum compound, the iron-vanadium alloy can also be magnetized via laser. ""In both cases, the material melts briefly at the irradiation point"", explains Rantej Bali. ""This causes the laser to erase the previous structure so that a small magnetic area is generated in both alloys.""An encouraging result: Apparently, the phenomenon is not limited to a specific material structure but can be observed in diverse atomic arrangements.The team is also keeping track of the temporal dynamics of the process: ""At least we now know in which time scales something happens,"" explains Theo Pflug. ""Within femtoseconds, the laser pulse excites the electrons in the material. Several picoseconds later, the excited electrons transfer their energy to the atomic nuclei.""Consequently, this energy transfer causes the rearrangement into a magnetic structure, which is stabilized by the subsequent rapid cooling. In follow-up experiments, the researchers aim to observe exactly how the atoms rearrange themselves by examining the magnetization process with intense X-rays.Sights set on applicationsAlthough still in the early stages, this work already provides initial ideas for possible applications: For example, placing tiny magnets on a chip surface via laser is conceivable. ""This could be useful for the production of sensitive magnetic sensors, such as those used in vehicles,"" speculates Rantej Bali. ""It could also find possible applications in magnetic data storage.""Additionally, the phenomenon appears relevant for a new type of electronics, namely spintronics. Here, magnetic signals should be used for digital computing processes instead of electrons passing through transistors as usual – offering a possible approach to computer technology of the future.

","score: 14.445669398907103, grade_level: '14'","score: 14.80577868852459, grade_levels: ['college_graduate'], ages: [24, 100]",10.1002/adfm.202311951,"Atomic scale reordering of lattices can induce local modulations of functional material properties, such as reflectance and ferromagnetism. Pulsed femtosecond laser irradiation enables lattice reordering in the picosecond range. However, the dependence of the phase transitions on the initial lattice order as well as the temporal dynamics of these transitions remain to be understood. This study investigates the laser‐induced atomic reordering and the concomitant onset of ferromagnetism in thin Fe‐based alloy films with vastly differing initial atomic orders. The optical response to single femtosecond laser pulses on selected prototype systems, one that initially possesses positional disorder, Fe60V40, and a second system initially in a chemically ordered state, Fe60Al40, has been tracked with time. Despite the vastly different initial atomic orders the structure in both systems converges to a positionally ordered but chemically disordered state, accompanied by the onset of ferromagnetism. Time‐resolved measurements of the transient reflectance combined with simulations of the electron and phonon temperatures reveal that the reordering processes occur via the formation of a transient molten state with an approximate lifetime of 200 ps. These findings provide insights into the fundamental processes involved in laser‐induced atomic reordering, paving the way for controlling material properties in the picosecond range."
"
On the highway of heat transfer, thermal energy is moved by way of quantum particles called phonons. But at the nanoscale of today's most cutting-edge semiconductors, those phonons don't remove enough heat. That's why Purdue University researchers are focused on opening a new nanoscale lane on the heat transfer highway by using hybrid quasiparticles called ""polaritons.""

Thomas Beechem loves heat transfer. He talks about it loud and proud, like a preacher at a big tent revival.
""We have several ways of describing energy,"" said Beechem, associate professor of mechanical engineering. ""When we talk about light, we describe it in terms of particles called 'photons.' Heat also carries energy in predictable ways, and we describe those waves of energy as 'phonons.' But sometimes depending on the material, photons and phonons will come together and make something new called a 'polariton.' It carries energy in its own way, distinct from both photons or phonons.""
Like photons and phonons, polaritons aren't physical particles you can see or capture. They are more like ways of describing energy exchange as if they were particles.
Still fuzzy? How about another analogy. ""Phonons are like internal combustion vehicles, and photons are like electric vehicles,"" Beechem said. ""Polaritons are a Toyota Prius. They are a hybrid of light and heat, and retain some of the properties of both. But they are their own special thing.""
Polaritons have been used in optical applications -- everything from stained glass to home health tests. But their ability to move heat has largely been ignored, because their impact becomes significant only when the size of materials becomes very small. ""We know that phonons do a majority of the work of transferring heat,"" said Jacob Minyard, a Ph.D. student in Beechem's lab. ""The effect of polaritons is only observable at the nanoscale. But we've never needed to address heat transfer at that level until now, because of semiconductors.""
""Semiconductors have become so incredibly small and complex,"" he continued. ""People who design and build these chips are discovering that phonons don't efficiently disperse heat at these very small scales. Our paper demonstrates that at those length scales, polaritons can contribute a larger share of thermal conductivity.""
Their research on polaritons has been selected as a Featured Article in the Journal of Applied Physics.

""We in the heat transfer community have been very material-specific in describing the effect of polaritons,"" said Beechem. ""Someone will observe it in this material or at that interface. It's all very disparate. Jacob's paper has established that this isn't some random thing. Polaritons begin to dominate the heat transfer on any surface thinner than 10 nanometers. That's twice as big as the transistors on an iPhone 15.""
Now Beechem gets really fired up. ""We've basically opened up a whole extra lane on the highway. And the smaller the scales get, the more important this extra lane becomes. As semiconductors continue to shrink, we need to think about designing the traffic flow to take advantage of both lanes: phonons and polaritons.""
Minyard's paper just scratches the surface of how this can happen practically. The complexity of semiconductors means that there are many opportunities to capitalize upon polariton-friendly designs. ""There are many materials involved in chipmaking, from the silicon itself to the dielectrics and metals,"" Minyard said. ""The way forward for our research is to understand how these materials can be used to conduct heat more efficiently, recognizing that polaritons provide a whole new lane to move energy.""
Recognizing this, Beechem and Minyard want to show chip manufacturers how to incorporate these polariton-based nanoscale heat transfer principles right into the physical design of the chip -- from the physical materials involved, to the shape and thickness of the layers.
While this work is theoretical now, physical experimentation is very much on the horizon -- which is why Beechem and Minyard are happy to be at Purdue.
""The heat transfer community here at Purdue is so robust,"" Beechem said. ""We can literally go upstairs and talk to Xianfan Xu, who had one of the first experimental realizations of this effect. Then we can walk over to Flex Lab and ask Xiulin Ruan about his pioneering work in phonon scattering. And we have the facilities here at Birck Nanotechnology Center to build nanoscale experiments, and use one-of-a-kind measurement tools to confirm our findings. It's really a researcher's dream.""

","score: 10.423639624724064, grade_level: '10'","score: 9.816146247240617, grade_levels: ['10'], ages: [15, 16]",10.1063/5.0173917,"The material dependence of phonon-polariton-based in-plane thermal conductance is investigated by examining systems composed of air and several wurtzite and zinc-blende crystals. Phonon-polariton-based thermal conductance varies by over an order of magnitude (∼0.5–60 nW/K), which is similar to the variation observed in the materials corresponding to bulk thermal conductivity. Regardless of the material, phonon-polaritons exhibit similar thermal conductance to that of phonons when layers become ultrathin (∼10 nm), suggesting the generality of the effect at these length-scales. A figure of merit is proposed to explain the large variation of in-plane polariton thermal conductance that is composed entirely of easily predicted and measured optical phonon energies and lifetimes. Using this figure of merit, in-plane phonon-polariton thermal conductance enlarges with increases in (1) optical phonon energies, (2) splitting between transverse and longitudinal mode pairs, and (3) phonon lifetimes."
"
Engineers at Duke University and Harvard Medical School have developed a bio-compatible ink that solidifies into different 3D shapes and structures by absorbing ultrasound waves. Because it responds to sound waves rather than light, the ink can be used in deep tissues for biomedical purposes ranging from bone healing to heart valve repair.

This work appears on December 7 in the journal Science.
The uses for 3D-printing tools are ever increasing. Printers create prototypes of medical devices, design flexible, lightweight electronics, and even engineer tissues used in wound healing. But many of these printing techniques involve building the object point-by-point in a slow and arduous process that often requires a robust printing platform.
To circumvent these issues over the past several years, researchers developed a photo-sensitive ink that responds directly to targeted beams of light and quickly hardens into a desired structure. While this printing technique can substantially improve the speed and quality of a print, researchers can only use transparent inks for the prints, and biomedical purposes are limited, as light can't reach beyond a few millimeters deep into tissue.
Now, Y. Shrike Zhang, associate bioengineer at Brigham and Women's Hospital and associate professor at Harvard Medical School, and Junjie Yao, associate professor of biomedical engineering at Duke, have developed a new printing method called deep-penetrating acoustic volumetric printing, or DVAP, that resolves these problems. This new technique involves a specialized ink that reacts to soundwaves rather than light, enabling them to create biomedically useful structures at unprecedented tissue depths.
""DVAP relies on the sonothermal effect, which occurs when soundwaves are absorbed and increase the temperature to harden our ink,"" explained Yao, who designed the ultrasound printing technology for DVAP. ""Ultrasound waves can penetrate more than 100 times deeper than light while still spatially confined, so we can reach tissues, bones and organs with high spatial precision that haven't been reachable with light-based printing methods.""
The first component of DVAP involves a sonicated ink, called sono-ink, that is a combination of hydrogels, microparticles and molecules designed to specifically react to ultrasound waves. Once the sono-ink is delivered into the target area, a specialized ultrasound printing probe sends focused ultrasound waves into the ink, hardening portions of it into intricate structures. These structures can range from a hexagonal scaffold that mimics the hardness of bone to a bubble of hydrogel that can be placed on an organ.

""The ink itself is a viscous liquid, so it can be injected into a targeted area fairly easily, and as you move the ultrasound printing probe around, the materials in the ink will link together and harden,"" said Zhang, who designed the sono-ink in his lab at the Brigham. ""Once it's done, you can remove any remaining ink that isn't solidified via a syringe.""
The different components of the sono-ink enable the researchers to adjust the formula for a wide variety uses. For example, if they want to create a scaffold to help heal a broken bone or make up for bone loss, they can add bone mineral particles to the ink. This flexibility also allows them to engineer the hardened formula to be more durable or more degradable, depending on its use. They can even adjust the colors of their final print.
The team conducted three tests as a proof-of-concept of their new technique. The first involved using the ink to seal off a section in a goat's heart. When a human has nonvalvular atrial fibrillation, the heart won't beat correctly, causing blood to pool in the organ. Traditional treatment often requires open-chest surgery to seal off the left atrial appendage to reduce the risk of blood clots and heart attack.
Instead, the team used a catheter to deliver their sono-ink to the left atrial appendage in a goat heart that was placed in a printing chamber. The ultrasound probe then delivered focused ultrasound waves through 12 mm of tissue, hardening the ink without damaging any of the surrounding organ. Once the process was complete, the ink was safely bonded to the heart tissue and was flexible enough to withstand movements that mimicked the heart beating.
Next, the team tested the potential for DVAP's use for tissue reconstruction and regeneration. After creating a bone defect model using a chicken leg, the team injected the sono-ink and hardened it through 10 mm of sample skin and muscle tissue layers. The resulting material bonded seamlessly to the bone and didn't negatively impact any of the surrounding tissues.
Finally, Yao and Zhang showed that DVAP could also be used for therapeutic drug delivery. In their example, they added a common chemotherapy drug to their ink, which they delivered to sample liver tissue. Using their probe, they hardened the sono-ink into hydrogels that slowly release the chemotherapy and diffuse into the liver tissue.
""We're still far from bringing this tool into the clinic, but these tests reaffirmed the potential of this technology,"" said Zhang. ""We're very excited to see where it can go from here.""
""Because we can print through tissue, it allows for a lot of potential applications in surgery and therapy that traditionally involve very invasive and disruptive methods,"" said Yao. ""This work opens up an exciting new avenue in the 3D printing world, and we're excited to explore the potential of this tool together.""

","score: 13.258151447661472, grade_level: '13'","score: 14.323918649630762, grade_levels: ['college_graduate'], ages: [24, 100]",10.1126/science.adi1563,"Volumetric printing, an emerging additive manufacturing technique, builds objects with enhanced printing speed and surface quality by forgoing the stepwise ink-renewal step. Existing volumetric printing techniques almost exclusively rely on light energy to trigger photopolymerization in transparent inks, limiting material choices and build sizes. We report a self-enhancing sonicated ink (or sono-ink) design and corresponding focused-ultrasound writing technique for deep-penetration acoustic volumetric printing (DAVP). We used experiments and acoustic modeling to study the frequency and scanning rate–dependent acoustic printing behaviors. DAVP achieves the key features of low acoustic streaming, rapid sonothermal polymerization, and large printing depth, enabling the printing of volumetric hydrogels and nanocomposites with various shapes regardless of their optical properties. DAVP also allows printing at centimeter depths through biological tissues, paving the way toward minimally invasive medicine."
"
The Chirik Group at the Princeton Department of Chemistry is chipping away at one of the great challenges of metal-catalyzed C-H functionalization with a new method that uses a cobalt catalyst to differentiate between bonds in fluoroarenes, functionalizing them based on their intrinsic electronic properties.

In a paper published this week in Science, researchers show they are able to bypass the need for steric control and directing groups to induce cobalt-catalyzed borylation that is meta-selective.
The lab's research showcases an innovative approach driven by deep insights into organometallic chemistry that have been at the heart of its mission for over a decade. In this case, the Chirik Lab drilled down into how transition metals break C-H bonds, uncovering a method that could have vast implications for the synthesis of medicines, natural products, and materials.
And their method is fast -- comparable in speed to those that rely on iridium.
The research is outlined in ""Kinetic and Thermodynamic Control of C(sp2)-H Activation Enable Site-Selective Borylation,"" by lead author Jose Roque, a former postdoc in the Chirik Group; postdoc Alex Shimozono; and P.I. Paul Chirik, the Edwards S. Sanford Professor of Chemistry and former lab members Tyler Pabst, Gabriele Hierlmeier, and Paul Peterson.
'Really fast, really selective'
""Chemists have been saying for decades, let's turn synthetic chemistry on its head and make the C-H bond a reactive part of the molecule. That would be incredibly important for drug discovery for the pharmaceutical industry, or for making materials,"" said Chirik.

""One of the ways we do this is called C-H borylation, in which you turn the C-H bond into something else, into a carbon-boron bond. Turning C-H to C-B is a gateway to great chemistry.""
Benzene rings are highly represented motifs in medicinal chemistry. However, chemists rely on traditional approaches to functionalize them. The Chirik Group develops new methods that access less-explored routes.
""Imagine you have a benzene ring and it has one substituent on it,"" Chikik added. ""The site next to it is called ortho, the one next to that is called meta, and the one opposite is called para. The meta C-H bond is the hardest one to do selectively. That's what Jose has done here with a cobalt catalyst, and no one's done it before.
""He's made a cobalt catalyst that is really fast and really selective.""
Roque, now an assistant professor in Princeton's Department of Chemistry, said rational design was at the heart of their solution.
""We started to get a glimpse of the high activity for C-H activation early during our stoichiometric studies,"" said Roque. ""The catalyst was rapidly activating the C-H bonds of aromatic solvents at room temperature. In order to isolate the catalyst, we had to avoid handling the catalyst in aromatic solvents,"" he added. ""We designed an electronically rich but sterically accessible pincer ligand that we posited -- based on some previous insights from our lab as well as some fundamental organometallic principles -- would lead to a more active catalyst.

""And it has.""
Chirik Lab Target Since 2014 
State-of-the-art borylation uses iridium as a catalyst for sterically driven C-H functionalization. It is highly reactive, and it is fast. But if you have a molecule with many C-H bonds, iridium catalysts fail to selectively functionalize the desired bond.
As a result, pharmaceutical companies have appealed for an alternative with more selectivity. And they've sought it among first-row transition metals like cobalt and iron, which are less expensive and more sustainable than iridium.
Since their first paper on C-H borylation in 2014, the Chirik Lab has articulated the concept of electronically controlled C-H activation as one answer to this challenge. Their idea is to differentiate between C-H bonds based on electronic properties in order to functionalize them. These properties are reflected in the metal-carbon bond strength. With the catalyst designed in this research, chemists can hit the selected bond and only the selected bond by tapping into these disparate strengths.
But they uncovered another result that makes their method advantageous: the site selectivity can be switched by exploiting the kinetic or thermodynamic preferences of C-H activation. This selectivity switch can be accomplished by choosing one reagent over another, a process that is as streamlined as it is cost-effective.
""Site-selective meta-to-fluorine functionalization was a huge challenge. We made some great progress toward that with this research and expanded the chemistry to include other substrate classes beyond fluoroarenes,"" said Roque. ""But as a function of studying first-row metals, we also found out, hey, we can switch the selectivity.""
Added Chirik: ""To me, this is a huge concept in C-H functionalization. Now we can look at metal-carbon bond strengths and predict where things are going to go. This opens a whole new opportunity. We're going to be able to do things that iridium doesn't do.""
Shimozono came to the project late in the game, after Roque had already discovered the pivotal catalyst. His role will deepen in the coming months as he seeks new advances in borylation.
""Jose's catalyst is groundbreaking. Usually, a completely different catalyst is required in order to change site-selectivity,"" said Shimozono. ""Counter to this dogma, Jose demonstrated that using B2Pin2 as the boron source affords meta selective chemistry, while using HBPin as the boron source gives ortho selective borylation using the same iPrACNCCo catalyst.
""In general, the more methods we have to install groups in specific sites in molecules, the better. This gives pharmaceutical chemists more tools to make and discover medications more efficiently.""

","score: 12.107030277185505, grade_level: '12'","score: 11.891663113006402, grade_levels: ['12'], ages: [17, 18]",10.1126/science.adj6527,"Catalysts that distinguish between electronically distinct carbon-hydrogen (C–H) bonds without relying on steric effects or directing groups are challenging to design. In this work, cobalt precatalysts supported by N -alkyl-imidazole–substituted pyridine dicarbene (ACNC) pincer ligands are described that enable undirected, remote borylation of fluoroaromatics and expansion of scope to include electron-rich arenes, pyridines, and tri- and difluoromethoxylated arenes, thereby addressing one of the major limitations of first-row transition metal C–H functionalization catalysts. Mechanistic studies established a kinetic preference for C–H bond activation at the meta -position despite cobalt-aryl complexes resulting from ortho C–H activation being thermodynamically preferred. Switchable site selectivity in C–H borylation as a function of the boron reagent was thereby preliminarily demonstrated using a single precatalyst."
"
A central goal in quantum optics and photonics is to increase the strength of the interaction between light and matter to produce, e.g., better photodetectors or quantum light sources. The best way to do that is to use optical resonators that store light for a long time, making it interact more strongly with matter. If the resonator is also very small, such that light is squeezed into a tiny region of space, the interaction is enhanced even further. The ideal resonator would store light for a long time in a region at the size of a single atom.

Physicists and engineers have struggled for decades with how small optical resonators can be made without making them very lossy, which is equivalent to asking how small you can make a semiconductor device. The semiconductor industry's roadmap for the next 15 years predicts that the smallest possible width of a semiconductor structure will be no less than 8 nm, which is several tens of atoms wide.
The team behind a new paper in Nature, Associate Professor Søren Stobbe and his colleagues at DTU Electro demonstrated 8 nm cavities last year, but now they propose and demonstrate a novel approach to fabricate a self-assembling cavity with an air void at the scale of a few atoms. Their paper 'Self-assembled photonic cavities with atomic-scale confinement' detailing the results is published today in Nature.
To briefly explain the experiment, two halves of silicon structures are suspended on springs, although in the first step, the silicon device is firmly attached to a layer of glass. The devices are made by conventional semiconductor technology, so the two halves are a few tens of nanometers apart. Upon selective etching of the glass, the structure is released and now only suspended by the springs, and because the two halves are fabricated so close to each other, they attract due to surface forces. By carefully engineering the design of the silicon structures, the result is a self-assembled resonator with bowtie-shaped gaps at the atomic scale surrounded by silicon mirrors.
""We are far from a circuit that builds itself completely. But we have succeeded in converging two approaches that have been travelling along parallel tracks so far. And it allowed us to build a silicon resonator with unprecedented miniaturization,"" says Søren Stobbe.
Two separate approaches
One approach -- the top-down approach -- is behind the spectacular development we have seen with silicon-based semiconductor technologies. Here, crudely put, you go from a silicon block and work on making nanostructures from them. The other approach -- the bottom-up approach -- is where you try to have a nanotechnological system assemble itself. It aims to mimic biological systems, such as plants or animals, built through biological or chemical processes. These two approaches are at the very core of what defines nanotechnology. But the problem is that these two approaches were so far disconnected: Semiconductors are scalable but cannot reach the atomic scale, and while self-assembled structures have long been operating at atomic scales, they offer no architecture for the interconnects to the external world.

""The interesting thing would be if we could produce an electronic circuit that built itself -- just like what happens with humans as they grow but with inorganic semiconductor materials. That would be true hierarchical self-assembly. We use the new self-assembly concept for photonic resonators, which may be used in electronics, nanorobotics, sensors, quantum technologies, and much more. Then, we would really be able to harvest the full potential of nanotechnology. The research community is many breakthroughs away from realizing that vision, but I hope we have taken the first steps,"" says Guillermo Arregui, who co-supervised the project.
Approaches converging
Supposing a combination of the two approaches is possible, the team at DTU Electro set out to create nanostructures that surpass the limits of conventional lithography and etching despite using nothing more than conventional lithography and etching. Their idea was to use two surface forces, namely the Casimir force for attracting the two halves and the van der Waals force for making them stick together. These two forces are rooted in the same underlying effect: quantum fluctuations (see Fact box).
The researchers made photonic cavities that confine photons to air gaps so small that determining their exact size was impossible, even with a transmission electron microscope. But the smallest they built are of a size of 1-3 silicon atoms.
""Even if the self-assembly takes care of reaching these extreme dimensions, the requirements for the nanofabrication are no less extreme. For example, structural imperfections are typically on the scale of several nanometers. Still, if there are defects at this scale, the two halves will only meet and touch at the three largest defects. We are really pushing the limits here, even though we make our devices in one of the very best university cleanrooms in the world,"" says Ali Nawaz Babar, a PhD student at the NanoPhoton Center of Excellence at DTU Electro and first author of the new paper.
""The advantage of self-assembly is that you can make tiny things. You can build unique materials with amazing properties. But today, you can't use it for anything you plug into a power outlet. You can't connect it to the rest of the world. So, you need all the usual semiconductor technology for making the wires or waveguides to connect whatever you have self-assembled to the external world.""
Robust and accurate self-assembly

The paper shows a possible way to link the two nanotechnology approaches by employing a new generation of fabrication technology that combines the atomic dimensions enabled by self-assembly with the scalability of semiconductors fabricated with conventional methods.
""We don't have to go in and find these cavities afterwards and insert them into another chip architecture. That would also be impossible because of the tiny size. In other words, we are building something on the scale of an atom already inserted in a macroscopic circuit. We are very excited about this new line of research, and plenty of work is ahead,"" says Søren Stobbe.
Surface forces
There are four known fundamental forces: Gravitational, electromagnetic, and strong and weak nuclear forces. Besides the forces due to static configurations, e.g., the attractive electromagnetic force between positively and negatively charged particles, there can also be forces due to fluctuations. Such fluctuations may be either thermal or quantum in origin, and they give rise to surface forces such as the van der Waals force and the Casimir force which act at different length scales but are rooted in the same underlying physics. Other mechanisms, such as electrostatic surface charges, can add to the net surface force. For example, geckos exploit surface forces to cling to walls and ceilings.
How it was done
The paper details three experiments that the researchers carried out in the labs at DTU:  No fewer than 2688 devices across two microchips were fabricated, each containing a platform that would either collapse onto a nearby silicon wall -- or not collapse, depending upon the surface area details, spring constant, and distance between platform and wall. This allowed the researchers to make a map of which parameters would -- and would not -- lead to deterministic self-assembly. Only 11 devices failed due to fabrication errors or other defects, a remarkably low number for a novel self-assembly process.  The researchers made self-assembled optical resonators whose optical properties were verified experimentally, and the atomic scale was confirmed by transmission electron microscopy.  The self-assembled cavities were embedded in a larger architecture consisting of self-assembled waveguides, springs, and photonic couplers to make the surrounding microchip circuitry in the same process.

","score: 13.429642964770036, grade_level: '13'","score: 14.422562244385446, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41586-023-06736-8,"Despite tremendous progress in research on self-assembled nanotechnological building blocks, such as macromolecules1, nanowires2 and two-dimensional materials3, synthetic self-assembly methods that bridge the nanoscopic to macroscopic dimensions remain unscalable and inferior to biological self-assembly. By contrast, planar semiconductor technology has had an immense technological impact, owing to its inherent scalability, yet it seems unable to reach the atomic dimensions enabled by self-assembly. Here, we use surface forces, including Casimir–van der Waals interactions4, to deterministically self-assemble and self-align suspended silicon nanostructures with void features well below the length scales possible with conventional lithography and etching5, despite using only conventional lithography and etching. The method is remarkably robust and the threshold for self-assembly depends monotonically on all the governing parameters across thousands of measured devices. We illustrate the potential of these concepts by fabricating nanostructures that are impossible to make with any other known method: waveguide-coupled high-Q silicon photonic cavities6,7 that confine telecom photons to 2 nm air gaps with an aspect ratio of 100, corresponding to mode volumes more than 100 times below the diffraction limit. Scanning transmission electron microscopy measurements confirm the ability to build devices with sub-nanometre dimensions. Our work constitutes the first steps towards a new generation of fabrication technology that combines the atomic dimensions enabled by self-assembly with the scalability of planar semiconductors."
"
Researchers have discovered magnetic monopoles -- isolated magnetic charges -- in a material closely related to rust, a result that could be used to power greener and faster computing technologies.

Researchers led by the University of Cambridge used a technique known as diamond quantum sensing to observe swirling textures and faint magnetic signals on the surface of hematite, a type of iron oxide.
The researchers observed that magnetic monopoles in hematite emerge through the collective behaviour of many spins (the angular momentum of a particle). These monopoles glide across the swirling textures on the surface of the hematite, like tiny hockey pucks of magnetic charge. This is the first time that naturally occurring emergent monopoles have been observed experimentally.
The research has also shown the direct connection between the previously hidden swirling textures and the magnetic charges of materials like hematite, as if there is a secret code linking them together. The results, which could be useful in enabling next-generation logic and memory applications, are reported in the journal Nature Materials.
According to the equations of James Clerk Maxwell, a giant of Cambridge physics, magnetic objects, whether a fridge magnet or the Earth itself, must always exist as a pair of magnetic poles that cannot be isolated.
""The magnets we use every day have two poles: north and south,"" said Professor Mete Atatüre, who led the research. ""In the 19th century, it was hypothesised that monopoles could exist. But in one of his foundational equations for the study of electromagnetism, James Clerk Maxwell disagreed.""
Atatüre is Head of Cambridge's Cavendish Laboratory, a position once held by Maxwell himself. ""If monopoles did exist, and we were able to isolate them, it would be like finding a missing puzzle piece that was assumed to be lost,"" he said.

About 15 years ago, scientists suggested how monopoles could exist in a magnetic material. This theoretical result relied on the extreme separation of north and south poles so that locally each pole appeared isolated in an exotic material called spin ice.
However, there is an alternative strategy to find monopoles, involving the concept of emergence. The idea of emergence is the combination of many physical entities can give rise to properties that are either more than or different to the sum of their parts.
Working with colleagues from the University of Oxford and the National University of Singapore, the Cambridge researchers used emergence to uncover monopoles spread over two-dimensional space, gliding across the swirling textures on the surface of a magnetic material.
The swirling topological textures are found in two main types of materials: ferromagnets and antiferromagnets. Of the two, antiferromagnets are more stable than ferromagnets, but they are more difficult to study, as they don't have a strong magnetic signature.
To study the behaviour of antiferromagnets, Atatüre and his colleagues use an imaging technique known as diamond quantum magnetometry. This technique uses a single spin -- the inherent angular momentum of an electron -- in a diamond needle to precisely measure the magnetic field on the surface of a material, without affecting its behaviour.
For the current study, the researchers used the technique to look at hematite, an antiferromagnetic iron oxide material. To their surprise, they found hidden patterns of magnetic charges within hematite, including monopoles, dipoles and quadrupoles.

""Monopoles had been predicted theoretically, but this is the first time we've actually seen a two-dimensional monopole in a naturally occurring magnet,"" said co-author Professor Paolo Radaelli, from the University of Oxford.
""These monopoles are a collective state of many spins that twirl around a singularity rather than a single fixed particle, so they emerge through many-body interactions. The result is a tiny, localised stable particle with diverging magnetic field coming out of it,"" said co-first author Dr Hariom Jani, from the University of Oxford.
""We've shown how diamond quantum magnetometry could be used to unravel the mysterious behaviour of magnetism in two-dimensional quantum materials, which could open up new fields of study in this area,"" said co-first author Dr Anthony Tan, from the Cavendish Laboratory. ""The challenge has always been direct imaging of these textures in antiferromagnets due to their weaker magnetic pull, but now we're able to do so, with a nice combination of diamonds and rust.""
The study not only highlights the potential of diamond quantum magnetometry but also underscores its capacity to uncover and investigate hidden magnetic phenomena in quantum materials. If controlled, these swirling textures dressed in magnetic charges could power super-fast and energy-efficient computer memory logic.
The research was supported in part by the Royal Society, the Sir Henry Royce Institute, the European Union, and the Engineering and Physical Sciences Research Council (EPSRC), part of UK Research and Innovation (UKRI).

","score: 15.567525252525254, grade_level: '16'","score: 15.880643939393941, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41563-023-01737-4,"Whirling topological textures play a key role in exotic phases of magnetic materials and are promising for logic and memory applications. In antiferromagnets, these textures exhibit enhanced stability and faster dynamics with respect to their ferromagnetic counterparts, but they are also difficult to study due to their vanishing net magnetic moment. One technique that meets the demand of highly sensitive vectorial magnetic field sensing with negligible backaction is diamond quantum magnetometry. Here we show that an archetypal antiferromagnet—haematite—hosts a rich tapestry of monopolar, dipolar and quadrupolar emergent magnetic charge distributions. The direct read-out of the previously inaccessible vorticity of an antiferromagnetic spin texture provides the crucial connection to its magnetic charge through a duality relation. Our work defines a paradigmatic class of magnetic systems to explore two-dimensional monopolar physics, and highlights the transformative role that diamond quantum magnetometry could play in exploring emergent phenomena in quantum materials."
"
Wearable devices that use sensors to monitor biological signals can play an important role in health care. These devices provide valuable information that allows providers to predict, diagnose and treat a variety of conditions while improving access to care and reducing costs.

However, wearables currently require significant infrastructure -- such as satellites or arrays of antennas that use cell signals -- to transmit data, making many of those devices inaccessible to rural and under-resourced communities.
A group of University of Arizona researchers has set out to change that with a wearable monitoring device system that can send health data up to 15 miles -- much farther than Wi-Fi or Bluetooth systems can -- without any significant infrastructure. Their device, they hope, will help make digital health access more equitable.
The researchers introduced novel engineering concepts that make their system possible in an upcoming paper in the journal Proceedings of the National Academy of Sciences.
Philipp Gutruf, an assistant professor of biomedical engineering and Craig M. Berge Faculty Fellow in the College of Engineering, directed the study in the Gutruf Lab. Co-lead authors are Tucker Stuart, a UArizona biomedical engineering doctoral alumnus, and Max Farley, an undergraduate student studying biomedical engineering.
Designed for ease, function and future
The COVID-19 pandemic, and the strain it placed on the global health care system, brought attention to the need for accurate, fast and robust remote patient monitoring, Gutruf said. Non-invasive wearable devices currently use the internet to connect clinicians to patient data for aggregation and investigation.

""These internet-based communication protocols are effective and well-developed, but they require cell coverage or internet connectivity and main-line power sources,"" said Gutruf, who is also a member of the UArizona BIO5 Institute. ""These requirements often leave individuals in remote or resource-constrained environments underserved.""
In contrast, the system the Gutruf Lab developed uses a low power wide area network, or LPWAN, that offers 2,400 times the distance of Wi-Fi and 533 times that of Bluetooth. The new system uses LoRa, a patented type of LPWAN technology.
""The choice of LoRa helped address previous limitations associated with power and electromagnetic constraints,"" Stuart said.
Alongside the implementation of this protocol, the lab developed circuitry and an antenna, which, in usual LoRa-enabled devices, is a large box that seamlessly integrates into the soft wearable. These electromagnetic, electronic and mechanical features enable it to send data to the receiver over a long distance. To make the device almost imperceptible to the wearer, the lab also enables recharge of its batteries over 2 meters of distance. The soft electronics, and the device's ability to harvest power, are the keys to the performance of this first-of-its-kind monitoring system, Gutruf said.
The Gutruf Lab calls the soft mesh wearable biosymbiotic, meaning it is custom 3D-printed to fit the user and is so unobtrusive it almost begins to feel like part of their body. The device, worn on the low forearm, stays in place even during exercise, ensuring high-quality data collection, Gutruf said. The user wears the device at all times, and it charges without removal or effort.
""Our device allows for continuous operation over weeks due to its wireless power transfer feature for interaction-free recharging -- all realized within a small package that even includes onboard computation of health metrics,"" Farley said.
Gutruf, Farley and Stuart plan to further improve and extend communication distances with the implementation of LoRa wireless area network gateways that could serve hundreds of square miles and hundreds of device users, using only a small number of connection points.
The wearable device and its communication system have the potential to aid remote monitoring in underserved rural communities, ensure high-fidelity recording in war zones, and monitor health in bustling cities, said Gutruf, whose long-term goal is to make the technology available to the communities with the most need.
""This effort is not just a scientific endeavor,"" he said. ""It's a step toward making digital medicine more accessible, irrespective of geographical and resource constraints.""

","score: 15.572763957987842, grade_level: '16'","score: 16.502795467108903, grade_levels: ['college_graduate'], ages: [24, 100]",10.1073/pnas.2307952120,"Remote patient monitoring is a critical component of digital medicine, and the COVID-19 pandemic has further highlighted its importance. Wearable sensors aimed at noninvasive extraction and transmission of high-fidelity physiological data provide an avenue toward at-home diagnostics and therapeutics; however, the infrastructure requirements for such devices limit their use to areas with well-established connectivity. This accentuates the socioeconomic and geopolitical gap in digital health technology and points toward a need to provide access in areas that have limited resources. Low-power wide area network (LPWAN) protocols, such as LoRa, may provide an avenue toward connectivity in these settings; however, there has been limited work on realizing wearable devices with this functionality because of power and electromagnetic constraints. In this work, we introduce wearables with electromagnetic, electronic, and mechanical features provided by a biosymbiotic platform to realize high-fidelity biosignals transmission of 15 miles without the need for satellite infrastructure. The platform implements wireless power transfer for interaction-free recharging, enabling long-term and uninterrupted use over weeks without the need for the user to interact with the devices. This work presents demonstration of a continuously wearable device with this long-range capability that has the potential to serve resource-constrained and remote areas, providing equitable access to digital health."
"
Inspired by a small and slow snail, scientists have developed a robot protype that may one day scoop up microplastics from the surfaces of oceans, seas and lakes.

The robot's design is based on the Hawaiian apple snail (Pomacea canaliculate), a common aquarium snail that uses the undulating motion of its foot to drive water surface flow and suck in floating food particles.
Currently, plastic collection devices mostly rely on drag nets or conveyor belts to gather and remove larger plastic debris from water, but they lack the fine scale required for retrieving microplastics. These tiny particles of plastic can be ingested and end up in the tissues of marine animals, thereby entering the food chain where they become a health issue and potentially carcinogenic to humans.
""We were inspired by how this snail collects food particles at the [water and air] interface to engineer a device that could possibly collect microplastics in the ocean or at a water body's surface, "" said Sunghwan ""Sunny"" Jung, professor in the department of biological and environmental engineering at Cornell University. Jung is senior author of a study, ""Optimal free-surface pumping by an undulating carpet,"" which published in Nature Communications. 
The prototype, modified from an existing design, would need to be scaled up to be practical in a real-world setting. The researchers used a 3D printer to make a flexible carpet-like sheet capable of undulating. A helical structure on the underside of the sheet rotates like a corkscrew to cause the carpet to undulate and create a travelling wave on the water.
Analyzing the motion of the fluid was key to this research. ""We needed to understand the fluid flow to characterize the pumping behavior,"" Jung said. The fluid-pumping system based on the snail's technique is open to the air. The researchers calculated that a similar closed system, where the pump is enclosed and uses a tube to suck in water and particles, would require high energy inputs to operate. On the other hand, the snail-like open system is far more efficient. For example, the prototype, though small, runs on only 5 volts of electricity while still effectively sucking in water, Jung said.
Due to the weight of a battery and motor, the researchers may need to attach a floatation device to the robot to keep it from sinking, Jung said.
Anupam Pandey, a former postdoctoral researcher in Jung's lab, currently an assistant professor of mechanical engineering at Syracuse University, is the paper's first author.
The study was funded by the National Science Foundation.

","score: 13.328242894056846, grade_level: '13'","score: 13.626304909560723, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41467-023-43059-8,"Examples of fluid flows driven by undulating boundaries are found in nature across many different length scales. Even though different driving mechanisms have evolved in distinct environments, they perform essentially the same function: directional transport of liquid. Nature-inspired strategies have been adopted in engineered devices to manipulate and direct flow. Here, we demonstrate how an undulating boundary generates large-scale pumping of a thin liquid near the liquid-air interface. Two dimensional traveling waves on the undulator, a canonical strategy to transport fluid at low Reynolds numbers, surprisingly lead to flow rates that depend non-monotonically on the wave speed. Through an asymptotic analysis of the thin-film equations that account for gravity and surface tension, we predict the observed optimal speed that maximizes pumping. Our findings reveal how proximity to free surfaces, which ensure lower energy dissipation, can be leveraged to achieve directional transport of liquids."
"
Graphene, that is extremely thin carbon, is considered a true miracle material. An international research team has now added another facet to its diverse properties with experiments at the Helmholtz-Zentrum Dresden-Rossendorf (HZDR): The experts, led by the University of Duisburg-Essen (UDE), fired short terahertz pulses at micrometer-sized discs of graphene, which briefly turned these minuscule objects into surprisingly strong magnets. This discovery may prove useful for developing future magnetic switches and storage devices.

Graphene consists of an ultra-thin sheet of just one layer of carbon atoms. But the material, which was only discovered as recently as 2004, displays remarkable properties. Among them is its ability to conduct electricity extremely well, and that is precisely what international researchers from Germany, Poland, India, and the USA took advantage of.
They applied thousands of tiny, micrometer-sized graphene discs onto a small chip using established semiconductor techniques. This chip was then exposed to a particular type of radiation situated between the microwave and infrared range: short terahertz pulses.
To achieve the best possible conditions, the working group, led by the UDE, used a particular light source for the experiment: The FELBE free-electron laser at the HZDR can generate extremely intense terahertz pulses. The astonishing result: ""The tiny graphene disks briefly turned into electromagnets,"" reports HZDR physicist Dr. Stephan Winnerl.
""We were able to generate magnetic fields in the range of 0.5 Tesla, which is roughly ten thousand times the Earth's magnetic field."" These were short magnetic pulses, only about ten picoseconds or one-hundredth of a billionth of a second long.
Radiation pulses stir electrons
The prerequisite for success: The researchers had to polarize the terahertz flashes in a specific way. Specialized optics changed the direction of oscillation of the radiation so that it moved, figuratively speaking, helically through space.

When these circularly polarized flashes hit the micrometer-sized graphene discs, the decisive effect occurred: Stimulated by the radiation, the free electrons in the discs began to circle -- just like water in a bucket stirred with a wooden spoon. And because, according to the basic laws of physics, a circulating current always generates a magnetic field, the graphene disks mutated into tiny electromagnets.
""The idea is actually quite simple,"" says Martin Mittendorff, professor at the University of Duisburg-Essen. ""In hindsight, we are surprised nobody had done it before."" Equally astonishing is the efficiency of the process: Compared to experiments irradiating nanoparticles of gold with light, the experiment at the HZDR was a million times more efficient -- an impressive increase. The new phenomenon could initially be used for scientific experiments in which material samples are exposed to short but strong magnetic pulses to investigate certain material properties in more detail.
The advantage: ""With our method, the magnetic field does not reverse polarity, as is the case with many other methods,"" explains Winnerl. ""It, therefore, remains unipolar."" In other words, during the ten picoseconds that the magnetic pulse from the graphene disks lasts, the north pole remains a north pole and the south pole a south pole -- a potential advantage for certain series of experiments.
The dream of magnetic electronics
In the long run, those minuscule magnets might even be useful for certain future technologies: As ultra-short radiation flashes generate them, the graphene discs could carry out extremely fast and precise magnetic switching operations. This would be interesting for magnetic storage technology, for example, but also for so-called spintronics -- a form of magnetic electronics.
Here, instead of electrical charges flowing in a processor, weak magnetic fields in the form of electron spins are passed on like tiny batons. This may, so it is hoped, significantly speed up the switching processes once again. Graphene disks could conceivably be used as switchable electromagnets to control future spintronic chips.
However, experts would have to invent very small, highly miniaturized terahertz sources for this purpose -- certainly still a long way to go. ""You cannot use a full-blown free-electron laser for this, like the one we used in our experiment,"" comments Stephan Winnerl. ""Nevertheless, radiation sources fitting on a laboratory table should be sufficient for future scientific experiments."" Such significantly more compact terahertz sources can already be found in some research facilities.

","score: 14.371370396600572, grade_level: '14'","score: 15.259408640226631, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41467-023-43412-x,"Strong circularly polarized excitation opens up the possibility to generate and control effective magnetic fields in solid state systems, e.g., via the optical inverse Faraday effect or the phonon inverse Faraday effect. While these effects rely on material properties that can be tailored only to a limited degree, plasmonic resonances can be fully controlled by choosing proper dimensions and carrier concentrations. Plasmon resonances provide new degrees of freedom that can be used to tune or enhance the light-induced magnetic field in engineered metamaterials. Here we employ graphene disks to demonstrate light-induced transient magnetic fields from a plasmonic circular current with extremely high efficiency. The effective magnetic field at the plasmon resonance frequency of the graphene disks (3.5 THz) is evidenced by a strong ( ~ 1°) ultrafast Faraday rotation ( ~ 20 ps). In accordance with reference measurements and simulations, we estimated the strength of the induced magnetic field to be on the order of 0.7 T under a moderate pump fluence of about 440 nJ cm−2."
"
In a new study, researchers at the University of Colorado Boulder have used doughnut-shaped beams of light to take detailed images of objects too tiny to view with traditional microscopes.

The new technique could help scientists improve the inner workings of a range of ""nanoelectronics,"" including the miniature semiconductors in computer chips. The discovery was highlighted Dec. 1 in a special issue of ""Optics & Photonics News"" called ""Optics in 2023.""
The research is the latest advance in the field of ptychography, a difficult to pronounce (the ""p"" is silent) but powerful technique for viewing very small things. Unlike traditional microscopes, ptychography tools don't directly view small objects. Instead, they shine lasers at a target, then measure how the light scatters away -- a bit like the microscopic equivalent of making shadow puppets on a wall.
So far, the approach has worked remarkably well, with one major exception, said study senior author and Distinguished Professor of physics Margaret Murnane.
""Until recently, it has completely failed for highly periodic samples, or objects with a regularly repeating pattern,"" said Murnane, fellow at JILA, a joint research institute of CU Boulder and the National Institute of Standards and Technology (NIST). ""It's a problem because that includes a lot of nanoelectronics.""
She noted that many important technologies like some semiconductors are made up of atoms like silicon or carbon joined together in regular patterns like a grid or mesh. To date, those structures have proved tricky for scientists to view up close using ptychography.
In the new study, however, Murnane and her colleagues came up with a solution. Instead of using traditional lasers in their microscopes, they produced beams of extreme ultraviolet light in the shape of doughnuts.

The team's novel approach can collect accurate images of tiny and delicate structures that are roughly 10 to 100 nanometers in size, or many times smaller than a millionth of an inch. In the future, the researchers expect to zoom in to view even smaller structures. The doughnut, or optical angular momentum, beams also won't harm tiny electronics in the process -- as some existing imaging tools, like electron microscopes, sometimes can.
""In the future, this method could be used to inspect the polymers used to make and print semiconductors for defects, without damaging those structures in the process,"" Murnane said.
Bin Wang and Nathan Brooks, who earned their doctoral degrees from JILA in 2023, were first authors of the new study.
Pushing the limits of microscopes
The research, Murnane said, pushes the fundamental limits of microscopes: Because of the physics of light, imaging tools using lenses can only see the world down to a resolution of about 200 nanometers -- which isn't accurate enough to capture many of the viruses, for example, that infect humans. Scientists can freeze and kill viruses to view them with powerful cryo-electron microscopes but can't yet capture these pathogens in action and in real time.
Ptychography, which was pioneered in the mid-2000s, could help researchers push past that limit.

To understand how, go back to those shadow puppets. Imagine that scientists want to collect a ptychographic image of a very small structure, perhaps letters spelling out ""CU."" To do that, they first zap a laser beam at the letters, scanning them multiple times. When the light hits the ""C"" and the ""U"" (in this case, the puppets), the beam will break apart and scatter, producing a complex pattern (the shadows). Employing sensitive detectors, scientists record those patterns, then analyze them with a series of mathematical equations. With enough time, Murnane explained, they recreate the shape of their puppets entirely from the shadows they cast.
""Instead of using a lens to retrieve the image, we use algorithms,"" Murnane said.
She and her colleagues have previously used such an approach to view submicroscopic shapes like letters or stars.
But the approach won't work with repeating structures like those silicon or carbon grids. If you shine a regular laser beam on a semiconductor with such regularity, for example, it will often produce a scatter pattern that is incredibly uniform -- ptychographic algorithms struggle to make sense of patterns that don't have much variation in them.
The problem has left physicists scratching their heads for close to a decade.
Doughnut microscopy
In the new study, however, Murnane and her colleagues decided to try something different. They didn't make their shadow puppets using regular lasers. Instead, they generated beams of extreme ultraviolet light, then employed a device called a spiral phase plate to twist those beams into the shape of a corkscrew, or vortex. (When such a vortex of light shines on a flat surface, it makes a shape like a doughnut).
The doughnut beams didn't have pink glaze or sprinkles, but they did the trick. The team discovered that when these types of beams bounced off repeating structures, they created much more complex shadow puppets than regular lasers.
To test out the new approach, the researchers created a mesh of carbon atoms with a tiny snap in one of the links. The group was able to spot that defect with precision not seen in other ptychographic tools.
""If you tried to image the same thing in a scanning electron microscope, you would damage it even further,"" Murnane said.
Moving forward, her team wants to make their doughnut strategy even more accurate, allowing them to view smaller and even more fragile objects -- including, one day, the workings of living, biological cells.
Other co-authors of the new study include Henry Kapteyn, professor of physics and fellow of JILA, and current and former JILA graduate students Peter Johnsen, Nicholas Jenkins, Yuka Esashi, Iona Binnie and Michael Tanksalvala.

","score: 11.982679263565892, grade_level: '12'","score: 13.140509447674418, grade_levels: ['college_graduate'], ages: [24, 100]",10.1364/OPTICA.498619,"Ptychographic coherent diffractive imaging enables diffraction-limited imaging of nanoscale structures at extreme ultraviolet and x-ray wavelengths, where high-quality image-forming optics are not available. However, its reliance on a set of diverse diffraction patterns makes it challenging to use ptychography to image highly periodic samples, limiting its application to defect inspection for electronic and photonic devices. Here, we use a vortex high harmonic light beam driven by a laser carrying orbital angular momentum to implement extreme ultraviolet ptychographic imaging of highly periodic samples with high fidelity and reliability. We also demonstrate, for the first time to our knowledge, ptychographic imaging of an isolated, near-diffraction-limited defect in an otherwise periodic sample using vortex high harmonic beams. This enhanced metrology technique can enable high-fidelity imaging and inspection of highly periodic structures for next-generation nano, energy, photonic, and quantum devices."
"
Hung from a common utility pole, a fiber optic cable -- the kind bringing high-speed internet to more and more American households -- can be turned into a sensor to detect temperature changes, vibrations, and even sound, through an emerging technology called distributed fiber optic sensing.

However, as NEC Labs America photonics researcher Sarper Ozharar, Ph.D., explains, acoustic sensing in fiber optic cables ""is limited to only nearby sound sources or very loud events, such as emergency vehicles, car alarms, or cicada emergences.""
Cicadas? Indeed, periodical cicadas -- the insects known for emerging by the billions on 13- or 17-year cycles and making a collective racket with their buzzy mating calls -- are loud enough to be detected through fiber optic acoustic sensing. And a new proof-of-concept study shows how the technology could open new pathways for charting the populations of these famously ephemeral bugs.
""I was surprised and excited to learn how much information about the calls was gathered, despite it being located near a busy section of Middlesex County in New Jersey,"" says entomologist Jessica Ware, Ph.D., associate curator and chair of the Division of Invertebrate Zoology at the American Museum of Natural History and co-author on the study, published in the Entomological Society of America's Journal of Insect Science.
As the researchers explain in their report, distributed fiber optic sensing is based on detecting and analyzing ""backscatter"" in a cable. When an optical pulse is sent through a fiber cable, tiny imperfections or disturbances in the cable cause a small fraction of the signal to bounce back to the source. Timing the arrival of the backscattered light can be used to calculate the exact point along the cable from which it bounced back. And, monitoring how the backscatter varies over time creates a signature of the disturbance -- which, in the case of acoustic sensing, can indicate volume and frequency of the sound.
A single sensor can be deployed on a huge segment of cable, too; the researchers offer an example of a 50-kilometer cable with a sensor that can detect the location of disturbances at a scale as precise as 1 meter. ""This is identical to installing 50,000 [acoustic] sensors in the monitored region that are inherently synchronized and do not require onsite power supply,"" they write.
In 2021, Brood X, the largest of several populations of cicadas that emerge on 17-year cycles, came out of the ground in at least 15 states and the District of Columbia in the Midwest and mid-Atlantic regions of the U.S., including New Jersey, where Ozharar works at NEC Laboratories America, Inc. There, Ozharar and colleagues used NEC's fiber-sensing test apparatus -- cable strung on three 35-foot utility poles on the grounds of NEC's lab in Princeton -- to see if they could detect and analyze the sound of Brood X cicadas buzzing in trees nearby between June 9 and June 24 that year.

Sure enough, the cicadas' buzzing was evident. It showed up as a strong signal at 1.33 kilohertz (kHz) via the fiber optic sensing, which matched the frequency of the cicadas' call measured with a traditional audio sensor placed in same location. The researchers also observed the cicadas' peak frequency varying between 1.2 kHz and 1.5 kHz, a pattern that appeared to follow changes in temperature at the test site. The overall intensity of the cicadas' buzzing was also observed through the fiber optic sensing, and the signal decreased over the course of the test period, as the cicadas' chorus peaked and then faded as they reached the end of their reproductive period.
""We think it is really exciting and interesting that this new technology, designed and optimized for other applications and seemingly unrelated to entomology, can support entomological studies,"" Ozharar says. Indeed, fiber optic sensors are multifunctional, meaning they can be installed and used for any number of purposes, detecting cicadas one day and some other disturbance the next.
Ware says fiber optic sensing could soon play a role in detecting a variety of insects. ""Periodical cicadas were a noisy cohort that was picked up by these systems, but it will be interesting to see if annual measurements of insect soundscapes and vibrations could be useful in monitoring insect abundance in an area across seasons and years,"" she says.
As for periodical cicadas, more than a dozen broods are known to emerge in different years and different areas of the eastern United States. The growing network of fiber optic infrastructure in the country -- with fiber internet available to more than 40 percent of U.S. households as of 2022, according to the Fiber Broadband Association -- could be incorporated into entomologists' efforts to observe and measure these emergences over time.
""Thanks to the booming development of broadband access and telecommunications, fiber cables are ubiquitously available across communities, weaving a vast network that not only provides high-speed internet but also serves as a foundation for the next generation of sensing technologies,"" Ozharar says.
Brood X cicadas will remain underground until 2038. Their brief appearances and massive numbers make them a challenge to study, but the long gap between their arrivals allows entomologists to make significant technological leaps in the interim. In 2021, Brood X was observed in unprecedented volume through a crowdsourced mobile smartphone app -- a method barely conceivable when Brood X had last emerged in 2004. By 2038, fiber optic sensing could well be the next avenue leading to a similar advance.

","score: 16.815023561971724, grade_level: '17'","score: 17.873514847782182, grade_levels: ['college_graduate'], ages: [24, 100]",10.1093/jisesa/iead090,"Brood X is the largest of the 15 broods of periodical cicadas, and individuals from this brood emerged across the Eastern United States in spring 2021. Using distributed acoustic sensing (DAS) technology, the activity of Brood X cicadas was monitored in their natural environment in Princeton, NJ. Critical information regarding their acoustic signatures and activity level is collected and analyzed using standard outdoor-grade telecommunication fiber cables. We believe these results have the potential to be a quantitative baseline for regional Brood X activity and pave the way for more detailed monitoring of insect populations to combat global insect decline. We also show that it is possible to transform readily available fiber optic networks into environmental sensors with no additional installation costs. To our knowledge, this is the first reported use case of a distributed fiber optic sensing system for entomological sciences and environmental studies."
"
Robotics researchers have already made great strides in developing sensors that can perceive changes in position, pressure, and temperature -- all of which are important for technologies like wearable devices and human-robot interfaces. But a hallmark of human perception is the ability to sense multiple stimuli at once, and this is something that robotics has struggled to achieve.

Now, Jamie Paik and colleagues in the Reconfigurable Robotics Lab (RRL) in EPFL's School of Engineering have developed a sensor that can perceive combinations of bending, stretching, compression, and temperature changes, all using a robust system that boils down to a simple concept: color.
Dubbed ChromoSense, the RRL's technology relies on a translucent rubber cylinder containing three sections dyed red, green, and blue. An LED at the top of the device sends light through its core, and changes in the light's path through the colors as the device is bent or stretched are picked up by a miniaturized spectral meter at the bottom.
""Imagine you are drinking three different flavors of slushie through three different straws at once: the proportion of each flavor you get changes if you bend or twist the straws. This is the same principle that ChromoSense uses: it perceives changes in light traveling through the colored sections as the geometry of those sections deforms,"" says Paik.
A thermosensitive section of the device also allows it to detect temperature changes, using a special dye -- similar to that in color-changing t-shirts or mood rings -- that desaturates in color when it is heated. The research has been published in Nature Communications and selected for the Editor's Highlights page.
A more streamlined approach to wearables
Paik explains that while robotic technologies that rely on cameras or multiple sensing elements are effective, they can make wearable devices heavier and more cumbersome, in addition to requiring more data processing.

""For soft robots to serve us better in our daily lives, they need to be able to sense what we are doing,"" she says. ""Traditionally, the fastest and most inexpensive way to do this has been through vision-based systems, which capture all of our activities and then extract the necessary data. ChromoSense allows for more targeted, information-dense readings, and the sensor can be easily embedded into different materials for different tasks.""
Thanks to its simple mechanical structure and use of color over cameras, ChromoSense could potentially lend itself to inexpensive mass production. In addition to assistive technologies, such as mobility-aiding exosuits, Paik sees everyday applications for ChromoSense in athletic gear or clothing, which could be used to give users feedback about their form and movements.
A strength of ChromoSense -- its ability to sense multiple stimuli at once -- can also be a weakness, as decoupling simultaneously applied stimuli is still a challenge the researchers are working on. At the moment, Paik says they are focusing on improving the technology to sense locally applied forces, or the exact boundaries of a material when it changes shape.
""If ChromoSense gains popularity and many people want to use it as a general-purpose robotic sensing solution, then I think further increasing the information density of the sensor could become a really interesting challenge,"" she says.
Looking ahead, Paik also plans to experiment with different formats for ChromoSense, which has been prototyped as a cylindrical shape and as part of a wearable soft exosuit, but could also be imagined in a flat form more suitable for the RRL's signature origami robots.
""With our technology, anything can become a sensor as long as light can pass through it,"" she summarizes.

","score: 15.795211055276383, grade_level: '16'","score: 17.802386934673372, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41467-023-42655-y,"Owing to the remarkable properties of the somatosensory system, human skin compactly perceives myriad forms of physical stimuli with high precision. Machines, conversely, are often equipped with sensory suites constituted of dozens of unique sensors, each made for detecting limited stimuli. Emerging high degree-of-freedom human-robot interfaces and soft robot applications are delimited by the lack of simple, cohesive, and information-dense sensing technologies. Stepping toward biological levels of proprioception, we present a sensing technology capable of decoding omnidirectional bending, compression, stretch, binary changes in temperature, and combinations thereof. This multi-modal deformation and temperature sensor harnesses chromaticity and intensity of light as it travels through patterned elastomer doped with functional dyes. Deformations and temperature shifts augment the light chromaticity and intensity, resulting in a one-to-one mapping between stimulus modes that are sequentially combined and the sensor output. We study the working principle of the sensor via a comprehensive opto-thermo-mechanical assay, and find that the information density provided by a single sensing element permits deciphering rich and diverse human-robot and robot-environmental interactions."
"
Researchers have developed a new experiment to better understand what people view as moral and immoral decisions related to driving vehicles, with the goal of collecting data to train autonomous vehicles how to make ""good"" decisions. The work is designed to capture a more realistic array of moral challenges in traffic than the widely discussed life-and-death scenario inspired by the so-called ""trolley problem.""

""The trolley problem presents a situation in which someone has to decide whether to intentionally kill one person (which violates a moral norm) in order to avoid the death of multiple people,"" says Dario Cecchini, first author of a paper on the work and a postdoctoral researcher at North Carolina State University.
""In recent years, the trolley problem has been utilized as a paradigm for studying moral judgment in traffic,"" Cecchini says. ""The typical situation comprises a binary choice for a self-driving car between swerving left, hitting a lethal obstacle, or proceeding forward, hitting a pedestrian crossing the street. However, these trolley-like cases are unrealistic. Drivers have to make many more realistic moral decisions every day. Should I drive over the speed limit? Should I run a red light? Should I pull over for an ambulance?""
""Those mundane decisions are important because they can ultimately lead to life-or-death situations,"" says Veljko Dubljevic, corresponding author of the paper and an associate professor in the Science, Technology & Society program at NC State.
""For example, if someone is driving 20 miles over the speed limit and runs a red light, then they may find themselves in a situation where they have to either swerve into traffic or get into a collision. There's currently very little data in the literature on how we make moral judgments about the decisions drivers make in everyday situations.""
To address that lack of data, the researchers developed a series of experiments designed to collect data on how humans make moral judgments about decisions that people make in low-stakes traffic situations. The researchers created seven different driving scenarios, such as a parent who has to decide whether to violate a traffic signal while trying to get their child to school on time. Each scenario is programmed into a virtual reality environment, so that study participants engaged in the experiment have audiovisual information about what drivers are doing when they make decisions, rather than simply reading about the scenario.
For this work, the researchers built on something called the Agent Deed Consequence (ADC) model, which posits that people take three things into account when making a moral judgment: the agent, which is the character or intent of the person who is doing something; the deed, or what is being done; and the consequence, or the outcome that resulted from the deed.

Researchers created eight different versions of each traffic scenario, varying the combinations of agent, deed and consequence. For example, in one version of the scenario where a parent is trying to get the child to school, the parent is caring, brakes at a yellow light, and gets the child to school on time. In a second version, the parent is abusive, runs a red light, and causes an accident. The other six versions alter the nature of the parent (the agent), their decision at the traffic signal (the deed), and/or the outcome of their decision (the consequence).
""The goal here is to have study participants view one version of each scenario and determine how moral the behavior of the driver was in each scenario, on a scale from one to 10,"" Cecchini says. ""This will give us robust data on what we consider moral behavior in the context of driving a vehicle, which can then be used to develop AI algorithms for moral decision making in autonomous vehicles.""
The researchers have done pilot testing to fine-tune the scenarios and ensure that they reflect believable and easily understood situations.
""The next step is to engage in large-scale data collection, getting thousands of people to participate in the experiments,"" says Dubljevic. ""We can then use that data to develop more interactive experiments with the goal of further fine-tuning our understanding of moral decision making. All of this can then be used to create algorithms for use in autonomous vehicles. We'll then need to engage in additional testing to see how those algorithms perform.""

","score: 14.037548679390788, grade_level: '14'","score: 14.81821573163679, grade_levels: ['college_graduate'], ages: [24, 100]",10.1007/s00146-023-01813-y,"The imminent deployment of autonomous vehicles requires algorithms capable of making moral decisions in relevant traffic situations. Some scholars in the ethics of autonomous vehicles hope to align such intelligent systems with human moral judgment. For this purpose, studies like the Moral Machine Experiment have collected data about human decision-making in trolley-like traffic dilemmas. This paper first argues that the trolley dilemma is an inadequate experimental paradigm for investigating traffic moral judgments because it does not include agents’ character-based considerations and is incapable of facilitating the investigation of low-stakes mundane traffic scenarios. In light of the limitations of the trolley paradigm, this paper presents an alternative experimental framework that addresses these issues. The proposed solution combines the creation of mundane traffic moral scenarios using virtual reality and the Agent-Deed-Consequences (ADC) model of moral judgment as a moral-psychological framework. This paradigm shift potentially increases the ecological validity of future studies by providing more realism and incorporating character considerations into traffic actions."
"
A researcher has just finished writing a scientific paper. She knows her work could benefit from another perspective. Did she overlook something? Or perhaps there's an application of her research she hadn't thought of. A second set of eyes would be great, but even the friendliest of collaborators might not be able to spare the time to read all the required background publications to catch up.

Kevin Yager -- leader of the electronic nanomaterials group at the Center for Functional Nanomaterials (CFN), a U.S. Department of Energy (DOE) Office of Science User Facility at DOE's Brookhaven National Laboratory -- has imagined how recent advances in artificial intelligence (AI) and machine learning (ML) could aid scientific brainstorming and ideation. To accomplish this, he has developed a chatbot with knowledge in the kinds of science he's been engaged in.
Rapid advances in AI and ML have given way to programs that can generate creative text and useful software code. These general-purpose chatbots have recently captured the public imagination. Existing chatbots -- based on large, diverse language models -- lack detailed knowledge of scientific sub-domains. By leveraging a document-retrieval method, Yager's bot is knowledgeable in areas of nanomaterial science that other bots are not. The details of this project and how other scientists can leverage this AI colleague for their own work have recently been published in Digital Discovery.
Rise of the Robots
""CFN has been looking into new ways to leverage AI/ML to accelerate nanomaterial discovery for a long time. Currently, it's helping us quickly identify, catalog, and choose samples, automate experiments, control equipment, and discover new materials. Esther Tsai, a scientist in the electronic nanomaterials group at CFN, is developing an AI companion to help speed up materials research experiments at the National Synchrotron Light Source II (NSLS-II)."" NSLS-II is another DOE Office of Science User Facility at Brookhaven Lab.
At CFN, there has been a lot of work on AI/ML that can help drive experiments through the use of automation, controls, robotics, and analysis, but having a program that was adept with scientific text was something that researchers hadn't explored as deeply. Being able to quickly document, understand, and convey information about an experiment can help in a number of ways -- from breaking down language barriers to saving time by summarizing larger pieces of work.
Watching Your Language
To build a specialized chatbot, the program required domain-specific text -- language taken from areas the bot is intended to focus on. In this case, the text is scientific publications. Domain-specific text helps the AI model understand new terminology and definitions and introduces it to frontier scientific concepts. Most importantly, this curated set of documents enables the AI model to ground its reasoning using trusted facts.

To emulate natural human language, AI models are trained on existing text, enabling them to learn the structure of language, memorize various facts, and develop a primitive sort of reasoning. Rather than laboriously retrain the AI model on nanoscience text, Yager gave it the ability to look up relevant information in a curated set of publications. Providing it with a library of relevant data was only half of the battle. To use this text accurately and effectively, the bot would need a way to decipher the correct context.
""A challenge that's common with language models is that sometimes they 'hallucinate' plausible sounding but untrue things,"" explained Yager. ""This has been a core issue to resolve for a chatbot used in research as opposed to one doing something like writing poetry. We don't want it to fabricate facts or citations. This needed to be addressed. The solution for this was something we call 'embedding,' a way of categorizing and linking information quickly behind the scenes.""
Embedding is a process that transforms words and phrases into numerical values. The resulting ""embedding vector"" quantifies the meaning of the text. When a user asks the chatbot a question, it's also sent to the ML embedding model to calculate its vector value. This vector is used to search through a pre-computed database of text chunks from scientific papers that were similarly embedded. The bot then uses text snippets it finds that are semantically related to the question to get a more complete understanding of the context.
The user's query and the text snippets are combined into a ""prompt"" that is sent to a large language model, an expansive program that creates text modeled on natural human language, that generates the final response. The embedding ensures that the text being pulled is relevant in the context of the user's question. By providing text chunks from the body of trusted documents, the chatbot generates answers that are factual and sourced.
""The program needs to be like a reference librarian,"" said Yager. ""It needs to heavily rely on the documents to provide sourced answers. It needs to be able to accurately interpret what people are asking and be able to effectively piece together the context of those questions to retrieve the most relevant information. While the responses may not be perfect yet, it's already able to answer challenging questions and trigger some interesting thoughts while planning new projects and research.""
Bots Empowering Humans
CFN is developing AI/ML systems as tools that can liberate human researchers to work on more challenging and interesting problems and to get more out of their limited time while computers automate repetitive tasks in the background. There are still many unknowns about this new way of working, but these questions are the start of important discussions scientists are having right now to ensure AI/ML use is safe and ethical.
""There are a number of tasks that a domain-specific chatbot like this could clear from a scientist's workload. Classifying and organizing documents, summarizing publications, pointing out relevant info, and getting up to speed in a new topical area are just a few potential applications,"" remarked Yager. ""I'm excited to see where all of this will go, though. We never could have imagined where we are now three years ago, and I'm looking forward to where we'll be three years from now.""

","score: 12.343352769679303, grade_level: '12'","score: 12.858017492711369, grade_levels: ['college'], ages: [18, 24]",10.1039/D3DD00112A,We demonstrate how large language models (LLMs) can be adapted to domain-specific science topics by connecting them to a corpus of trusted documents.
"
Researchers at the University of Sydney Nano Institute have invented a compact silicon semiconductor chip that integrates electronics with photonic, or light, components. The new technology significantly expands radio-frequency (RF) bandwidth and the ability to accurately control information flowing through the unit.

Expanded bandwidth means more information can flow through the chip and the inclusion of photonics allows for advanced filter controls, creating a versatile new semiconductor device.
Researchers expect the chip will have application in advanced radar, satellite systems, wireless networks and the roll-out of 6G and 7G telecommunications and also open the door to advanced sovereign manufacturing. It could also assist in the creation of high-tech value-add factories at places like Western Sydney's Aerotropolis precinct.
The chip is built using an emerging technology in silicon photonics that allows integration of diverse systems on semiconductors less than 5 millimetres wide. Pro-Vice-Chancellor (Research) Professor Ben Eggleton, who guides the research team, likened it to fitting together Lego building blocks, where new materials are integrated through advanced packaging of components, using electronic 'chiplets'.
The research for this invention has been published in Nature Communications.
Dr Alvaro Casas Bedoya, Associate Director for Photonic Integration in the School of Physics, who led the chip design, said the unique method of heterogenous materials integration has been 10 years in the making.
""The combined use of overseas semiconductor foundries to make the basic chip wafer with local research infrastructure and manufacturing has been vital in developing this photonic integrated circuit,"" he said.

""This architecture means Australia could develop its own sovereign chip manufacturing without exclusively relying on international foundries for the value-add process.""
Professor Eggleton highlighted the fact that most of the items on the Federal Government's List of Critical Technologies in the National Interest depend upon semiconductors.
He said the invention means the work at Sydney Nano fits well with initiatives like the Semiconductor Sector Service Bureau (S3B), sponsored by the NSW Government, which aims to develop the local semiconductor ecosystem.
Dr Nadia Court, Director of S3B, said, ""This work aligns with our mission to drive advancements in semiconductor technology, holding great promise for the future of semiconductor innovation in Australia. The result reinforces local strength in research and design at a pivotal time of increased global focus and investment in the sector.""
Designed in collaboration with scientists at the Australian National University, the integrated circuit was built at the Core Research Facility cleanroom at the University of Sydney Nanoscience Hub, a purpose-built $150 million building with advanced lithography and deposition facilities.
The photonic circuit in the chip means a device with an impressive 15 gigahertz bandwidth of tunable frequencies with spectral resolution down to just 37 megahertz, which is less than a quarter of one percent of the total bandwidth.

Professor Eggleton said: ""Led by our impressive PhD student Matthew Garrett, this invention is a significant advance for microwave photonics and integrated photonics research.
""Microwave photonic filters play a crucial role in modern communication and radar applications, offering the flexibility to precisely filter different frequencies, reducing electromagnetic interference and enhancing signal quality.
""Our innovative approach of integrating advanced functionalities into semiconductor chips, particularly the heterogenous integration of chalcogenide glass with silicon, holds the potential to reshape the local semiconductor landscape.""
Co-author and Senior Research Fellow Dr Moritz Merklein said: ""This work paves the way for a new generation of compact, high-resolution RF photonic filters with wideband frequency tunability, particularly beneficial in air and spaceborne RF communication payloads, opening possibilities for enhanced communications and sensing capabilities.""

","score: 19.13601250601251, grade_level: '19'","score: 20.560533910533913, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41467-023-43404-x,"Microwave photonics (MWP) has unlocked a new paradigm for Radio Frequency (RF) signal processing by harnessing the inherent broadband and tunable nature of photonic components. Despite numerous efforts made to implement integrated MWP filters, a key RF processing functionality, it remains a long-standing challenge to achieve a fully integrated photonic circuit that can merge the megahertz-level spectral resolution required for RF applications with key electro-optic components. Here, we overcome this challenge by introducing a compact 5 mm × 5 mm chip-scale MWP filter with active E-O components, demonstrating 37 MHz spectral resolution. We achieved this device by heterogeneously integrating chalcogenide waveguides, which provide Brillouin gain, in a complementary metal-oxide-semiconductor (CMOS) foundry-manufactured silicon photonic chip containing integrated modulators and photodetectors. This work paves the way towards a new generation of compact, high-resolution RF photonic filters with wideband frequency tunability demanded by future applications, such as air and spaceborne RF communication payloads."
"
Researchers at Tufts University and Harvard University's Wyss Institute have created tiny biological robots that they call Anthrobots from human tracheal cells that can move across a surface and have been found to encourage the growth of neurons across a region of damage in a lab dish.

The multicellular robots, ranging in size from the width of a human hair to the point of a sharpened pencil, were made to self-assemble and shown to have a remarkable healing effect on other cells. The discovery is a starting point for the researchers' vision to use patient-derived biobots as new therapeutic tools for regeneration, healing, and treatment of disease.
The work follows from earlier research in the laboratories of Michael Levin, Vannevar Bush Professor of Biology at Tufts University School of Arts & Sciences, and Josh Bongard at the University of Vermont in which they created multicellular biological robots from frog embryo cells called Xenobots, capable of navigating passageways, collecting material, recording information, healing themselves from injury, and even replicating for a few cycles on their own. At the time, researchers did not know if these capabilities were dependent on their being derived from an amphibian embryo, or if biobots could be constructed from cells of other species.
In the current study, published in Advanced Science, Levin, along with PhD student Gizem Gumuskaya discovered that bots can in fact be created from adult human cells without any genetic modification and they are demonstrating some capabilities beyond what was observed with the Xenobots. The discovery starts to answer a broader question that the lab has posed -- what are the rules that govern how cells assemble and work together in the body, and can the cells be taken out of their natural context and recombined into different ""body plans"" to carry out other functions by design?
In this case, researchers gave human cells, after decades of quiet life in the trachea, a chance to reboot and find ways of creating new structures and tasks. ""We wanted to probe what cells can do besides create default features in the body,"" said Gumuskaya, who earned a degree in architecture before coming into biology. ""By reprogramming interactions between cells, new multicellular structures can be created, analogous to the way stone and brick can be arranged into different structural elements like walls, archways or columns."" The researchers found that not only could the cells create new multicellular shapes, but they could move in different ways over a surface of human neurons grown in a lab dish and encourage new growth to fill in gaps caused by scratching the layer of cells.
Exactly how the Anthrobots encourage growth of neurons is not yet clear, but the researchers confirmed that neurons grew under the area covered by a clustered assembly of Anthrobots, which they called a ""superbot.""
""The cellular assemblies we construct in the lab can have capabilities that go beyond what they do in the body,"" said Levin, who also serves as the director of the Allen Discovery Center at Tufts and is an associate faculty member of the Wyss Institute. ""It is fascinating and completely unexpected that normal patient tracheal cells, without modifying their DNA, can move on their own and encourage neuron growth across a region of damage,"" said Levin. ""We're now looking at how the healing mechanism works, and asking what else these constructs can do.""
The advantages of using human cells include the ability to construct bots from a patient's own cells to perform therapeutic work without the risk of triggering an immune response or requiring immunosuppressants. They only last a few weeks before breaking down, and so can easily be re-absorbed into the body after their work is done.

In addition, outside of the body, Anthrobots can only survive in very specific laboratory conditions, and there is no risk of exposure or unintended spread outside the lab. Likewise, they do not reproduce, and they have no genetic edits, additions or deletions, so there is no risk of their evolving beyond existing safeguards.
How Are Anthrobots Made?
Each Anthrobot starts out as a single cell, derived from an adult donor. The cells come from the surface of the trachea and are covered with hairlike projections called cilia that wave back and forth. The cilia help the tracheal cells push out tiny particles that find their way into air passages of the lung. We all experience the work of ciliated cells when we take the final step of expelling the particles and excess fluid by coughing or clearing our throats. Earlier studies by others had shown that when the cells are grown in the lab, they spontaneously form tiny multicellular spheres called organoids.
The researchers developed growth conditions that encouraged the cilia to face outward on organoids. Within a few days they started moving around, driven by the cilia acting like oars. They noted different shapes and types of movement -- the first. important feature observed of the biorobotics platform. Levin says that if other features could be added to the Anthrobots (for example, contributed by different cells), they could be designed to respond to their environment, and travel to and perform functions in the body, or help build engineered tissues in the lab.
The team, with the help of Simon Garnier at the New Jersey Institute of Technology, characterized the different types of Anthrobots that were produced. They observed that bots fell into a few discrete categories of shape and movement, ranging in size from 30 to 500 micrometers (from the thickness of a human hair to the point of a sharpened pencil), filling an important niche between nanotechnology and larger engineered devices.
Some were spherical and fully covered in cilia, and some were irregular or football shaped with more patchy coverage of cilia, or just covered with cilia on one side. They traveled in straight lines, moved in tight circles, combined those movements, or just sat around and wiggled. The spherical ones fully covered with cilia tended to be wigglers. The Anthrobots with cilia distributed unevenly tended to move forward for longer stretches in straight or curved paths. They usually survived about 45-60 days in laboratory conditions before they naturally biodegraded.

""Anthrobots self-assemble in the lab dish,"" said Gumuskaya, who created the Anthrobots. ""Unlike Xenobots, they don't require tweezers or scalpels to give them shape, and we can use adult cells -- even cells from elderly patients -- instead of embryonic cells. It's fully scalable -- we can produce swarms of these bots in parallel, which is a good start for developing a therapeutic tool.""
Little Healers
Because Levin and Gumuskaya ultimately plan to make Anthrobots with therapeutic applications, they created a lab test to see how the bots might heal wounds. The model involved growing a two-dimensional layer of human neurons, and simply by scratching the layer with a thin metal rod, they created an open 'wound' devoid of cells.
To ensure the gap would be exposed to a dense concentration of Anthrobots, they created ""superbots"" a cluster that naturally forms when the Anthrobots are confined to a small space. The superbots were made up primarily of circlers and wigglers, so they would not wander too far away from the open wound.
Although it might be expected that genetic modifications of Anthrobot cells would be needed to help the bots encourage neural growth, surprisingly the unmodified Anthrobots triggered substantial regrowth, creating a bridge of neurons as thick as the rest of the healthy cells on the plate. Neurons did not grow in the wound where Anthrobots were absent. At least in the simplified 2D world of the lab dish, the Anthrobot assemblies encouraged efficient healing of live neural tissue.
According to the researchers, further development of the bots could lead to other applications, including clearing plaque buildup in the arteries of atherosclerosis patients, repairing spinal cord or retinal nerve damage, recognizing bacteria or cancer cells, or delivering drugs to targeted tissues. The Anthrobots could in theory assist in healing tissues, while also laying down pro-regenerative drugs.
Making New Blueprints, Restoring Old Ones
Gumuskaya explained that cells have the innate ability to self-assemble into larger structures in certain fundamental ways. ""The cells can form layers, fold, make spheres, sort and separate themselves by type, fuse together, or even move,"" Gumuskaya said. ""Two important differences from inanimate bricks are that cells can communicate with each other and create these structures dynamically, and each cell is programmed with many functions, like movement, secretion of molecules, detection of signals and more. We are just figuring out how to combine these elements to create new biological body plans and functions -- different than those found in nature.""
Taking advantage of the inherently flexible rules of cellular assembly helps the scientists construct the bots, but it can also help them understand how natural body plans assemble, how the genome and environment work together to create tissues, organs, and limbs, and how to restore them with regenerative treatments.

","score: 14.68591020728952, grade_level: '15'","score: 16.317213626092936, grade_levels: ['college_graduate'], ages: [24, 100]",10.1002/advs.202303575,"Fundamental knowledge gaps exist about the plasticity of cells from adult soma and the potential diversity of body shape and behavior in living constructs derived from genetically wild‐type cells. Here anthrobots are introduced, a spheroid‐shaped multicellular biological robot (biobot) platform with diameters ranging from 30 to 500 microns and cilia‐powered locomotive abilities. Each Anthrobot begins as a single cell, derived from the adult human lung, and self‐constructs into a multicellular motile biobot after being cultured in extra cellular matrix for 2 weeks and transferred into a minimally viscous habitat. Anthrobots exhibit diverse behaviors with motility patterns ranging from tight loops to straight lines and speeds ranging from 5–50 microns s−1. The anatomical investigations reveal that this behavioral diversity is significantly correlated with their morphological diversity. Anthrobots can assume morphologies with fully polarized or wholly ciliated bodies and spherical or ellipsoidal shapes, each related to a distinct movement type. Anthrobots are found to be capable of traversing, and inducing rapid repair of scratches in, cultured human neural cell sheets in vitro. By controlling microenvironmental cues in bulk, novel structures, with new and unexpected behavior and biomedically‐relevant capabilities, can be discovered in morphogenetic processes without direct genetic editing or manual sculpting."
"
Researchers at Weill Cornell Medicine, Cornell Tech and Cornell's Ithaca campus have demonstrated the use of AI-selected natural images and AI-generated synthetic images as neuroscientific tools for probing the visual processing areas of the brain. The goal is to apply a data-driven approach to understand how vision is organized while potentially removing biases that may arise when looking at responses to a more limited set of researcher-selected images.

In the study, published Oct. 23 in Communications Biology, the researchers had volunteers look at images that had been selected or generated based on an AI model of the human visual system. The images were predicted to maximally activate several visual processing areas. Using functional magnetic resonance imaging (fMRI) to record the brain activity of the volunteers, the researchers found that the images did activate the target areas significantly better than control images.
The researchers also showed that they could use this image-response data to tune their vision model for individual volunteers, so that images generated to be maximally activating for a particular individual worked better than images generated based on a general model.
""We think this is a promising new approach to study the neuroscience of vision,"" said study senior author Dr. Amy Kuceyeski, a professor of mathematics in radiology and of mathematics in neuroscience in the Feil Family Brain and Mind Research Institute at Weill Cornell Medicine.
The study was a collaboration with the laboratory of Dr. Mert Sabuncu, a professor of electrical and computer engineering at Cornell Engineering and Cornell Tech, and of electrical engineering in radiology at Weill Cornell Medicine. The study's first author was Dr. Zijin Gu, a who was a doctoral student co-mentored by Dr. Sabuncu and Dr. Kuceyeski at the time of the study.
Making an accurate model of the human visual system, in part by mapping brain responses to specific images, is one of the more ambitious goals of modern neuroscience. Researchers have found for example, that one visual processing region may activate strongly in response to an image of a face whereas another may respond to a landscape. Scientists must rely mainly on non-invasive methods in pursuit of this goal, given the risk and difficulty of recording brain activity directly with implanted electrodes. The preferred non-invasive method is fMRI, which essentially records changes in blood flow in small vessels of the brain -- an indirect measure of brain activity -- as subjects are exposed to sensory stimuli or otherwise perform cognitive or physical tasks. An fMRI machine can read out these tiny changes in three dimensions across the brain, at a resolution on the order of cubic millimeters.
For their own studies, Dr. Kuceyeski and Dr. Sabuncu and their teams used an existing dataset comprising tens of thousands of natural images, with corresponding fMRI responses from human subjects, to train an AI-type system called an artificial neural network (ANN) to model the human brain's visual processing system. They then used this model to predict which images, across the dataset, should maximally activate several targeted vision areas of the brain. They also coupled the model with an AI-based image generator to generate synthetic images to accomplish the same task.

""Our general idea here has been to map and model the visual system in a systematic, unbiased way, in principle even using images that a person normally wouldn't encounter,"" Dr. Kuceyeski said.
The researchers enrolled six volunteers and recorded their fMRI responses to these images, focusing on the responses in several visual processing areas. The results showed that, for both the natural images and the synthetic images, the predicted maximal activator images, on average across the subjects, did activate the targeted brain regions significantly more than a set of images that were selected or generated to be only average activators. This supports the general validity of the team's ANN-based model and suggests that even synthetic images may be useful as probes for testing and improving such models.
In a follow-on experiment, the team used the image and fMRI-response data from the first session to create separate ANN-based visual system models for each of the six subjects. They then used these individualized models to select or generate predicted maximal-activator images for each subject. The fMRI responses to these images showed that, at least for the synthetic images, there was greater activation of the targeted visual region, a face-processing region called FFA1, compared to the responses to images based on the group model. This result suggests that AI and fMRI can be useful for individualized visual-system modeling, for example to study differences in visual system organization across populations.
The researchers are now running similar experiments using a more advanced version of the image generator, called Stable Diffusion.
The same general approach could be useful in studying other senses such as hearing, they noted.
Dr. Kuceyeski also hopes ultimately to study the therapeutic potential of this approach.
""In principle, we could alter the connectivity between two parts of the brain using specifically designed stimuli, for example to weaken a connection that causes excess anxiety,"" she said.

","score: 17.054546161083163, grade_level: '17'","score: 17.967472082464106, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s42003-023-05440-7,"Understanding how human brains interpret and process information is important. Here, we investigated the selectivity and inter-individual differences in human brain responses to images via functional MRI. In our first experiment, we found that images predicted to achieve maximal activations using a group level encoding model evoke higher responses than images predicted to achieve average activations, and the activation gain is positively associated with the encoding model accuracy. Furthermore, anterior temporal lobe face area (aTLfaces) and fusiform body area 1 had higher activation in response to maximal synthetic images compared to maximal natural images. In our second experiment, we found that synthetic images derived using a personalized encoding model elicited higher responses compared to synthetic images from group-level or other subjects’ encoding models. The finding of aTLfaces favoring synthetic images than natural images was also replicated. Our results indicate the possibility of using data-driven and generative approaches to modulate macro-scale brain region responses and probe inter-individual differences in and functional specialization of the human visual system."
"
Long before researchers discovered the electron and its role in generating electrical current, they knew about electricity and were exploring its potential. One thing they learned early on was that metals were great conductors of both electricity and heat.

And in 1853, two scientists showed that those two admirable properties of metals were somehow related: At any given temperature, the ratio of electronic conductivity to thermal conductivity was roughly the same in any metal they tested. This so-called Wiedemann-Franz law has held ever since -- except in quantum materials, where electrons stop behaving as individual particles and glom together into a sort of electron soup. Experimental measurements have indicated that the 170-year-old law breaks down in these quantum materials, and by quite a bit.
Now, a theoretical argument put forth by physicists at the Department of Energy's SLAC National Accelerator Laboratory, Stanford University and the University of Illinois suggests that the law should, in fact, approximately hold for one type of quantum material -- the copper oxide superconductors, or cuprates, which conduct electricity with no loss at relatively high temperatures.
In a paper published in Science today, they propose that the Wiedemann-Franz law should still roughly hold if one considers only the electrons in cuprates. They suggest that other factors, such as vibrations in the material's atomic latticework, must account for experimental results that make it look like the law does not apply.
This surprising result is important to understanding unconventional superconductors and other quantum materials, said Wen Wang, lead author of the paper and a PhD student with the Stanford Institute for Materials and Energy Sciences (SIMES) at SLAC.
""The original law was developed for materials where electrons interact with each other weakly and behave like little balls that bounce off defects in the material's lattice,"" Wang said. ""We wanted to test the law theoretically in systems where neither of these things was true.""
Peeling a quantum onion
Superconducting materials, which carry electric current without resistance, were discovered in 1911. But they operated at such extremely low temperatures that their usefulness was quite limited.

That changed in 1986, when the first family of so-called high-temperature or unconventional superconductors -- the cuprates -- was discovered. Although cuprates still require extremely cold conditions to work their magic, their discovery raised hopes that superconductors could someday work at much closer to room temperature -- making revolutionary technologies like no-loss power lines possible.
After nearly four decades of research, that goal is still elusive, although a lot of progress has been made in understanding the conditions in which superconducting states flip in and out of existence.
Theoretical studies, performed with the help of powerful supercomputers, have been essential for interpreting the results of experiments on these materials and for understanding and predicting phenomena that are out of experimental reach.
For this study, the SIMES team ran simulations based on what's known as the Hubbard model, which has become an essential tool for simulating and describing systems where electrons stop acting independently and join forces to produce unexpected phenomena.
The results show that when you only take electron transport into account, the ratio of electronic conductivity to thermal conductivity approaches what the Wiedemann-Franz law predicts, Wang said. ""So, the discrepancies that have been seen in experiments should be coming from other things like phonons, or lattice vibrations, that are not in the Hubbard model,"" she said.
SIMES staff scientist and paper co-author Brian Moritz said that although the study did not investigate how vibrations cause the discrepancies, ""somehow the system still knows that there is this correspondence between charge and heat transport amongst the electrons. That was the most surprising result.""
From here, he added, ""maybe we can peel the onion to understand a little bit more.""
Major funding for this study came from the DOE Office of Science. Computational work was carried out at Stanford University and on resources of the National Energy Research Scientific Computing Center, which is a DOE Office of Science user facility.

","score: 15.709442750373693, grade_level: '16'","score: 17.569865470852015, grade_levels: ['college_graduate'], ages: [24, 100]",10.1126/science.ade3232,"Many metallic quantum materials display anomalous transport phenomena that defy a Fermi liquid description. Here, we use numerical methods to calculate thermal and charge transport in the doped Hubbard model and observe a crossover separating high- and low-temperature behaviors. Distinct from the behavior at high temperatures, the Lorenz number L becomes weakly doping dependent and less sensitive to parameters at low temperatures. At the lowest numerically accessible temperatures, L roughly approaches the Wiedemann-Franz constant L 0 , even in a doped Mott insulator that lacks well-defined quasiparticles. Decomposing the energy current operator indicates a compensation between kinetic and potential contributions, which may help to clarify the interpretation of transport experiments beyond Boltzmann theory in strongly correlated metals."
"
Researchers have developed an AI model that can predict where a drug molecule can be chemically altered.

A team of researchers from LMU, ETH Zurich, and Roche Pharma Research and Early Development (pRED) Basel has used artificial intelligence (AI) to develop an innovative method that predicts the optimal method for synthesizing drug molecules. ""This method has the potential to significantly reduce the number of required lab experiments, thereby increasing both the efficiency and sustainability of chemical synthesis,"" says David Nippa, lead author of the corresponding paper, which has been published in the journal Nature Chemistry. Nippa is a doctoral student in Dr. David Konrad's research group at the Faculty of Chemistry and Pharmacy at LMU and at Roche.
Active pharmaceutical ingredients typically consist of a framework to which functional groups are attached. These groups enable a specific biological function. To achieve new or improved medical effects, functional groups are altered and added to new positions in the framework. However, this process is particularly challenging in chemistry, as the frameworks, which mainly consist of carbon and hydrogen atoms, are hardly reactive themselves. One method of activating the framework is the so-called borylation reaction. In this process, a chemical group containing the element boron is attached to a carbon atom of the framework. This boron group can then be replaced by a variety of medically effective groups. Although borylation has great potential, it is difficult to control in the lab.
Together with Kenneth Atz, a doctoral student at ETH Zurich, David Nippa developed an AI model that was trained on data from trustworthy scientific works and experiments from an automated lab at Roche. It can successfully predict the position of borylation for any molecule and provides the optimal conditions for the chemical transformation. ""Interestingly, the predictions improved when the three-dimensional information of the starting materials were taken into account, not just their two-dimensional chemical formulas,"" says Atz.
The method has already been successfully used to identify positions in existing active ingredients where additional active groups can be introduced. This helps researchers develop new and more effective variants of known drug active ingredients more quickly.

","score: 14.840680766688699, grade_level: '15'","score: 14.998509583608723, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41557-023-01360-5,"Late-stage functionalization is an economical approach to optimize the properties of drug candidates. However, the chemical complexity of drug molecules often makes late-stage diversification challenging. To address this problem, a late-stage functionalization platform based on geometric deep learning and high-throughput reaction screening was developed. Considering borylation as a critical step in late-stage functionalization, the computational model predicted reaction yields for diverse reaction conditions with a mean absolute error margin of 4–5%, while the reactivity of novel reactions with known and unknown substrates was classified with a balanced accuracy of 92% and 67%, respectively. The regioselectivity of the major products was accurately captured with a classifier F-score of 67%. When applied to 23 diverse commercial drug molecules, the platform successfully identified numerous opportunities for structural diversification. The influence of steric and electronic information on model performance was quantified, and a comprehensive simple user-friendly reaction format was introduced that proved to be a key enabler for seamlessly integrating deep learning and high-throughput experimentation for late-stage functionalization."
"
Noise on the radio when reception is poor is a typical example of how fluctuations mask a physical signal. In fact, such interference or noise occurs in every physical measurement in addition to the actual signal. ""Even in the loneliest place in the universe, where there should be nothing at all, there are still fluctuations of the electromagnetic field,"" says physicist Ulrich Nowak. In the Collaborative Research Centre (CRC) 1432 ""Fluctuations and Nonlinearities in Classical and Quantum Matter beyond Equilibrium"" at the University of Konstanz, researchers do not see this omnipresent noise as a disturbing factor that needs to be eliminated as far as possible, but as a source of information that tells us something about the signal.

No magnetic effect, but fluctuations
This approach has now proved successful when investigating antiferromagnets. Antiferromagnets are magnetic materials in which the magnetizations of several sub-lattices cancel out each other. Nevertheless, antiferromagnetic insulators are considered promising for energy-efficient components in the field of information technology. As they have hardly any magnetic fields on the outside, they are very difficult to characterize physically. Yet, antiferromagnets are surrounded by magnetic fluctuations, which can tell us a lot about this weakly magnetic material.
In this spirit, the groups of the two materials scientists Ulrich Nowak and Sebastian Gönnenwein analysed the fluctuations of antiferromagnetic materials in the context of the CRC. The decisive factor in their theoretical as well as experimental study, recently published in the journal Nature Communications, was the specific frequency range. ""We measure very fast fluctuations and have developed a method with which fluctuations can still be detected on the ultrashort time scale of femtoseconds,"" says experimental physicist Sebastian Gönnenwein. A femtosecond is one millionth of a billionth of a second.
New experimental approach for ultrafast time scales
On slower time scales, one could use electronics that are fast enough to measure these fluctuations. On ultrafast time scales, this no longer works, which is why a new experimental approach had to be developed. It is based on an idea from the research group of Alfred Leitenstorfer, who is also a member of the Collaborative Research Centre. Employing laser technology, the researchers use pulse sequences or pulse pairs in order to obtain information about fluctuations. Initially, this measurement approach was developed to investigate quantum fluctuations, and has now been extended to fluctuations in magnetic systems. Takayuki Kurihara from the University of Tokyo played a key role in this development as the third cooperation partner. He was a member of the Leitenstorfer research group and the Zukunftskolleg at the University of Konstanz from 2018 to 2020.
Detection of fluctuations using ultrashort light pulses
In the experiment, two ultrashort light pulses are transmitted through the magnet with a time delay, testing the magnetic properties during the transit time of each pulse, respectively. The light pulses are then checked for similarity using sophisticated electronics. The first pulse serves as a reference, the second contains information about how much the antiferromagnet has changed in the time between the first and second pulse. Different measurement results at the two points of time confirm the fluctuations. Ulrich Nowak's research group also modelled the experiment in elaborate computer simulations in order to better understand its results.
One unexpected result was the discovery of what is known as telegraph noise on ultrashort time scales. This means that there is not only unsorted noise, but also fluctuations in which the system switches back and forth between two well-defined states.Such fast, purely random switching has never been observed before and could be interesting for applications such as random number generators. In any case, the new methodological possibilities for analyzing fluctuations on ultrashort time scales offer great potential for further discoveries in the field of functional materials.

","score: 14.86314536340852, grade_level: '15'","score: 15.885519480519484, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41467-023-43318-8,"Owing to their high magnon frequencies, antiferromagnets are key materials for future high-speed spintronics. Picosecond switching of antiferromagnetic spin systems has been viewed a milestone for decades and pursued only by using ultrafast external perturbations. Here, we show that picosecond spin switching occurs spontaneously due to thermal fluctuations in the antiferromagnetic orthoferrite Sm0.7Er0.3FeO3. By analysing the correlation between the pulse-to-pulse polarisation fluctuations of two femtosecond optical probes, we extract the autocorrelation of incoherent magnon fluctuations. We observe a strong enhancement of the magnon fluctuation amplitude and the coherence time around the critical temperature of the spin reorientation transition. The spectrum shows two distinct features, one corresponding to the quasi-ferromagnetic mode and another one which has not been previously reported in pump-probe experiments. Comparison to a stochastic spin dynamics simulation reveals this new mode as smoking gun of ultrafast spontaneous spin switching within the double-well anisotropy potential."
"
New technology often calls for new materials -- and with supercomputers and simulations, researchers don't have to wade through inefficient guesswork to invent them from scratch.

The Materials Project, an open-access database founded at the Department of Energy's Lawrence Berkeley National Laboratory (Berkeley Lab) in 2011, computes the properties of both known and predicted materials. Researchers can focus on promising materials for future technologies -- think lighter alloys that improve fuel economy in cars, more efficient solar cells to boost renewable energy, or faster transistors for the next generation of computers.
Now, Google DeepMind -- Google's artificial intelligence lab -- is contributing nearly 400,000 new compounds to the Materials Project, expanding the amount of information researchers can draw upon. The dataset includes how the atoms of a material are arranged (the crystal structure) and how stable it is (formation energy).
""We have to create new materials if we are going to address the global environmental and climate challenges,"" said Kristin Persson, the founder and director of the Materials Project at Berkeley Lab and a professor at UC Berkeley. ""With innovation in materials, we can potentially develop recyclable plastics, harness waste energy, make better batteries, and build cheaper solar panels that last longer, among many other things.""
To generate the new data, Google DeepMind developed a deep learning tool called Graph Networks for Materials Exploration, or GNoME. Researchers trained GNoME using workflows and data that were developed over a decade by the Materials Project, and improved the GNoME algorithm through active learning. GNoME researchers ultimately produced 2.2 million crystal structures, including 380,000 that they are adding to the Materials Project and predict are stable, making them potentially useful in future technologies. The new results from Google DeepMind are published today in the journal Nature.
Some of the computations from GNoME were used alongside data from the Materials Project to test A-Lab, a facility at Berkeley Lab where artificial intelligence guides robots in making new materials. A-Lab's first paper, also published today in Nature, showed that the autonomous lab can quickly discover novel materials with minimal human input.
Over 17 days of independent operation, A-Lab successfully produced 41 new compounds out of an attempted 58 -- a rate of more than two new materials per day. For comparison, it can take a human researcher months of guesswork and experimentation to create one new material, if they ever reach the desired material at all.

To make the novel compounds predicted by the Materials Project, A-Lab's AI created new recipes by combing through scientific papers and using active learning to make adjustments. Data from the Materials Project and GNoME were used to evaluate the materials' predicted stability.
""We had this staggering 71% success rate, and we already have a few ways to improve it,"" said Gerd Ceder, the principal investigator for A-Lab and a scientist at Berkeley Lab and UC Berkeley. ""We've shown that combining the theory and data side with automation has incredible results. We can make and test materials faster than ever before, and adding more data points to the Materials Project means we can make even smarter choices.""
The Materials Project is the most widely used open-access repository of information on inorganic materials in the world. The database holds millions of properties on hundreds of thousands of structures and molecules, information primarily processed at Berkeley Lab's National Energy Research Science Computing Center. More than 400,000 people are registered as users of the site and, on average, more than four papers citing the Materials Project are published every day. The contribution from Google DeepMind is the biggest addition of structure-stability data from a group since the Materials Project began.
""We hope that the GNoME project will drive forward research into inorganic crystals,"" said Ekin Dogus Cubuk, lead of Google DeepMind's Materials Discovery team. ""External researchers have already verified more than 736 of GNoME's new materials through concurrent, independent physical experiments, demonstrating that our model's discoveries can be realized in laboratories.""
The Materials Project is now processing the compounds from Google DeepMind and adding them into the online database. The new data will be freely available to researchers, and also feed into projects such as A-Lab that partner with the Materials Project.
""I'm really excited that people are using the work we've done to produce an unprecedented amount of materials information,"" said Persson, who is also the director of Berkeley Lab's Molecular Foundry. ""This is what I set out to do with the Materials Project: To not only make the data that I produced free and available to accelerate materials design for the world, but also to teach the world what computations can do for you. They can scan large spaces for new compounds and properties more efficiently and rapidly than experiments alone can.""
By following promising leads from data in the Materials Project over the past decade, researchers have experimentally confirmed useful properties in new materials across several areas. Some show potential for use: in carbon capture (pulling carbon dioxide from the atmosphere) as photocatalysts (materials that speed up chemical reactions in response to light and could be used to break down pollutants or generate hydrogen) as thermoelectrics (materials that could help harness waste heat and turn it into electrical power) as transparent conductors (which might be useful in solar cells, touch screens, or LEDs)Of course, finding these prospective materials is only one of many steps to solving some of humanity's big technology challenges.
""Making a material is not for the faint of heart,"" Persson said. ""It takes a long time to take a material from computation to commercialization. It has to have the right properties, work within devices, be able to scale, and have the right cost efficiency and performance. The goal with the Materials Project and facilities like A-Lab is to harness data, enable data-driven exploration, and ultimately give companies more viable shots on goal.""

","score: 15.48491150080454, grade_level: '15'","score: 16.866432759702185, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41586-023-06734-w,"To close the gap between the rates of computational screening and experimental realization of novel materials1,2, we introduce the A-Lab, an autonomous laboratory for the solid-state synthesis of inorganic powders. This platform uses computations, historical data from the literature, machine learning (ML) and active learning to plan and interpret the outcomes of experiments performed using robotics. Over 17 days of continuous operation, the A-Lab realized 41 novel compounds from a set of 58 targets including a variety of oxides and phosphates that were identified using large-scale ab initio phase-stability data from the Materials Project and Google DeepMind. Synthesis recipes were proposed by natural-language models trained on the literature and optimized using an active-learning approach grounded in thermodynamics. Analysis of the failed syntheses provides direct and actionable suggestions to improve current techniques for materials screening and synthesis design. The high success rate demonstrates the effectiveness of artificial-intelligence-driven platforms for autonomous materials discovery and motivates further integration of computations, historical knowledge and robotics."
"
Researchers report that they have developed a new composite material designed to change behaviors depending on temperature in order to perform specific tasks. These materials are poised to be part of the next generation of autonomous robotics that will interact with the environment.

The new study conducted by University of Illinois Urbana-Champaign civil and environmental engineering professor Shelly Zhang and graduate student Weichen Li, in collaboration with professor Tian Chen and graduate student Yue Wang from the University of Houston, uses computer algorithms, two distinct polymers and 3D printing to reverse engineer a material that expands and contracts in response to temperature change with or without human intervention.
The study findings are reported in the journal Science Advances.
""Creating a material or device that will respond in specific ways depending on its environment is very challenging to conceptualize using human intuition alone -- there are just so many design possibilities out there,"" Zhang said. ""So, instead, we decided to work with a computer algorithm to help us determine the best combination of materials and geometry.""
The team first used computer modeling to conceptualize a two-polymer composite that can behave differently under various temperatures based on user input or autonomous sensing.
""For this study, we developed a material that can behave like soft rubber in low temperatures and as a stiff plastic in high temperatures,"" Zhang said.
Once fabricated into a tangible device, the team tested the new composite material's ability to respond to temperature changes to perform a simple task -- switch on LED lights.

""Our study demonstrates that it is possible to engineer a material with intelligent temperature sensing capabilities, and we envision this being very useful in robotics,"" Zhang said. ""For example, if a robot's carrying capacity needs to change when the temperature changes, the material will 'know' to adapt its physical behavior to stop or perform a different task.""
Zhang said that one of the hallmarks of the study is the optimization process that helps the researchers interpolate the distribution and geometries of the two different polymer materials needed.
""Our next goal is to use this technique to add another level of complexity to a material's programmed or autonomous behavior, such as the ability to sense the velocity of some sort of impact from another object,"" she said. ""This will be critical for robotics materials to know how to respond to various hazards in the field.""
The National Science Foundation supported this research.

","score: 16.256271132376394, grade_level: '16'","score: 17.01110845295056, grade_levels: ['college_graduate'], ages: [24, 100]",10.1126/sciadv.adk0620,"We envision programmable matters that can alter their physical properties in desirable manners based on user input or autonomous sensing. This vision motivates the pursuit of mechanical metamaterials that interact with the environment in a programmable fashion. However, this has not been systematically achieved for soft metamaterials because of the highly nonlinear deformation and underdevelopment of rational design strategies. Here, we use computational morphogenesis and multimaterial polymer 3D printing to systematically create soft metamaterials with arbitrarily programmable temperature-switchable nonlinear mechanical responses under large deformations. This is made possible by harnessing the distinct glass transition temperatures of different polymers, which, when optimally synthesized, produce local and giant stiffness changes in a controllable manner. Featuring complex geometries, the generated structures and metamaterials exhibit fundamentally different yet programmable nonlinear force-displacement relations and deformation patterns as temperature varies. The rational design and fabrication establish an objective-oriented synthesis of metamaterials with freely tunable thermally adaptive behaviors. This imbues structures and materials with environment-aware intelligence."
"
Particle accelerators hold great potential for semiconductor applications, medical imaging and therapy, and research in materials, energy and medicine. But conventional accelerators require plenty of elbow room -- kilometers -- making them expensive and limiting their presence to a handful of national labs and universities.

Researchers from The University of Texas at Austin, several national laboratories, European universities and the Texas-based company TAU Systems Inc. have demonstrated a compact particle accelerator less than 20 meters long that produces an electron beam with an energy of 10 billion electron volts (10 GeV). There are only two other accelerators currently operating in the U.S. that can reach such high electron energies, but both are approximately 3 kilometers long.
""We can now reach those energies in 10 centimeters,"" said Bjorn ""Manuel"" Hegelich, associate professor of physics at UT and CEO of TAU Systems, referring to the size of the chamber where the beam was produced. He is the senior author on a recent paper describing their achievement in the journal Matter and Radiation at Extremes.
Hegelich and his team are currently exploring the use of their accelerator, called an advanced wakefield laser accelerator, for a variety of purposes. They hope to use it to test how well space-bound electronics can withstand radiation, to image the 3D internal structures of new semiconductor chip designs, and even to develop novel cancer therapies and advanced medical-imaging techniques.
This kind of accelerator could also be used to drive another device called an X-ray free electron laser, which could take slow-motion movies of processes on the atomic or molecular scale. Examples of such processes include drug interactions with cells, changes inside batteries that might cause them to catch fire, chemical reactions inside solar panels, and viral proteins changing shape when infecting cells.
The concept for wakefield laser accelerators was first described in 1979. An extremely powerful laser strikes helium gas, heats it into a plasma and creates waves that kick electrons from the gas out in a high-energy electron beam. During the past couple of decades, various research groups have developed more powerful versions. Hegelich and his team's key advance relies on nanoparticles. An auxiliary laser strikes a metal plate inside the gas cell, which injects a stream of metal nanoparticles that boost the energy delivered to electrons from the waves.
The laser is like a boat skimming across a lake, leaving behind a wake, and electrons ride this plasma wave like surfers.

""It's hard to get into a big wave without getting overpowered, so wake surfers get dragged in by Jet Skis,"" Hegelich said. ""In our accelerator, the equivalent of Jet Skis are nanoparticles that release electrons at just the right point and just the right time, so they are all sitting there in the wave. We get a lot more electrons into the wave when and where we want them to be, rather than statistically distributed over the whole interaction, and that's our secret sauce.""
For this experiment, the researchers used one of the world's most powerful pulsed lasers, the Texas Petawatt Laser, which is housed at UT and fires one ultra-intense pulse of light every hour. A single petawatt laser pulse contains about 1,000 times the installed electrical power in the U.S. but lasts only 150 femtoseconds, less than a billionth as long as a lightning discharge. The team's long-term goal is to drive their system with a laser they're currently developing that fits on a tabletop and can fire repeatedly at thousands of times per second, making the whole accelerator far more compact and usable in much wider settings than conventional accelerators.
The study's co-first authors are Constantin Aniculaesei, corresponding author now at Heinrich Heine University Düsseldorf, Germany; and Thanh Ha, doctoral student at UT and researcher at TAU Systems. Other UT faculty members are professors Todd Ditmire and Michael Downer.
Hegelich and Aniculaesei have submitted a patent application describing the device and method to generate nanoparticles in a gas cell. TAU Systems, spun out of Hegelich's lab, holds an exclusive license from the University for this foundational patent. As part of the agreement, UT has been issued shares in TAU Systems.
Support for this research was provided by the U.S. Air Force Office of Scientific Research, the U.S. Department of Energy, the U.K. Engineering and Physical Sciences Research Council and the European Union's Horizon 2020 research and innovation program.

","score: 14.98609129676294, grade_level: '15'","score: 15.958136266718356, grade_levels: ['college_graduate'], ages: [24, 100]",10.1063/5.0161687,"An intense laser pulse focused onto a plasma can excite nonlinear plasma waves. Under appropriate conditions, electrons from the background plasma are trapped in the plasma wave and accelerated to ultra-relativistic velocities. This scheme is called a laser wakefield accelerator. In this work, we present results from a laser wakefield acceleration experiment using a petawatt-class laser to excite the wakefields as well as nanoparticles to assist the injection of electrons into the accelerating phase of the wakefields. We find that a 10-cm-long, nanoparticle-assisted laser wakefield accelerator can generate 340 pC, 10 ± 1.86 GeV electron bunches with a 3.4 GeV rms convolved energy spread and a 0.9 mrad rms divergence. It can also produce bunches with lower energies in the 4–6 GeV range."
"
Physicists at Martin Luther University Halle-Wittenberg (MLU) and Central South University in China have demonstrated that, combining specific materials, heat in technical devices can be used in computing. Their discovery is based on extensive calculations and simulations. The new approach demonstrates how heat signals can be steered and amplified for use in energy-efficient data processing. The team's research findings have been published in the journal Advanced Electronic Materials.

Electric current flow heats up electronic device. The generated heat is dissipated and energy is lost. ""For decades, people have been looking for ways to re-use this lost energy in electronics,"" explains Dr Jamal Berakdar, a professor of physics at MLU. This is extremely challenging, he says, due to the difficulty in directing and controlling accurately heat signals. However, both are necessary if heat signals are to be used to reliably process data.
Berakdar carried out extensive calculations together with two colleagues from Central South University in China. The idea: instead of conventional electronic circuits, non-conductive magnetic strips are used in conjunction with normal metal spacers. ""This unusual combination makes it possible to conduct and amplify heat signals in a controlled manner in order to power logical computing operations and heat diodes,"" explains Berakdar.
One disadvantage of the new method, however, is its speed. ""This method does not produce the kind of computing speeds we see in modern smartphones,"" says Berakdar. That is why the new method is currently probably less relevant for use in everyday electronics and is better suited for next generation computers which will be used to perform energy-saving calculations. ""Our technology can contribute to saving energy in information technology by making good use of surplus heat,"" Berakdar concludes.
The study was funded by the German Research Foundation (DFG), the National Natural Science Foundation of China, the Natural Science Foundation of Hunan Province and as part of the Central South University Innovation-Driven Research Program.

","score: 13.95136222910217, grade_level: '14'","score: 13.865634674922603, grade_levels: ['college_graduate'], ages: [24, 100]",10.1002/aelm.202300325,"Devices for performing computation and logic operations with low‐energy consumption are of key importance for environmentally friendly data processing and information technology. Here, a design for magnetic elements that use excess heat to perform logic operations is presented. The basic information channel is coupled non‐conductive magnetic stripes with a normal metal spacer. The thermal information signal is embodied in magnetic excitations and it can be transported, locally enhanced, and controllably steered by virtue of charge current pulses in the spacer. Functionality of essential thermal logic gates is demonstrated by material‐specific simulations. The operation principle takes advantage of the special material architecture with a balanced gain/loss mechanism for magnetic excitation which renders the circuit parity‐time symmetric with exceptional points tunable by the current strength in the spacer. Heat flow at these points can be enhanced, be non‐reciprocal, or may oscillate between the information channels enabling controlled thermal diode and thermal gate operations. The findings point to a new route for exploiting heat for useful work on the nanoscale."
"
For 200 years, scientists have failed to grow a common mineral in the laboratory under the conditions believed to have formed it naturally. Now, a team of researchers from the University of Michigan and Hokkaido University in Sapporo, Japan have finally pulled it off, thanks to a new theory developed from atomic simulations.

Their success resolves a long-standing geology mystery called the ""Dolomite Problem."" Dolomite -- a key mineral in the Dolomite mountains in Italy, Niagara Falls, the White Cliffs of Dover and Utah's Hoodoos -- is very abundant in rocks older than 100 million years, but nearly absent in younger formations.
""If we understand how dolomite grows in nature, we might learn new strategies to promote the crystal growth of modern technological materials,"" said Wenhao Sun, the Dow Early Career Professor of Materials Science and Engineering at U-M and the corresponding author of the paper published today in Science.
The secret to finally growing dolomite in the lab was removing defects in the mineral structure as it grows. When minerals form in water, atoms usually deposit neatly onto an edge of the growing crystal surface. However, the growth edge of dolomite consists of alternating rows of calcium and magnesium. In water, calcium and magnesium will randomly attach to the growing dolomite crystal, often lodging into the wrong spot and creating defects that prevent additional layers of dolomite from forming. This disorder slows dolomite growth to a crawl, meaning it would take 10 million years to make just one layer of ordered dolomite.
Luckily, these defects aren't locked in place. Because the disordered atoms are less stable than atoms in the correct position, they are the first to dissolve when the mineral is washed with water. Repeatedly rinsing away these defects -- for example, with rain or tidal cycles -- allows a dolomite layer to form in only a matter of years. Over geologic time, mountains of dolomite can accumulate.
To simulate dolomite growth accurately, the researchers needed to calculate how strongly or loosely atoms will attach to an existing dolomite surface. The most accurate simulations require the energy of every single interaction between electrons and atoms in the growing crystal. Such exhaustive calculations usually require huge amounts of computing power, but software developed at U-M's Predictive Structure Materials Science (PRISMS) Center offered a shortcut.
""Our software calculates the energy for some atomic arrangements, then extrapolates to predict the energies for other arrangements based on the symmetry of the crystal structure,"" said Brian Puchala, one of the software's lead developers and an associate research scientist in U-M's Department of Materials Science and Engineering.

That shortcut made it feasible to simulate dolomite growth over geologic timescales.
""Each atomic step would normally take over 5,000 CPU hours on a supercomputer. Now, we can do the same calculation in 2 milliseconds on a desktop,"" said Joonsoo Kim, a doctoral student of materials science and engineering and the study's first author.
The few areas where dolomite forms today intermittently flood and later dry out, which aligns well with Sun and Kim's theory. But such evidence alone wasn't enough to be fully convincing. Enter Yuki Kimura, a professor of materials science from Hokkaido University, and Tomoya Yamazaki, a postdoctoral researcher in Kimura's lab. They tested the new theory with a quirk of transmission electron microscopes.
""Electron microscopes usually use electron beams just to image samples,"" Kimura said. ""However, the beam can also split water, which makes acid that can cause crystals to dissolve. Usually this is bad for imaging, but in this case, dissolution is exactly what we wanted.""
After placing a tiny dolomite crystal in a solution of calcium and magnesium, Kimura and Yamazaki gently pulsed the electron beam 4,000 times over two hours, dissolving away the defects. After the pulses, dolomite was seen to grow approximately 100 nanometers -- around 250,000 times smaller than an inch. Although this was only 300 layers of dolomite, never had more than five layers of dolomite been grown in the lab before.
The lessons learned from the Dolomite Problem can help engineers manufacture higher-quality materials for semiconductors, solar panels, batteries and other tech.
""In the past, crystal growers who wanted to make materials without defects would try to grow them really slowly,"" Sun said. ""Our theory shows that you can grow defect-free materials quickly, if you periodically dissolve the defects away during growth.""
The research was funded by the American Chemical Society PRF New Doctoral Investigator grant, the U.S. Department of Energy and the Japanese Society for the Promotion of Science.

","score: 13.50751137635211, grade_level: '14'","score: 13.849906751212238, grade_levels: ['college_graduate'], ages: [24, 100]",10.1126/science.adi3690,"Crystals grow in supersaturated solutions. A mysterious counterexample is dolomite CaMg(CO 3 ) 2 , a geologically abundant sedimentary mineral that does not readily grow at ambient conditions, not even under highly supersaturated solutions. Using atomistic simulations, we show that dolomite initially precipitates a cation-disordered surface, where high surface strains inhibit further crystal growth. However, mild undersaturation will preferentially dissolve these disordered regions, enabling increased order upon reprecipitation. Our simulations predict that frequent cycling of a solution between supersaturation and undersaturation can accelerate dolomite growth by up to seven orders of magnitude. We validated our theory with in situ liquid cell transmission electron microscopy, directly observing bulk dolomite growth after pulses of dissolution. This mechanism explains why modern dolomite is primarily found in natural environments with pH or salinity fluctuations. More generally, it reveals that the growth and ripening of defect-free crystals can be facilitated by deliberate periods of mild dissolution."
"
Animal embryos go through a series of characteristic developmental stages on their journey from a fertilized egg cell to a functional organism. This biological process is largely genetically controlled and follows a similar pattern across different animal species. Yet, there are differences in the details -- between individual species and even among embryos of the same species. For example, the tempo at which individual embryonic stages are passed through can vary. Such variations in embryonic development are considered an important driver of evolution, as they can lead to new characteristics, thus promoting evolutionary adaptations and biodiversity.

Studying the embryonic development of animals is therefore of great importance to better understand evolutionary mechanisms. But how can differences in embryonic development, such as the timing of developmental stages, be recorded objectively and efficiently? Researchers at the University of Konstanz led by systems biologist Patrick Müller are developing and using methods based on artificial intelligence (AI). In their current article in Nature Methods, they describe a novel approach that automatically captures the tempo of development processes and recognizes characteristic stages without human input -- standardized and across species boundaries.
Every embryo is a little different
Our current knowledge of animal embryogenesis and individual developmental stages is based on studies in which embryos of different ages were observed under the microscope and described in detail. Thanks to this painstaking manual work, reference books with idealized depictions of individual embryonic stages are available for many animal species today. ""However, embryos often do not look exactly the same under the microscope as they do in the schematic drawings. And the transitions between individual stages are not abrupt, but more gradual,"" explains Müller. Manually assigning an embryo to the various stages of development is therefore not trivial even for experts and a bit subjective.
What makes it even more difficult: Embryonic development does not always follow the expected timetable. ""Various factors can influence the timing of embryonic development, such as temperature,"" explains Müller. The AI-supported method he and his colleagues developed is a substantial step forward. For a first application example, the researchers trained their Twin Network with more than 3 million images of zebrafish embryos that were developing healthily. They then used the resulting AI model to automatically determine the developmental age of other zebrafish embryos.
Objective, accurate and generalizable
The researchers were able to demonstrate that the AI is capable of identifying key steps in zebrafish embryogenesis and detecting individual stages of development fully automatically and without human input. In their study, the researchers used the AI system to compare the developmental stage of embryos and describe the temperature dependence of embryonic development in zebrafish. Although the AI was trained with images of normally developing embryos, it was also able to identify malformations that can occur spontaneously in a certain percentage of embryos or that may be triggered by environmental toxins.
In a final step, the researchers transferred the method to other animal species, such as sticklebacks or the worm Caenorhabditis elegans, which is evolutionarily quite distant from zebrafish. ""Once the necessary image material is available, our Twin Network-based method can be used to analyze the embryonic development of various animal species in terms of time and stages. Even if no comparative data for the animal species exists, our system works in an objective, standardized way,"" Müller explains. The method therefore holds great potential for studying the development and evolution of previously uncharacterized animal species.

","score: 16.188107023411373, grade_level: '16'","score: 16.495170568561875, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41592-023-02083-8,"During animal development, embryos undergo complex morphological changes over time. Differences in developmental tempo between species are emerging as principal drivers of evolutionary novelty, but accurate description of these processes is very challenging. To address this challenge, we present here an automated and unbiased deep learning approach to analyze the similarity between embryos of different timepoints. Calculation of similarities across stages resulted in complex phenotypic fingerprints, which carry characteristic information about developmental time and tempo. Using this approach, we were able to accurately stage embryos, quantitatively determine temperature-dependent developmental tempo, detect naturally occurring and induced changes in the developmental progression of individual embryos, and derive staging atlases for several species de novo in an unsupervised manner. Our approach allows us to quantify developmental time and tempo objectively and provides a standardized way to analyze early embryogenesis."
"
Until today, dry stone wall construction has involved vast amounts of manual labour. A multidisciplinary team of ETH Zurich researchers developed a method of using an autonomous excavator to construct a dry-​stone wall that is six metres high and sixty-​five metres long. Dry stone walls are resource efficient as they use locally sourced materials, such as concrete slabs that are low in embodied energy.

ETH Zurich researchers deployed an autonomous excavator, called HEAP, to build a six metre-high and sixty-five-metre-long dry-stone wall. The wall is embedded in a digitally planned and autonomously excavated landscape and park.
The team of researchers included: Gramazio Kohler Research, the Robotics Systems Lab, Vision for Robotics Lab, and the Chair of Landscape Architecture. They developed this innovative design application as part of the National Centre of Competence in Research for Digital Fabrication (NCCR dfab).
Using sensors, the excavator can autonomously draw a 3D map of the construction site and localise existing building blocks and stones for the wall's construction. Specifically designed tools and machine vision approaches enable the excavator to scan and grab large stones in its immediate environment. It can also register their approximate weight as well as their centre of gravity.
An algorithm determines the best position for each stone, and the excavator then conducts the task itself by placing the stones in the desired location. The autonomous machine can place 20 to 30 stones in a single consignment -- about as many as one delivery could supply.

","score: 13.103655913978496, grade_level: '13'","score: 13.87772849462366, grade_levels: ['college_graduate'], ages: [24, 100]",10.1126/scirobotics.abp9758,"Automated building processes that enable efficient in situ resource utilization can facilitate construction in remote locations while simultaneously offering a carbon-reducing alternative to commonplace building practices. Toward these ends, we present a robotic construction pipeline that is capable of planning and building freeform stone walls and landscapes from highly heterogeneous local materials using a robotic excavator equipped with a shovel and gripper. Our system learns from real and simulated data to facilitate the online detection and segmentation of stone instances in spatial maps, enabling robotic grasping and textured 3D scanning of individual stones and rubble elements. Given a limited inventory of these digitized stones, our geometric planning algorithm uses a combination of constrained registration and signed-distance-field classification to determine how these should be positioned toward the formation of stable and explicitly shaped structures. We present a holistic approach for the robotic manipulation of complex objects toward dry stone construction and use the same hardware and mapping to facilitate autonomous terrain-shaping on a single construction site. Our process is demonstrated with the construction of a freestanding stone wall (10 meters by 1.7 meters by 4 meters) and a permanent retaining wall (65.5 meters by 1.8 meters by 6 meters) that is integrated with robotically contoured terraces (665 square meters). The work illustrates the potential of autonomous heavy construction vehicles to build adaptively with highly irregular, abundant, and sustainable materials that require little to no transportation and preprocessing."
"
Tandem solar cells based on perovskite semiconductors convert sunlight to electricity more efficiently than conventional silicon solar cells. In order to make this technology ready for the market, further improvements with regard to stability and manufacturing processes are required. Researchers of Karlsruhe Institute of Technology (KIT) and of two Helmholtz platforms -- Helmholtz Imaging at the German Cancer Research Center (DKFZ) and Helmholtz AI -- have succeeded in finding a way to predict the quality of the perovskite layers and consequently that of the resulting solar cells: Assisted by Machine Learning and new methods in Artificial Intelligence (AI), it is possible assess their quality from variations in light emission already in the manufacturing process.

Perovskite tandem solar cells combine a perovskite solar cell with a conventional solar cell, for example based on silicon. These cells are considered a next-generation technology: They boast an efficiency of currently more than 33 percent, which is much higher than that of conventional silicon solar cells. Moreover, they use inexpensive raw materials and are easily manufactured. To achieve this level of efficiency, an extremely thin high-grade perovskite layer, whose thickness is only a fraction of that of human hair, has to be produced. ""Manufacturing these high-grade, multi-crystalline thin layers without any deficiencies or holes using low-cost and scalable methods is one of the biggest challenges,"" says tenure-track professor Ulrich W. Paetzold who conducts research at the Institute of Microstructure Technology and the Light Technology Institute of KIT. Even under apparently perfect lab conditions, there may be unknown factors that cause variations in semiconductor layer quality: ""This drawback eventually prevents a quick start of industrial-scale production of these highly efficient solar cells, which are needed so badly for the energy turnaround,"" explains Paetzold.
AI Finds Hidden Signs of Effective Coating
To find the factors that influence coating, an interdisciplinary team consisting of the perovskite solar cell experts of KIT has joined forces with specialists for Machine Learning and Explainable Artificial Intelligence (XAI) of Helmholtz Imaging and Helmholtz AI at the DKFZ in Heidelberg. The researchers developed AI methods that train and analyze neural networks using a huge dataset. This dataset includes video recordings that show the photoluminescence of the thin perovskite layers during the manufacturing process. Photoluminescence refers to the radiant emission of the semiconductor layers that have been excited by an external light source. ""Since even experts could not see anything particular on the thin layers, the idea was born to train an AI system for Machine Learning (Deep Learning) to detect hidden signs of good or poor coating from the millions of data items on the videos,"" Lukas Klein and Sebastian Ziegler from Helmholtz Imaging at the DKFZ explain.
To filter and analyze the widely scattered indications output by the Deep Learning AI system, the researchers subsequently relied on methods of Explainable Artificial Intelligence.
""A Blueprint for Follow-Up Research""
The researchers found out experimentally that the photoluminescence varies during production and that this phenomenon has an influence on the coating quality. ""Key to our work was the targeted use of XAI methods to see which factors have to be changed to obtain a high-grade solar cell,"" Klein and Ziegler say. This is not the usual approach. In most cases, XAI is only used as a kind of guardrail to avoid mistakes when building AI models. ""This is a change of paradigm: Gaining highly relevant insights in materials science in such a systematic way is a totally new experience."" It was indeed the conclusion drawn from the photoluminescence variation that enabled the researchers to take the next step. After the neural networks had been trained accordingly, the AI was able to predict whether each solar cell would achieve a low or a high level of efficiency based on which variation of light emission occurred at what point in the manufacturing process. ""These are extremely exciting results,"" emphasizes Ulrich W. Paetzold. ""Thanks to the combined use of AI, we have a solid clue and know which parameters need to be changed in the first place to improve production. Now we are able to conduct our experiments in a more targeted way and are no longer forced to look blindfolded for the needle in a haystack. This is a blueprint for follow-up research that also applies to many other aspects of energy research and materials science.""

","score: 15.873690838493022, grade_level: '16'","score: 17.449878266344847, grade_levels: ['college_graduate'], ages: [24, 100]",10.1002/adma.202307160,"Large‐area processing of perovskite semiconductor thin‐films is complex and evokes unexplained variance in quality, posing a major hurdle for the commercialization of perovskite photovoltaics. Advances in scalable fabrication processes are currently limited to gradual and arbitrary trial‐and‐error procedures. While the in situ acquisition of photoluminescence (PL) videos has the potential to reveal important variations in the thin‐film formation process, the high dimensionality of the data quickly surpasses the limits of human analysis. In response, this study leverages deep learning (DL) and explainable artificial intelligence (XAI) to discover relationships between sensor information acquired during the perovskite thin‐film formation process and the resulting solar cell performance indicators, while rendering these relationships humanly understandable. The study further shows how gained insights can be distilled into actionable recommendations for perovskite thin‐film processing, advancing toward industrial‐scale solar cell manufacturing. This study demonstrates that XAI methods will play a critical role in accelerating energy materials science."
"
Hopfions, magnetic spin structures predicted decades ago, have become a hot and challenging research topic in recent years. In a study published in Nature today, the first experimental evidence is presented by a Swedish-German-Chinese research collaboration.

""Our results are important from both a fundamental and applied point of view, as a new bridge has emerged between experimental physics and abstract mathematical theory, potentially leading to hopfions finding an application in spintronics,"" says Philipp Rybakov, researcher at the Department of Physics and Astronomy at Uppsala University, Sweden.
A deeper understanding of how different components of materials function is important for the development of innovative materials and future technology. The research field of spintronics, for example, which studies the spin of electrons, has opened up promising possibilities to combine the electrons electricity and magnetism for applications such as new electronics, etc.
Magnetic skyrmions and hopfions are topological structures -- well-localized field configurations that have been a hot research topic over the past decade owing to their unique particle-like properties, which make them promising objects for spintronic applications. Skyrmions are two-dimensional, resembling vortex-like strings, while hopfions are three-dimensional structures within a magnetic sample volume resembling closed, twisted skyrmion strings in the shape of a donut-shaped ring in the simplest case.
Despite extensive research in recent years, direct observation of magnetic hopfions has only been reported in synthetic material. This current work is the first experimental evidence of such states stabilised in a crystal of B20-type FeGe plates using transmission electron microscopy and holography. The results are highly reproducible and in full agreement with micromagnetic simulations. The researchers provide a unified skyrmion-hopfion homotopy classification and offer an insight into the diversity of topological solitons in three-dimensional chiral magnets.
The findings open up new fields in experimental physics: identifying other crystals in which hopfions are stable, studying how hopfions interact with electric and spin currents, hopfion dynamics, and more.
""Since the object is new and many of its interesting properties remain to be discovered, it is difficult to make predictions about specific spintronic applications. However, we can speculate that hopfions may be of greatest interest when upgrading to the third dimension of almost any technology being developed with magnetic skyrmions: racetrack memory, neuromorphic computing, and qubits (basic unit of quantum information). Compared to skyrmions, hopfions have an additional degree of freedom due to three-dimensionality and thus can move in three rather than two dimensions,"" explains Rybakov.

","score: 18.240864197530865, grade_level: '18'","score: 20.14392592592592, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41586-023-06658-5,"Magnetic skyrmions and hopfions are topological solitons1—well-localized field configurations that have gained considerable attention over the past decade owing to their unique particle-like properties, which make them promising objects for spintronic applications. Skyrmions2,3 are two-dimensional solitons resembling vortex-like string structures that can penetrate an entire sample. Hopfions4–9 are three-dimensional solitons confined within a magnetic sample volume and can be considered as closed twisted skyrmion strings that take the shape of a ring in the simplest case. Despite extensive research on magnetic skyrmions, the direct observation of magnetic hopfions is challenging10 and has only been reported in a synthetic material11. Here we present direct observations of hopfions in crystals. In our experiment, we use transmission electron microscopy to observe hopfions forming coupled states with skyrmion strings in B20-type FeGe plates. We provide a protocol for nucleating such hopfion rings, which we verify using Lorentz imaging and electron holography. Our results are highly reproducible and in full agreement with micromagnetic simulations. We provide a unified skyrmion–hopfion homotopy classification and offer insight into the diversity of topological solitons in three-dimensional chiral magnets."
"
Using artificial intelligence technology and mathematical modeling, a research group led by Nagoya University has revealed that human behavior, such as lockdowns and isolation measures, affect the evolution of new strains of COVID-19. SARS-CoV-2, the virus that causes COVID-19, developed to become more transmissible earlier in its lifecycle. The researcher's findings, published in Nature Communications, provide new insights into the relationship between how people behave and disease-causing agents.

As with any other living organism, viruses evolve over time. Those with survival advantages become dominant in the gene pool. Many environmental factors influence this evolution, including human behavior. By isolating sick people and using lockdowns to control outbreaks, humans may alter virus evolution in complicated ways. Predicting how these changes occur is vital to develop adaptive treatments and interventions.
An important concept in this interaction is viral load, which refers to the amount or concentration of a virus present per ml of a bodily fluid. In SARS-CoV-2, a higher viral load in respiratory secretions increases the risk of transmission through droplets. Viral load relates to the potential to transmit a virus to others. For example, a virus like Ebola has an exceptionally high viral load, whereas the common cold has a low one. However, viruses must perform a careful balancing act, as increasing the maximum viral load can be advantageous, but an excessive viral load may cause individuals to become too sick to transmit the virus to others.
The research group led by Professor Shingo Iwami at the Nagoya University Graduate School of Science identified trends using mathematical modeling with an artificial intelligence component to investigate previously published clinical data. They found that the SARS-CoV-2 variants that were most successful at spreading had an earlier and higher peak in viral load. However, as the virus evolved from the pre-Alpha to the Delta variants, it had a shorter duration of infection. The researchers also found that the decreased incubation period and the increased proportion of asymptomatic infections recorded as the virus mutated also affected virus evolution.
The results showed a clear difference. As the virus evolved from the Wuhan strain to the Delta strain, they found a 5-fold increase in the maximum viral load and a 1.5-fold increase in the number of days before the viral load peaked.
Iwami and his colleagues suggest that human behavioral changes in response to the virus, designed to limit transmission, were increasing the selection pressure on the virus. This caused SARS-CoV-2 to be transmitted mainly during the asymptomatic and presymptomatic periods, which occur earlier in its infectious cycle. As a result, the viral load peak advanced to this period to spread more effectively in the earlier pre-symptomatic stages.
When evaluating public health strategies in response to COVID-19 and any future potentially pandemic-causing pathogens, it is necessary to consider the impact of changes in human behavior on virus evolution patterns. ""We expect that immune pressure from vaccinations and/or previous infections drives the evolution of SARS-CoV-2,"" Iwami said. ""However, our study found that human behavior can also contribute to the virus's evolution in a more complicated manner, suggesting the need to reevaluate virus evolution.""
Their study suggests the possibility that new strains of coronavirus evolved because of a complex interaction between clinical symptoms and human behavior. The group hopes that their research will speed up the establishment of testing regimes for adaptive treatment, effective screening, and isolation strategies.

","score: 14.399288244766506, grade_level: '14'","score: 14.701295974235101, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41467-023-43043-2,"During the COVID-19 pandemic, human behavior change as a result of nonpharmaceutical interventions such as isolation may have induced directional selection for viral evolution. By combining previously published empirical clinical data analysis and multi-level mathematical modeling, we find that the SARS-CoV-2 variants selected for as the virus evolved from the pre-Alpha to the Delta variant had earlier and higher peak in viral load dynamics but a shorter duration of infection. Selection for increased transmissibility shapes the viral load dynamics, and the isolation measure is likely to be a driver of these evolutionary transitions. In addition, we show that a decreased incubation period and an increased proportion of asymptomatic infection are also positively selected for as SARS-CoV-2 mutated to adapt to human behavior (i.e., Omicron variants). The quantitative information and predictions we present here can guide future responses in the potential arms race between pandemic interventions and viral evolution."
"
Your phone may have more than 15 billion tiny transistors packed into its microprocessor chips. The transistors are made of silicon, metals like gold and copper, and insulators that together take an electric current and convert it to 1s and 0s to communicate information and store it. The transistor materials are inorganic, basically derived from rock and metal.

But what if you could make these fundamental electronic components part biological, able to respond directly to the environment and change like living tissue?
This is what a team at Tufts University Silklab did when they created transistors replacing the insulating material with biological silk. They reported their findings in Advanced Materials.
Silk fibroin -- the structural protein of silk fibers -- can be precisely deposited onto surfaces and easily modified with other chemical and biological molecules to change its properties. Silk functionalized in this manner can pick up and detect a wide range of components from the body or environment.
The team's first demonstration of a prototype device used the hybrid transistors to make a highly sensitive and ultrafast breath sensor, detecting changes in humidity. Further modifications of the silk layer could enable devices to detect some cardiovascular and pulmonary diseases, as well as sleep apnea, or pick up carbon dioxide levels and other gases and molecules in the breath that might provide diagnostic information. Used with blood plasma, they could potentially provide information on levels of oxygenation and glucose, circulating antibodies, and more.
Prior to the development of the hybrid transistors, the Silklab, led by Fiorenzo Omenetto, the Frank C. Doble Professor of engineering, had already used fibroin to make bioactive inks for fabrics that can detect changes in the environment or on the body, sensing tattoos that can be placed under the skin or on the teeth to monitor health and diet, and sensors that can be printed on any surface to detect pathogens like the virus responsible for COVID19.
How It Works
A transistor is simply an electrical switch, with a metal electrical lead coming in and another going out. In between the leads is the semiconductor material, so-called because it's not able to conduct electricity unless coaxed.

Another source of electrical input called a gate is separated from everything else by an insulator. The gate acts as the ""key"" to turn the transistor on and off. It triggers the on-state when a threshold voltage- which we will call ""1"" -- creates an electric field across the insulator, priming electron movement in the semiconductor and starting the flow of current through the leads.
In a biological hybrid transistor, a silk layer is used as the insulator, and when it absorbs moisture, it acts like a gel carrying whatever ions (electrically charged molecules) are contained within. The gate triggers the on-state by rearranging ions in the silk gel. By changing the ionic composition in the silk, the transistor operation changes, allowing it to be triggered by any gate value between zero and one.
""You could imagine creating circuits that make use of information that is not represented by the discrete binary levels used in digital computing, but can process variable information as in analog computing, with the variation caused by changing what's inside the silk insulator"" said Omenetto. ""This opens up the possibility of introducing biology into computing within modern microprocessors,"" said Omenetto. Of course, the most powerful known biological computer is the brain, which processes information with variable levels of chemical and electrical signals.
The technical challenge in creating hybrid biological transistors was to achieve silk processing at the nanoscale, down to 10nm or less than 1/10000th the diameter of a human hair. ""Having achieved that, we can now make hybrid transistors with the same fabrication processes that are used for commercial chip manufacturing,"" said Beom Joon Kim, postdoctoral researcher at the School of Engineering. ""This means you can make a billion of these with capabilities available today.""
Having billions of transistor nodes with connections reconfigured by biological processes in the silk could lead to microprocessors which could act like the neural networks used in AI. ""Looking ahead, one could imagine have integrated circuits that train themselves, respond to environmental signals, and record memory directly in the transistors rather than sending it to separate storage,"" said Omenetto.
Devices detecting and responding to more complex biological states, as well as large-scale analog and neuromorphic computing are yet to be created. Omenetto is optimistic for future opportunities. ""This opens up a new way of thinking about the interface between electronics and biology, with many important fundamental discoveries and applications ahead.""

","score: 14.95241270369139, grade_level: '15'","score: 15.686167692051875, grade_levels: ['college_graduate'], ages: [24, 100]",10.1002/adma.202302062,"In recent years, increased control over naturally derived structural protein formulations and their self‐assembly has enabled the application of high‐resolution manufacturing techniques to silk‐based materials, leading to bioactive interfaces with unprecedented miniaturized formats and functionalities. Here, a hybrid biopolymer–semiconductor device, obtained by integrating nanoscale silk layers in a well‐established class of inorganic field‐effect transistors (silk‐FETs), is presented. The devices offer two distinct modes of operation—either traditional field‐effect or electrolyte‐gated—enabled by the precisely controlled thickness, morphology, and biochemistry of the integrated silk layers. The different operational modes are selectively accessed by dynamically modulating the free‐water content within the nanoscale protein layer from the vapor phase. The utility of these hybrid devices is illustrated in a highly sensitive and ultrafast breath sensor, highlighting the opportunities offered by the integration of nanoscale biomaterial interfaces in conjunction with traditional semiconductor devices, enabling functional outcomes at the intersection between the worlds of microelectronics and biology."
"
A new artificial intelligence computer program created by researchers at the University of Florida and NVIDIA can generate doctors' notes so well that two physicians couldn't tell the difference, according to an early study from both groups.

In this proof-of-concept study, physicians reviewed patient notes -- some written by actual medical doctors while others were created by the new AI program -- and the physicians identified the correct author only 49% of the time.
A team of 19 researchers from NVIDIA and the University of Florida said their findings, published Nov. 16 in the Nature journal npj Digital Medicine, open the door for AI to support health care workers with groundbreaking efficiencies.
The researchers trained supercomputers to generate medical records based on a new model, GatorTronGPT, that functions similarly to ChatGPT. The free versions of GatorTron™ models have more than 430,000 downloads from Hugging Face, an open-source AI website. GatorTron™ models are the site's only models available for clinical research, according to the article's lead author Yonghui Wu, Ph.D., from the UF College of Medicine's department of health outcomes and biomedical informatics.
""In health care, everyone is talking about these models. GatorTron™ and GatorTronGPT are unique AI models that can power many aspects of medical research and health care. Yet, they require massive data and extensive computing power to build. We are grateful to have this supercomputer, HiPerGator, from NVIDIA to explore the potential of AI in health care,"" Wu said.
UF alumnus and NVIDIA co-founder Chris Malachowsky is the namesake of UF's new Malachowsky Hall for Data Science & Information Technology. A public-private partnership between UF and NVIDIA helped to fund this $150 million structure. In 2021, UF upgraded its HiPerGator supercomputer to elite status with a multimillion-dollar infrastructure package from NVIDIA, the first at a university.
For this research, Wu and his colleagues developed a large language model that allows computers to mimic natural human language. These models work well with standard writing or conversations, but medical records bring additional hurdles, such as needing to protect patients' privacy and being highly technical. Digital medical records cannot be Googled or shared on Wikipedia.

To overcome these obstacles, the researchers stripped UF Health medical records of identifying information from 2 million patients while keeping 82 billion useful medical words. Combining this set with another dataset of 195 billion words, they trained the GatorTronGPT model to analyze the medical data with GPT-3 architecture, or Generative Pre-trained Transformer, a form of neural network architecture. That allowed GatorTronGPT to write clinical text similar to medical doctors' notes.
""This GatorTronGPT model is one of the first major products from UF's initiative to incorporate AI across the university. We are so pleased with how the partnership with NVIDIA is already bearing fruit and setting the stage for the future of medicine,"" said Elizabeth Shenkman, Ph.D., a co-author and chair of UF's department of health outcomes and biomedical informatics.
Of the many possible uses for a medical GPT, one idea involves replacing the tedium of documentation with notes recorded and transcribed by AI. Wu says that UF has an innovation center that is pursuing a commercial version of the software.
For an AI tool to reach such parity with human writing, programmers spend weeks programming supercomputers with clinical vocabulary and language usage based on billions upon billions of words. One resource providing the necessary clinical data is the OneFlorida+ Clinical Research Network, coordinated at UF and representing many health care systems.
""It's critical to have such massive amounts of UF Health clinical data not only available but ready for AI. Only a supercomputer could handle such a big dataset of 277 billion words. We are excited to implement GatorTron™ and GatorTronGPT models to real-world health care at UF Health,"" said Jiang Bian, Ph.D., a co-author and UF Health's chief data scientist and chief research information officer.
A cross-section of 14 UF and UF Health faculty contributed to this study, including researchers from Research Computing, Integrated Data Repository Research Services within the Clinical and Translational Science Institute, and from departments and divisions within the College of Medicine, including neurosurgery, endocrinology, diabetes and metabolism, cardiovascular medicine, and health outcomes and biomedical informatics.
The study was partially funded by grants from the Patient-Centered Outcomes Research Institute, the National Cancer Institute and the National Institute on Aging.
Here are two paragraphs that reference two patient cases one written by a human and one created by GatorTronGPT -- can you tell whether the author was machine or human?

","score: 15.044042245739302, grade_level: '15'","score: 16.273239831697055, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41746-023-00958-w,"There are enormous enthusiasm and concerns in applying large language models (LLMs) to healthcare. Yet current assumptions are based on general-purpose LLMs such as ChatGPT, which are not developed for medical use. This study develops a generative clinical LLM, GatorTronGPT, using 277 billion words of text including (1) 82 billion words of clinical text from 126 clinical departments and approximately 2 million patients at the University of Florida Health and (2) 195 billion words of diverse general English text. We train GatorTronGPT using a GPT-3 architecture with up to 20 billion parameters and evaluate its utility for biomedical natural language processing (NLP) and healthcare text generation. GatorTronGPT improves biomedical natural language processing. We apply GatorTronGPT to generate 20 billion words of synthetic text. Synthetic NLP models trained using synthetic text generated by GatorTronGPT outperform models trained using real-world clinical text. Physicians’ Turing test using 1 (worst) to 9 (best) scale shows that there are no significant differences in linguistic readability (p = 0.22; 6.57 of GatorTronGPT compared with 6.93 of human) and clinical relevance (p = 0.91; 7.0 of GatorTronGPT compared with 6.97 of human) and that physicians cannot differentiate them (p < 0.001). This study provides insights into the opportunities and challenges of LLMs for medical research and healthcare."
"
Intense focus pervades the EEG laboratory at the University of Konstanz on this day of experimentation. In separate labs, two participants, connected by screens, engage in the computer game Pacman. The burning question: Can strangers, unable to communicate directly, synchronize their efforts to conquer the digital realm together?

Doctoral candidate Karl-Philipp Flösch is leading today's experiment. He states: ""Our research revolves around cooperative behaviour and the adoption of social roles."" However, understanding brain processes underlying cooperative behaviour is still in its infancy, presenting a central challenge for cognitive neuroscience. How can cooperative behaviour be brought into a highly structured EEG laboratory environment without making it feel artificial or boring for study participants?
Pacman as a scientific ""playground""
The research team, led by Harald Schupp, Professor of Biological Psychology at the University of Konstanz, envisioned using the well-known computer game Pacman as a natural medium to study cooperative behaviour in the EEG laboratory. Conducting the study as part of the Cluster of Excellence Centre for the Advanced Study of Collective Behaviour, they recently published their findings in Psychophysiology.
""Pacman is a cultural icon. Many have navigated the voracious Pacman through mazes in their youth, aiming to devour fruits and outsmart hostile ghosts,"" reminisces Karl-Philipp Flösch. Collaborating with colleagues, co-author Tobias Flaisch adapted the game. In the EEG version, two players instead of one must collaboratively guide Pacman to the goal. Flaisch explains: ""Success hinges on cooperative behaviour, as players must seamlessly work together.""
However, the researchers have built in a special hurdle: the labyrinth's path is concealed. Only one of the two players can see where Pacman is going next. Flösch elaborates: ""The active player can communicate the direction to the partner, but only indirectly using pre-agreed symbols, communicated solely through the computer screen."" If you do not remember quickly enough that a crescent moon on the screen means that Pacman should move right, and that only the banana on the keyboard can make Pacman move to the right, you're making a mistake. ""From the perspective of classical psychological research, the game combines various skills inherent in natural social situations,"" notes Harald Schupp.
EEG measures event-related potentials
During each game, the players' brain reactions were measured using EEG. Calculating event-related potentials provides a detailed view of the effects elicited by different game roles with millisecond-level temporal precision. The team hypothesized that the game role significantly influences brain reactions. Therefore, they examined the P3 component, a well-studied brain reaction exhibiting a stronger deflection in the presence of significant and task-relevant stimuli. The results confirmed their assumption: ""The P3 was increased not only when the symbol indicated the next move's direction but also when observing whether the game partner selected the correct symbol,"" says Flösch. The team concludes that the role we take on during cooperation determines the informational value of environmental stimuli situationally. EEG measurements allow the brain processes involved to be dynamically mapped.
""Cooperative role adoption structures our entire society,"" summarizes Schupp, providing context for the study. ""An individual achieves little alone, but collectively, humanity even reaches the moon. Our technological society hinges on cooperative behavior,"" says Flösch, adding that children early on take individual roles, thereby learning the art of complex cooperation. Consequently, this role adoption occurs nearly effortlessly and automatically for us every day. ""Our brains are practically 'built' for it, as evidenced by the results of our study.""

","score: 14.008798359383661, grade_level: '14'","score: 14.493333887595611, grade_levels: ['college_graduate'], ages: [24, 100]",10.1111/psyp.14433,"Humans are highly co‐operative and thus cognitively, affectively, and motivationally tuned to pursue shared goals. Yet, cooperative tasks typically require people to constantly take and switch individual roles. Task relevance is dictated by these roles and thereby dynamically changing. Here, we designed a dyadic game to test whether the family of P3 components can trace this dynamic allocation of task relevance. We demonstrate that late positive event‐related potential (ERP) modulations not only reflect predictable asymmetries between receiving and sending information but also differentiate whether the receiver's role is related to correct decision making or action monitoring. Furthermore, similar results were observed when playing the game with a computer, suggesting that experimental games may motivate humans to similarly cooperate with an artificial agent. Overall, late positive ERP waves provide a real‐time measure of how role taking dynamically shapes the meaning and relevance of stimuli within collaborative contexts. Our results, therefore, shed light on how the processes of mutual coordination unfold during dyadic cooperation."
"
Machine learning algorithms designed to diagnose a common infection that affects women showed a diagnostic bias among ethnic groups, University of Florida researchers found.

While artificial intelligence tools offer great potential for improving health care delivery, practitioners and scientists warn of their risk for perpetuating racial inequities. Published Friday in the Nature journal Digital Medicine, this is the first paper to evaluate fairness among these tools in connection to a women's health issue.
""Machine learning can be a great tool in medical diagnostics, but we found it can show bias toward different ethnic groups,"" said Ruogu Fang, an associate professor in the J. Crayton Pruitt Family Department of Biomedical Engineering and the study's author. ""This is alarming for women's health as there already are existing disparities that vary by ethnicity.""
The researchers evaluated the fairness of machine learning in diagnosing bacterial vaginosis, or BV, a common condition affecting women of reproductive age, which has clear diagnostic differences among ethnic groups.
Fang and co-corresponding author Ivana Parker, both faculty members in the Herbert Wertheim College of Engineering, pulled data from 400 women, comprising 100 from each of the ethnic groups represented -- white, Black, Asian, and Hispanic.
In investigating the ability of four machine learning models to predict BV in women with no symptoms, researchers say the accuracy varied among ethnicities. Hispanic women had the most false-positive diagnoses, and Asian women received the most false-negative. Algorithm
""The models performed highest for white women and lowest for Asian women,"" said the Parker, an assistant professor of bioengineering. ""This tells us machine learning methods are not treating ethnic groups equally well.""
Parker said that while they were interested in understanding how AI tools predict disease for specific ethnicities, their study also helps medical scientists understand the factors associated with bacteria in women of varying ethnic backgrounds, which can lead to improved treatments.

BV, one of the most common vaginal infections, can cause discomfort and pain and happens when natural bacteria levels are out of balance. While there are symptoms associate with BV, many people have no symptoms, making it difficult to diagnose.
It doesn't often cause complications, but in some cases, BV can increase the risk of sexually transmitted infections, miscarriage, and premature births.
The researchers said their findings demonstrate the need for improved methods for building the AI tools to mitigate health care bias.

","score: 15.179483082706767, grade_level: '15'","score: 17.008674812030073, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41746-023-00953-1,"While machine learning (ML) has shown great promise in medical diagnostics, a major challenge is that ML models do not always perform equally well among ethnic groups. This is alarming for women’s health, as there are already existing health disparities that vary by ethnicity. Bacterial Vaginosis (BV) is a common vaginal syndrome among women of reproductive age and has clear diagnostic differences among ethnic groups. Here, we investigate the ability of four ML algorithms to diagnose BV. We determine the fairness in the prediction of asymptomatic BV using 16S rRNA sequencing data from Asian, Black, Hispanic, and white women. General purpose ML model performances vary based on ethnicity. When evaluating the metric of false positive or false negative rate, we find that models perform least effectively for Hispanic and Asian women. Models generally have the highest performance for white women and the lowest for Asian women. These findings demonstrate a need for improved methodologies to increase model fairness for predicting BV."
"
Treating cancer is becoming increasingly complex, but also offers more and more possibilities. After all, the better a tumor's biology and genetic features are understood, the more treatment approaches there are. To be able to offer patients personalized therapies tailored to their disease, laborious and time-consuming analysis and interpretation of various data is required. Researchers at Charité -- Universitätsmedizin Berlin and Humboldt-Universität zu Berlin have now studied whether generative artificial intelligence (AI) tools such as ChatGPT can help with this step. This is one of many projects at Charité analyzing the opportunities unlocked by AI in patient care.

If the body can no longer repair certain genetic mutations itself, cells begin to grow unchecked, producing a tumor. The crucial factor in this phenomenon is an imbalance of growth-inducing and growth-inhibiting factors, which can result from changes in oncogenes -- genes with the potential to cause cancer -- for example. Precision oncology, a specialized field of personalized medicine, leverages this knowledge by using specific treatments such as low-molecular weight inhibitors and antibodies to target and disable hyperactive oncogenes.
The first step in identifying which genetic mutations are potential targets for treatment is to analyze the genetic makeup of the tumor tissue. The molecular variants of the tumor DNA that are necessary for precision diagnosis and treatment are determined. Then the doctors use this information to craft individual treatment recommendations. In especially complex cases, this requires knowledge from various fields of medicine. At Charité, this is when the ""molecular tumor board"" (MTB) meets: Experts from the fields of pathology, molecular pathology, oncology, human genetics, and bioinformatics work together to analyze which treatments seem most promising based on the latest studies. It is a very involved process, ultimately culminating in a personalized treatment recommendation.
Can artificial intelligence help with treatment decisions?
Dr. Damian Rieke, a doctor at Charité, Prof. Ulf Leser and Xing David Wang of Humboldt-Universität zu Berlin, and Dr. Manuela Benary, a bioinformatics specialist at Charité, wondered whether artificial intelligence might be able to help at this juncture. In a study just recently published in the journal JAMA Network Open, they worked with other researchers to examine the possibilities and limitations of large language models such as ChatGPT in automatically scanning scientific literature with an eye to selecting personalized treatments.
""We prompted the models to identify personalized treatment options for fictitious cancer patients and then compared the results with the recommendations made by experts,"" Rieke explains. His conclusion: ""AI models were able to identify personalized treatment options in principle -- but they weren't even close to the abilities of human experts.""
The team created ten molecular tumor profiles of fictitious patients for the experiment. A human physician specialist and four large language models were then tasked with identifying a personalized treatment option. These results were presented to the members of the MTB for assessment, without them knowing where which recommendation came from.

Improved AI models hold promise for future uses
""There were some surprisingly good treatment options identified by AI in isolated cases,"" Benary reports. ""But large language models perform much worse than human experts."" Beyond that, data protection, privacy, and reproducibility pose particular challenges in relation to the use of artificial intelligence with real-world patients, she notes.
Still, Rieke is fundamentally optimistic about the potential uses of AI in medicine: ""In the study, we also showed that the performance of AI models is continuing to improve as the models advance. This could mean that AI can provide more support for even complex diagnostic and treatment processes in the future -- as long as humans are the ones to check the results generated by AI and have the final say about treatment.""
AI projects at Charité aim to improve patient care 
Prof. Felix Balzer, Director of the Institute of Medical Informatics, is also certain medicine will benefit from AI. In his role as Chief Medical Information Officer (CMIO) within IT, he is responsible for the digital transformation of patient care at Charité. ""One special area of focus when it comes to greater efficiency in patient care is digitalization, which also means the use of automation and artificial intelligence,"" Balzer explains.
His institute is working on AI models to help with fall prevention in long-term care, for example. Other areas at Charité are also conducting extensive research on AI: The Charité Lab for Artificial Intelligence in Medicine is working to develop tools for AI-based prognosis following strokes, and the TEF-Health project, led by Prof. Petra Ritter of the Berlin Institute of Health at Charité (BIH), is working to facilitate the validation and certification of AI and robotics in medical devices.

","score: 15.671372107969152, grade_level: '16'","score: 16.449630462724933, grade_levels: ['college_graduate'], ages: [24, 100]",10.1001/jamanetworkopen.2023.43689,"Clinical interpretation of complex biomarkers for precision oncology currently requires manual investigations of previous studies and databases. Conversational large language models (LLMs) might be beneficial as automated tools for assisting clinical decision-making. To assess performance and define their role using 4 recent LLMs as support tools for precision oncology. This diagnostic study examined 10 fictional cases of patients with advanced cancer with genetic alterations. Each case was submitted to 4 different LLMs (ChatGPT, Galactica, Perplexity, and BioMedLM) and 1 expert physician to identify personalized treatment options in 2023. Treatment options were masked and presented to a molecular tumor board (MTB), whose members rated the likelihood of a treatment option coming from an LLM on a scale from 0 to 10 (0, extremely unlikely; 10, extremely likely) and decided whether the treatment option was clinically useful. Number of treatment options, precision, recall, F1 score of LLMs compared with human experts, recognizability, and usefulness of recommendations. For 10 fictional cancer patients (4 with lung cancer, 6 with other; median [IQR] 3.5 [3.0-4.8] molecular alterations per patient), a median (IQR) number of 4.0 (4.0-4.0) compared with 3.0 (3.0-5.0), 7.5 (4.3-9.8), 11.5 (7.8-13.0), and 13.0 (11.3-21.5) treatment options each was identified by the human expert and 4 LLMs, respectively. When considering the expert as a criterion standard, LLM-proposed treatment options reached F1 scores of 0.04, 0.17, 0.14, and 0.19 across all patients combined. Combining treatment options from different LLMs allowed a precision of 0.29 and a recall of 0.29 for an F1 score of 0.29. LLM-generated treatment options were recognized as AI-generated with a median (IQR) 7.5 (5.3-9.0) points in contrast to 2.0 (1.0-3.0) points for manually annotated cases. A crucial reason for identifying AI-generated treatment options was insufficient accompanying evidence. For each patient, at least 1 LLM generated a treatment option that was considered helpful by MTB members. Two unique useful treatment options (including 1 unique treatment strategy) were identified only by LLM. In this diagnostic study, treatment options of LLMs in precision oncology did not reach the quality and credibility of human experts; however, they generated helpful ideas that might have complemented established procedures. Considering technological progress, LLMs could play an increasingly important role in assisting with screening and selecting relevant biomedical literature to support evidence-based, personalized treatment decisions."
"
When researchers asked hundreds of people to watch other people shake boxes, it took just seconds for almost all of them to figure out what the shaking was for.

The deceptively simple work by Johns Hopkins University perception researchers is the first to demonstrate that people can tell what others are trying to learn just by watching their actions. Published today in the journal Proceedings of the National Academy of Sciences, the study reveals a key yet neglected aspect of human cognition, and one with implications for artificial intelligence.
""Just by looking at how someone's body is moving, you can tell what they are trying to learn about their environment,"" said author Chaz Firestone, an assistant professor of psychological and brain sciences who investigates how vision and thought interact. ""We do this all the time, but there has been very little research on it.""
Recognizing another person's actions is something we do every day, whether it's guessing which way someone is headed or figuring out what object they're reaching for. These are known as ""pragmatic actions."" Numerous studies have shown people can quickly and accurately identify these actions just by watching them. The new Johns Hopkins work investigates a different kind of behavior: ""epistemic actions,"" which are performed when someone is trying to learn something.
For instance, someone might put their foot in a swimming pool because they're going for a swim or they might put their foot in a pool to test the water. Though the actions are similar, there are differences and the Johns Hopkins team surmised observers would be able to detect another person's ""epistemic goals"" just by watching them.
Across several experiments, researchers asked a total of 500 participants to watch two videos in which someone picks up a box full of objects and shakes it around. One shows someone shaking a box to figure out the number of objects inside it. The other shows someone shaking a box to figure out the shape of the objects inside. Almost every participant knew who was shaking for the number and who was shaking for shape.
""What is surprising to me is how intuitive this is,"" said lead author Sholei Croom, a Johns Hopkins graduate student. ""People really can suss out what others are trying to figure out, which shows how we can make these judgments even though what we're looking at is very noisy and changes from person to person.""
Added Firestone, ""When you think about all the mental calculations someone must make to understand what someone else is trying to learn, it's a remarkably complicated process. But our findings show it's something people do easily.""

The findings could also inform the development of artificial intelligence systems designed to interact with humans. A commercial robot assistant, for example, that can look at a customer and guess what they're looking for.
""It's one thing to know where someone is headed or what product they are reaching for,"" Firestone said. ""But it's another thing to infer whether someone is lost or what kind of information they are seeking.""
In the future the team would like to pursue whether people can observe someone's epistemic intent versus their pragmatic intent -- what are they up to when they dip their foot in the pool. They're also interested in when these observational skills emerge in human development and if it's possible to build computational models to detail exactly how observed physical actions reveal epistemic intent.
The Johns Hopkins team also included Hanbei Zhou, a sophomore studying neuroscience.

","score: 11.855904228222112, grade_level: '12'","score: 13.049159449821701, grade_levels: ['college_graduate'], ages: [24, 100]",10.1073/pnas.2303162120,"Many actions have instrumental aims, in which we move our bodies to achieve a physical outcome in the environment. However, we also perform actions with epistemic aims, in which we move our bodies to acquire information and learn about the world. A large literature on action recognition investigates how observers represent and understand the former class of actions; but what about the latter class? Can one person tell, just by observing another person’s movements, what they are trying to learn? Here, five experiments exploreepistemic action understanding. We filmed volunteers playing a “physics game” consisting of two rounds: Players shook an opaque box and attempted to determine i) the number of objects hidden inside, or ii) the shape of the objects inside. Then, independent subjects watched these videos and were asked to determine which videos came from which round: Who was shaking for number and who was shaking for shape? Across several variations, observers successfully determined what an actor was trying to learn, based only on their actions (i.e., how they shook the box)—even when the box’s contents were identical across rounds. These results demonstrate that humans can infer epistemic intent from physical behaviors, adding a new dimension to research on action understanding."
"
Cambridge scientists have shown that placing physical constraints on an artificially-intelligent system -- in much the same way that the human brain has to develop and operate within physical and biological constraints -- allows it to develop features of the brains of complex organisms in order to solve tasks.

As neural systems such as the brain organise themselves and make connections, they have to balance competing demands. For example, energy and resources are needed to grow and sustain the network in physical space, while at the same time optimising the network for information processing. This trade-off shapes all brains within and across species, which may help explain why many brains converge on similar organisational solutions.
Jascha Achterberg, a Gates Scholar from the Medical Research Council Cognition and Brain Sciences Unit (MRC CBSU) at the University of Cambridge said: ""Not only is the brain great at solving complex problems, it does so while using very little energy. In our new work we show that considering the brain's problem solving abilities alongside its goal of spending as few resources as possible can help us understand why brains look like they do.""
Co-lead author Dr Danyal Akarca, also from the MRC CBSU, added: ""This stems from a broad principle, which is that biological systems commonly evolve to make the most of what energetic resources they have available to them. The solutions they come to are often very elegant and reflect the trade-offs between various forces imposed on them.""
In a study published today in Nature Machine Intelligence, Achterberg, Akarca and colleagues created an artificial system intended to model a very simplified version of the brain and applied physical constraints. They found that their system went on to develop certain key characteristics and tactics similar to those found in human brains.
Instead of real neurons, the system used computational nodes. Neurons and nodes are similar in function, in that each takes an input, transforms it, and produces an output, and a single node or neuron might connect to multiple others, all inputting information to be computed.
In their system, however, the researchers applied a 'physical' constraint on the system. Each node was given a specific location in a virtual space, and the further away two nodes were, the more difficult it was for them to communicate. This is similar to how neurons in the human brain are organised.

The researchers gave the system a simple task to complete -- in this case a simplified version of a maze navigation task typically given to animals such as rats and macaques when studying the brain, where it has to combine multiple pieces of information to decide on the shortest route to get to the end point.
One of the reasons the team chose this particular task is because to complete it, the system needs to maintain a number of elements -- start location, end location and intermediate steps -- and once it has learned to do the task reliably, it is possible to observe, at different moments in a trial, which nodes are important. For example, one particular cluster of nodes may encode the finish locations, while others encode the available routes, and it is possible to track which nodes are active at different stages of the task.
Initially, the system does not know how to complete the task and makes mistakes. But when it is given feedback it gradually learns to get better at the task. It learns by changing the strength of the connections between its nodes, similar to how the strength of connections between brain cells changes as we learn. The system then repeats the task over and over again, until eventually it learns to perform it correctly.
With their system, however, the physical constraint meant that the further away two nodes were, the more difficult it was to build a connection between the two nodes in response to the feedback. In the human brain, connections that span a large physical distance are expensive to form and maintain.
When the system was asked to perform the task under these constraints, it used some of the same tricks used by real human brains to solve the task. For example, to get around the constraints, the artificial systems started to develop hubs -- highly connected nodes that act as conduits for passing information across the network.
More surprising, however, was that the response profiles of individual nodes themselves began to change: in other words, rather than having a system where each node codes for one particular property of the maze task, like the goal location or the next choice, nodes developed a flexible coding scheme. This means that at different moments in time nodes might be firing for a mix of the properties of the maze. For instance, the same node might be able to encode multiple locations of a maze, rather than needing specialised nodes for encoding specific locations. This is another feature seen in the brains of complex organisms.

Co-author Professor Duncan Astle, from Cambridge's Department of Psychiatry, said: ""This simple constraint -- it's harder to wire nodes that are far apart -- forces artificial systems to produce some quite complicated characteristics. Interestingly, they are characteristics shared by biological systems like the human brain. I think that tells us something fundamental about why our brains are organised the way they are.""
Understanding the human brain
The team are hopeful that their AI system could begin to shed light on how these constraints, shape differences between people's brains, and contribute to differences seen in those that experience cognitive or mental health difficulties.
Co-author Professor John Duncan from the MRC CBSU said: ""These artificial brains give us a way to understand the rich and bewildering data we see when the activity of real neurons is recorded in real brains.""
Achterberg added: ""Artificial 'brains' allow us to ask questions that it would be impossible to look at in an actual biological system. We can train the system to perform tasks and then play around experimentally with the constraints we impose, to see if it begins to look more like the brains of particular individuals.""
Implications for designing future AI systems
The findings are likely to be of interest to the AI community, too, where they could allow for the development of more efficient systems, particularly in situations where there are likely to be physical constraints.
Dr Akarca said: ""AI researchers are constantly trying to work out how to make complex, neural systems that can encode and perform in a flexible way that is efficient. To achieve this, we think that neurobiology will give us a lot of inspiration. For example, the overall wiring cost of the system we've created is much lower than you would find in a typical AI system.""
Many modern AI solutions involve using architectures that only superficially resemble a brain. The researchers say their works shows that the type of problem the AI is solving will influence which architecture is the most powerful to use.
Achterberg said: ""If you want to build an artificially-intelligent system that solves similar problems to humans, then ultimately the system will end up looking much closer to an actual brain than systems running on large compute cluster that specialise in very different tasks to those carried out by humans. The architecture and structure we see in our artificial 'brain' is there because it is beneficial for handling the specific brain-like challenges it faces.""
This means that robots that have to process a large amount of constantly changing information with finite energetic resources could benefit from having brain structures not dissimilar to ours.
Achterberg added: ""Brains of robots that are deployed in the real physical world are probably going to look more like our brains because they might face the same challenges as us. They need to constantly process new information coming in through their sensors while controlling their bodies to move through space towards a goal. Many systems will need to run all their computations with a limited supply of electric energy and so, to balance these energetic constraints with the amount of information it needs to process, it will probably need a brain structure similar to ours.""
The research was funded by the Medical Research Council, Gates Cambridge, the James S McDonnell Foundation, Templeton World Charity Foundation and Google DeepMind.

","score: 13.98618275862069, grade_level: '14'","score: 15.955409482758618, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s42256-023-00748-9,"Brain networks exist within the confines of resource limitations. As a result, a brain network must overcome the metabolic costs of growing and sustaining the network within its physical space, while simultaneously implementing its required information processing. Here, to observe the effect of these processes, we introduce the spatially embedded recurrent neural network (seRNN). seRNNs learn basic task-related inferences while existing within a three-dimensional Euclidean space, where the communication of constituent neurons is constrained by a sparse connectome. We find that seRNNs converge on structural and functional features that are also commonly found in primate cerebral cortices. Specifically, they converge on solving inferences using modular small-world networks, in which functionally similar units spatially configure themselves to utilize an energetically efficient mixed-selective code. Because these features emerge in unison, seRNNs reveal how many common structural and functional brain motifs are strongly intertwined and can be attributed to basic biological optimization processes. seRNNs incorporate biophysical constraints within a fully artificial system and can serve as a bridge between structural and functional research communities to move neuroscientific understanding forwards."
"
Top Olympic achievers are awarded the gold medal, a symbol revered for wealth and honor both in the East and the West. This metal also serves as a key element in diverse fields due to its stability in air, exceptional electrical conductivity, and biocompatibility. It's highly favored in medical and energy sectors as the 'preferred catalyst' and is increasingly finding application in cutting-edge wearable technologies.

A research team led by Professor Sei Kwang Hahn and Dr. Tae Yeon Kim from the Department of Materials Science and Engineering at Pohang University of Science and Technology (POSTECH) developed an integrated wearable sensor device that effectively measures and processes two bio-signals simultaneously. Their research findings were featured in Advanced Materials, an international top journal in the materials field.
Wearable devices, available in various forms like attachments and patches, play a pivotal role in detecting physical, chemical, and electrophysiological signals for disease diagnosis and management. Recent strides in research focus on devising wearables capable of measuring multiple bio-signals concurrently. However, a major challenge has been the disparate materials needed for each signal measurement, leading to interface damage, complex fabrication, and reduced device stability. Additionally, these varied signals analysis requires further signal processing systems and algorithms.
The team tackled this challenge using various shapes of gold (Au) nanowires. While silver (Ag) nanowires, known for their extreme thinness, lightness, and conductivity, are commonly used in wearable devices, the team fused them with gold. Initially, they developed bulk gold nanowires by coating the exterior of the silver nanowires, suppressing the galvanic phenomenon. Subsequently, they created hollow gold nanowires by selectively etching the silver from the gold-coated nanowires. The bulk gold nanowires responded sensitively to temperature variations, whereas the hollow gold nanowires showed high sensitivity to minute changes in strain.
These nanowires were then patterned onto a substrate made of styrene-ethylene-butylene-styrene (SEBS) polymer, seamlessly integrated without separations. By leveraging two types of gold nanowires, each with distinct properties, they engineered an integrated sensor capable of measuring both temperature and strain. Additionally, they engineered a logic circuit for signal analysis, utilizing the negative gauge factor resulting from introducing micrometer-scale corrugations into the pattern. This approach led to the successful creation of an intelligent wearable device system that not only captures but also analyzes signals simultaneously, all using a single material of Au.
The team's sensors exhibited remarkable performance in detecting subtle muscle tremors, identifying heartbeat patterns, recognizing speech through vocal cord tremors, and monitoring changes in body temperature. Notably, these sensors maintained high stability without causing damage to the material interfaces. Their flexibility and excellent stretchability enabled them to conform to curved skin seamlessly.
Professor Sei Kwang Hahn stated, ""This research underscores the potential for the development of a futuristic bioelectronics platform capable of analyzing a diverse range of bio-signals."" He added, ""We envision new prospects across various industries including healthcare and integrated electronic systems.""
The research was sponsored by the Basic Research Program and the Biomedical Technology Development Program of the National Research Foundation of Korea, and POSCO Holdings.

Top Olympic achievers are awarded the gold medal, a symbol revered for wealth and honor both in the East and the West. This metal also serves as a key element in diverse fields due to its stability in air, exceptional electrical conductivity, and biocompatibility. It's highly favored in medical and energy sectors as the 'preferred catalyst' and is increasingly finding application in cutting-edge wearable technologies.
A research team led by Professor Sei Kwang Hahn and Dr. Tae Yeon Kim from the Department of Materials Science and Engineering at Pohang University of Science and Technology (POSTECH) developed an integrated wearable sensor device that effectively measures and processes two bio-signals simultaneously. Their research findings were featured in Advanced Materials, an international top journal in the materials field.
Wearable devices, available in various forms like attachments and patches, play a pivotal role in detecting physical, chemical, and electrophysiological signals for disease diagnosis and management. Recent strides in research focus on devising wearables capable of measuring multiple bio-signals concurrently. However, a major challenge has been the disparate materials needed for each signal measurement, leading to interface damage, complex fabrication, and reduced device stability. Additionally, these varied signals analysis requires further signal processing systems and algorithms.
The team tackled this challenge using various shapes of gold (Au) nanowires. While silver (Ag) nanowires, known for their extreme thinness, lightness, and conductivity, are commonly used in wearable devices, the team fused them with gold. Initially, they developed bulk gold nanowires by coating the exterior of the silver nanowires, suppressing the galvanic phenomenon. Subsequently, they created hollow gold nanowires by selectively etching the silver from the gold-coated nanowires. The bulk gold nanowires responded sensitively to temperature variations, whereas the hollow gold nanowires showed high sensitivity to minute changes in strain.
These nanowires were then patterned onto a substrate made of styrene-ethylene-butylene-styrene (SEBS) polymer, seamlessly integrated without separations. By leveraging two types of gold nanowires, each with distinct properties, they engineered an integrated sensor capable of measuring both temperature and strain. Additionally, they engineered a logic circuit for signal analysis, utilizing the negative gauge factor resulting from introducing micrometer-scale corrugations into the pattern. This approach led to the successful creation of an intelligent wearable device system that not only captures but also analyzes signals simultaneously, all using a single material of Au.
The team's sensors exhibited remarkable performance in detecting subtle muscle tremors, identifying heartbeat patterns, recognizing speech through vocal cord tremors, and monitoring changes in body temperature. Notably, these sensors maintained high stability without causing damage to the material interfaces. Their flexibility and excellent stretchability enabled them to conform to curved skin seamlessly.
Professor Sei Kwang Hahn stated, ""This research underscores the potential for the development of a futuristic bioelectronics platform capable of analyzing a diverse range of bio-signals."" He added, ""We envision new prospects across various industries including healthcare and integrated electronic systems.""
The research was sponsored by the Basic Research Program and the Biomedical Technology Development Program of the National Research Foundation of Korea, and POSCO Holdings.

","score: 16.87038778877888, grade_level: '17'","score: 17.733229372937295, grade_levels: ['college_graduate'], ages: [24, 100]",10.1002/adma.202303401,"Although multifunctional wearable devices have been widely investigated for healthcare systems, augmented/virtual realities, and telemedicines, there are few reports on multiple signal monitoring and logical signal processing by using one single nanomaterial without additional algorithms or rigid application‐specific integrated circuit chips. Here, multifunctional intelligent wearable devices are developed using monolithically patterned gold nanowires for both signal monitoring and processing. Gold bulk and hollow nanowires show distinctive electrical properties with high chemical stability and high stretchability. In accordance, the monolithically patterned gold nanowires can be used to fabricate the robust interfaces, programmable sensors, on‐demand heating systems, and strain‐gated logical circuits. The stretchable sensors show high sensitivity for strain and temperature changes on the skin. Furthermore, the micro‐wrinkle structures of gold nanowires exhibit the negative gauge factor, which can be used for strain‐gated logical circuits. Taken together, this multifunctional intelligent wearable device would be harnessed as a promising platform for futuristic electronic and biomedical applications."
"
Quantum scientists have discovered a rare phenomenon that could hold the key to creating a 'perfect switch' in quantum devices which flips between being an insulator and superconductor.

The research, led by the University of Bristol and published in Science, found these two opposing electronic states exist within purple bronze, a unique one-dimensional metal composed of individual conducting chains of atoms.
Tiny changes in the material, for instance prompted by a small stimulus like heat or light, may trigger an instant transition from an insulating state with zero conductivity to a superconductor with unlimited conductivity, and vice versa. This polarised versatility, known as 'emergent symmetry', has the potential to offer an ideal On/Off switch in future quantum technology developments.
Lead author Nigel Hussey, Professor of Physics at the University of Bristol, said: ""It's a really exciting discovery which could provide a perfect switch for quantum devices of tomorrow.
""The remarkable journey started 13 years ago in my lab when two PhD students, Xiaofeng Xu and Nick Wakeham, measured the magnetoresistance -- the change in resistance caused by a magnetic field -- of purple bronze.""
In the absence of a magnetic field, the resistance of purple bronze was highly dependent on the direction in which the electrical current is introduced. Its temperature dependence was also rather complicated. Around room temperature, the resistance is metallic, but as the temperature is lowered, this reverses and the material appears to be turning into an insulator. Then, at the lowest temperatures, the resistance plummets again as it transitions into a superconductor. Despite this complexity, surprisingly, the magnetoresistance was found to be extremely simple. It was essentially the same irrespective of the direction in which the current or field were aligned and followed a perfect linear temperature dependence all the way from room temperature down to the superconducting transition temperature.
""Finding no coherent explanation for this puzzling behaviour, the data lay dormant and published unpublished for the next seven years. A hiatus like this is unusual in quantum research, though the reason for it was not a lack of statistics,"" Prof Hussey explained.

""Such simplicity in the magnetic response invariably belies a complex origin and as it turns out, its possible resolution would only come about through a chance encounter.""
In 2017, Prof Hussey was working at Radboud University and saw advertised a seminar by physicist Dr Piotr Chudzinski on the subject of purple bronze. At the time few researchers were devoting an entire seminar to this little-known material, so his interest was piqued.
Prof Hussey said: ""In the seminar Chudzinski proposed that the resistive upturn may be caused by interference between the conduction electrons and elusive, composite particles known as 'dark excitons'. We chatted after the seminar and together proposed an experiment to test his theory. Our subsequent measurements essentially confirmed it.""
Buoyed by this success, Prof Hussey resurrected Xu and Wakeham's magnetoresistance data and showed them to Dr Chudzinski. The two central features of the data -- the linearity with temperature and the independence on the orientation of current and field -- intrigued Chudzinski, as did the fact that the material itself could exhibit both insulating and superconducting behaviour depending on how the material was grown.
Dr Chudzinski wondered whether rather than transforming completely into an insulator, the interaction between the charge carriers and the excitons he'd introduced earlier could cause the former to gravitate towards the boundary between the insulating and superconducting states as the temperature is lowered. At the boundary itself, the probability of the system being an insulator or a superconductor is essentially the same.
Prof Hussey said: ""Such physical symmetry is an unusual state of affairs and to develop such symmetry in a metal as the temperature is lowered, hence the term 'emergent symmetry', would constitute a world-first.""
Physicists are well versed in the phenomenon of symmetry breaking: lowering the symmetry of an electron system upon cooling. The complex arrangement of water molecules in an ice crystal is an example of such broken symmetry. But the converse is an extremely rare, if not unique, occurrence. Returning to the water/ice analogy, it is as though upon cooling the ice further, the complexity of the ice crystals 'melts' once again into something as symmetric and smooth as the water droplet.

Dr Chudzinski, now a Research Fellow at Queen's University Belfast, said: ""Imagine a magic trick where a dull, distorted figure transforms into a beautiful, perfectly symmetric sphere. This is, in a nutshell, the essence of emergent symmetry. The figure in question is our material, purple bronze, while our magician is nature itself.""
To further test whether the theory held water, an additional 100 individual crystals, some insulating and others superconducting, were investigated by another PhD student, Maarten Berben, working at Radboud University.
Prof Hussey added: ""After Maarten's Herculean effort, the story was complete and the reason why different crystals exhibited such wildly different ground states became apparent. Looking ahead, it might be possible to exploit this 'edginess' to create switches in quantum circuits whereby tiny stimuli induce profound, orders-of-magnitude changes in the switch resistance.""

","score: 15.443500417710947, grade_level: '15'","score: 16.025478696741857, grade_levels: ['college_graduate'], ages: [24, 100]",10.1126/science.abp8948,"Upon cooling, condensed-matter systems typically transition into states of lower symmetry. The converse—i.e., the emergence of higher symmetry at lower temperatures—is extremely rare. In this work, we show how an unusually isotropic magnetoresistance in the highly anisotropic, one-dimensional conductor Li 0.9 Mo 6 O 17 and its temperature dependence can be interpreted as a renormalization group (RG) flow toward a so-called separatrix. This approach is equivalent to an emergent symmetry in the system. The existence of two distinct ground states, Mott insulator and superconductor, can then be traced back to two opposing RG trajectories. By establishing a direct link between quantum field theory and an experimentally measurable quantity, we uncover a path through which emergent symmetry might be identified in other candidate materials."
"
As possibilities have changed and technology has advanced, memories and nostalgia are now a significant part of our use of social media. This is shown in a study from the University of Gothenburg and University West.

Researchers at the University of Gothenburg and University West have been following a group of eleven active social media users for ten years, allowing them to describe and reflect on how they use the platforms to document and share their lives. The study provides insight into the role of technology in creating experiences and reliving meaningful moments.
""These types of studies help us look back and understand the culture as it was in the 2010s and 2020s when social media was a central part of it,"" says Beata Jungselius, senior lecturer of informatics at University West and one of the researchers behind the study.
Social media users engage in what researchers define as ""social media nostalgizing,"" meaning they actively seek out content that evokes feelings of nostalgia.
Alexandra Weilenmann, professor of interaction design at the University of Gothenburg, explains that participants in the study have described it as ""treating themselves"" to a nostalgia trip now and then.
""Going back and remembering what has happened earlier in life becomes a bigger part of it over time than posting new content,"" she says, and explains that in later interviews, it becomes clear that the platforms often serve as diary-like tools that allow memories to be relived.
Social media platforms are introducing increasingly advanced features to help users interact with older content. Personal, music-infused photo albums generated for us or reminders of pictures we posted on the same date one, three, or ten years ago allow for nostalgic experiences, which are often seen as positive. The study describes how these features can lead to users reconnecting with old friends by ""tagging"" them in a shared memory. Alexandra Weilenmann and Beata Jungselius believe this could be a deliberate move by social media platforms to encourage users to stay active since the publication of new content has decreased.
The researchers have noted that it's not just the content itself that evokes feelings of nostalgia but also memories of the actual usage of social media play a significant role. For example, one of the interviewees reminisces about how rewarding the intense communication in forums was and how it often led to real-life meetings and interactions.
""It's only now that we've lived with social media long enough to make and draw conclusions from a study like this. Through our method of studying the same users over ten years, we've been able to follow how their usage and attitudes toward the platforms have changed as they have evolved,"" says Beata Jungselius.

","score: 15.144867136659439, grade_level: '15'","score: 16.413863882863346, grade_levels: ['college_graduate'], ages: [24, 100]",10.1177/20563051231207850,"In this article, we present findings from an analysis of social media users’ own descriptions of having lived with social media for over a decade. In doing so, we draw upon the users’ reflections as related in data collected over 10 years. We present findings from a unique dataset of 36 stimulated-recall interviews, where we have studied the same group of informants in 2012, 2017, and 2022. While previous work on reminiscing, memories, and social media have relied on descriptions of practices as they are remembered, our approach has allowed us to follow and examine how users reflect upon their own practices over time. In this article, we focus on social media reminiscing practices and show how social media users seek and engage with previously posted social media content to reminisce and how their reflecting upon how their social media practices have evolved over time evoke ambiguous feelings. Drawing upon previous work and our own empirical material, we define and discuss social media nostalgia. We describe how social media users experience both personal social media nostalgia (referring to how I was), and historical social media nostalgia (referring to how it was) when reflecting upon past social media practices and demonstrate how social media users nostalgize as they interact with and through social media memories. Finally, we discuss our findings in relation to the interplay between reminiscing practices and technology and point to how social media memories represent a detailed insight into an ongoing social transformation of everyday life."
"
Biological materials are made of individual components, including tiny motors that convert fuel into motion. This creates patterns of movement, and the material shapes itself with coherent flows by constant consumption of energy. Such continuously driven materials are called ""active matter."" The mechanics of cells and tissues can be described by active matter theory, a scientific framework to understand shape, flows, and form of living materials. The active matter theory consists of many challenging mathematical equations. 

Scientists from the Max Planck Institute of Molecular Cell Biology and Genetics (MPI-CBG) in Dresden, the Center for Systems Biology Dresden (CSBD), and the TU Dresden have now developed an algorithm, implemented in an open-source supercomputer code, that can for the first time solve the equations of active matter theory in realistic scenarios. These solutions bring us a big step closer to solving the century-old riddle of how cells and tissues attain their shape and to designing artificial biological machines.
Biological processes and behaviors are often very complex. Physical theories provide a precise and quantitative framework for understanding them. The active matter theory offers a framework to understand and describe the behavior of active matter -- materials composed of individual components capable of converting a chemical fuel (""food"") into mechanical forces. Several scientists from Dresden were key in developing this theory, among others Frank Jülicher, director at the Max Planck Institute for the Physics of Complex Systems, and Stephan Grill, director at the MPI-CBG. With these principles of physics, the dynamics of active living matter can be described and predicted by mathematical equations. However, these equations are extremely complex and hard to solve. Therefore, scientists require the power of supercomputers to comprehend and analyze living materials. There are different ways to predict the behavior of active matter, with some focusing on the tiny individual particles, others studying active matter at the molecular level, and yet others studying active fluids on a large scale. These studies help scientists see how active matter behaves at different scales in space and over time.
Solving complex mathematical equations
Scientists from the research group of Ivo Sbalzarini, TU Dresden Professor at the Center for Systems Biology Dresden (CSBD), research group leader at the Max Planck Institute of Molecular Cell Biology and Genetics (MPI-CBG), and Dean of the Faculty of Computer Science at TU Dresden, have now developed a computer algorithm to solve the equations of active matter. Their work was published in the journal ""Physics of Fluids"" and was featured on the cover. They present an algorithm that can solve the complex equations of active matter in three dimensions and in complex-shaped spaces. ""Our approach can handle different shapes in three dimensions over time,"" says one of the first authors of the study, Abhinav Singh, a studied mathematician. He continues, ""Even when the data points are not regularly distributed, our algorithm employs a novel numerical approach that works seamlessly for complex biologically realistic scenarios to accurately solve the theory's equations. Using our approach, we can finally understand the long-term behavior of active materials in both moving and non-moving scenarios for predicting their dynamics. Further, the theory and simulations could be used to program biological materials or create engines at the nano-scale to extract useful work."" The other first author, Philipp Suhrcke, a graduate of TU Dresden's Computational Modeling and Simulation M.Sc. program, adds, ""thanks to our work, scientists can now, for example, predict the shape of a tissue or when a biological material is going to become unstable or dysregulated, with far-reaching implications in understanding the mechanisms of growth and disease.""
A powerful code for everyone to use
The scientists implemented their software using the open-source library OpenFPM, meaning that it is freely available for others to use. OpenFPM is developed by the Sbalzarini group for democratizing large-scale scientific computing. The authors first developed a custom computer language that allows computational scientists to write supercomputer codes by specifying the equations in mathematical notation and let the computer do the work to create a correct program code. As a result, they do not have to start from scratch every time they write a code, effectively reducing code development times in scientific research from months or years to days or weeks, providing enormous productivity gains. Due to the tremendous computational demands of studying three-dimensional active materials, the new code is scalable on shared and distributed-memory multi-processor parallel supercomputers, thanks to the use of OpenFPM. Although the application is designed to run on powerful supercomputers, it can also run on regular office computers for studying two-dimensional materials.

The Principal Investigator of the study, Ivo Sbalzarini, summarizes: ""Ten years of our research went into creating this simulation framework and enhancing the productivity of computational science. This now all comes together in a tool for understanding the three-dimensional behavior of living materials. Open-source, scalable, and capable of handling complex scenarios, our code opens new avenues for modeling active materials. This may finally lead us to understand how cells and tissues attain their shape, addressing the fundamental question of morphogenesis that has puzzled scientist for centuries. But it may also help us design artificial biological machines with minimal numbers of components.""
The computer code that support the findings of this study are openly available in the 3Dactive-hydrodynamics github repository located at https://github.com/mosaic-group/3Dactive-hydrodynamics
The open source framework OpenFPM is available at https://github.com/mosaic-group/openfpm_pdata
Related Publications for the embedded computer language and the OpenFPM software library: https://doi.org/10.1016/j.cpc.2019.03.007 and https://doi.org/10.1140/epje/s10189-021-00121-x

","score: 15.93713336287107, grade_level: '16'","score: 17.887389455028796, grade_levels: ['college_graduate'], ages: [24, 100]",10.1063/5.0169546,"We present a higher-order convergent numerical solver for active polar hydrodynamics in three-dimensional domains of arbitrary shape, along with a scalable open-source software implementation for shared- and distributed-memory parallel computers. This enables the computational study of the nonlinear dynamics of out-of-equilibrium materials from first principles. We numerically solve the nonlinear active Ericksen–Leslie hydrodynamic equations of three-dimensional (3D) active nematics using both a meshfree and a hybrid particle-mesh method in either the Eulerian or Lagrangian frame of reference. The solver is validated against a newly derived analytical solution in 3D and implemented using the OpenFPM software library for scalable scientific computing. We then apply the presented method to studying the transition of 3D active polar fluids to spatiotemporal chaos, the emergence of coherent angular motion in a 3D annulus, and chiral vortices in symmetric and asymmetric 3D shapes resembling dividing cells. Overall, this provides a robust and efficient open-source simulation framework for 3D active matter with verified numerical convergence and scalability on parallel computers."
"
In the background of image recognition software that can ID our friends on social media and wildflowers in our yard are neural networks, a type of artificial intelligence inspired by how own our brains process data. While neural networks sprint through data, their architecture makes it difficult to trace the origin of errors that are obvious to humans -- like confusing a Converse high-top with an ankle boot -- limiting their use in more vital work like health care image analysis or research. A new tool developed at Purdue University makes finding those errors as simple as spotting mountaintops from an airplane.

""In a sense, if a neural network were able to speak, we're showing you what it would be trying to say,"" said David Gleich, a Purdue professor of computer science in the College of Science who developed the tool, which is featured in a paper published in Nature Machine Intelligence. ""The tool we've developed helps you find places where the network is saying, 'Hey, I need more information to do what you've asked.' I would advise people to use this tool on any high-stakes neural network decision scenarios or image prediction task.""
Code for the tool is available on GitHub, as are use case demonstrations. Gleich collaborated on the research with Tamal K. Dey, also a Purdue professor of computer science, and Meng Liu, a former Purdue graduate student who earned a doctorate in computer science.
In testing their approach, Gleich's team caught neural networks mistaking the identity of images in databases of everything from chest X-rays and gene sequences to apparel. In one example, a neural network repeatedly mislabeled images of cars from the Imagenette database as cassette players. The reason? The pictures were drawn from online sales listings and included tags for the cars' stereo equipment.
Neural network image recognition systems are essentially algorithms that process data in a way that mimics the weighted firing pattern of neurons as an image is analyzed and identified. A system is trained to its task -- such as identifying an animal, a garment or a tumor -- with a ""training set"" of images that includes data on each pixel, tagging and other information, and the identity of the image as classified within a particular category. Using the training set, the network learns, or ""extracts,"" the information it needs in order to match the input values with the category. This information, a string of numbers called an embedded vector, is used to calculate the probability that the image belongs to each of the possible categories. Generally speaking, the correct identity of the image is within the category with the highest probability.
But the embedded vectors and probabilities don't correlate to a decision-making process that humans would recognize. Feed in 100,000 numbers representing the known data, and the network produces an embedded vector of 128 numbers that don't correspond to physical features, although they do make it possible for the network to classify the image. In other words, you can't open the hood on the algorithms of a trained system and follow along. Between the input values and the predicted identity of the image is a proverbial ""black box"" of unrecognizable numbers across multiple layers.
""The problem with neural networks is that we can't see inside the machine to understand how it's making decisions, so how can we know if a neural network is making a characteristic mistake?"" Gleich said.

Rather than trying to trace the decision-making path of any single image through the network, Gleich's approach makes it possible to visualize the relationship that the computer sees among all the images in an entire database. Think of it like a bird's-eye view of all the images as the neural network has organized them.
The relationship among the images (like network's prediction of the identity classification of each of the images in the database) is based on the embedded vectors and probabilities the network generates. To boost the resolution of the view and find places where the network can't distinguish between two different classifications, Gleich's team first developed a method of splitting and overlapping the classifications to identify where images have a high probability of belonging to more than one classification.
The team then maps the relationships onto a Reeb graph, a tool taken from the field of topological data analysis. On the graph, each group of images the network thinks are related is represented by a single dot. Dots are color coded by classification. The closer the dots, the more similar the network considers groups to be, and most areas of the graph show clusters of dots in a single color. But groups of images with a high probability of belonging to more than one classification will be represented by two differently colored overlapping dots. With a single glance, areas where the network cannot distinguish between two classifications appear as a cluster of dots in one color, accompanied by a smattering of overlapping dots in a second color. Zooming in on the overlapping dots will show an area of confusion, like the picture of the car that's been labeled both car and cassette player.
""What we're doing is taking these complicated sets of information coming out of the network and giving people an 'in' into how the network sees the data at a macroscopic level,"" Gleich said. ""The Reeb map represents the important things, the big groups and how they relate to each other, and that makes it possible to see the errors.""
""Topological Structure of Complex Predictions"" was produced with the support of the National Science Foundation and the U.S. Department of Energy.

","score: 13.682389803052263, grade_level: '14'","score: 14.481035893938099, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s42256-023-00749-8,"Current complex prediction models are the result of fitting deep neural networks, graph convolutional networks or transducers to a set of training data. A key challenge with these models is that they are highly parameterized, which makes describing and interpreting the prediction strategies difficult. We use topological data analysis to transform these complex prediction models into a simplified topological view of the prediction landscape. The result is a map of the predictions that enables inspection of the model results with more specificity than dimensionality-reduction methods such as tSNE and UMAP. The methods scale up to large datasets across different domains. We present a case study of a transformer-based model previously designed to predict expression levels of a piece of DNA in thousands of genomic tracks. When the model is used to study mutations in the BRCA1 gene, our topological analysis shows that it is sensitive to the location of a mutation and the exon structure of BRCA1 in ways that cannot be found with tools based on dimensionality reduction. Moreover, the topological framework offers multiple ways to inspect results, including an error estimate that is more accurate than model uncertainty. Further studies show how these ideas produce useful results in graph-based learning and image classification."
"
Investigators from the UCLA Health Jonsson Comprehensive Cancer Center have developed an artificial intelligence (AI) model based on epigenetic factors that is able to predict patient outcomes successfully across multiple cancer types.

The researchers found that by examining the gene expression patterns of epigenetic factors -- factors that influence how genes are turned on or off -- in tumors, they could categorize them into distinct groups to predict patient outcomes across various cancer types better than traditional measures like cancer grade and stage.
These findings, described in Communications Biology, also lay the groundwork for developing targeted therapies aimed at regulating epigenetic factors in cancer therapy, such as histone acetyltransferases and SWI/SNF chromatin remodelers.
""Traditionally, cancer has been viewed as primarily a result of genetic mutations within oncogenes or tumor suppressors,"" said co-senior author Hilary Coller, professor of molecular, cell, and developmental biology and a member of the UCLA Health Jonsson Comprehensive Cancer Center and the Eli and Edythe Broad Center of Regenerative Medicine and Stem Cell Research at UCLA. ""However, the emergence of advanced next-generation sequencing technologies has made more people realize that the state of the chromatin and the levels of epigenetic factors that maintain this state are important for cancer and cancer progression. There are different aspects of the state of the chromatin -- like whether the histone proteins are modified, or whether the nucleic acid bases of the DNA contain extra methyl groups -- that can affect cancer outcomes. Understanding these differences between tumors could help us learn more about why some patients respond differently to treatments and why their outcomes vary.""
While previous studies have shown that mutations in the genes that encode epigenetic factors can affect an individual's cancer susceptibility, little is known about how the levels of these factors impact cancer progression. This knowledge gap is crucial in fully understanding how epigenetics affects patient outcomes, noted Coller.
To see if there was a relationship between epigenetic patterns and clinical outcomes, the researchers analyzed the expression patterns of 720 epigenetic factors to classify tumors from 24 different cancer types into distinct clusters.
Out of the 24 adult cancer types, the team found that for 10 of the cancers, the clusters were associated with significant differences in patient outcomes, including progression-free survival, disease-specific survival, and overall survival.

This was especially true for adrenocortical carcinoma, kidney renal clear cell carcinoma, brain lower grade glioma, liver hepatocellular carcinoma and lung adenocarcinoma, where the differences were significant for all the survival measurements.
The clusters with poor outcomes tended to have higher cancer stage, larger tumor size, or more severe spread indicators.
""We saw that the prognostic efficacy of an epigenetic factor was dependent on the tissue-of-origin of the cancer type,"" said Mithun Mitra, co-senior author of the study and an associate project scientist in the Coller laboratory. ""We even saw this link in the few pediatric cancer types we analyzed. This may be helpful in deciding the cancer-specific relevance of therapeutically targeting these factors.""
The team then used epigenetic factor gene expression levels to train and test an AI model to predict patient outcomes. This model was specifically designed to predict what might happen for the five cancer types that had significant differences in survival measurements.
The scientists found the model could successfully divide patients with these five cancer types into two groups: one with a significantly higher chance of better outcomes and another with a higher chance of poorer outcomes.
They also saw that the genes that were most crucial for the AI model had a significant overlap with the cluster-defining signature genes.
""The pan-cancer AI model is trained and tested on the adult patients from the TCGA cohort and it would be good to test this on other independent datasets to explore its broad applicability,"" said Mitra. ""Similar epigenetic factor-based models could be generated for pediatric cancers to see what factors influence the decision-making process compared to the models built on adult cancers.""
""Our research helps provide a roadmap for similar AI models that can be generated through publicly-available lists of prognostic epigenetic factors,"" said the study's first author, Michael Cheng, a graduate student in the Bioinformatics Interdepartmental Program at UCLA. ""The roadmap demonstrates how to identify certain influential factors in different types of cancer and contains exciting potential for predicting specific targets for cancer treatment.""
The study was funded in part by grants from the National Cancer Institute, Cancer Research Institute, Melanoma Research Alliance, Melanoma Research Foundation, National Institutes of Health and the UCLA Spore in Prostate Cancer.

","score: 18.126366013071898, grade_level: '18'","score: 20.17211764705882, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s42003-023-05459-w,"Oncogenic pathways that drive cancer progression reflect both genetic changes and epigenetic regulation. Here we stratified primary tumors from each of 24 TCGA adult cancer types based on the gene expression patterns of epigenetic factors (epifactors). The tumors for five cancer types (ACC, KIRC, LGG, LIHC, and LUAD) separated into two robust clusters that were better than grade or epithelial-to-mesenchymal transition in predicting clinical outcomes. The majority of epifactors that drove the clustering were also individually prognostic. A pan-cancer machine learning model deploying epifactor expression data for these five cancer types successfully separated the patients into poor and better outcome groups. Single-cell analysis of adult and pediatric tumors revealed that expression patterns associated with poor or worse outcomes were present in individual cells within tumors. Our study provides an epigenetic map of cancer types and lays a foundation for discovering pan-cancer targetable epifactors."
"
The widespread adoption of nuclear power was predicted by computer simulations more than four decades ago but the continued reliance on fossil fuels for energy shows these simulations need improvement, a new study has shown.

In order to assess the efficacy of energy policies implemented today, a team of researchers looked back at the influential 1980s model that predicted nuclear power would expand dramatically. Energy policies shapes how we produce and use energy, impacting jobs, costs, climate, and security. These policies are generated using simulations (also known as mathematical models) which forecast things like electricity demand and technology costs. But forecasts may miss the point altogether.
Results published today (Wednesday, 15 November) in the journal Risk Analysis showed the team found simulations that inform energy policy had unreliable assumptions built into them and that they need more transparency about their limitations. To amend this, they recommend new ways to test simulations and be upfront about their uncertainties. This includes methods like 'sensitivity auditing', which evaluates model assumptions. The goal is to improve modelling and open up decision-making.
Lead researcher Dr Samuele Lo Piano, of the University of Reading, said: ""Energy policy affects everybody, so it's worrying when decisions rely on just a few models without questioning their limits. By questioning assumptions and exploring what we don't know, we can get better decision making. We have to acknowledge that no model can perfectly predict the future. But by being upfront about model limitations, democratic debate on energy policy will improve.""
Modelling politics
A chapter of a new book, The politics of modelling(to be published on November 20), written by lead author Dr Lo Piano, highlights why the research matters for all the fields where mathematical models are used to inform decision and policy-making. The chapter considers the inherent complexities and uncertainties posed by human-caused socio-economic and environmental changes.
Entitled 'Sensitivity auditing -- A practical checklist for auditing decision-relevant models', the chapter presents four real-world applications of sensitivity auditing in public health, education, human-water systems, and food provision systems.

","score: 15.434701327433629, grade_level: '15'","score: 15.895431415929203, grade_levels: ['college_graduate'], ages: [24, 100]",10.1111/risa.14248,"This article explores how the modeling of energy systems may lead to an undue closure of alternatives by generating an excess of certainty around some of the possible policy options. We retrospectively exemplify the problem with the case of the International Institute for Applied Systems Analysis (IIASA) global modeling in the 1980s. We discuss different methodologies for quality assessment that may help mitigate this issue, which include Numeral Unit Spread Assessment Pedigree (NUSAP), diagnostic diagrams, and sensitivity auditing (SAUD). We illustrate the potential of these reflexive modeling practices in energy policy‐making with three additional cases: (i) the case of the energy system modeling environment (ESME) for the creation of UK energy policy; (ii) the negative emission technologies (NETs) uptake in integrated assessment models (IAMs); and (iii) the ecological footprint indicator. We encourage modelers to adopt these approaches to achieve more robust, defensible, and inclusive modeling activities in the field of energy research."
"
Stroke is a leading cause of long-term disability worldwide. Each year more than 15 million people worldwide have strokes, and three-quarters of stroke survivors will experience impairment, weakness and paralysis in their arms and hands.

Many stroke survivors rely on their stronger arm to complete daily tasks, from carrying groceries to combing their hair, even when the weaker arm has the potential to improve. Breaking this habit, known as ""arm nonuse"" or ""learned nonuse,"" can improve strength and prevent injury.
But, determining how much a patient is using their weaker arm outside of the clinic is challenging. In a classic case of observer's paradox, the measurement has to be covert for the patient to behave spontaneously.
Now, USC researchers have developed a novel robotic system for collecting precise data on how people recovering from stroke use their arms spontaneously. The first-of-its-kind method is outlined in a paper published in the November 15 issue of Science Robotics.
Using a robotic arm to track 3D spatial information, and machine learning techniques to process the data, the method generates an ""arm nonuse"" metric, which could help clinicians accurately assess a patient's rehabilitation progress. A socially assistive robot (SAR) provides instructions and encouragement throughout the challenge.
""Ultimately, we are trying to assess how much someone's performance in physical therapy transfers into real life,"" said Nathan Dennler, the paper's lead author and a computer science doctoral student.
The research involved combined efforts from researchers in USC's Thomas Lord Department of Computer Science and the Division of Biokinesiology and Physical Therapy. ""This work brings together quantitative user-performance data collected using a robot arm, while also motivating the user to provide a representative performance thanks to a socially assistive robot,"" said Maja Matari?, study co-author and Chan Soon-Shiong Chair and Distinguished Professor of Computer Science, Neuroscience, and Pediatrics. ""This novel combination can serve as a more accurate and more motivating process for stroke patient assessment.""
Additional authors are Stefanos Nikolaidis, an assistant professor of computer science; Amelia Cain, an assistant professor of clinical physical therapy, Carolee J. Winstein, a professor emeritus and an adjunct professor in the Neuroscience Graduate Program, and computer science students Erica De Guzmann and Claudia Chiu.

Mirroring everyday use 
For the study, the research team recruited 14 participants who were right-hand dominant before the stroke. The participant placed their hands on the device's home position -- a 3D-printed box with touch sensors.
A socially assistive robot (SAR) described the system's mechanics and provided positive feedback, while the robot arm moved a button to different target locations in front of the participant (100 locations in total). The ""reaching trial"" begins when the button lights up, and the SAR cues the participant to move.
In the first phase, the participants were directed to reach for the button using whichever hand came naturally, mirroring everyday use. In the second phase, they were instructed to use the stroke-affected arm only, mirroring performance in physiotherapy or other clinical settings.
Using machine learning, the team analyzed three measurements to determine a metric for arm nonuse: arm use probability, time to reach, and successful reach. A noticeable difference in performance between the phases would suggest nonuse of the affected arm.
""The participants have a time limit to reach the button, so even though they know they're being tested, they still have to react quickly,"" said Dennler. ""This way, we're measuring gut reaction to the light turning on -- which hand will you use on the spot?""
Safe and easy to use 

In chronic stroke survivors, the researchers observed high variability in hand choice and in the time to reach targets in the workspace. The method was reliable across repeated sessions, and participants rated it as simple to use, with above-average user experience scores. All participants found the interaction to be safe and easy to use.
Crucially, the researchers found differences in arm use between participants, which could be used by healthcare professionals to more accurately track a patient's stroke recovery.
""For example, one participant whose right side was more affected by their stroke exhibited lower use of their right arm specifically in areas higher on their right side, but maintained a high probability of using their right arm for lower areas on the same side,"" said Dennler.
""Another participant exhibited more symmetric use but also compensated with their less-affected side slightly more often for higher-up points that were close to the mid-line.""
Participants felt that the system could be improved through personalization, which the team hopes to explore in future studies, in addition to incorporating other behavioral data such as facial expressions and different types of tasks.
As a physiotherapist, Cain said the technology addresses many issues encountered with traditional methods of assessment, which ""require the patient not to know they're being tested, and are based on the tester's observation which can leave more room for error.""
""This type of technology could provide rich, objective information about a stroke survivor's arm use to their rehabilitation therapist,"" said Cain. ""The therapist could then integrate this information into their clinical decision-making process and better tailor their interventions to address the patient's areas of weakness and build upon areas of strength.""

","score: 14.780082783056852, grade_level: '15'","score: 16.278238041552584, grade_levels: ['college_graduate'], ages: [24, 100]",10.1126/scirobotics.adf7723,"An overreliance on the less-affected limb for functional tasks at the expense of the paretic limb and in spite of recovered capacity is an often-observed phenomenon in survivors of hemispheric stroke. The difference between capacity for use and actual spontaneous use is referred to as arm nonuse. Obtaining an ecologically valid evaluation of arm nonuse is challenging because it requires the observation of spontaneous arm choice for different tasks, which can easily be influenced by instructions, presumed expectations, and awareness that one is being tested. To better quantify arm nonuse, we developed the bimanual arm reaching test with a robot (BARTR) for quantitatively assessing arm nonuse in chronic stroke survivors. The BARTR is an instrument that uses a robot arm as a means of remote and unbiased data collection of nuanced spatial data for clinical evaluations of arm nonuse. This approach shows promise for determining the efficacy of interventions designed to reduce paretic arm nonuse and enhance functional recovery after stroke. We show that the BARTR satisfies the criteria of an appropriate metric for neurorehabilitative contexts: It is valid, reliable, and simple to use."
"
3D printing is advancing rapidly, and the range of materials that can be used has expanded considerably. While the technology was previously limited to fast-curing plastics, it has now been made suitable for slow-curing plastics as well. These have decisive advantages as they have enhanced elastic properties and are more durable and robust.

The use of such polymers is made possible by a new technology developed by researchers at ETH Zurich and a US start-up. As a result, researchers can now 3D print complex, more durable robots from a variety of high-quality materials in one go. This new technology also makes it easy to combine soft, elastic, and rigid materials. The researchers can also use it to create delicate structures and parts with cavities as desired.
Materials that return to their original state
Using the new technology, researchers at ETH Zurich have succeeded for the first time in printing a robotic hand with bones, ligaments and tendons made of different polymers in one go. ""We wouldn't have been able to make this hand with the fast-curing polyacrylates we've been using in 3D printing so far,"" explains Thomas Buchner, a doctoral student in the group of ETH Zurich robotics professor Robert Katzschmann and first author of the study. ""We're now using slow-curing thiolene polymers. These have very good elastic properties and return to their original state much faster after bending than polyacrylates."" This makes thiolene polymers ideal for producing the elastic ligaments of the robotic hand.
In addition, the stiffness of thiolenes can be fine-tuned very well to meet the requirements of soft robots. ""Robots made of soft materials, such as the hand we developed, have advantages over conventional robots made of metal. Because they're soft, there is less risk of injury when they work with humans, and they are better suited to handling fragile goods,"" Katzschmann explains.
Scanning instead of scraping
3D printers typically produce objects layer by layer: nozzles deposit a given material in viscous form at each point; a UV lamp then cures each layer immediately. Previous methods involved a device that scraped off surface irregularities after each curing step. This works only with fast-curing polyacrylates. Slow-curing polymers such as thiolenes and epoxies would gum up the scraper.
To accommodate the use of slow-curing polymers, the researchers developed 3D printing further by adding a 3D laser scanner that immediately checks each printed layer for any surface irregularities. ""A feedback mechanism compensates for these irregularities when printing the next layer by calculating any necessary adjustments to the amount of material to be printed in real time and with pinpoint accuracy,"" explains Wojciech Matusik, a professor at the Massachusetts Institute of Technology (MIT) in the US and co-author of the study. This means that instead of smoothing out uneven layers, the new technology simply takes the unevenness into account when printing the next layer.
Inkbit, an MIT spin-off, was responsible for developing the new printing technology. The ETH Zurich researchers developed several robotic applications and helped optimise the printing technology for use with slow-curing polymers. The researchers from Switzerland and the US have now jointly published the technology and their sample applications in the journal Nature.
At ETH Zurich, Katzschmann's group will use the technology to explore further possibilities and to design even more sophisticated structures and develop additional applications. Inkbit is planning to use the new technology to offer a 3D printing service to its customers and to sell the new printers.

","score: 13.363973353732394, grade_level: '13'","score: 14.062839293682664, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41586-023-06684-3,"Recreating complex structures and functions of natural organisms in a synthetic form is a long-standing goal for humanity1. The aim is to create actuated systems with high spatial resolutions and complex material arrangements that range from elastic to rigid. Traditional manufacturing processes struggle to fabricate such complex systems2. It remains an open challenge to fabricate functional systems automatically and quickly with a wide range of elastic properties, resolutions, and integrated actuation and sensing channels2,3. We propose an inkjet deposition process called vision-controlled jetting that can create complex systems and robots. Hereby, a scanning system captures the three-dimensional print geometry and enables a digital feedback loop, which eliminates the need for mechanical planarizers. This contactless process allows us to use continuously curing chemistries and, therefore, print a broader range of material families and elastic moduli. The advances in material properties are characterized by standardized tests comparing our printed materials to the state-of-the-art. We directly fabricated a wide range of complex high-resolution composite systems and robots: tendon-driven hands, pneumatically actuated walking manipulators, pumps that mimic a heart and metamaterial structures. Our approach provides an automated, scalable, high-throughput process to manufacture high-resolution, functional multimaterial systems."
"
With 3D inkjet printing systems, engineers can fabricate hybrid structures that have soft and rigid components, like robotic grippers that are strong enough to grasp heavy objects but soft enough to interact safely with humans.

These multimaterial 3D printing systems utilize thousands of nozzles to deposit tiny droplets of resin, which are smoothed with a scraper or roller and cured with UV light. But the smoothing process could squish or smear resins that cure slowly, limiting the types of materials that can be used.
Researchers from MIT, the MIT spinout Inkbit, and ETH Zurich have developed a new 3D inkjet printing system that works with a much wider range of materials. Their printer utilizes computer vision to automatically scan the 3D printing surface and adjust the amount of resin each nozzle deposits in real time to ensure no areas have too much or too little material.
Since it does not require mechanical parts to smooth the resin, this contactless system works with materials that cure more slowly than the acrylates which are traditionally used in 3D printing. Some slower-curing material chemistries can offer improved performance over acrylates, such as greater elasticity, durability, or longevity.
In addition, the automatic system makes adjustments without stopping or slowing the printing process, making this production-grade printer about 660 times faster than a comparable 3D inkjet printing system.
The researchers used this printer to create complex, robotic devices that combine soft and rigid materials. For example, they made a completely 3D-printed robotic gripper shaped like a human hand and controlled by a set of reinforced, yet flexible, tendons.
""Our key insight here was to develop a machine vision system and completely active feedback loop. This is almost like endowing a printer with a set of eyes and a brain, where the eyes observe what is being printed, and then the brain of the machine directs it as to what should be printed next,"" says co-corresponding author Wojciech Matusik, a professor of electrical engineering and computer science at MIT who leads the Computational Design and Fabrication Group within the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL).

He is joined on the paper by lead author Thomas Buchner, a doctoral student at ETH Zurich, co-corresponding author Robert Katzschmann, PhD '18, assistant professor of robotics who leads the Soft Robotics Laboratory at ETH Zurich; as well as others at ETH Zurich and Inkbit. The research will appear in Nature.
Contact free
This paper builds off a low-cost, multimaterial 3D printer known as MultiFab that the researchers introduced in 2015. By utilizing thousands of nozzles to deposit tiny droplets of resin that are UV-cured, MultiFab enabled high-resolution 3D printing with up to 10 materials at once.
With this new project, the researchers sought a contactless process that would expand the range of materials they could use to fabricate more complex devices.
They developed a technique, known as vision-controlled jetting, which utilizes four high-frame-rate cameras and two lasers that rapidly and continuously scan the print surface. The cameras capture images as thousands of nozzles deposit tiny droplets of resin.
The computer vision system converts the image into a high-resolution depth map, a computation that takes less than a second to perform. It compares the depth map to the CAD (computer-aided design) model of the part being fabricated, and adjusts the amount of resin being deposited to keep the object on target with the final structure.

The automated system can make adjustments to any individual nozzle. Since the printer has 16,000 nozzles, the system can control fine details of the device being fabricated.
""Geometrically, it can print almost anything you want made of multiple materials. There are almost no limitations in terms of what you can send to the printer, and what you get is truly functional and long-lasting,"" says Katzschmann.
The level of control afforded by the system enables it to print very precisely with wax, which is used as a support material to create cavities or intricate networks of channels inside an object. The wax is printed below the structure as the device is fabricated. After it is complete, the object is heated so the wax melts and drains out, leaving open channels throughout the object.
Because it can automatically and rapidly adjust the amount of material being deposited by each of the nozzles in real time, the system doesn't need to drag a mechanical part across the print surface to keep it level. This enables the printer to use materials that cure more gradually, and would be smeared by a scraper.
Superior materials
The researchers used the system to print with thiol-based materials, which are slower-curing than the traditional acrylic materials used in 3D printing. However, thiol-based materials are more elastic and don't break as easily as acrylates. They also tend to be more stable over a wider range of temperatures and don't degrade as quickly when exposed to sunlight.
""These are very important properties when you want to fabricate robots or systems that need to interact with a real-world environment,"" says Katzschmann.
The researchers used thiol-based materials and wax to fabricate several complex devices that would otherwise be nearly impossible to make with existing 3D printing systems. For one, they produced a functional, tendon-driven robotic hand that has 19 independently actuatable tendons, soft fingers with sensor pads, and rigid, load-bearing bones.
""We also produced a six-legged walking robot that can sense objects and grasp them, which was possible due to the system's ability to create airtight interfaces of soft and rigid materials, as well as complex channels inside the structure,"" says Buchner.
The team also showcased the technology through a heart-like pump with integrated ventricles and artificial heart valves, as well as metamaterials that can be programmed to have non-linear material properties.
""This is just the start. There is an amazing number of new types of materials you can add to this technology. This allows us to bring in whole new material families that couldn't be used in 3D printing before,"" Matusik says.
The researchers are now looking at using the system to print with hydrogels, which are used in tissue-engineering applications, as well as silicon materials, epoxies, and special types of durable polymers.
They also want to explore new application areas, such as printing customizable medical devices, semiconductor polishing pads, and even more complex robots.
This research was funded, in part, by Credit Suisse, the Swiss National Science Foundation, the Defense Advanced Research Projects Agency (DARPA), and the National Science Foundation (NSF).

","score: 13.957466577540107, grade_level: '14'","score: 15.327799966577537, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41586-023-06684-3,"Recreating complex structures and functions of natural organisms in a synthetic form is a long-standing goal for humanity1. The aim is to create actuated systems with high spatial resolutions and complex material arrangements that range from elastic to rigid. Traditional manufacturing processes struggle to fabricate such complex systems2. It remains an open challenge to fabricate functional systems automatically and quickly with a wide range of elastic properties, resolutions, and integrated actuation and sensing channels2,3. We propose an inkjet deposition process called vision-controlled jetting that can create complex systems and robots. Hereby, a scanning system captures the three-dimensional print geometry and enables a digital feedback loop, which eliminates the need for mechanical planarizers. This contactless process allows us to use continuously curing chemistries and, therefore, print a broader range of material families and elastic moduli. The advances in material properties are characterized by standardized tests comparing our printed materials to the state-of-the-art. We directly fabricated a wide range of complex high-resolution composite systems and robots: tendon-driven hands, pneumatically actuated walking manipulators, pumps that mimic a heart and metamaterial structures. Our approach provides an automated, scalable, high-throughput process to manufacture high-resolution, functional multimaterial systems."
"
Researchers have developed a new deep learning AI tool that generates life-like birdsongs to train bird identification tools, helping ecologists to monitor rare species in the wild. The findings are presented in the British Ecological Society journal, Methods in Ecology and Evolution.

Identifying common bird species through their song has never been easier, with numerous phone apps and software available to both ecologists and the public. But what if the identification software has never heard a particular bird before, or only has a small sample of recordings to reference? This is a problem facing ecologists and conservationists monitoring some of the world's rarest birds.
To overcome this problem, researchers at the University of Moncton, Canada, have developed ECOGEN, a first of its kind deep learning tool, that can generate lifelike bird sounds to enhance the samples of underrepresented species. These can then be used to train audio identification tools used in ecological monitoring, which often have disproportionately more information on common species.
The researchers found that adding artificial birdsong samples generated by ECOGEN to a birdsong identifier improved the bird song classification accuracy by 12% on average.
Dr Nicolas Lecomte, one of the lead researchers, said: ""Due to significant global changes in animal populations, there is an urgent need for automated tools, such acoustic monitoring, to track shifts in biodiversity. However, the AI models used to identify species in acoustic monitoring lack comprehensive reference libraries.
""With ECOGEN, you can address this gap by creating new instances of bird sounds to support AI models. Essentially, for species with limited wild recordings, such as those that are rare, elusive, or sensitive, you can expand your sound library without further disrupting the animals or conducting additional fieldwork.""
The researchers say that creating synthetic bird songs in this way can contribute to the conservation of endangered bird species and also provide valuable insight into their vocalisations, behaviours and habitat preferences.

The ECOGEN tool has other potential applications. For instance, it could be used to help conserve extremely rare species, like the critically endangered regent honeyeaters, where young individuals are unable to learn their species' songs because there aren't enough adult birds to learn from.
The tool could benefit other types of animal as well. Dr Lecomte added: ""While ECOGEN was developed for birds, we're confident that it could be applied to mammals, fish (yes they can produce sounds!), insects and amphibians.""
As well as its versatility, a key advantage of the ECOGEN tool is its accessibility, due to it being open source and able to used on even basic computers.
ECOGEN works by converting real recordings of bird songs into spectrograms (visual representations of sounds) and then generating new AI images from these to increase the dataset for rare species with few recordings. These spectrograms are then converted back into audio to train bird sound identifiers. In this study the researchers used a dataset of 23,784 wild bird recordings from around the world, covering 264 species.

","score: 14.408203723986855, grade_level: '14'","score: 15.093302300109528, grade_levels: ['college_graduate'], ages: [24, 100]",10.1111/2041-210X.14239,"Large‐scale acoustic projects generate vast amounts of data that can now be efficiently processed using deep learning tools. However, these tools often face limitations due to sound labeling and imbalanced sampling. Data augmentation can help overcome such challenges, particularly through the generation of synthetic and lifelike sounds. Synthetic samples can be valuable not only for deep learning but also for species with limited available data. Despite advancements in computer power, sound generation remains a time‐consuming process, even requiring a substantial number of samples. We present ECOGEN, a novel deep learning approach designed to generate realistic bird songs for biologists and ecologists. The primary objective of ECOGEN is to enhance the number of samples in under‐represented bird song classes, thereby improving the performance and robustness of classifiers in ecological research.The ECOGEN framework employs spectrograms as a representation of bird songs and leverages proven image generation techniques to create new spectrograms, subsequently converted back to digital audio signals. As a class‐agnostic tool, ECOGEN is applicable to a wide range of biophonic sounds, including mammal and insect calls. We show that adding samples generated by ECOGEN to a bird song classifier improved the classification accuracy by 12% on average and improved results compared with classic data augmentation techniques 80% of the time. Our approach is both fast and efficient, enabling the generation of synthetic bird songs on standard computing resources. By facilitating the creation of synthetic bird songs, ECOGEN can contribute to the conservation of endangered bird species, while providing valuable insights into their vocalizations, behaviours and habitat preferences. Future development of ECOGEN can be easily implemented and could focus on incorporating additional configurable parameters during the generation phase for increased control over the output, catering to the specific needs of biologists. Large‐scale acoustic projects generate vast amounts of data that can now be efficiently processed using deep learning tools. However, these tools often face limitations due to sound labeling and imbalanced sampling. Data augmentation can help overcome such challenges, particularly through the generation of synthetic and lifelike sounds. Synthetic samples can be valuable not only for deep learning but also for species with limited available data. Despite advancements in computer power, sound generation remains a time‐consuming process, even requiring a substantial number of samples. We present ECOGEN, a novel deep learning approach designed to generate realistic bird songs for biologists and ecologists. The primary objective of ECOGEN is to enhance the number of samples in under‐represented bird song classes, thereby improving the performance and robustness of classifiers in ecological research.The ECOGEN framework employs spectrograms as a representation of bird songs and leverages proven image generation techniques to create new spectrograms, subsequently converted back to digital audio signals. As a class‐agnostic tool, ECOGEN is applicable to a wide range of biophonic sounds, including mammal and insect calls. We show that adding samples generated by ECOGEN to a bird song classifier improved the classification accuracy by 12% on average and improved results compared with classic data augmentation techniques 80% of the time. Our approach is both fast and efficient, enabling the generation of synthetic bird songs on standard computing resources. By facilitating the creation of synthetic bird songs, ECOGEN can contribute to the conservation of endangered bird species, while providing valuable insights into their vocalizations, behaviours and habitat preferences. Future development of ECOGEN can be easily implemented and could focus on incorporating additional configurable parameters during the generation phase for increased control over the output, catering to the specific needs of biologists."
"
Researchers from ICIQ in Spain have designed micromotors that move around on their own to purify wastewater. The process creates ammonia, which can serve as a green energy source. Now, an AI method developed at the University of Gothenburg will be used to tune the motors to achieve the best possible results.

Micromotors have emerged as a promising tool for environmental remediation, largely due to their ability to autonomously navigate and perform specific tasks on a microscale. The micromotor is comprised of a tube made of silicon and manganese dioxide in which chemical reactions cause the release of bubbles from one end. These bubbles act as a motor that sets the tube in motion.
Researchers from the Institute of Chemical Research of Catalonia (ICIQ) have built a micromotor covered with the chemical compound laccase, which accelerates the conversion of urea found in polluted water into ammonia when it comes into contact with the motor.
Green energy source
""This is an interesting discovery. Today, water treatment plants have trouble breaking down all the urea, which results in eutrophication when the water is released. This is a serious problem in urban areas in particular,"" says Rebeca Ferrer, a PhD student at Doctor Katherine Villa´s group at ICIQ.
Converting urea into ammonia offers other advantages as well. If you can extract the ammonia from the water, you also have a source of green energy as ammonia can be converted into hydrogen.
There is a great deal of development work to be done, with the bubbles produced by the micromotors posing a problem for researchers.

""We need to optimise the design so that the tubes can purify the water as efficiently as possible. To do this, we need to see how they move and how long they continue working, but this is difficult to see under a microscope because the bubbles obscure the view,"" Ferrer explains.
Much development work remains
However, thanks to an AI method developed by researchers at the University of Gothenburg, it is possible to estimate the movements of the micromotors under a microscope. Machine learning enables several motors in the liquid to be monitored simultaneously.
""If we cannot monitor the micromotor, we cannot develop it. Our AI works well in a laboratory environment, which is where the development work is currently under way,"" says Harshith Bachimanchi, a PhD student at the Department of Physics, University of Gothenburg.
The researchers have trouble saying how long it will be before urban water treatment plants can also become energy producers. Much development work remains, including on the AI method, which needs to be modified to work in large-scale trials.
""Our goal is to tune the motors to perfection,"" Bachimanchi ends.

","score: 12.701885816590416, grade_level: '13'","score: 12.052884424109806, grade_levels: ['college'], ages: [18, 24]",10.1039/D3NR03804A,"Here, we introduce self-propelled biocatalytic micromotors for simultaneous organic pollutant removal and green energy generation. The study demonstrates remarkable results, showcasing the potential to generate ammonia from wastewater in short time."
"
Virtual reality (VR) is not only a technology for games and entertainment, but also has potential in science and medicine. Researchers at Ruhr University Bochum, Germany, have now gained new insights into human perception with the help of VR. They used virtual reality scenarios in which subjects touched their own bodies with a virtual object. To the researchers' surprise, this led to a tingling sensation at the spot where the avatarized body was touched. This effect occurred even though there was no real physical contact between the virtual object and the body. The scientists led by Dr. Artur Pilacinski and Professor Christian Klaes from the Department of Neurotechnology describe this phenomenon as a phantom touch illusion. They published their results in the journal Scientific Reports of the Nature Publishing Group in September 2023.

""People in virtual reality sometimes have the feeling that they are touching things, although they are actually only encountering virtual objects,"" says first author Artur Pilacinski from the Knappschaftskrankenhaus Bochum Langendreer, University Clinic of Ruhr University Bochum, explaining the origin of the research question. ""We show that the phantom touch illusion is described by most subjects as a tingling or prickling, electrifying sensation or as if the wind was passing through their hand.""
Body sensation arises from complex combination of different sensory perceptions
The neuroscientists wanted to understand what is behind this phenomenon and find out which processes in the brain and body play a role in it. They observed that the phantom touch illusion also occurred when the subjects touched parts of their bodies that were not visible in virtual reality. Second author Marita Metzler adds: ""This suggests that human perception and body sensation are not only based on vision, but on a complex combination of many sensory perceptions and the internal representation of our body.""
This study involved 36 volunteers wearing VR glasses. First, they got used to the VR environment by moving around and touching virtual objects. Then they were given the task of touching their hand in the virtual environment with a virtual stick.
Comparison between virtual and suggested touch sensations
Participants were asked if they felt anything. If not, they were allowed to continue touching and the question was asked again later. If they felt sensations, they were asked to describe them and rate their intensity on different hand locations. This process was repeated for both hands. There was a consistent reporting of the sensation as ""tingling"" by a majority of participants.

In a control experiment, it was investigated whether similar sensations could also be perceived without visual contact with virtual objects purely due to experimental situation demands. Here, a small laser pointer was used instead of virtual objects to touch the hand. This control experiment did not result in phantom touch suggesting that phantom touch illusion was unique to virtual touch.
The discovery of the phantom touch illusion opens up new possibilities for further research into human perception and could also be applied in the fields of virtual reality and medicine. Christian Klaes, member of the Research Department of Neuroscience at Ruhr University, says: ""It could even help to deepen the understanding of neurological diseases and disorders that affect the perception of one's own body.""
Further collaboration with the University of Sussex
The Bochum team plans to continue their research on the phantom touch illusion and the underlying processes. For this reason, a collaboration with the University of Sussex has been started. ""It is important to first distinguish between the actual sensations of phantom touch and other cognitive processes that may be involved in reporting such embodied sensations, such as suggestion, or experimental situation demands,"" says Artur Pilacinski. ""We also want to further explore and understand the neural basis of the phantom touch illusion in collaboration with other partners.""
The research of Artur Pilacinski and Christian Klaes took place within the Research Department of Neuroscience (RDN). The RDN further develops and consolidates a long-established, outstanding research strength of Ruhr University Bochum in the field of systems neuroscience research.

","score: 13.568020936952301, grade_level: '14'","score: 14.61854057126616, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41598-023-42683-0,"We report the presence of a tingling sensation perceived during self-touch without physical stimulation. We used immersive virtual reality scenarios in which subjects touched their body using a virtual object. This touch resulted in a tingling sensation corresponding to the location touched on the virtual body. We called it “phantom touch illusion” (PTI). Interestingly, the illusion was also reported when subjects touched invisible (inferred) parts of their limb. We reason that this PTI results from tactile gating process during self-touch if there is no tactile input to supress. The reported PTI when touching invisible body parts indicates that tactile gating is not exclusively based on vision, but rather on multi-sensory, top-down input involving body schema. This supplementary finding shows that representations of one's own body are defined top-down, beyond the available sensory information."
"
An innovative approach to artificial intelligence (AI) enables reconstructing a broad field of data, such as overall ocean temperature, from a small number of field-deployable sensors using low-powered ""edge"" computing, with broad applications across industry, science and medicine.

""We developed a neural network that allows us to represent a large system in a very compact way,"" said Javier Santos, a Los Alamos National Laboratory researcher who applies computational science to geophysical problems. ""That compactness means it requires fewer computing resources compared to state-of-the-art convolutional neural network architectures, making it well-suited to field deployment on drones, sensor arrays and other edge-computing applications that put computation closer to its end use.""
Novel AI approach boosts computing efficiency
Santos is first author of a paper published by a team of Los Alamos researchers in Nature Machine Intelligence on the novel AI technique, which they dubbed Senseiver. The work, which builds on an AI model called Perceiver IO developed by Google, applies the techniques of natural-language models such as ChatGPT to the problem of reconstructing information about a broad area -- such as the ocean -- from relatively few measurements.
The team realized the model would have broad application because of its efficiency. ""Using fewer parameters and less memory requires fewer central processing unit cycles on the computer, so it runs faster on smaller computers,"" said Dan O'Malley, a coauthor of the paper and Los Alamos researcher who applies machine learning to geoscience problems.
In a first in the published literature, Santos and his Los Alamos colleagues validated the model by demonstrating its effectiveness on real-world sets of sparse data -- meaning information taken from sensors that cover only a tiny portion of the field of interest -- and on complex data sets of three-dimensional fluids.
In a demonstration of the real-world utility of the Senseiver, the team applied the model to a National Oceanic and Atmospheric Administration sea-surface-temperature dataset. The model was able to integrate a multitude of measurements taken over decades from satellites and sensors on ships. From these sparse point measurements, the model forecast temperatures across the entire body of the ocean, which provides information useful to global climate models.

Bringing AI to drones and sensor networks
The Senseiver is well-suited to a variety of projects and research areas of interest to Los Alamos.
""Los Alamos has a wide range of remote sensing capabilities, but it's not easy to use AI because models are too big and don't fit on devices in the field, which leads us to edge computing,"" said Hari Viswanathan, Los Alamos National Laboratory Fellow, environmental scientist and coauthor of the paper about the Senseiver. ""Our work brings the benefits of AI to drones, networks of field-based sensors and other applications currently beyond the reach of cutting-edge AI technology.""
The AI model will be particularly useful in the Lab's work identifying and characterizing orphaned wells. The Lab leads the Department of Energy-funded Consortium Advancing Technology for Assessment of Lost Oil & Gas Wells (CATALOG), a federal program tasked with locating and characterizing undocumented orphaned wells and measuring their methane emissions. Viswanathan is the lead scientist of CATALOG.
The approach offers improved capabilities for large, practical applications such as self-driving cars, remote modeling of assets in oil and gas, medical monitoring of patients, cloud gaming, content delivery and contaminant tracing.

","score: 18.436594982078855, grade_level: '18'","score: 19.83150537634409, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s42256-023-00746-x,"The reconstruction of complex time-evolving fields from sensor observations is a grand challenge. Frequently, sensors have extremely sparse coverage and low-resource computing capacity for measuring highly nonlinear phenomena. While numerical simulations can model some of these phenomena using partial differential equations, the reconstruction problem is ill-posed. Data-driven-strategies provide crucial disambiguation, but these suffer in cases with small amounts of data, and struggle to handle large domains. Here we present the Senseiver, an attention-based framework that excels in reconstructing complex spatial fields from few observations with low overhead. The Senseiver reconstructs n-dimensional fields by encoding arbitrarily sized sparse sets of inputs into a latent space using cross-attention, producing uniform-sized outputs regardless of the number of observations. This allows efficient inference by decoding only a sparse set of output observations, while a dense set of observations is needed to train. This framework enables training of data with complex boundary conditions and extremely large fine-scale simulations. We build on the Perceiver IO by enabling training models with fewer parameters, which facilitates field deployment, and a training framework that allows a flexible number of sensors as input, which is critical for real-world applications. We show that the Senseiver advances the state-of-the-art of field reconstruction in many applications."
"
Distributed cloud storage is a hot topic for security researchers around the globe pursuing secure data storage, and a team in China is now merging quantum physics with mature cryptography and storage techniques to achieve a cost-effective cloud storage solution.

Shamir's secret sharing, a known method, is a key distribution algorithm. It involves distributing private information to a group so that ""the secret"" can be revealed only when a majority pools their knowledge. It's common to combine quantum key distribution (QKD) and Shamir's secret sharing algorithm for secure storage -- at an utmost security level. But utmost security solutions tend to bring substantial cost baggage, including significant cloud storage space requirements.
In AIP Advances, the team presents its method that uses quantum random numbers as encryption keys, disperses the keys via Sharmir's secret sharing algorithm, applies erasure coding within ciphertext, and securely transmits the data through QKD-protected networks to distributed clouds.
Their method not only provides quantum security to the entire system but also offers fault tolerance and efficient storage -- and this may help speed the adoption of quantum technologies.
""In essence, our solution is quantum-secure and serves as a practical application of the fusion between quantum and cryptography technologies,"" said corresponding author Yong Zhao, vice president of QuantumCTek Co. Ltd., a quantum information technology company. ""QKD-generated keys secure both user data uploads to servers and data transmissions to dispersed cloud storage nodes.""
The team explored whether quantum security services could expand beyond secure data transmission to offer a richer spectrum of quantum security applications such as data storage and processing.
They came up with a more secure and cost-effective fault-tolerant cloud storage solution. ""It not only achieves quantum security but also saves storage space when compared to traditional mirroring methods or ones based on Shamir's secret sharing, which is commonly used for distributed management of sensitive data,"" said Zhao.
When the team ran the solution through experimental tests ranging from encryption/decryption, key preservation, and data storage, it proved to be effective.
The solution is currently feasible from both technological and engineering perspectives: It meets the requirement for relevant quantum and cryptographic standards to ensure a secure storage solution capable of withstanding the challenges posed by quantum computing.
""In the future, we plan to drive the commercial implementation of this technology to offer practical services,"" said Zhao. ""We'll explore various usage models in multiuser scenarios, and we're also considering integrating more quantum technologies, such as quantum secret sharing, into cloud storage.""

","score: 16.466179078014182, grade_level: '16'","score: 18.367402482269505, grade_levels: ['college_graduate'], ages: [24, 100]",10.1063/5.0172384,"With the increasing prominence of data security in cloud storage, we propose a practical and robust cloud storage scheme, which uses quantum random numbers as encryption keys, disperses the keys using Shamir’s secret sharing scheme, applies erasure coding to the ciphertext, and securely transmits the data through quantum key distribution protected networks to the distributed clouds. This system offers several key advantages, including quantum-level security, fault tolerance, and storage space saving. To validate its feasibility, we conduct comprehensive experimental tests covering essential functionalities such as encryption/decryption, key preservation, and data storage. By successfully demonstrating the effectiveness of our proposal, we aim to accelerate the application of quantum technology in cloud storage."
"
Researchers at the Complexity Science Hub have examined 1.2 million criminal incidents and developed an innovative method to identify patterns in criminal trajectories.

When it comes to preventing future crimes, it is essential to understand how past criminal behavior relates to future offenses. One key question is whether criminals tend to specialize in specific types of crimes or exhibit a generalist approach by engaging in a variety of illegal activities.
Despite the potential significance of systematically identifying patterns in criminal careers, especially in preventing recurrent offenses, there is a scarcity of comprehensive empirical studies on this subject.
""To address this gap, we conducted an exhaustive examination of over 1.2 million criminal incidents,"" elaborates Stefan Thurner of the Complexity Science Hub. This comprehensive dataset encompassed all criminal reports filed against individuals over six years in a small Central European country.
Specialists With Certain Features 
Criminal offenders who specialize in specific types of crimes typically are older and more frequently female than individuals involved in a broader range of offenses.
""These individuals, referred to as specialists, also tend to operate within a more confined geographic area, suggesting their dependence on local knowledge and potentially receiving support from individuals within that specific region, as opposed to offenders with a wider focus,"" Thurner explains one of the study's results. Furthermore, the researchers observed that these specialists tend to collaborate in tighter-knit local networks, increasing the likelihood of recurring partnerships.

Data-Driven Clustering
In developing this method, researchers initially categorized all offenses into 21 categories, including corruption and sexual crimes, for example. ""Subsequently, we clustered the criminal offenders based on the crimes they committed,"" explaines Georg Heiler from CSH.
To this anonymized dataset, the scientists added socio-demographic information such as age and gender, as well as details about the nature and severity of the committed offenses and the geographical region where they occurred. ""The resulting clustering allows for data-driven categorization of crimes, revealing patterns of criminal behavior,"" Thurner says.
Strengths of the Method 
Regardless of the type and frequency of crimes, this new method's strength lies in the fact that each cluster may consist of varying numbers of crime types and offenses. The fact that some offenses (such as fraud or drug possession) occur more frequently than others (like counterfeiting or data misuse) does not influence the results.
The method also takes into account how often individuals commit specific types of offenses. Researchers found, among other things, that a transition between certain types of crimes occurs significantly more often than others. ""This suggests that specialization in certain categories is more likely than in others,"" Thurner notes. These include, for example, environmental crimes, terrorism, or prostitution crimes.

Preventing Repeat Offenses 
According to a report by Statistics Austria, the reconviction rate in 2022 was 30%. Of the 581,000 individuals involved in this study, nearly a quarter committed more than one offense. If these repeat offenders specialize in specific crimes like burglaries, drug-related offenses, or hacking, this knowledge could assist law enforcement agencies in better anticipating criminal developments. Tailored measures in the areas of policing, prevention, and rehabilitation could have an even greater impact.
The close collaboration between law enforcement agencies and science has already demonstrated in previous projects how the development of new tools based on scientific methods can support police work in terms of resource allocation, planning, and execution of actions, as well as the efficiency, relevance, and quality of results. This is, of course, done while adhering to all legal standards, especially national and international data protection standards.
""While this dataset has the usual limitation of not containing information about undetected or unsolved crimes, we hope that with this method, we can support the work of law enforcement agencies from a scientific standpoint,"" Thurner concludes.

","score: 16.222482018773622, grade_level: '16'","score: 17.770320614409364, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41598-023-43552-6,"We use a comprehensive longitudinal dataset on criminal acts over 6 years in a European country to study specialization in criminal careers. We present a method to cluster crime categories by their relative co-occurrence within criminal careers, deriving a natural, data-based taxonomy of criminal specialization. Defining specialists as active criminals who stay within one category of offending behavior, we study their socio-demographic attributes, geographic range, and positions in their collaboration networks relative to their generalist counterparts. Compared to generalists, specialists tend to be older, are more likely to be women, operate within a smaller geographic range, and collaborate in smaller, more tightly-knit local networks. We observe that specialists are more intensely embedded in criminal networks, suggesting a potential source of self-reinforcing dynamics in criminal careers."
"
Researchers at the Max Planck Institute for the Structure and Dynamics of Matter (MPSD) in Hamburg, Germany, have shown that a previously demonstrated ability to turn on superconductivity with a laser beam can be integrated on a chip, opening up a route toward opto-electronic applications.

Their work, now published in Nature Communications, also shows that the electrical response of photo-excited K3C60 is not linear, that is, the resistance of the sample depends on the applied current. This is a key feature of superconductivity, validates some of the previous observations and provides new information and perspectives on the physics of K3C60 thin films.
The optical manipulation of materials to produce superconductivity at high temperatures is a key research focus of the MPSD. So far, this strategy has proven successful in several quantum materials, including cuprates, k-(ET)2-X and K3C60. Enhanced electrical coherence and vanishing resistance have been observed in previous studies on the optically driven states in these materials.
In this study, researchers from the Cavalleri group deployed on-chip non-linear THz spectroscopy to open up the realm of picosecond transport measurements (a picosecond is a trillionth of a second). They connected thin films of K3C60 to photo-conductive switches with co-planar waveguides. Using a visible laser pulse to trigger the switch, they sent a strong electrical current pulse lasting just one picosecond through the material. After travelling through the solid at around half the speed of light, the current pulse reached another switch which served as a detector to reveal important information, such as the characteristic electrical signatures of superconductivity.
By simultaneously exposing the K3C60 films to mid-infrared light, the researchers were able to observe non-linear current changes in the optically excited material. This so-called critical current behavior and the Meissner effect are the two key features of superconductors. However, neither has been measured so far -- making this demonstration of critical current behavior in the excited solid particularly significant. Moreover, the team discovered that the optically driven state of K3C60 resembled that of a so-called granular superconductor, consisting of weakly connected superconducting islands.
The MPSD is uniquely placed to carry out such measurements on the picosecond scale, with the on-chip set-up having been designed and built in-house. ""We developed a technique platform which is perfect for probing non-linear transport phenomena away from equilibrium, like the non-linear and anomalous Hall effects, the Andreev reflection and others,"" says lead author Eryin Wang, a staff scientist in the Cavalleri group. In addition, the integration of non-equilibrium superconductivity into opto-electronic platforms may lead to new devices based on this effect.
Andrea Cavalleri, who has founded and is currently leading the research group, adds: ""This work underscores the scientific and technological developments within the MPSD in Hamburg, where new experimental methods are constantly being developed to achieve new scientific understanding. We have been working on ultrafast electrical transport methods for nearly a decade and are now in a position to study so many new phenomena in non-equilibrium materials, and potentially to introduce lasting changes in technology.""
The research underpinning these results was carried out in the laboratories of the MPSD at the Center for Free-Electron Laser Science (CFEL) in Hamburg, Germany.

","score: 16.744150943396225, grade_level: '17'","score: 18.329301886792457, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41467-023-42989-7,"Optically driven quantum materials exhibit a variety of non-equilibrium functional phenomena, which to date have been primarily studied with ultrafast optical, X-Ray and photo-emission spectroscopy. However, little has been done to characterize their transient electrical responses, which are directly associated with the functionality of these materials. Especially interesting are linear and nonlinear current-voltage characteristics at frequencies below 1 THz, which are not easily measured at picosecond temporal resolution. Here, we report on ultrafast transport measurements in photo-excited K3C60. Thin films of this compound were connected to photo-conductive switches with co-planar waveguides. We observe characteristic nonlinear current-voltage responses, which in these films point to photo-induced granular superconductivity. Although these dynamics are not necessarily identical to those reported for the powder samples studied so far, they provide valuable new information on the nature of the light-induced superconducting-like state above equilibrium Tc. Furthermore, integration of non-equilibrium superconductivity into optoelectronic platforms may lead to integration in high-speed devices based on this effect."
"
White faces generated by artificial intelligence (AI) now appear more real than human faces, according to new research led by experts at The Australian National University (ANU).

In the study, more people thought AI-generated white faces were human than the faces of real people. The same wasn't true for images of people of colour.
The reason for the discrepancy is that AI algorithms are trained disproportionately on white faces, Dr Amy Dawel, the senior author of the paper, said.
""If white AI faces are consistently perceived as more realistic, this technology could have serious implications for people of colour by ultimately reinforcing racial biases online,"" Dr Dawel said.
""This problem is already apparent in current AI technologies that are being used to create professional-looking headshots. When used for people of colour, the AI is altering their skin and eye colour to those of white people.""
One of the issues with AI 'hyper-realism' is that people often don't realise they're being fooled, the researchers found.
""Concerningly, people who thought that the AI faces were real most often were paradoxically the most confident their judgements were correct,"" Elizabeth Miller, study co-author and PhD candidate at ANU, said.

""This means people who are mistaking AI imposters for real people don't know they are being tricked.""
The researchers were also able to discover why AI faces are fooling people.
""It turns out that there are still physical differences between AI and human faces, but people tend to misinterpret them. For example, white AI faces tend to be more in-proportion and people mistake this as a sign of humanness,"" Dr Dawel said.
""However, we can't rely on these physical cues for long. AI technology is advancing so quickly that the differences between AI and human faces will probably disappear soon.""
The researchers argue this trend could have serious implications for the proliferation of misinformation and identity theft, and that action needs to be taken.
""AI technology can't become sectioned off so only tech companies know what's going on behind the scenes. There needs to be greater transparency around AI so researchers and civil society can identify issues before they become a major problem,"" Dr Dawel said.
Raising public awareness can also play a significant role in reducing the risks posed by the technology, the researchers argue.
""Given that humans can no longer detect AI faces, society needs tools that can accurately identify AI imposters,"" Dr Dawel said.
""Educating people about the perceived realism of AI faces could help make the public appropriately sceptical about the images they're seeing online.""

","score: 12.003570351217412, grade_level: '12'","score: 12.430687351863824, grade_levels: ['college'], ages: [18, 24]",10.1177/09567976231207095,"Recent evidence shows that AI-generated faces are now indistinguishable from human faces. However, algorithms are trained disproportionately on White faces, and thus White AI faces may appear especially realistic. In Experiment 1 ( N = 124 adults), alongside our reanalysis of previously published data, we showed that White AI faces are judged as human more often than actual human faces—a phenomenon we term AI hyperrealism. Paradoxically, people who made the most errors in this task were the most confident (a Dunning-Kruger effect). In Experiment 2 ( N = 610 adults), we used face-space theory and participant qualitative reports to identify key facial attributes that distinguish AI from human faces but were misinterpreted by participants, leading to AI hyperrealism. However, the attributes permitted high accuracy using machine learning. These findings illustrate how psychological theory can inform understanding of AI outputs and provide direction for debiasing AI algorithms, thereby promoting the ethical use of AI."
"
A form of brain-inspired computing that exploits the intrinsic physical properties of a material to dramatically reduce energy use is now a step closer to reality, thanks to a new study led by UCL and Imperial College London researchers.

In the new study, published in the journal Nature Materials, an international team of researchers used chiral (twisted) magnets as their computational medium and found that, by applying an external magnetic field and changing temperature, the physical properties of these materials could be adapted to suit different machine-learning tasks.
Such an approach, known as physical reservoir computing, has until now been limited due to its lack of reconfigurability. This is because a material's physical properties may allow it to excel at a certain subset of computing tasks but not others.
In the new study, published in the journal Nature Materials, an international team of researchers used chiral (twisted) magnets as their computational medium and found that, by applying an external magnetic field and changing temperature, the physical properties of these materials could be adapted to suit different machine-learning tasks.
Dr Oscar Lee (London Centre for Nanotechnology at UCL and UCL Department of Electronic & Electrical Engineering), the lead author of the paper, said: ""This work brings us a step closer to realising the full potential of physical reservoirs to create computers that not only require significantly less energy, but also adapt their computational properties to perform optimally across various tasks, just like our brains.
""The next step is to identify materials and device architectures that are commercially viable and scalable.""
Traditional computing consumes large amounts of electricity. This is partly because it has separate units for data storage and processing, meaning information has to be shuffled constantly between the two, wasting energy and producing heat. This is particularly a problem for machine learning, which requires vast datasets for processing. Training one large AI model can generate hundreds of tonnes of carbon dioxide.

Physical reservoir computing is one of several neuromorphic (or brain inspired) approaches that aims to remove the need for distinct memory and processing units, facilitating more efficient ways to process data. In addition to being a more sustainable alternative to conventional computing, physical reservoir computing could be integrated into existing circuitry to provide additional capabilities that are also energy efficient.
In the study, involving researchers in Japan and Germany, the team used a vector network analyser to determine the energy absorption of chiral magnets at different magnetic field strengths and temperatures ranging from -269 °C to room temperature.
They found that different magnetic phases of chiral magnets excelled at different types of computing task. The skyrmion phase, where magnetised particles are swirling in a vortex-like pattern, had a potent memory capacity apt for forecasting tasks. The conical phase, meanwhile, had little memory, but its non-linearity was ideal for transformation tasks and classification -- for instance, identifying if an animal is a cat or dog.
Co-author Dr Jack Gartside, of Imperial College London, said: ""Our collaborators at UCL in the group of Professor Hidekazu Kurebayashi recently identified a promising set of materials for powering unconventional computing. These materials are special as they can support an especially rich and varied range of magnetic textures. Working with the lead author Dr Oscar Lee, the Imperial College London group [led by Dr Gartside, Kilian Stenning and Professor Will Branford] designed a neuromorphic computing architecture to leverage the complex material properties to match the demands of a diverse set of challenging tasks. This gave great results, and showed how reconfiguring physical phases can directly tailor neuromorphic computing performance.""
The work also involved researchers at the University of Tokyo and Technische Universität München and was supported by the Leverhulme Trust, Engineering and Physical Sciences Research Council (EPSRC), Imperial College London President's Excellence Fund for Frontier Research, Royal Academy of Engineering, the Japan Science and Technology Agency, Katsu Research Encouragement Award, Asahi Glass Foundation, and the DFG (German Research Foundation).

","score: 18.23, grade_level: '18'","score: 19.910318181818184, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41563-023-01698-8,"Reservoir computing is a neuromorphic architecture that may offer viable solutions to the growing energy costs of machine learning. In software-based machine learning, computing performance can be readily reconfigured to suit different computational tasks by tuning hyperparameters. This critical functionality is missing in ‘physical’ reservoir computing schemes that exploit nonlinear and history-dependent responses of physical systems for data processing. Here we overcome this issue with a ‘task-adaptive’ approach to physical reservoir computing. By leveraging a thermodynamical phase space to reconfigure key reservoir properties, we optimize computational performance across a diverse task set. We use the spin-wave spectra of the chiral magnet Cu2OSeO3 that hosts skyrmion, conical and helical magnetic phases, providing on-demand access to different computational reservoir responses. The task-adaptive approach is applicable to a wide variety of physical systems, which we show in other chiral magnets via above (and near) room-temperature demonstrations in Co8.5Zn8.5Mn3 (and FeGe)."
"
With Amazon aiming to make 10,000 deliveries with drones in Europe this year and Walmart planning to expand its drone delivery services to an additional 60,000 homes this year in the states, companies are investing more research and development funding into drone delivery, But are consumers ready to accept this change as the new normal?

Northwestern University's Mobility and Behavior Lab, led by Amanda Stathopoulos, an associate professor of civil and environmental engineering, wanted to know if consumers were ready for robots to replace delivery drivers, in the form of automated vehicles, drones and robots. The team found that societally, there's work to do to shift public perceptions of the near-future technology.
""We need to think really carefully about the effect of these new technologies on people and communities, and to tune in to what they think about these changes,"" Stathopoulos, the study's senior author, said.
The study, titled ""Robots at your doorstep: Acceptance of near-future technologies for automated parcel delivery,"" published last week in the journal Scientific Reports. Researchers noted a ""complex and multifaceted"" relationship between behavior and acceptance of near-future technologies for automated parcel delivery.
While people were generally more willing to accept an automated vehicle as a substitute for a delivery person -- perhaps because there already is familiarity with self-driving cars -- people disliked drones and robots as options. However, as delivery speed increased and price decreased, likelihood to accept the technology increased.
They also found that tech-savvy consumers were more accepting of the near-future technologies than populations less familiar with the technology.
Stathopoulos is the William Patterson Junior professor of civil and environmental engineering at Northwestern's McCormick School of Engineering, where she studies the human aspects of new systems of mobility. She also is a faculty affiliate of Northwestern's Transportation Center. She said especially after the pandemic, people have come to expect efficient delivery from e-commerce purchases as they increasingly work from home.

Maher Said, a graduate of Stathopoulos's lab, is the study's lead author.
""There's a paradox: We're having a hard time reconciling the convenience and the benefit of getting speedy, efficient delivery with its consequences, like poor labor conditions in warehouses, air pollution and congested streets,"" Stathopoulos said. ""We don't really see that other role that we play as citizens or as users of the city. And one role is directly affecting the other role, and we are both. With automated delivery, we could reduce some of these issues.""
The team designed a survey to assess preferences of 692 U.S. respondents, asking questions about different delivery options and variables like delivery speed, package handling and general perceptions.
Stathopoulos said that while new modes of delivery present an exciting opportunity, societally, ""we're not there just yet."" As companies ramp up drone deliveries due in part to labor shortages and in part because existing systems cannot satisfy the sheer volume of e-commerce deliveries, the researchers caution that these innovations may fail because of a lack of public acceptance.
Stathopoulos said she thinks shipping and logistics centers should be placed at the ""front and center"" of city planning and design, as in some European cities, to recognize its importance and role in quality of life. Policy makers will also need to become part of the conversation as more drones enter the airspace and labor shifts. None of this will work, Stathopoulos argued, until companies begin to consolidate their unique systems.
""On the planning side, we need to make sure that we embrace the fact that the massive amount of deliveries is going to shape our cities,"" Stathopoulos said. ""Collaboration, coordination, and information sharing between companies has been a running challenge -- but it's not going to work if everyone has their own technology. It just destroys the purpose and builds redundant and overlapping systems.""
However, by listening to and conducting more frequent assessments of user acceptance of technologies, Stathopoulos argues that policy makers and companies can prepare for the future and work to overcome anxiety and reluctance to accept new technologies.
The study was supported by the National Science Foundation Career program.

","score: 15.150353535353535, grade_level: '15'","score: 16.14655844155844, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41598-023-45371-1,"The logistics and delivery industry is undergoing a technology-driven transformation, with robotics, drones, and autonomous vehicles expected to play a key role in meeting the growing challenges of last-mile delivery. To understand the public acceptability of automated parcel delivery options, this U.S. study explores customer preferences for four innovations: autonomous vehicles, aerial drones, sidewalk robots, and bipedal robots. We use an Integrated Nested Choice and Correlated Latent Variable (INCLV) model to reveal substitution effects among automated delivery modes in a sample of U.S. respondents. The study finds that acceptance of automated delivery modes is strongly tied to shipment price and time, underscoring the importance of careful planning and incentives to maximize the trialability of innovative logistics options. Older individuals and those with concerns about package handling exhibit a lower preference for automated modes, while individuals with higher education and technology affinity exhibit greater acceptance. These findings provide valuable insights for logistics companies and retailers looking to introduce automation technologies in their last-mile delivery operations, emphasizing the need to tailor marketing and communication strategies to meet customer preferences. Additionally, providing information about appropriate package handling by automated technologies may alleviate concerns and increase the acceptance of these modes among all customer groups."
"
Deep within every piece of magnetic material, electrons dance to the invisible tune of quantum mechanics. Their spins, akin to tiny atomic tops, dictate the magnetic behavior of the material they inhabit. This microscopic ballet is the cornerstone of magnetic phenomena, and it's these spins that a team of JILA researchers -- headed by JILA Fellows and University of Colorado Boulder professors Margaret Murnane and Henry Kapteyn -- has learned to control with remarkable precision, potentially redefining the future of electronics and data storage.

In a new Science Advances publication, the JILA team -- along with collaborators from universities in Sweden, Greece, and Germany -- probed the spin dynamics within a special material known as a Heusler compound: a mixture of metals that behaves like a single magnetic material. For this study, the researchers utilized a compound of cobalt, manganese, and gallium, which behaved as a conductor for electrons whose spins were aligned upwards and as an insulator for electrons whose spins were aligned downwards.
Using a form of light called extreme ultraviolet high-harmonic generation (EUV HHG) as a probe, the researchers could track the re-orientations of the spins inside the compound after exciting it with a femtosecond laser, which caused the sample to change its magnetic properties. The key to accurately interpreting the spin re-orientations was the ability to tune the color of the EUV HHG probe light.
""In the past, people haven't done this color tuning of HHG,"" explained co-first author and JILA graduate student Sinéad Ryan. ""Usually, scientists only measured the signal at a few different colors, maybe one or two per magnetic element at most."" In a monumental first, the JILA team tuned their EUV HHG light probe across the magnetic resonances of each element within the compound to track the spin changes with a precision down to femtoseconds (a quadrillionth of a second).
""On top of that, we also changed the laser excitation fluence, so we were changing how much power we used to manipulate the spins,"" Ryan elaborated, highlighting that that step was also an experimental first for this type of research.
Along with their novel approach, the researchers collaborated with theorist and co-first author Mohamed Elhanoty of Uppsala University, who visited JILA, to compare theoretical models of spin changes to their experimental data. Their results showed strong correspondence between data and theory. ""We felt that we'd set a new standard with the agreement between the theory and the experiment,"" added Ryan.
Fine Tuning Light Energy 
To dive into the spin dynamics of their Heusler compound, the researchers brought an innovative tool to the table: extreme ultraviolet high-harmonic probes. To produce the probes, the researchers focused 800-nanometer laser light into a tube filled with neon gas, where the laser's electric field pulled the electrons away from their atoms and then pushed them back. When the electrons snapped back, they acted like rubber bands released after being stretched, creating purple bursts of light at a higher frequency (and energy) than the laser that kicked them out. Ryan tuned these bursts to resonate with the energies of the cobalt and the manganese within the sample, measuring element-specific spin dynamics and magnetic behaviors within the material that the team could further manipulate.

A Competition of Spin Effects 
From their experiment, the researchers found that by tuning the power of the excitation laser and the color (or the photon energy) of their HHG probe, they could determine which spin effects were dominant at different times within their compound. They compared their measurements to a complex computational model called time-dependent density functional theory (TD-DFT). This model predicts how a cloud of electrons in a material will evolve from moment to moment when exposed to various inputs.
Using the TD-DFT framework, Elhanoty found agreement between the model and the experimental data due to three competing spin effects within the Heusler compound. ""What he found in the theory was that the spin flips were quite dominant on early timescales, and then the spin transfers became more dominant,"" explained Ryan. ""Then, as time progressed, more de-magnetization effects take over, and the sample de-magnetizes.""
The phenomena of spin flips happen within one element in the sample as the spins shift their orientation from up to down and vice versa. In contrast, spin transfers happen within multiple elements, in this case, the cobalt and manganese, as they transfer spins between each other, causing each material to become more or less magnetic as time progresses.
Understanding which effects were dominant at which energy levels and times allowed the researchers to understand better how spins could be manipulated to give materials more powerful magnetic and electronic properties.
""There's this concept of spintronics, which takes the electronics that we currently have, and instead of using only the electron's charge, we also use the electron's spin,"" elaborated Ryan. ""So, spintronics also have a magnetic component. The reason to use spin instead of electronic charge is that it could create devices with less resistance and less thermal heating, making devices faster and more efficient.""
From their work with Elhanoty and their other collaborators, the JILA team gained a deeper insight into spin dynamics within Heusler compounds. Ryan said: ""It was really rewarding to see such a good agreement with the theory and experiment when it came from this really close and productive collaboration as well."" The JILA researchers are hopeful to continue this collaboration in studying other compounds to understand better how light can be used to manipulate spin patterns.

","score: 15.461799201518424, grade_level: '15'","score: 16.999730676091367, grade_levels: ['college_graduate'], ages: [24, 100]",10.1126/sciadv.adi1428,"The direct manipulation of spins via light may provide a path toward ultrafast energy-efficient devices. However, distinguishing the microscopic processes that can occur during ultrafast laser excitation in magnetic alloys is challenging. Here, we study the Heusler compound Co 2 MnGa, a material that exhibits very strong light-induced spin transfers across the entire M-edge. By combining the element specificity of extreme ultraviolet high-harmonic probes with time-dependent density functional theory, we disentangle the competition between three ultrafast light-induced processes that occur in Co 2 MnGa: same-site Co-Co spin transfer, intersite Co-Mn spin transfer, and ultrafast spin flips mediated by spin-orbit coupling. By measuring the dynamic magnetic asymmetry across the entire M-edges of the two magnetic sublattices involved, we uncover the relative dominance of these processes at different probe energy regions and times during the laser pulse. Our combined approach enables a comprehensive microscopic interpretation of laser-induced magnetization dynamics on time scales shorter than 100 femtoseconds."
"
Physical properties (stability, solubility, etc.), critical to the performance of pharmaceutical and functional materials, are known to strongly depend on the solid-state form and environmental factors, such as temperature and relative humidity. Recognising that late appearing, more stable forms can lead to disappearing polymorphs and potentially market withdrawal of a life-saving medicine, the pharmaceutical industry has heavily invested in solid form screening platforms.

Quantitatively measuring the free energy differences between crystalline forms is no small challenge. Metastable crystal forms can be difficult to prepare in pure form and they are frequently susceptible to converting to more stable forms. Thus, having the ability to computationally model free energies means that the risks posed by physical instability can be understood and mitigated for all systems, including those that are experimentally intractable. The lack of reliable experimental benchmark data has been a major bottleneck in developing computational methods for accurately predicting solid-solid free energy differences. Reports in the literature are sparse and much of the experimental data on free energy determinations for molecules of pharmaceutical interest is simply not in the public domain.
To overcome this challenge, experts in academia and industry have compiled the first ever reliable experimental benchmark of solid-solid free energy differences for chemically diverse, industrially relevant systems. They then predicted these free energy differences using several methods pioneered by the group of Prof. Alexandre Tkatchenko within the Department of Physics and Materials Science at the University of Luxembourg, and further improved by Dr. Marcus Neumann and his team of researchers at Avant-garde Materials Simulation. Without using any empirical input, these calculations leveraging high performance computing (HPC) were able to predict and explain data from seven pharmaceutical companies with surprising accuracy. The potential future implications of this work are manifold, and this latest development is just one of many potential application of quantum mechanical calculations in the pharmaceutical industry.
""I am thrilled to see how computational methods developed in my academic group have been quickly adopted to reliably predict the energetics of drug crystal forms in the pharmaceutical industry in a matter of years, breaking the traditional barrier between research and industrial innovation,"" remarks Prof. Tkatchenko.
""We owe a fair part of our success to the visionaries among our customers who have enabled us to create an industrial working environment with an academic touch that promotes creativity based on core values such as honesty, integrity, perseverance, team-spirit and genuine care for people and the environment,"" points out Dr Marcus Neuman, founder and CEO of AMS.
""Building links between fundamental science, high performance computing, and major industry players in order to make a lasting impact for the future of health is no small feat,"" said Prof. Jens Kreisel, Rector of the University of Luxembourg. ""We take very seriously our mission of nurturing an ecosystem where researchers can drive societal change for good.""

","score: 19.041364822546978, grade_level: '19'","score: 20.530294885177454, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41586-023-06587-3,"The physicochemical properties of molecular crystals, such as solubility, stability, compactability, melting behaviour and bioavailability, depend on their crystal form1. In silico crystal form selection has recently come much closer to realization because of the development of accurate and affordable free-energy calculations2–4. Here we redefine the state of the art, primarily by improving the accuracy of free-energy calculations, constructing a reliable experimental benchmark for solid–solid free-energy differences, quantifying statistical errors for the computed free energies and placing both hydrate crystal structures of different stoichiometries and anhydrate crystal structures on the same energy landscape, with defined error bars, as a function of temperature and relative humidity. The calculated free energies have standard errors of 1–2 kJ mol−1 for industrially relevant compounds, and the method to place crystal structures with different hydrate stoichiometries on the same energy landscape can be extended to other multi-component systems, including solvates. These contributions reduce the gap between the needs of the experimentalist and the capabilities of modern computational tools, transforming crystal structure prediction into a more reliable and actionable procedure that can be used in combination with experimental evidence to direct crystal form selection and establish control5."
"
Over the past decade, AI has permeated nearly every corner of science: Machine learning models have been used to predict protein structures, estimate the fraction of the Amazon rainforest that has been lost to deforestation and even classify faraway galaxies that might be home to exoplanets.

But while AI can be used to speed scientific discovery -- helping researchers make predictions about phenomena that may be difficult or costly to study in the real world -- it can also lead scientists astray. In the same way that chatbots sometimes ""hallucinate,"" or make things up, machine learning models can sometimes present misleading or downright false results.
In a paper published online today (Thursday, Nov. 9) in Science, researchers at the University of California, Berkeley, present a new statistical technique for safely using the predictions obtained from machine learning models to test scientific hypotheses.
The technique, called prediction-powered inference (PPI), uses a small amount of real-world data to correct the output of large, general models -- such as AlphaFold, which predicts protein structures -- in the context of specific scientific questions.
""These models are meant to be general: They can answer many questions, but we don't know which questions they answer well and which questions they answer badly -- and if you use them naively, without knowing which case you're in, you can get bad answers,"" said study author Michael Jordan, the Pehong Chen Distinguished Professor of electrical engineering and computer science and of statistics at UC Berkeley. ""With PPI, you're able to use the model, but correct for possible errors, even when you don't know the nature of those errors at the outset.""
The risk of hidden biases
When scientists conduct experiments, they're not just looking for a single answer -- they want to obtain a range of plausible answers. This is done by calculating a ""confidence interval,"" which, in the simplest case, can be found by repeating an experiment many times and seeing how the results vary.

In most science studies, a confidence interval usually refers to a summary or combined statistic, not individual data points. Unfortunately, machine learning systems focus on individual data points, and thus do not provide scientists with the kinds of uncertainty assessments that they care about. For instance, AlphaFold predicts the structure of a single protein, but it doesn't provide a notion of confidence for that structure, nor a way to obtain confidence intervals that refer to general properties of proteins.
Scientists may be tempted to use the predictions from AlphaFold as if they were data to compute classical confidence intervals, ignoring the fact that these predictions are not data. The problem with this approach is that machine learning systems have many hidden biases that can skew the results. These biases arise, in part, from the data on which they are trained, which are generally existing scientific research that may not have had the same focus as the current study.
""Indeed, in scientific problems, we're often interested in phenomena which are at the edge between the known and the unknown,"" Jordan said. ""Very often, there aren't much data from the past that are at that edge, and that makes generative AI models even more likely to 'hallucinate,' producing output that is unrealistic.""
Calculating valid confidence intervals
PPI allows scientists to incorporate the predictions from models like AlphaFold without making any assumptions about how the model was built or the data it was trained on. To do this, PPI requires a small amount of data that is unbiased, with respect to the specific hypothesis being investigated, paired with machine learning predictions corresponding to that data. By bringing these two sources of evidence together, PPI is able to form valid confidence intervals.
For example, the research team applied the PPI technique to algorithms that can pinpoint areas of deforestation in the Amazon using satellite imagery. These models were accurate, overall, when tested individually on regions in the forest; however, when these assessments were combined to estimate deforestation across the entire Amazon, the confidence intervals became highly skewed. This is likely because the model struggled to recognize certain newer patterns of deforestation.

With PPI, the team was able to correct for the bias in the confidence interval using a small number of human-labeled regions of deforestation.
The team also showed how the technique can be applied to a variety of other research, including questions about protein folding, galaxy classification, gene expression levels, counting plankton, and the relationship between income and private health insurance.
""There's really no limit on the type of questions that this approach could be applied to,"" Jordan said. ""We think that PPI is a much-needed component of modern data-intensive, model-intensive and collaborative science.""
Additional co-authors include Anastasios N. Angelopoulos, Stephen Bates, Clara Fannjiang and Tijana Zrnic of UC Berkeley. This research was supported by the Office of Naval Research (N00014-21-1-2840) and the National Science Foundation.

","score: 15.450620857454485, grade_level: '15'","score: 17.34672036244651, grade_levels: ['college_graduate'], ages: [24, 100]",10.1126/science.adi6000,"Prediction-powered inference is a framework for performing valid statistical inference when an experimental dataset is supplemented with predictions from a machine-learning system. The framework yields simple algorithms for computing provably valid confidence intervals for quantities such as means, quantiles, and linear and logistic regression coefficients without making any assumptions about the machine-learning algorithm that supplies the predictions. Furthermore, more accurate predictions translate to smaller confidence intervals. Prediction-powered inference could enable researchers to draw valid and more data-efficient conclusions using machine learning. The benefits of prediction-powered inference were demonstrated with datasets from proteomics, astronomy, genomics, remote sensing, census analysis, and ecology."
"
Scientists at Oak Ridge National Laboratory used their expertise in quantum biology, artificial intelligence and bioengineering to improve how CRISPR Cas9 genome editing tools work on organisms like microbes that can be modified to produce renewable fuels and chemicals.

CRISPR is a powerful tool for bioengineering, used to modify genetic code to improve an organism's performance or to correct mutations. The CRISPR Cas9 tool relies on a single, unique guide RNA that directs the Cas9 enzyme to bind with and cleave the corresponding targeted site in the genome. Existing models to computationally predict effective guide RNAs for CRISPR tools were built on data from only a few model species, with weak, inconsistent efficiency when applied to microbes.
""A lot of the CRISPR tools have been developed for mammalian cells, fruit flies or other model species. Few have been geared towards microbes where the chromosomal structures and sizes are very different,"" said Carrie Eckert, leader of the Synthetic Biology group at ORNL. ""We had observed that models for designing the CRISPR Cas9 machinery behave differently when working with microbes, and this research validates what we'd known anecdotally.""
To improve the modeling and design of guide RNA, the ORNL scientists sought a better understanding of what's going on at the most basic level in cell nuclei, where genetic material is stored. They turned to quantum biology, a field bridging molecular biology and quantum chemistry that investigates the effects that electronic structure can have on the chemical properties and interactions of nucleotides, the molecules that form the building blocks of DNA and RNA.
The way electrons are distributed in the molecule influences reactivity and conformational stability, including the likelihood that the Cas9 enzyme-guide RNA complex will effectively bind with the microbe's DNA, said Erica Prates, computational systems biologist at ORNL.
The best guide through a forest of decisions
The scientists built an explainable artificial intelligence model called iterative random forest. They trained the model on a dataset of around 50,000 guide RNAs targeting the genome of E. coli bacteria while also taking into account quantum chemical properties, in an approach described in the journal Nucleic Acids Research.

The model revealed key features about nucleotides that can enable the selection of better guide RNAs. ""The model helped us identify clues about the molecular mechanisms that underpin the efficiency of our guide RNAs,"" Prates said, ""giving us a rich library of molecular information that can help us improve CRISPR technology.""
ORNL researchers validated the explainable AI model by conducting CRISPR Cas9 cutting experiments on E. coli with a large group of guides selected by the model.
Using explainable AI gave scientists an understanding of the biological mechanisms that drove results, rather than a deep learning model rooted in a ""black box"" algorithm that lacks interpretability, said Jaclyn Noshay, a former ORNL computational systems biologist who is first author on the paper.
""We wanted to improve our understanding of guide design rules for optimal cutting efficiency with a microbial species focus given knowledge of the incompatibility of models trained across [biological] kingdoms,"" Noshay said.
The explainable AI model, with its thousands of features and iterative nature, was trained using the Summit supercomputer at ORNL's Oak Ridge Leadership Computer Facility, or OLCF, a DOE Office of Science user facility.
Eckert said her synthetic biology team plans to work with computational science colleagues at ORNL to take what they've learned with the new microbial CRISPR Cas9 model and improve it further using data from lab experiments or a variety of microbial species.

Better CRISPR Cas9 tools for every species
Taking quantum properties into consideration opens the door to Cas9 guide improvements for every species. ""This paper even has implications across the human scale,"" Eckert said. ""If you're looking at any sort of drug development, for instance, where you're using CRISPR to target a specific region of the genome, you must have the most accurate model to predict those guides.""
Refining CRISPR Cas9 models gives scientists a higher-throughput pipeline to link genotype to phenotype, or genes to physical traits, a field known as functional genomics. The research has implications for the work of the ORNL-led Center for Bioenergy Innovation (CBI), for example, to improve bioenergy feedstock plants and bacterial fermentation of biomass.
""We're greatly improving our predictions of guide RNA with this research,"" Eckert said. ""The better we understand the biological processes at play and the more data we can feed into our predictions, the better our targets will be, improving the precision and speed of our research.""
""A major goal of our research is to improve the ability to predictively modify the DNA of more organisms using CRISPR tools. This study represents an exciting advancement toward,,, understanding how we can avoid making costly 'typos' in an organism's genetic code,"" said ORNL's Paul Abraham, a bioanalytical chemist who leads the DOE Genomic Science Program's Secure Ecosystem Engineering and Design Science Focus Area, or SEED SFA, that supported the CRISPR research. ""I am eager to learn how much more these predictions can improve as we generate additional training data and continue to leverage explainable AI modeling.""
Co-authors on the publication included ORNL's William Alexander, Dawn Klingeman, Erica Prates, Carrie Eckert, Stephan Irle and Daniel Jacobson; Tyler Walker, Jonathan Romero and Angelica Walker of the Bredesen Center for Interdisciplinary Research and Graduate Education at the University of Tennessee, Knoxville; and Jaclyn Noshay and David Kainer, who were formerly with ORNL and now with Bayer and the University of Queensland, respectively.
Funding for the project was provided by the SEED SFA and CBI, both part of the DOE Office of Science Biological and Environmental Research Program, by ORNL's Lab-Directed Research and Development program, and by the high-performance computing resources of the OLCF and Compute and Data Environment for Science, both also supported by the Office of Science.

","score: 17.51009281041539, grade_level: '18'","score: 19.347379072540363, grade_levels: ['college_graduate'], ages: [24, 100]",10.1093/nar/gkad736,"CRISPR-Cas9 tools have transformed genetic manipulation capabilities in the laboratory. Empirical rules-of-thumb have been developed for only a narrow range of model organisms, and mechanistic underpinnings for sgRNA efficiency remain poorly understood. This work establishes a novel feature set and new public resource, produced with quantum chemical tensors, for interpreting and predicting sgRNA efficiency. Feature engineering for sgRNA efficiency is performed using an explainable-artificial intelligence model: iterative Random Forest (iRF). By encoding quantitative attributes of position-specific sequences for Escherichia coli sgRNAs, we identify important traits for sgRNA design in bacterial species. Additionally, we show that expanding positional encoding to quantum descriptors of base-pair, dimer, trimer, and tetramer sequences captures intricate interactions in local and neighboring nucleotides of the target DNA. These features highlight variation in CRISPR-Cas9 sgRNA dynamics between E. coli and H. sapiens genomes. These novel encodings of sgRNAs enhance our understanding of the elaborate quantum biological processes involved in CRISPR-Cas9 machinery."
"
Leveraging artificial intelligence and the largest pediatric brain MRI dataset to date, researchers have now developed a growth chart for tracking muscle mass in growing children. The new study led by investigators from Brigham and Women's Hospital, a founding member of the Mass General Brigham healthcare system, found that their artificial intelligence-based tool is the first to offer a standardized, accurate, and reliable way to assess and track indicators of muscle mass on routine MRI. Their results were published today in Nature Communications.

""Pediatric cancer patients often struggle with low muscle mass, but there is no standard way to measure this. We were motivated to use artificial intelligence to measure temporalis muscle thickness and create a standardized reference,"" said senior author Ben Kann, MD, a radiation oncologist in the Brigham's Department of Radiation Oncology and Mass General Brigham's Artificial Intelligence in Medicine Program. ""Our methodology produced a growth chart that we can use to track muscle thickness within developing children quickly and in real-time. Through this, we can determine whether they are growing within an ideal range.""
Lean muscle mass in humans has been linked to quality of life, daily functional status, and is an indicator of overall health and longevity. Individuals with conditions such as sarcopenia or low lean muscle mass are at risk of dying earlier, or otherwise being prone to various diseases that can affect their quality of life. Historically, there has not been a widespread or practical way to track lean muscle mass, with body mass index (BMI) serving as a default form of measurement. The weakness in using BMI is that while it considers weight, it does not indicate how much of that weight is muscle. For decades, scientists have known that the thickness of the temporalis muscle outside the skull is associated with lean muscle mass in the body. However, the thickness of this muscle has been difficult to measure in real-time in the clinic and there was no way to diagnose normal from abnormal thickness. Traditional methods have typically involved manual measurements, but these practices are time consuming and are not standardized.
To address this, the research team applied their deep learning pipeline to MRI scans of patients with pediatric brain tumors treated at Boston Children's Hospital/Dana-Farber Cancer Institute in collaboration with Boston Children's Radiology Department. The team analyzed 23,852 normal healthy brain MRIs from individuals aged 4 through 35 to calculate temporalis muscle thickness (iTMT) and develop normal-reference growth charts for the muscle. MRI results were aggregated to create sex-specific iTMT normal growth charts with percentiles and ranges. They found that iTMT is accurate for a wide range of patients and is comparable to the analysis of trained human experts.
""The idea is that these growth charts can be used to determine if a patient's muscle mass is within a normal range, in a similar way that height and weight growth charts are typically used in the doctor's office,"" said Kann.
In essence, the new method could be used to assess patients who are already receiving routine brain MRIs that track medical conditions such as pediatric cancers and neurodegenerative diseases. The team hopes that the ability to monitor the temporalis muscle instantly and quantitatively will enable clinicians to quickly intervene for patients who demonstrate signs of muscle loss, and thus prevent the negative effects of sarcopenia and low muscle mass.
One of the limitations lies in the algorithms reliance on scan quality, and how a suboptimal resolution can affect measurements and the interpretation of results. Another drawback is the limited amount of MRI datasets available outside of the United States and Europe that can give an accurate global picture.
""In the future, we may want to explore if the utility of iTMT will be high enough to justify getting MRIs on a regular basis for more patients,"" said Kann. ""We plan to improve model performance by training it on more challenging and variable cases. Future applications of iTMT could allow us to track and predict morbidity, as well as reveal critical physiologic states in patients that require intervention.""

","score: 14.649895868323814, grade_level: '15'","score: 16.2090930466913, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41467-023-42501-1,"Lean muscle mass (LMM) is an important aspect of human health. Temporalis muscle thickness is a promising LMM marker but has had limited utility due to its unknown normal growth trajectory and reference ranges and lack of standardized measurement. Here, we develop an automated deep learning pipeline to accurately measure temporalis muscle thickness (iTMT) from routine brain magnetic resonance imaging (MRI). We apply iTMT to 23,876 MRIs of healthy subjects, ages 4 through 35, and generate sex-specific iTMT normal growth charts with percentiles. We find that iTMT was associated with specific physiologic traits, including caloric intake, physical activity, sex hormone levels, and presence of malignancy. We validate iTMT across multiple demographic groups and in children with brain tumors and demonstrate feasibility for individualized longitudinal monitoring. The iTMT pipeline provides unprecedented insights into temporalis muscle growth during human development and enables the use of LMM tracking to inform clinical decision-making."
"
Technology is edging closer and closer to the super-speed world of computing with artificial intelligence. But is the world equipped with the proper hardware to be able to handle the workload of new AI technological breakthroughs?

""The brain-inspired codes of the AI revolution are largely being run on conventional silicon computer architectures which were not designed for it,"" explains Erica Carlson, 150th Anniversary Professor of Physics and Astronomy at Purdue University.
A joint effort between Physicists from Purdue University, University of California San Diego (USCD) and École Supérieure de Physique et de Chimie Industrielles (ESPCI) in Paris, France, believe they may have discovered a way to rework the hardware…. By mimicking the synapses of the human brain. They published their findings, ""Spatially Distributed Ramp Reversal Memory in VO2"" in Advanced Electronic Materials which is featured on the back cover of the October 2023 edition.
New paradigms in hardware will be necessary to handle the complexity of tomorrow's computational advances. According to Carlson, lead theoretical scientist of this research, ""neuromorphic architectures hold promise for lower energy consumption processors, enhanced computation, fundamentally different computational modes, native learning and enhanced pattern recognition.""
Neuromorphic architecture basically boils down to computer chips mimicking brain behavior. Neurons are cells in the brain that transmit information. Neurons have small gaps at their ends that allow signals to pass from one neuron to the next which are called synapses. In biological brains, these synapses encode memory. This team of scientists concludes that vanadium oxides show tremendous promise for neuromorphic computing because they can be used to make both artificial neurons and synapses.
""The dissonance between hardware and software is the origin of the enormously high energy cost of training, for example, large language models like ChatGPT,"" explains Carlson. ""By contrast, neuromorphic architectures hold promise for lower energy consumption by mimicking the basic components of a brain: neurons and synapses. Whereas silicon is good at memory storage, the material does not easily lend itself to neuron-like behavior. Ultimately, to provide efficient, feasible neuromorphic hardware solutions requires research into materials with radically different behavior from silicon -- ones that can naturally mimic synapses and neurons. Unfortunately, the competing design needs of artificial synapses and neurons mean that most materials that make good synaptors fail as neuristors, and vice versa. Only a handful of materials, most of them quantum materials, have the demonstrated ability to do both.""
The team relied on a recently discovered type of non-volatile memory which is driven by repeated partial temperature cycling through the insulator-to-metal transition. This memory was discovered in vanadium oxides.

Alexandre Zimmers, lead experimental scientist from Sorbonne University and École Supérieure de Physique et de Chimie Industrielles, Paris, explains, ""Only a few quantum materials are good candidates for future neuromorphic devices, i.e., mimicking artificial synapses and neurons. For the first time, in one of them, vanadium dioxide, we can see optically what is changing in the material as it operates as an artificial synapse. We find that memory accumulates throughout the entirety of the sample, opening new opportunities on how and where to control this property.""
""The microscopic videos show that, surprisingly, the repeated advance and retreat of metal and insulator domains causes memory to be accumulated throughout the entirety of the sample, rather than only at the boundaries of domains,"" explains Carlson. ""The memory appears as shifts in the local temperature at which the material transitions from insulator to metal upon heating, or from metal to insulator upon cooling. We propose that these changes in the local transition temperature accumulate due to the preferential diffusion of point defects into the metallic domains that are interwoven through the insulator as the material is cycled partway through the transition.""
Now that the team has established that vanadium oxides are possible candidates for future neuromorphic devices, they plan to move forward in the next phase of their research.
""Now that we have established a way to see inside this neuromorphic material, we can locally tweak and observe the effects of, for example, ion bombardment on the material's surface,"" explains Zimmers. ""This could allow us to guide the electrical current through specific regions in the sample where the memory effect is at its maximum. This has the potential to significantly enhance the synaptic behavior of this neuromorphic material.""

","score: 15.433633323064736, grade_level: '15'","score: 16.236536988953922, grade_levels: ['college_graduate'], ages: [24, 100]",10.1002/aelm.202300085,"Ramp‐reversal memory has recently been discovered in several insulator‐to‐metal transition materials where a non‐volatile resistance change can be set by repeatedly driving the material partway through the transition. This study uses optical microscopy to track the location and internal structure of accumulated memory as a thin film of VO2 is temperature cycled through multiple training subloops. These measurements reveal that the gain of insulator phase fraction between consecutive subloops occurs primarily through front propagation at the insulator‐metal boundaries. By analyzing transition temperature maps, it is found, surprisingly, that the memory is also stored deep inside both insulating and metallic clusters throughout the entire sample, making the metal‐insulator coexistence landscape more rugged. This non‐volatile memory is reset after heating the sample to higher temperatures, as expected. Diffusion of point defects is proposed to account for the observed memory writing and subsequent erasing over the entire sample surface. By spatially mapping the location and character of non‐volatile memory encoding in VO2, this study results enable the targeting of specific local regions in the film where the full insulator‐to‐metal resistivity change can be harnessed in order to maximize the working range of memory elements for conventional and neuromorphic computing applications."
"
Researchers in the Department of Mechanical Engineering at Carnegie Mellon University, in collaboration with paleontologists from Spain and Poland, used fossil evidence to engineer a soft robotic replica of pleurocystitid, a marine organism that existed nearly 450 million years ago and is believed to be one of the first echinoderms capable of movement using a muscular stem.

Published today in The Proceedings of the National Academy of Science (PNAS), the research seeks to broaden modern perspective of animal design and movement by introducing a new a field of study -- Paleobionics -- aimed at using Softbotics, robotics with flexible electronics and soft materials, to understand the biomechanical factors that drove evolution using extinct organisms.
""Softbotics is another approach to inform science using soft materials to construct flexible robot limbs and appendages. Many fundamental principles of biology and nature can only fully be explained if we look back at the evolutionary timeline of how animals evolved. We are building robot analogues to study how locomotion has changed,"" said Carmel Majidi, lead author and Professor of Mechanical Engineering at Carnegie Mellon University.
With humans' time on earth representing only 0.007% of the planet's history, the modern-day animal kingdom that influences understanding of evolution and inspires today's mechanical systems is only a fraction of all creatures that have existed through history.
Using fossil evidence to guide their design and a combination of 3D printed elements and polymers to mimic the flexible columnar structure of the moving appendage, the team demonstrated that pleurocystitids were likely able to move over the sea bottom by means of a muscular stem that pushed the animal forward. Despite the absence of a current day analogue (echinoderms have since evolved to include modern day starfish and sea urchins), pleurocystitids have been of interest to paleontologists due to their pivotal role in echinoderm evolution.
The team determined that wide sweeping movements were likely the most effective motion and that increasing the length of the stem significantly increased the animals' speed without forcing it to exert more energy.
""Researchers in the bio-inspired robotics community need to pick and choose important features worth adopting from organisms,"" explained Richard Desatnik, PhD candidate and co-first author.

""Essentially, we have to decide on good locomotion strategies to get our robots moving. For example, would a starfish robot really need to use 5 limbs for locomotion or can we find a better strategy?"" added Zach Patterson, CMU alumnus and co-first author.
Now that the team has demonstrated that they can use Softbotics to engineer extinct organisms, they hope to explore other animals, like the first organism that could travel from sea to land -- something that can't be studied in the same way using conventional robot hardware.
""Bringing a new life to something that existed nearly 500 million years ago is exciting in and of itself, but what really excites us about this breakthrough is how much we will be able to learn from it,"" said Phil LeDuc, co-author, and Professor of Mechanical Engineering at Carnegie Mellon University. ""We aren't just looking at fossils in the ground, we are trying to better understand life through working with amazing paleontologists.""
Additional collaborators include Przemyslaw Gorzelak, Institute of Paleobiology, Polish Academy of Sciences, and Samuel Zamora, The Geological and Mining Institute of Spain.

","score: 18.62144771832784, grade_level: '19'","score: 20.0200042548665, grade_levels: ['college_graduate'], ages: [24, 100]",10.1073/pnas.2306580120,"The transition from sessile suspension to active mobile detritus feeding in early echinoderms (c.a. 500 Mya) required sophisticated locomotion strategies. However, understanding locomotion adopted by extinct animals in the absence of trace fossils and modern analogues is extremely challenging. Here, we develop a biomimetic soft robot testbed with accompanying computational simulation to understand fundamental principles of locomotion in one of the most enigmatic mobile groups of early stalked echinoderms—pleurocystitids. We show that these Paleozoic echinoderms were likely able to move over the sea bottom by means of a muscular stem that pushed the animal forward (anteriorly). We also demonstrate that wide, sweeping gaits could have been the most effective for these echinoderms and that increasing stem length might have significantly increased velocity with minimal additional energy cost. The overall approach followed here, which we call “Paleobionics,” is a nascent but rapidly developing research agenda in which robots are designed based on extinct organisms to generate insights in engineering and evolution."
"
A speech prosthetic developed by a collaborative team of Duke neuroscientists, neurosurgeons, and engineers can translate a person's brain signals into what they're trying to say.

Appearing Nov. 6 in the journal Nature Communications, the new technology might one day help people unable to talk due to neurological disorders regain the ability to communicate through a brain-computer interface.
""There are many patients who suffer from debilitating motor disorders, like ALS (amyotrophic lateral sclerosis) or locked-in syndrome, that can impair their ability to speak,"" said Gregory Cogan, Ph.D., a professor of neurology at Duke University's School of Medicine and one of the lead researchers involved in the project. ""But the current tools available to allow them to communicate are generally very slow and cumbersome.""
Imagine listening to an audiobook at half-speed. That's the best speech decoding rate currently available, which clocks in at about 78 words per minute. People, however, speak around 150 words per minute.
The lag between spoken and decoded speech rates is partially due the relatively few brain activity sensors that can be fused onto a paper-thin piece of material that lays atop the surface of the brain. Fewer sensors provide less decipherable information to decode.
To improve on past limitations, Cogan teamed up with fellow Duke Institute for Brain Sciences faculty member Jonathan Viventi, Ph.D., whose biomedical engineering lab specializes in making high-density, ultra-thin, and flexible brain sensors.
For this project, Viventi and his team packed an impressive 256 microscopic brain sensors onto a postage stamp-sized piece of flexible, medical-grade plastic. Neurons just a grain of sand apart can have wildly different activity patterns when coordinating speech, so it's necessary to distinguish signals from neighboring brain cells to help make accurate predictions about intended speech.

After fabricating the new implant, Cogan and Viventi teamed up with several Duke University Hospital neurosurgeons, including Derek Southwell, M.D., Ph.D., Nandan Lad, M.D., Ph.D., and Allan Friedman, M.D., who helped recruit four patients to test the implants. The experiment required the researchers to place the device temporarily in patients who were undergoing brain surgery for some other condition, such as treating Parkinson's disease or having a tumor removed. Time was limited for Cogan and his team to test drive their device in the OR.
""I like to compare it to a NASCAR pit crew,"" Cogan said. ""We don't want to add any extra time to the operating procedure, so we had to be in and out within 15 minutes. As soon as the surgeon and the medical team said 'Go!' we rushed into action and the patient performed the task.""
The task was a simple listen-and-repeat activity. Participants heard a series of nonsense words, like ""ava,"" ""kug,"" or ""vip,"" and then spoke each one aloud. The device recorded activity from each patient's speech motor cortex as it coordinated nearly 100 muscles that move the lips, tongue, jaw, and larynx.
Afterwards, Suseendrakumar Duraivel, the first author of the new report and a biomedical engineering graduate student at Duke, took the neural and speech data from the surgery suite and fed it into a machine learning algorithm to see how accurately it could predict what sound was being made, based only on the brain activity recordings.
For some sounds and participants, like /g/ in the word ""gak,"" the decoder got it right 84% of the time when it was the first sound in a string of three that made up a given nonsense word.
Accuracy dropped, though, as the decoder parsed out sounds in the middle or at the end of a nonsense word. It also struggled if two sounds were similar, like /p/ and /b/.

Overall, the decoder was accurate 40% of the time. That may seem like a humble test score, but it was quite impressive given that similar brain-to-speech technical feats require hours or days-worth of data to draw from. The speech decoding algorithm Duraivel used, however, was working with only 90 seconds of spoken data from the 15-minute test.
Duraivel and his mentors are excited about making a cordless version of the device with a recent $2.4M grant from the National Institutes of Health.
""We're now developing the same kind of recording devices, but without any wires,"" Cogan said. ""You'd be able to move around, and you wouldn't have to be tied to an electrical outlet, which is really exciting.""
While their work is encouraging, there's still a long way to go for Viventi and Cogan's speech prosthetic to hit the shelves anytime soon.
""We're at the point where it's still much slower than natural speech,"" Viventi said in a recent Duke Magazine piece about the technology, ""but you can see the trajectory where you might be able to get there.""
This work was supported by grants from the National Institutes for Health (R01DC019498, UL1TR002553), Department of Defense (W81XWH-21-0538), Klingenstein-Simons Foundation, and an Incubator Award from the Duke Institute for Brain Sciences.

","score: 12.761844155844155, grade_level: '13'","score: 13.54158205430933, grade_levels: ['college_graduate'], ages: [24, 100]",10.1038/s41467-023-42555-1,"Patients suffering from debilitating neurodegenerative diseases often lose the ability to communicate, detrimentally affecting their quality of life. One solution to restore communication is to decode signals directly from the brain to enable neural speech prostheses. However, decoding has been limited by coarse neural recordings which inadequately capture the rich spatio-temporal structure of human brain signals. To resolve this limitation, we performed high-resolution, micro-electrocorticographic (µECoG) neural recordings during intra-operative speech production. We obtained neural signals with 57× higher spatial resolution and 48% higher signal-to-noise ratio compared to macro-ECoG and SEEG. This increased signal quality improved decoding by 35% compared to standard intracranial signals. Accurate decoding was dependent on the high-spatial resolution of the neural interface. Non-linear decoding models designed to utilize enhanced spatio-temporal neural information produced better results than linear techniques. We show that high-density µECoG can enable high-quality speech decoding for future neural speech prostheses."
"
Researchers from the Universities of Freiburg and Ulm have developed a monolithically integrated photo battery using organic materials.

Networked intelligent devices and sensors can improve the energy efficiency of consumer products and buildings by monitoring their consumption in real time. Miniature devices like these being developed under the concept of the Internet of Things require energy sources that are as compact as possible in order to function autonomously. Monolithically integrated batteries that simultaneously generate, convert, and store energy in a single system could be used for this purpose.
A team of scientists at the University of Freiburg's Cluster of Excellence Living, Adaptive, and Energy-Autonomous Materials Systems (livMatS) has developed a monolithically integrated photo battery consisting of an organic polymer-based battery and a multi-junction organic solar cell. The battery, presented by Rodrigo Delgado Andrés andDr. Uli Würfel, University Freiburg, and Robin Wessling and Prof. Dr. Birgit Esser, University of Ulm, is the first monolithically integrated photo battery made of organic materials to achieve a discharge potential of 3.6 volts. It is thus among the first systems of this kind capable of powering miniature devices. The team published their results in the journal Energy & Environmental Science.
Combination of a multi-junction solar cell and a dual-ion battery
The researchers developed a scalable method for the photo battery which allows them to manufacture organic solar cells out of five active layers. ""The system achieves relatively high voltages of 4.2 volts with this solar cell,"" explains Wessling. The team combined this multi-junction solar cell with a so-called dual-ion battery, which is capable of being charged at high currents, unlike the cathodes of conventional lithium batteries. With careful control of illumination intensity and discharge rates, a photo battery constructed in this way is capable of rapid charging in less than 15 minutes at discharge capacities of up to 22 milliampere hours per gram (mAh g-1). In combination with the averaged discharge potential of 3.6 volts, the devices can provide an energy density of 69 milliwatt hours per gram (mWh g-1) and a power density of 95 milliwatts per gram (mW g-1). ""Our system thus lays the foundation for more in-depth research and further developments in the area of organic photo batteries,"" says Wessling.

","score: 16.191083778966135, grade_level: '16'","score: 16.576452762923353, grade_levels: ['college_graduate'], ages: [24, 100]",10.1039/d3ee01822a,"Development of a first of its kind monolithically integrated photo-battery, capable of photo-charging within minutes and a discharge voltage of 3.6 V with all-organic active materials."
"
A team of researchers from UCLA has unveiled a first-of-its-kind stable and fully solid-state thermal transistor that uses an electric field to control a semiconductor device's heat movement.

The group's study, which will be published in the Nov. 3 issue of Science, details how the device works and its potential applications. With top speed and performance, the transistor could open new frontiers in heat management of computer chips through an atomic-level design and molecular engineering. The advance could also further the understanding of how heat is regulated in the human body.
""The precision control of how heat flows through materials has been a long-held but elusive dream for physicists and engineers,"" said the study's co-author Yongjie Hu, a professor of mechanical and aerospace engineering at the UCLA Samueli School of Engineering."" This new design principle takes a big leap toward that, as it manages the heat movement with the on-off switching of an electric field, just like how it has been done with electrical transistors for decades.""
Electrical transistors are the foundational building blocks of modern information technology. They were first developed by Bell Labs in the 1940s and have three terminals -- a gate, a source and a sink. When an electrical field is applied through the gate, it regulates how electricity (in the form of electrons) moves through the chip. These semiconductor devices can amplify or switch electrical signals and power. But as they continue to shrink in size over the years, billions of transistors can fit on one chip, resulting in more heat generated from the movement of electrons, which affects chip performance. Conventional heat sinks passively draw heat away from hotspots, but it has remained a challenge to find a more dynamic control to actively regulate heat.
While there have been efforts in tuning thermal conductivity, their performances have suffered due to reliance on moving parts, ionic motions, or liquid solution components. This has resulted in slow switching speeds for heat movement on the order of minutes or far slower, creating issues in performance reliability as well as incompatibility with semiconductor manufacturing.
The new thermal transistor, which boasts a field effect (the modulation of the thermal conductivity of a material by the application of an external electric field) and a full solid state (no moving parts), offers high performance and compatibility with integrated circuits in semiconductor manufacturing processes. The team's design incorporates the field effect on charge dynamics at an atomic interface to allow high performance using a negligible power to switch and amplify a heat flux continuously.
The UCLA team demonstrated electrically gated thermal transistors that achieved record-high performance with switching speed of more than 1 megahertz, or 1 million cycles per second. They also offered a 1,300% tunability in thermal conductance and reliable performance for more than 1 million switching cycles.

""This work is the result of a terrific collaboration in which we are able to leverage our detailed understanding of molecules and interfaces to make a major step forward in the control of important materials properties with the potential for real-world impact,"" said co-author Paul Weiss, a professor of chemistry and biochemistry. ""We have been able to improve both the speed and size of the thermal switching effect by orders of magnitude over what was previously possible.""
In the team's proof-of-concept design, a self-assembled molecular interface is fabricated and acts as a conduit for heat movement. Switching an electrical field on and off through a third-terminal gate controls the thermal resistance across the atomic interfaces and thereby allowing heat to move through the material with precision. The researchers validated the transistor's performance with spectroscopy experiments and conducted first-principles theory computations that accounted for the field effects on the characteristics of atoms and molecules.
The study presents a scalable technology innovation for sustainable energy in chip manufacturing and performance. Hu suggested the concept also offers a new way to understand heat management in the human body.
""At the very fundamental level, the platform could provide insights for the molecular-level mechanisms for living cells,"" Hu added.
Other authors on the paper -- all from UCLA -- include Man Li, Huan Wu, Erin Avery, Zihao Qin, Dominic Goronzy, Huu Duy Nguyen and Tianhan Liu. Hu and Weiss are also affiliated with the California NanoSystems Institute, as well as UCLA Samueli's departments of Bioengineering and Materials Science and Engineering.
The research was supported by grants from the National Institutes of Health, the Alfred P. Sloan Foundation and the National Science Foundation. Technical support was provided by the UCLA Nanolab and the California NanoSystems Institute at UCLA. Computational resources were provided by the UCLA Institute for Digital Research and Education and by Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support.

","score: 15.54299258105505, grade_level: '16'","score: 16.81844038201418, grade_levels: ['college_graduate'], ages: [24, 100]",10.1126/science.abo4297,"Controlling heat flow is a key challenge for applications ranging from thermal management in electronics to energy systems, industrial processing, and thermal therapy. However, progress has generally been limited by slow response times and low tunability in thermal conductance. In this work, we demonstrate an electronically gated solid-state thermal switch using self-assembled molecular junctions to achieve excellent performance at room temperature. In this three-terminal device, heat flow is continuously and reversibly modulated by an electric field through carefully controlled chemical bonding and charge distributions within the molecular interface. The devices have ultrahigh switching speeds above 1 megahertz, have on/off ratios in thermal conductance greater than 1300%, and can be switched more than 1 million times. We anticipate that these advances will generate opportunities in molecular engineering for thermal management systems and thermal circuit design."
"
Researchers from Lancaster University in the UK have discovered how superfluid helium 3He would feel if you could put your hand into it.

The interface between the exotic world of quantum physics and classical physics of the human experience is one of the major open problems in modern physics.
Dr Samuli Autti is the lead author of the research published in Nature Communications.
Dr Autti said: ""In practical terms, we don't know the answer to the question 'how does it feel to touch quantum physics?'
""These experimental conditions are extreme and the techniques complicated, but I can now tell you how it would feel if you could put your hand into this quantum system.
""Nobody has been able to answer this question during the 100-year history of quantum physics. We now show that, at least in superfluid 3He, this question can be answered.""
The experiments were carried out at about a 10000th of a degree above absolute zero in a special refrigerator and made use of mechanical resonator the size of a finger to probe the very cold superfluid.

When stirred with a rod, superfluid 3He carries the generated heat away along the surfaces of the container. The bulk of the superfluid behaves like a vacuum and remains entirely passive.
Dr Autti said: ""This liquid would feel two-dimensional if you could stick your finger into it. The bulk of the superfluid feels empty, while heat flows in a two-dimensional subsystem along the edges of the bulk -- in other words, along your finger.""
The researchers conclude that the bulk of superfluid 3He is wrapped by an independent two-dimensional superfluid that interacts with mechanical probes instead of the bulk superfluid, only providing access to the bulk superfluid if given a sudden burst of energy.
That is, superfluid 3He at the lowest temperatures and applied energies is thermo-mechanically two dimensional.
""This also redefines our understanding of superfluid 3He. For the scientist, that may be even more influential than hands-in quantum physics.""
Superfluid 3He is one of the most versatile macroscopic quantum systems in the laboratory. It often influences seemingly distant fields such as particle physics (for example the Higgs mechanism), cosmology (Kibble mechanism), and quantum information processing (time crystals).
A redefinition of its basic structure may therefore have far-reaching consequences.

","score: 12.060594315245478, grade_level: '12'","score: 12.413745410036718, grade_levels: ['college'], ages: [18, 24]",10.1038/s41467-023-42520-y,"The B phase of superfluid 3He can be cooled into the pure superfluid regime, where the thermal quasiparticle density is negligible. The bulk superfluid is surrounded by a quantum well at the boundaries of the container, confining a sea of quasiparticles with energies below that of those in the bulk. We can create a non-equilibrium distribution of these states within the quantum well and observe the dynamics of their motion indirectly. Here we show that the induced quasiparticle currents flow diffusively in the two-dimensional system. Combining this with a direct measurement of energy conservation, we conclude that the bulk superfluid 3He is effectively surrounded by an independent two-dimensional superfluid, which is isolated from the bulk superfluid but which readily interacts with mechanical probes. Our work shows that this two-dimensional quantum condensate and the dynamics of the surface bound states are experimentally accessible, opening the possibility of engineering two-dimensional quantum condensates of arbitrary topology."
"
Researchers at Delft University of Technology, led by assistant professor Richard Norte, have unveiled a remarkable new material with potential to impact the world of material science: amorphous silicon carbide (a-SiC). Beyond its exceptional strength, this material demonstrates mechanical properties crucial for vibration isolation on a microchip. Amorphous silicon carbide is therefore particularly suitable for making ultra-sensitive microchip sensors.

The range of potential applications is vast. From ultra-sensitive microchip sensors and advanced solar cells, to pioneering space exploration and DNA sequencing technologies. The advantages of this material's strength combined with its scalability make it exceptionally promising.
Ten medium-sized cars
""To better understand the crucial characteristic of ""amorphous,"" think of most materials as being made up of atoms arranged in a regular pattern, like an intricately built Lego tower,"" explains Norte. ""These are termed as ""crystalline"" materials, like for example, a diamond. It has carbon atoms perfectly aligned, contributing to its famed hardness."" However, amorphous materials are akin to a randomly piled set of Legos, where atoms lack consistent arrangement. But contrary to expectations, this randomisation doesn't result in fragility. In fact, amorphous silicon carbide is a testament to strength emerging from such randomness.
The tensile strength of this new material is 10 GigaPascal (GPa). ""To grasp what this means, imagine trying to stretch a piece of duct tape until it breaks. Now if you'd want to simulate the tensile stress equivalent to 10 GPa, you'd need to hang about ten medium-sized cars end-to-end off that strip before it breaks,"" says Norte.
Nanostrings
The researchers adopted an innovative method to test this material's tensile strength. Instead of traditional methods that might introduce inaccuracies from the way the material is anchored, they turned to microchip technology. By growing the films of amorphous silicon carbide on a silicon substrate and suspending them, they leveraged the geometry of the nanostrings to induce high tensile forces. By fabricating many such structures with increasing tensile forces, they meticulously observed the point of breakage. This microchip-based approach not only ensures unprecedented precision but also paves the way for future material testing.

Why the focus on nanostrings? ""Nanostrings are fundamental building blocks, the very foundation that can be used to construct more intricate suspended structures. Demonstrating high yield strength in a nanostring translates to showcasing strength in its most elemental form.""
From micro to macro 
And what finally sets this material apart is its scalability. Graphene, a single layer of carbon atoms, is known for its impressive strength but is challenging to produce in large quantities. Diamonds, though immensely strong, are either rare in nature or costly to synthesize. Amorphous silicon carbide, on the other hand, can be produced at wafer scales, offering large sheets of this incredibly robust material.
""With amorphous silicon carbide's emergence, we're poised at the threshold of microchip research brimming with technological possibilities,"" concludes Norte.

","score: 13.00324514991182, grade_level: '13'","score: 13.579991181657846, grade_levels: ['college_graduate'], ages: [24, 100]",10.1002/adma.202306513,"For decades, mechanical resonators with high sensitivity have been realized using thin‐film materials under high tensile loads. Although there are remarkable strides in achieving low‐dissipation mechanical sensors by utilizing high tensile stress, the performance of even the best strategy is limited by the tensile fracture strength of the resonator materials. In this study, a wafer‐scale amorphous thin film is uncovered, which has the highest ultimate tensile strength ever measured for a nanostructured amorphous material. This silicon carbide (SiC) material exhibits an ultimate tensile strength of over 10 GPa, reaching the regime reserved for strong crystalline materials and approaching levels experimentally shown in graphene nanoribbons. Amorphous SiC strings with high aspect ratios are fabricated, with mechanical modes exceeding quality factors 108 at room temperature, the highest value achieves among SiC resonators. These performances are demonstrated faithfully after characterizing the mechanical properties of the thin film using the resonance behaviors of free‐standing resonators. This robust thin‐film material has significant potential for applications in nanomechanical sensors, solar cells, biological applications, space exploration, and other areas requiring strength and stability in dynamic environments. The findings of this study open up new possibilities for the use of amorphous thin‐film materials in high‐performance applications."
